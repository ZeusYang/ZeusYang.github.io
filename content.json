{"meta":{"title":"YangWC's Blog","subtitle":null,"description":"Personal blog website.","author":"WC Yang","url":"https://yangwc.com","root":"/"},"pages":[{"title":"404 Not Found","date":"2019-11-02T01:23:05.017Z","updated":"2019-11-02T01:23:05.017Z","comments":true,"path":"404.html","permalink":"https://yangwc.com/404.html","excerpt":"","text":"**404 Not Found** Sorry, the page you visited does not exist. It may be that the input address is wrong or the address has been deleted."},{"title":"大佬的博客","date":"2019-09-10T02:18:51.844Z","updated":"2019-04-27T10:52:08.692Z","comments":true,"path":"friends/index.html","permalink":"https://yangwc.com/friends/index.html","excerpt":"","text":"名称： YangWC’s Blog头像： https://cdn.jsdelivr.net/gh/ZeusYang/CDN-for-yangwc.com@1.1.4//globalImage/avator.jpg网址： https://yangwc.com"},{"title":"所有分类","date":"2019-04-27T08:54:04.778Z","updated":"2019-04-27T08:54:04.778Z","comments":true,"path":"categories/index.html","permalink":"https://yangwc.com/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2019-04-27T08:02:52.306Z","updated":"2019-04-27T08:02:52.306Z","comments":true,"path":"tags/index.html","permalink":"https://yangwc.com/tags/index.html","excerpt":"","text":""},{"title":"about","date":"2021-04-12T06:53:35.899Z","updated":"2021-04-12T06:53:35.899Z","comments":true,"path":"about/index.html","permalink":"https://yangwc.com/about/index.html","excerpt":"","text":"Wencong Yang(杨文聪)I am a second-year master student in Intelligent and Multimedia Science Laboratory at Sun Yat-sen University, advised by Prof. Chengying GAO. I do research in Computer Graphics, with primary focus on fluid simulation. Currently I am investigating the applications of fluid simulation in virtual oil painting. LocationGuangzhou, ChinaSun Yat-sen University (SYSU)（中山大学）School Of Computer Science And Engineering（计算机学院（软件学院））Intelligent and Multimedia Science Laboratory（智能与多媒体科学实验室） Education 2015.8 ~ 2019.7: Bachelor of Engineering in Computer Science in Sun Yat-sen University 2019.9 ~ 2021.7: Master of Engineering in Software Engineering in Sun Yat-sen University PublicationWencong Yang, Chengying Gao, A completely parallel surface reconstruction method for particle-based fluids, Computer Graphics International (CGI 2020) We propose a fast, simple and extremely accurate narrow-band method of fluid surface, which makes the surface reconstruction algorithm accurately process the valid fluid surface area. Meanwhile, the whole process of fluid surface reconstruction is completely parallelized, which greatly speeds up the efficiency of surface reconstruction. InternsIn the summer of 2020 (7.13 ~ 9.18), I worked as an intern of Rendering Engine R &amp; D Engineer of douyin department of Bytedance in Shenzhen. In July 2021, I’ll be a regular employee of Bytedance in Guangzhou. Projects FluidEngineAn offline fluid simulation engine for computer graphics applications. Features: Basic math and geometry operations and data structures; Jacobi, Gauss-Seidel, SOR, MG, CG, ICCG, and MGPCG linear system solvers; Spherical, SPH, Zhu & Bridson, and Anisotropic kernel for points-to-surface converter; Intel TBB multi-threading backends; SPH and PCISPH fluid simulators; Converters between signed distance function and triangular mesh; Stable fluids-based smoke simulator (Pure Euler fluid solver); Level set-based liquid simulator. TinySoftRendererA soft renderer using C++ from scratch. The original intention of building such a 3D rendering system from scratch without any help of graphics library is to get a thorough understanding of the three-dimensional rendering process. This project was totally refactored based on previous naive version I built 2 years ago. Features: Perspective correct per vertex parameter interpolation; Back face culling; Z-buffering (reversed z);Sutherland Hodgeman homogeneous cliping; Accelerated edge function-based triangle rasterization;Texture mapping, nearest texture sampling, and bilinear texture sampling;Tiling and morton curve memory layout for accessing to texture;Phong/Blinn-Phong illumination;Mipmap texture mapping;Reinhard tone mapping;Multi sampling anti-aliasing (MSAA 4X, and MSAA 8X);Alpha blending, and alpha to coverage;Multi-thread parallelization using tbb as backend. Ray-TracerAn offline renderer : path tracer. Just a personal toy for learning and playing. Features: BVH tree for fast intersection detection; Intel TBB multi-threading for rendering acceleration; Monte Carlo method for important sampling; Lambertian, dielectric and metal material. Octree data structure for acceleration of intersection detection. PhysicallyBasedRendererPhysically based renderer using OpenGL library. Features: High Dynamic Range; Bloom Effect for glowing; Cook-Torrance(BRDF lighting model) Physically Based Rendering pipeline; Screen Space Ambient Occlusion for dark details; Defered Shading for large amount of light sources; Image Based Lighting for enviroment map. Nowadays , physically based rendering is popular for its realistic and awesome visual effect. ContactPlease feel free to let me know if you catch any error or typo in my blog. Academic discussions are welcome. Github: yangwc Wechat: ywc1579148717 E-mail: 1579148717@qq.com ResourcesSome pretty useful and interseting resources and links collected. Conferences/Periodicals Papers: SIGGRAPH、SIGGRAPH Asia、EG、PG、I3D; Physics-Based Animation; Pixar Technical Memos 3D Models Downloading: Free 3D Pbr Models; Free 3D models for CG digital design and artwork; Humster3D: the biggest online collection of cars, electronics, weapons and other hot things.; Yobi3D: Free 3D Models Search Engine; Rendering Resources; The Stanford 3D Scanning Repository; Standord Models; 3D Models - CMLab Graphics Tutorials/Courses on Computer Graphics: Learn OpenGL; Learn Vulkan; GPU Gems; Fluid Simulation for Video Games; Learn Computer Graphics From Scratch!; 3d-game-shaders-for-beginners; TU Wien Rendering/Ray Tracing Course; CMU 15-462/662: Computer Graphics Course; Stanford CS148: Introduction to Computer Graphics and Imaging; UCSB CS291A: Real-Time High Quality Rendering Course; DTU 02941: Physically Based Rendering and Material Appearance Modelling; Advances in Real-Time Rendering in 3D Graphics and Games; Rasterization in One Weekend Books on Computer Graphics: 《Real-Time Rendering》; 《Physically Based Rendering: From Theory To Implementation》; 《全局光照技术》; 《Fluid Engine Development》; 《Fluid Simulation For Computer Graphics》; 《Ray Tracing in One Weekend》; 《Ray Tracing: the Next Week》; 《Ray Tracing: the Rest of Your Life》; 《Ray Tracing Gems》; 《Fundamentals of Computer Graphics, Fourth Edition》; 《Real-Time Rendering 3rd》提炼总结; 《Real-Time Collision Detection, Christer Ericson》; 《The art of fluid animation》 Researchers and Scholars: Robert Bridson; Doyub Kim; Christopher Batty; Xinxin Zhang（张心欣）; Lingqi Yan（闫令琪）; Chenfanfu Jiang（蒋陈凡夫）; Prof. Dr. Ligang Liu（刘利刚）; Tamar Shinar; Yuanming Hu（胡渊鸣）; Yang Gao（高阳）; Xiao Zhai（翟骁） Something Interesting: GLSL Sandbox; Shader toy; Jiumo Search; McGuire Computer Graphics Archive; Awesome Creative Coding; Coding Labs; HUMUS; TYLER HOBBS; casual-effects"}],"posts":[{"title":"高质量实时渲染：实时软阴影","slug":"PCSS","date":"2021-04-14T14:52:52.835Z","updated":"2021-04-14T15:07:26.318Z","comments":true,"path":"2021/04/14/PCSS/","link":"","permalink":"https://yangwc.com/2021/04/14/PCSS/","excerpt":"本文是闫令琪老师GAMES202高质量实时渲染课程的学习笔记和总结，这一节的主题为高质量实时阴影。","text":"本文是闫令琪老师GAMES202高质量实时渲染课程的学习笔记和总结，这一节的主题为高质量实时阴影。 一、阴影贴图&emsp;&emsp;阴影贴图技术由来已久，它是一种基于图像空间的方法，因其简单、高效等因素被广泛应用于当今的实时渲染引用中，甚至早期的离线渲染都曾用该技术来生成阴影。阴影贴图概括起来： 是一种两个pass的算法，先从光照的视角渲染成并生成深度图，然后再从观察视角渲染并用深度图计算阴影； 是一种图像空间的方法，因而无需关注场景中的具体几何形状，但也带来了一些局限性（例如走样）； 图1 Shadow Map的两个pass &emsp;&emsp;阴影贴图的原理如图1所示，比较简单，这里不再赘述。简单粗暴的阴影贴图有很大局限性，主要体现在： 由于深度图的有限分辨率带来的自遮挡阴影失真，在光与阴影接收面法线夹角很大的时候非常明显； 由于深度图的有限分辨率带来的阴影轮廓走样锯齿问题； 阴影太过生硬，阴影区域到非阴影区域无平滑柔和的过度，即只能产生硬阴影。 &emsp;&emsp;自遮挡问题的产生原因见如图2所示，其中黄色线代表深度图存储的最近深度。受限于分辨率，在距离光源比较远的情况下，多个片段可能从深度贴图的同一个值中去采样，因此存在一定的深度偏差，在某些地方读取的阴影贴图深度偏大，有些地方偏小，最终导致产生隔间性的阴影条带，如图3所示。 图2 自遮挡问题产生原因 图3 自遮挡问题导致的条带状 &emsp;&emsp;这种失真现象在光与接收平面法线夹角增大时更加明显（注：此失真现象并非为摩尔纹）。一种解决方案就是使用阴影偏移，对深度图采样得到的深度值减去一个$\\epsilon$，本质上就是把最近的深度往光的方向再移动了一下。但阴影偏移的技巧容易产生阴影悬浮，即稍微阴影偏离了本来的位置，使得产生阴影的物体看起来像是漂浮起来了。为了解决悬浮问题，可以在第一个pass的时候设置前向面剔除。 &emsp;&emsp;另一种解决自遮挡问题的方法则是用深度图存储次小深度。如下图4所示，这种方法实现起来也简单，但需要两个pass来生成阴影贴图，第一个pass绘制时设置为背向面剔除，第二个pass绘制时设置为前向面剔除，这样就可以得到介于两者之间的深度值，用以计算阴影。 图4 次小深度阴影贴图生成 &emsp;&emsp;但这种方法要求投射阴影的物体必须为闭合曲面（watertight），而且再多一个pass必然带来更大的开销，因此并没有得到广泛的应用（实时渲染不相信算法渐进复杂度）。阴影贴图的另外两个问题（锯齿和硬阴影）的解决，将在后面叙述。 二、阴影贴图的数学原理&emsp;&emsp;实时渲染领域并不关注严谨、严格的数学证明，只关注是否能够work。一个常用的积分近似公式如下所示： \\int_{\\Omega} f(x)g(x)dx\\approx \\frac{\\int_{\\Omega}f(x)dx}{\\int_{\\Omega}dx}\\cdot \\int_{\\Omega}g(x)dx \\tag {1}&emsp;&emsp;当满足以下任一条件时，公式$(1)$的近似能够得到一个相对准确的结果： 当$g(x)$在积分域$\\Omega$内的实际贡献很小，即$g(x)$在$\\Omega$内绝大部分取值为零，例如一个脉冲函数，仅在$x=0$不为零，其他地方均为零（即狄拉克函数）； 当$g(x)$在积分域$\\Omega$内变化不大，即最大值和最小值差别很小（甚至相同，即常量函数）。 &emsp;&emsp;回到Shadow mapping原理上，其本质上就是把渲染方程中的可见性函数提取出来单独进行积分，从而得到近似的积分结果： \\begin{align} L_o(p,\\omega_o) =&\\int_{\\Omega^+}L_i(p,\\omega_i)f_r(p,\\omega_i,\\omega_o)cos\\theta_i V(p,\\omega_i)d\\omega_i\\\\ \\approx&\\frac{f_{\\Omega^+}V(p,\\omega_i)d\\omega_i}{\\int_{\\Omega^+}d\\omega_i} \\cdot \\int_{\\Omega^+}L_i(p,\\omega_i)f_r(p,\\omega_i,\\omega_o)cos\\theta_i d\\omega_i \\end{align} \\tag {2}&emsp;&emsp;这种近似在实时应用领域应用广泛，通常的应用场景下，$L_i$来自于点光源或者平行光源，因而满足上面提到的条件一，或者$f_r$函数取值变化不大（例如漫反射brdf），符合上面提到的条件二。阴影贴图做的事就是把公式$(2)$的可见性函数先计算出来，然后再与光照项进行相乘，得到最终的渲染结果。 三、PCSS软阴影算法&emsp;&emsp;前面提到，阴影贴图的一个缺点就是受限于贴图的分辨率大小容易产生明显的走样现象，根本原因还是采样频率没有跟上。暴力增加分辨率进行超采样的方法这里不再赘述。常用的解决方法就是对采样得到可见性信号进行滤波，这就是PCF（Percentage Close Filtering）。在计算可见性时，PCF不再仅仅只查单独的一个深度值，而是会将着色器点$p$的深度值与深度贴图上的周围最近深度进行比较，最终对比较的结果进行一个加权平均，得到最终的结果。这样本质上就是对可见性信号进行了滤波（注意：并非是对深度图进行了滤波）。经过PCF滤波的阴影在边缘部分能够得到明显的缓和，如图5所示。 图5 有无PCF的阴影比较 &emsp;&emsp;PCF的滤波半径决定了阴影边缘的柔和程度，一般来说滤波半径越大则柔和，相反阴影边缘越锐利，如图6所示。由此启发了PCSS（Percentage Close Soft Shadow）软阴影算法的核心原理，对于那些我们需要实现软阴影的边缘用更大的滤波半径进行滤波，而对于那些不需要太过柔和阴影边缘的地方，则采用更小的滤波半径，这是一种自适应滤波半径的机制。 图6 不同PCF滤波半径得到的阴影 &emsp;&emsp;PCSS的本质，概括起来就是自适应PCF滤波半径的阴影贴图算法。其关键核心在于如何制定自适应滤波半径的机制。仔细观察图7，可以看到阴影区域在越靠近遮挡物的地方越硬，而越原理遮挡物的地方则越软，因此我们希望根据投影区域到遮挡物的距离来调整PCF滤波半径的大小。 图7 阴影柔和程度的变化 &emsp;&emsp;在此之前，先明确一个概念，点光源作为理想中的光源类型，不存在软阴影，而现实世界的光都具有一定的面积。阴影的软化程度本质上取决于半影区域的大小，半影区域越大则阴影越柔和，因此我们首先推算给定光源、给定遮挡物下和给定投影区域上的半影大小（即如下图8所示的$W_{Penumbra}$）。 图8 半影长度推算 &emsp;&emsp;由三角形相似可以得到： W_{Penumbra}=(d_{Receiver}-d_{Blocker})\\cdot w_{light}/d_{Blocker} \\tag {3}&emsp;&emsp;其中，$d_{Receiver}$是阴影接收区域到光源的距离，$d_{Blocker}$是遮挡物到光源的距离，这两个参数都可以直接从第一个pass生成的深度图获取得到，光源面积$w_{light}$由我们自己手动指定。公式$(3)$计算得到的半影长度就可以作为PCF滤波半径的大小参数。因此，完整的PCSS算法如下所示： 图9 PCSS算法步骤 &emsp;&emsp;其中第一步用于计算$d_{Blocker}$，方式与PCF类似，对周围深度信息进行采样，如果判断为是遮挡物，则获取它的深度值并进行累加，最后取平均得到$d_{Blocker}$。第二步则是直接用公式$(3)$计算半影大小，第三步根据半影大小进行PCF滤波。这里贴一下核心的实现代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950float findBlocker( sampler2D shadowMap, vec2 uv, float zReceiver ) &#123; const int radius = 40; const vec2 texelSize = vec2(1.0/2048.0, 1.0/2048.0); float cnt = 0.0, blockerDepth = 0.0; int flag = 0; for(int ns = 0;ns &lt; BLOCKER_SEARCH_NUM_SAMPLES;++ns) &#123; vec2 sampleCoord = (vec2(radius) * poissonDisk[ns]) * texelSize + uv; float cloestDepth = unpack(texture2D(shadowMap, sampleCoord)); if(zReceiver - 0.002 &gt; cloestDepth) &#123; blockerDepth += cloestDepth; cnt += 1.0; flag = 1; &#125; &#125; if(flag == 1) &#123; return blockerDepth / cnt; &#125; return 1.0;&#125;float PCF(sampler2D shadowMap, vec4 shadowCoord, float radius) &#123; const vec2 texelSize = vec2(1.0/2048.0, 1.0/2048.0); float visibility = 0.0, cnt = 0.0; for(int ns = 0;ns &lt; PCF_NUM_SAMPLES;++ns) &#123; vec2 sampleCoord = (vec2(radius) * poissonDisk[ns]) * texelSize + shadowCoord.xy; float cloestDepth = unpack(texture2D(shadowMap, sampleCoord)); visibility += ((shadowCoord.z - 0.001) &gt; cloestDepth ? 0.0 : 1.0); cnt += 1.0; &#125; return visibility/cnt;&#125;float PCSS(sampler2D shadowMap, vec4 shadowCoord)&#123; // STEP 1: avgblocker depth float avgBlockerDepth = findBlocker(shadowMap, shadowCoord.xy, shadowCoord.z); // STEP 2: penumbra size const float lightWidth = 50.0; float penumbraSize = max(shadowCoord.z-avgBlockerDepth,0.0)/avgBlockerDepth*lightWidth; // STEP 3: filtering return PCF(shadowMap, shadowCoord, penumbraSize); //return 1.0;&#125; &emsp;&emsp;实现的效果见本文的开头。 四、PCSS效率改进——VSSM&emsp;&emsp;相比最初的PCF阴影贴图算法，PCSS算法新增了$d_{Blocker}$的计算过程，由此需要再花费更多的对深度图的纹理访问。纹理的访问对性能的损害非常大，由此启发人们寻找更为快速的$d_{Blocker}$计算方法和快速的PCF滤波方法，VSSM（Variance Soft Shadow Mapping）由此诞生。 &emsp;&emsp;PCSS的第一步和第三步本质上都是根据给定的滤波半径去查找深度纹理的深度值，都是一个类似的卷积滤波过程。第一步根据读取得到的深度值判断是否为遮挡物，如果是遮挡物则会累加其深度值，最后做加权平均，得到$d_{Blocker}$；而第三步根据计算得到的滤波半径大小，读取深度纹理值，进行可见性（即是否为遮挡物）的判断并累加，最后加权平均。第一步和第三步的区别仅在于，第一步需要对遮挡物的深度值进行加权平均（而非对滤波区域内的所有深度值进行加权平均）。因此，VSM算法尝试用同一种思路快速计算PCSS的第一步和第三步。 &emsp;&emsp;我们先把关注点放在PCF滤波上，PCSS第三步的PCF滤波可以用如下的卷积公式表示： V(x)=\\sum_{q\\in N(p)}w(p,q)\\cdot \\chi^+[D_{SM}(q)-D_{scene}(x)] \\tag {4}&emsp;&emsp;其中，$V(x)$是$x$点处的可见性，$\\omega(p,q)$是滤波权重函数（例如高斯权重、均值权重等），$\\chi^+$函数对大于$0$的输入返回$1$，对小于$0$的输入返回$0$。$D_{SM}(q)$表示Shadow Map上$q$处的深度值，$D_{scene}(x)$则为场景中$x$处在光的视角下的深度值。直观上，公式$(4)$可以看成在滤波范围内有多少百分比的深度值超过了$x$处的深度值，而$V(x)$就是这个百分比取值，如果我们能够快速得到这个百分比，那么也就不需要循坏迭代采样滤波了！公式$(4)$的计算可以转化成等价问题：对于给定$x$处的深度值$D_{scene}(x)$，$P(D_{SM}(q)&gt;D_{scene(x)})$概率为多少？其中P(D_{SM}(q)>D_{scene(x)})就等价于$V(x)$。 &emsp;&emsp;为了计算$V(x)$，VSSM引入了概率论的方法。在概率论中，有如下的公式成立： Var(X)=E(X^2)-E^2(X) \\tag {5}&emsp;&emsp;即如果知道$X$和$X^2$的数学期望（或者说均值），那么我们就可以根据上述公式计算其方差。而如果知道均值$\\mu$和其方差$\\sigma^2$，则有如下的切比雪夫不等式（Chebychev’s inequality）成立： P(x>t)\\leq \\frac{\\sigma^2}{\\sigma^2+(t-\\mu)^2} \\tag {6}&emsp;&emsp;VSSM直接把上述公式$(6)$的不等符号$\\leq$当成约等符号$\\approx$用，从而快速求解出$P(D_{SM}(q)&gt;D_{scene(x)})$，这就是它的快速计算原理！但在此之前，我们需要计算给定任意一点$q$和滤波半径$r$，计算深度纹理上该范围内的深度均值和深度平方的均值。这里不需要额外新增一个pass，用深度纹理的r通道和g通道分别存储深度值和深度值的平方即可。问题在于如何计算任意一点$q$、任意滤波半径$r$大小下的两个均值$E(X^2)$和$E(X)$，关于这个问题有以下两种解决方案： 借助于纹理的MIPMAP机制，MIPMAP的生成过程，后一层纹理是由前一层纹理的均值降采样得到，因此可以快速得到纹理上任意一点的均值，只需根据滤波半径$r$大小去寻找对应的层级即可（两个层级之间可以再次插值提升准确度）； 图10 纹理MIPMAP 借助于Summed-Area table（简称SAT），即二维的面积前缀和，需要额外自己主动实现，不再赘述。 &emsp;&emsp;MIPMAP直接用底层硬件可以自动生成，但MIPMAP仅仅局限于方形区域的均值查找，相比之下SAT更具优势。但无论是MIPMAP还是SAT，只要光源发生了移动、旋转等变化，那么均需要重新计算。得到的深度均值和深度平方的均值，那么就可以计算出深度方差，然后直接用公式$(6)$就可以快速近似得到PCF滤波结果。 &emsp;&emsp;解决了PCF快速滤波问题，现在问题转到PCSS的第一步。我们知道，第一步计算$d_{Blocker}$并不是简单地对滤波范围内的深度值取加权平均，而是对滤波范围内的遮挡物的深度值取平均，对于那些非遮挡物我们不会累加其深度值。因此我们不能直接从MIPMAP或SAT查找得到$d_{Blocker}$。但通过公式$(6)$我们近似知道滤波范围内，$P(D_{SM}(q)&gt;D_{scene(x)})$取值是多少，相应的$P(D_{SM}(q)D_{scene(x)})$。有以下的关系成立： P(D_{SM}(q)>D_{scene}(x))\\cdot z_{unocc}+P(D_{SM}(q)\\leq D_{scene}(x))\\cdot z_{occ}=z_{avg} \\tag {7}&emsp;&emsp;其中$z_{avg}$是通过MIPMAP或者SAT查找得到的滤波范围的深度均值，$z_{unocc}$、$z_{occ}$分别是非遮挡物、遮挡物的深度均值，两个$P$分别对应他们的百分比。我们现在要求$z_{occ}$（即$d_{Blocker}$），但上述公式还有一个未知量$z_{unocc}$，即非遮挡物的深度均值。VSSM进一步做了个大胆的假设，直接假设$z_{unocc}$取值为$D_{scene}(x)$，即非遮挡物的平均深度刚好与$x$点处的深度值一样（在该假设下，阴影接收物是一个平面）。从而最终计算根据公式$(7)$得到$z_{occ}$。 &emsp;&emsp;VSSM和PCSS的区别在于求解方法的效率，VSSM通过大胆的假设和近似实现了快速的$d_{Blocker}$计算和PCF滤波过程，避免大量的纹理访问过程，显著地提升了阴影生成效率。 五、VSSM改进——MSM&emsp;&emsp;VSSM虽然高效，但其大胆的假设和近似也带来了一些问题。VSSM的缺点主要来源于近似带来的误差。近似结果偏大，那么阴影会偏暗一点，结果尚可接受；而如果近似结果偏小，那么会导致漏光（Light Leaking）现象的产生，这是一种严重的失真（如下图11车底所示）。此外，VSSM容易在非平面阴影接收物上产生的奇怪的阴影（来源于前面提到的$z_{unocc}$的深度假设）。 图11 VSSM漏光现象 &emsp;&emsp;VSSM用深度的均值$\\mu$和方差$\\sigma$来逼近可见性的累积分布函数（简称CDF，即前面提到的$P(D_{SM}(q)&gt;D_{scene(x)})$），本质上就是用深度值分布的一阶原始矩和二阶中心矩。为了进一步提升近似准确率，MSM（Moment Shadow Mapping）采用了更高阶的矩来进行估算（前四阶矩）。 &emsp;&emsp;在概率论中，矩（moment）是对变量分布和形态特点的一组度量。n阶矩被定义为变量的n次方与其概率密度函数（PDF）之积的积分。直接使用变量计算的矩被称为原始矩（raw moment），移除均值后计算的矩被称为中心矩（central moment）。变量的一阶原始矩等价于数学期望（expectation）、二至四阶中心矩被定义为方差（variance）、偏度（skewness）和峰度（kurtosis）。在这里，我们用纹理的四个通道分别存储$z$、$z^2$、$z^3$和$z^4$，其中指数代表几阶矩。 &emsp;&emsp;MSM基于这样的一个结论：我们可以用前$m$阶矩来表示具有$m/2$个台阶的阶跃函数。对于深度值的分布估计，通常情况下四阶矩已经可以得到非常不错的结果。下图12展示了PCF与前四阶矩的拟合结果比较，理想情况下，越靠近PCF结果越好！四阶矩的逼近结果已经非常接近PCF了。 图12 PCF、2阶、3阶、4阶近似逼近的结果比较 &emsp;&emsp;MSM的原理比较复杂，闫老师没有仔细展开。我查了相关资料，也没有仔细看，这里仅仅解读一下实现的伪代码，如图13所示。算法的输入为存储深度$z$值前四阶矩的预过滤结果$b\\in \\mathbf{R}^4$、当前片元在光视角下的深度$z_{f}\\in \\mathbf{R}$、偏移值$\\alpha &gt;0$；算法的输出为阴影强度$G(b,z_f)$。 图13 MSM实现伪代码 &emsp;&emsp;第一步对$b$做一定的偏移；第二步用Cholesky分解求解$c$；第三步求解图中所示一元二次方程，记求解结果为$z_2$和$z_3$，其中$z_2\\leq z_3$。最后阴影强度$G$由如下公式给出： G= \\begin{cases} 0 & z_{f}\\leq z_2\\\\ \\frac{z_f\\cdot z_3-b_1'\\cdot(z_f+z_3)+b_2'}{(z_3-z_2)\\cdot(z_f-z_2)} & z_2< z_f\\leq z_3\\\\ 1-\\frac{z_2\\cdot z_3-b_1'\\cdot(z_2+z_3)+b_2'}{(z_f-z_2)\\cdot(z_f-z_3)} & z_3 < z_f \\end{cases}&emsp;&emsp;图13给出了VSM与MSM的结果比较，相比于VSM，MSM的阴影估算准确了很多，额外的性能开销还尚可观。 图13 MSM实现伪代码 六、基于距离场的软阴影&emsp;&emsp;有向距离场（Signed Distance Field，简称SDF）表示为空间中的一个点到场景中所有物体表面的最近距离，其中的符号表示点在内部（负号）还是在外部（正号）。光线步进（Ray Marching）借助于SDF来感知当前位置到最近物体表面的距离，并以此作为最长的步进距离向前移动，一直步进到某个表面上或者超过给定的距离。 &emsp;&emsp;基于距离场的软阴影结合了光线步进的思想，其本质上并非物理准确（比PCSS还要更fake一些），但仍然能够产生非常不错的软阴影效果。该方法在shader toy上非常常用。核心的原理如图14所示，从需要计算阴影强度的着色点出发，向光源发射一条Shadow Ray，沿着该方向进行光线步进。每步进到一个点，我们可以得到一个圆心为当前点、半径为当前点的SDF值的圆（三维情况下为球体），从出发点向该圆作一条切线，可以得到一个夹角（如图所示的$\\theta_1$、$\\theta_2$和$\\theta_3$），取所有这些夹角中的最小值，根据这个值来确定半影大小。 图14 基于SDF的软阴影 &emsp;&emsp;由简单的三角关系可以的得到夹角$arcsin\\frac{SDF(p)}{|p-o|}$，其中$o$为起始点，$p$为当前所在位置。这个公式计算需要用到非常耗时的反三角函数，为了避免反三角函数的开销。一个更为快速的方法是用下面的公式： \\min \\{\\frac{k\\cdot SDF(p)}{|p-o|},1.0\\}&emsp;&emsp;上述公式直接把反三角函数去掉了。上述公式的$k$为缩放系数，$k$越大，则计算得到的半影大小会越小（因为角度越大，则表示视角越大，越不在遮挡范围内），阴影越硬，正如图15所示。 图15 不同k值的阴影柔和效果比较 &emsp;&emsp;基于SDF的方法能够生成比较高质量的软阴影，但它也有一个致命的缺点：需要预计算场景的SDF并存储（例如用3D纹理进行存储）。如何减小内存开销是一个非常重要的问题。除此之外，该方法容易在边界处产生一些artifact。 Reference$[1]$ Summed-Area Variance Shadow Maps $[2]$ Peters C, Klein R. Moment shadow mapping[C]//Proceedings of the 19th Symposium on Interactive 3D Graphics and Games. 2015: 7-14. $[3]$ GAMES202: 高质量实时渲染","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Real-time Rendering","slug":"Real-time-Rendering","permalink":"https://yangwc.com/categories/Real-time-Rendering/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Real-time Rendering","slug":"Real-time-Rendering","permalink":"https://yangwc.com/tags/Real-time-Rendering/"}]},{"title":"Fluid Simulation：栅格Boltzmann流体模拟","slug":"LBM","date":"2020-06-24T02:09:37.724Z","updated":"2021-04-14T14:46:42.674Z","comments":true,"path":"2020/06/24/LBM/","link":"","permalink":"https://yangwc.com/2020/06/24/LBM/","excerpt":"相对于基于N-S流体方程的模拟方法，栅格Boltzmann流体模拟方法（Lattice Boltzmann Method，简称LBM）是一种截然不同的流体模拟算法，表面上看起来跟Navier-Stokes没什么太大的关联，但依旧能够实现非常逼真的流体动画（特别是细粒度流体）。LBM容易理解，实现起来也不难。","text":"相对于基于N-S流体方程的模拟方法，栅格Boltzmann流体模拟方法（Lattice Boltzmann Method，简称LBM）是一种截然不同的流体模拟算法，表面上看起来跟Navier-Stokes没什么太大的关联，但依旧能够实现非常逼真的流体动画（特别是细粒度流体）。LBM容易理解，实现起来也不难。 &emsp;&emsp;LBM方法本质上是一种基于拉格朗日视角的模拟方法，形式上却跟欧拉视角下的网格有点相似，它将微观粒子的动力学与宏观流体规律相结合，同时具有较好的精度，因此是很好的弱可压缩（甚至不可压缩）N-S方程二阶精准求解方法。在图形学领域，我们模拟的绝大部分都是不可压流体，因此契合得很好。 &emsp;&emsp;LBM是一种介于宏观和微观之间的介观尺度的方法。一方面它将流体空间离散为一个个栅格，另一方面又通过栅格上的概率密度函数来描述微观尺度下的粒子分布情况。如下图所示： 一、LBM方程&emsp;&emsp;在了解LBM方程之前，有必要对LBM方法是如何规划模拟区域的。与将空间均匀划分的欧拉网格相似，LBM方法也是先均匀分割流体所在的区域，从而得到一个个小的格子。但与欧拉网格方法不同，LBM格子之间的连接方式略有不同。以三维为例，欧拉网格的格子邻居仅仅有上下、左右、前后六个，但LBM格子的邻居除了这六个之外，还可以有八个对角上的邻居，甚至可以有八个仅仅是边相连的邻居。通常用DdQq来表示某种LBM格子模式，其中d表示维度，q表示联通的格子数目（包含它自己）。常用的有D1Q3、D2Q9、D3Q15、D3Q19和D3Q27。这些不难理解。 &emsp;&emsp;目前我们以D2Q9即二维的LBM为例。每个LBM格子与9个格子相连（上下左右，以及四个corner，还有自身），相应的我们有9个速度方向，这些速度方向指向连通的格子，例如下面的九个，每个方向我们记为$c_i$, 其中$i=0,…,8$： 1[0, 0], [1, 0], [0, 1], [-1, 0], [0, -1], [1, 1], [-1, 1], [-1, -1], [1, -1] &emsp;&emsp;每个LBM格子中心存储粒子分布函数$f$，用以描述流体粒子的分布情况。$f$是一个关于空间位置$x$和时间轴$t$的函数，写作$f(x,t)$。需要注意的是，粒子分布函数在每个方向上都有一个，因此一个LBM格子的有$9$个粒子分布函数，记为$f_i(x,t)\\ , i=0,…,8$，分别对应每个速度方向上的粒子分布情况，这里说的粒子仅仅是概念上的虚拟粒子。 &emsp;&emsp;有了上述的栅格粒子分布函数，如何得到流场的速度场？一般通过下面的公式来得到流体的速度场$u(x,t)$： \\begin{align} \\rho(x,t)&=\\Sigma_i f_i(x,t)\\\\ u(x,t)&=\\frac{1}{\\rho(x,t)}\\Sigma_i c_i f_i(x,t), \\ \\ i=0,...,8 \\end{align} \\tag {1}&emsp;&emsp;这里的$\\rho(x,t)$是虚拟粒子的密度，并非流体真正的密度场。相关概念介绍得差不多了。现在来看看LBM的核心公式，LBM方法的核心可以分成两个步骤，分别是碰撞和流动，我们先来看看碰撞步骤。碰撞步骤的核心计算公式如下所示： f_i(x, t+\\Delta t)=f_i(x, t)+\\frac{f_i^{eq}(x,t)-f_i(x, t)}{\\tau_f} \\tag {2}&emsp;&emsp;上述公式使用了BGK碰撞算子（Bhatnagar Gross and Krook）。其中$\\tau_f$被称为松弛时间，与运动粘度息息相关，一般来说$\\tau_f$越大则模拟出来的流体表现得越粘。运动粘度$\\mu$与松弛时间的关系为$\\mu =c_s^2(\\tau_f -\\frac{\\Delta t}{2})$，其中$c_s$是声波在流体中的传播速度。而上述公式中的$f_i^{eq}(x,t)$是流场平衡状态下的分布函数，它的计算公式为： f_i^{eq}(x,t)=\\omega_i \\rho(x,t)(1+\\frac{u(x,t)\\cdot c_i}{c_s^2}+\\frac{(u(x,t)\\cdot c_i)^2}{2c_s^4}-\\frac{u(x,t)\\cdot u(x,t)}{2c_s^2}) \\tag {3}&emsp;&emsp;$\\omega_i$为在该方向上的权重值。碰撞步骤可以理解为处理流体粒子的碰撞，使得流体粒子碰撞之后，能够朝向平衡状态变化。碰撞之后，我们紧接着进行流动步骤，流动步骤的核心公式如下所示： f_i(x+c_i \\Delta t, t+\\Delta t) = f_i(x,t) \\tag {4}&emsp;&emsp;乍一看，这个流动的公式颇有半拉格朗日对流算法的味道。上述的公式并不难理解，它本质上就是在相连的栅格之间传播$f_i(x,t)$函数值，从而形成流体流动的效果。一般情况下我们把流动和碰撞这两个步骤合在一起实现，也就是把公式$(2)$和公式$(4)$合并成下面的形式： f_i(x+c_i \\Delta t, t+\\Delta t)=f_i(x,t)-\\frac{\\Delta t}{\\tau_f}(f_i(x,t)-f_i^{eq}(x,t)) \\tag {5}&emsp;&emsp;公式$(5)$就是栅格Boltzmann流体模拟的核心方程，就是这么朴实无华、简单（没有偏微分）。但要实现一个完整的LBM流体解算器还需要考虑一些其他方面的东西，例如初始状态设定、边界条件处理等等。公式$(5)$也可以写成如下的等价形式： f_i(x, t+\\Delta t)=f_i(x-c_i \\Delta t,t)-\\frac{\\Delta t}{\\tau_f}(f_i(x-c_i \\Delta t,t)-f_i^{eq}(x-c_i \\Delta t,t)) \\tag {6}&emsp;&emsp;实现的时候我们更倾向于使用公式$(6)$，遍历所有的格子，并按照公式更新每个格子对应的值。 二、LBM解算步骤&emsp;&emsp;首先是关于系统初始状态的设定。在初始情况下（也就是$t=0$时），我们令相关变量的取值如下： \\begin{align} \\rho(x,t=0) &= 1\\\\ u(x,t=0) &= 0\\\\ \\end{align} \\tag {6}&emsp;&emsp;然后在每一个时间步上进行迭代求解。每个时间步主要的迭代过程分成如下几步： Step (1)：利用公式$(5)$进行碰撞和流动，更新每个格子每个方向上的粒子分布函数$f_i(x,t)$； Step (2)：利用公式$(1)$计算每个格子的$\\rho(x,t)$和流体的速度场$u(x,t)$ Step (3)：处理边界条件，处理流体与边界、固体的碰撞，对流体的速度场$u(x,t)$进行边界修正 &emsp;&emsp;一般情况默认时间步长$\\Delta t = 1$。这里提一下流体与固体的碰撞处理，一种最简单的方案就是构建一个与LBM网格一样大小的栅格数据结构，每个栅格上存储的是布尔变量。若当前格子被固体占据，则设为True，反之设为False。这样就能快速判断当前的格子是不是在固体内部，如果在固体内部则需要做进一步的修正处理。（至于如何构建这样的一个mask结构，你可以去了解一下网格体素化的概念） &emsp;&emsp;而在边界处理方面，按照处理方法的不同可分成狄拉克边界条件（Dirichlet boundary condition）和冯诺伊曼边界条件（Neumann边界）。狄拉克边界条件直接设定边界上的物理值，例如我直接令边界上的速度场为零；而冯诺伊曼边界条件通常直接设定的是物理值在边界上的偏导数值，例如设置边界上的偏导值为零，则表示该物理量在边界上不会发生任何变化，因此应该等于靠近边上的物理量值。关于这两种边界条件的细节我不再多说。 三、二维solver的实现&emsp;&emsp;这里以一个二维的LBM解算器实现为例展开相关的实现细节，主要参考了taichi论坛上的一位搞CFD大佬的代码。首先是数据结构的声明和创建。在这里我们需要为$\\rho(x,t)$、$u(x,t)$、$f_i(x,t))$和固体$mask$开辟一个二维数组，其中$f_i(x,t))$需要开辟两个一样的数组，用以迭代交换。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def __init__(self, nx, # domain size ny, niu, # viscosity of fluid bc_type, # [left,top,right,bottom] boundary conditions: 0 -&gt; Dirichlet ; 1 -&gt; Neumann bc_value, # if bc_type = 0, we need to specify the velocity in bc_value cy = 0, # whether to place a cylindrical obstacle cy_para = [0.0, 0.0, 0.0], # location and radius of the cylinder steps = 60000): # total steps to run self.nx = nx # by convention, dx = dy = dt = 1.0 (lattice units) self.ny = ny self.niu = niu self.tau = 3.0 * niu + 0.5 self.inv_tau = 1.0 / self.tau # density. self.rho = ti.var(dt=ti.f32, shape=(nx, ny)) # velocity. self.vel = ti.Vector(2, dt=ti.f32, shape=(nx, ny)) self.mask = ti.var(dt=ti.f32, shape=(nx, ny)) # particle density function. self.f_old = ti.Vector(9, dt=ti.f32, shape=(nx, ny)) self.f_new = ti.Vector(9, dt=ti.f32, shape=(nx, ny)) # weights for velocity. self.w = ti.var(dt=ti.f32, shape=9) # lattice vector self.e = ti.var(dt=ti.i32, shape=(9, 2)) # boundary condition. self.bc_type = ti.var(dt=ti.i32, shape=4) # boundary value if we use dirichlet boundary condition. self.bc_value = ti.var(dt=ti.f32, shape=(4, 2)) # whether to place a cylindrical obstacle. self.cy = cy self.cy_para = ti.var(dt=ti.f32, shape=3) self.bc_type.from_numpy(np.array(bc_type, dtype=np.int32)) self.bc_value.from_numpy(np.array(bc_value, dtype=np.float32)) self.cy_para.from_numpy(np.array(cy_para, dtype=np.float32)) # number of substeps. self.steps = steps # 1/36 1/9 1/36 # 1/9 4/9 1/9 # 1/36 1/9 1/36 arr = np.array([ 4.0 / 9.0, 1.0 / 9.0, 1.0 / 9.0, 1.0 / 9.0, 1.0 / 9.0, 1.0 / 36.0, 1.0 / 36.0, 1.0 / 36.0, 1.0 / 36.0], dtype=np.float32) self.w.from_numpy(arr) arr = np.array([[0, 0], [1, 0], [0, 1], [-1, 0], [0, -1], [1, 1], [-1, 1], [-1, -1], [1, -1]], dtype=np.int32) self.e.from_numpy(arr) &emsp;&emsp;然后就是系统状态的初始化，我们给场景中加了一个原型的固体，因此在初始化的时候相应地也设置mask： 12345678910111213@ti.kerneldef init(self): for i, j in self.rho: self.vel[i, j] = ti.Vector([0.0, 0.0]) self.rho[i, j] = 1.0 self.mask[i, j] = 0.0 for k in ti.static(range(9)): self.f_new[i, j][k] = self.f_eq(i, j, k) self.f_old[i, j][k] = self.f_new[i, j][k] if(self.cy==1): if ((ti.cast(i, ti.f32) - self.cy_para[0])**2.0 + (ti.cast(j, ti.f32) - self.cy_para[1])**2.0 &lt;= self.cy_para[2]**2.0): self.mask[i, j] = 1.0 &emsp;&emsp;在进行碰撞和流体之前，我们根据公式$(3)$实现平衡状态的$f_i^{eq}(x,t)$的计算： 123456@ti.func # compute equilibrium distribution functiondef f_eq(self, i, j, k): eu = ti.cast(self.e[k, 0], ti.f32) * self.vel[i, j][0] + ti.cast(self.e[k, 1], ti.f32) * self.vel[i, j][1] uv = self.vel[i, j][0]**2.0 + self.vel[i, j][1]**2.0 return self.w[k] * self.rho[i, j] * (1.0 + 3.0 * eu + 4.5 * eu**2 - 1.5 * uv) &emsp;&emsp;然后套用前面的公式$(6)$进行碰撞和流动的处理，得到新的$f_i(x,t)$： 12345678@ti.kerneldef collide_and_stream(self): # lbm core equation for i, j in ti.ndrange((1, self.nx - 1), (1, self.ny - 1)): for k in ti.static(range(9)): ip = i - self.e[k, 0] jp = j - self.e[k, 1] self.f_new[i,j][k] = (1.0-self.inv_tau)*self.f_old[ip,jp][k] + \\ self.f_eq(ip,jp,k)*self.inv_tau &emsp;&emsp;随后利用新的$f_i(x,t)$套用公式$(1)$计算$\\rho(x,t)$和$u(x,t)$： 123456789101112131415@ti.kerneldef update_macro_var(self): # compute rho u v for i, j in ti.ndrange((1, self.nx - 1), (1, self.ny - 1)): self.rho[i, j] = 0.0 self.vel[i, j][0] = 0.0 self.vel[i, j][1] = 0.0 for k in ti.static(range(9)): self.f_old[i, j][k] = self.f_new[i, j][k] self.rho[i, j] += self.f_new[i, j][k] self.vel[i, j][0] += (ti.cast(self.e[k, 0], ti.f32) * self.f_new[i, j][k]) self.vel[i, j][1] += (ti.cast(self.e[k, 1], ti.f32) * self.f_new[i, j][k]) self.vel[i, j][0] /= self.rho[i, j] self.vel[i, j][1] /= self.rho[i, j] &emsp;&emsp;最后需要对流体的属性根据边界条件和与固体的碰撞进行修正： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@ti.kerneldef apply_bc(self): # impose boundary conditions # left and right for j in ti.ndrange(1, self.ny - 1): # left: dr = 0; ibc = 0; jbc = j; inb = 1; jnb = j self.apply_bc_core(1, 0, 0, j, 1, j) # right: dr = 2; ibc = nx-1; jbc = j; inb = nx-2; jnb = j self.apply_bc_core(1, 2, self.nx - 1, j, self.nx - 2, j) # top and bottom for i in ti.ndrange(self.nx): # top: dr = 1; ibc = i; jbc = ny-1; inb = i; jnb = ny-2 self.apply_bc_core(1, 1, i, self.ny - 1, i, self.ny - 2) # bottom: dr = 3; ibc = i; jbc = 0; inb = i; jnb = 1 self.apply_bc_core(1, 3, i, 0, i, 1) # cylindrical obstacle # Note: for cuda backend, putting 'if statement' inside loops can be much faster! for i, j in ti.ndrange(self.nx, self.ny): if (self.cy == 1 and self.mask[i, j] == 1): self.vel[i, j][0] = 0.0 # velocity is zero at solid boundary self.vel[i, j][1] = 0.0 inb = 0 jnb = 0 if (ti.cast(i,ti.f32) &gt;= self.cy_para[0]): inb = i + 1 else: inb = i - 1 if (ti.cast(j,ti.f32) &gt;= self.cy_para[1]): jnb = j + 1 else: jnb = j - 1 self.apply_bc_core(0, 0, i, j, inb, jnb)@ti.funcdef apply_bc_core(self, outer, dr, ibc, jbc, inb, jnb): if (outer == 1): # handle outer boundary # Dirichlet boundary condition if (self.bc_type[dr] == 0): self.vel[ibc, jbc][0] = self.bc_value[dr, 0] self.vel[ibc, jbc][1] = self.bc_value[dr, 1] # Neumann boundary condition elif (self.bc_type[dr] == 1): self.vel[ibc, jbc][0] = self.vel[inb, jnb][0] self.vel[ibc, jbc][1] = self.vel[inb, jnb][1] self.rho[ibc, jbc] = self.rho[inb, jnb] for k in ti.static(range(9)): self.f_old[ibc,jbc][k] = self.f_eq(ibc,jbc,k) - self.f_eq(inb,jnb,k) + \\ self.f_old[inb,jnb][k] &emsp;&emsp;把上面的过程串起来作为一个迭代，则LBM的迭代解算过程如下，后面的一些流程可视化方法这里就不提了。总的来说，LBM方法挺简单的，不用求解大规模的PDE。 1234567891011121314151617181920212223242526def solve(self): gui = ti.GUI('lbm solver', (self.nx, 2*self.ny)) self.init() for i in range(self.steps): self.collide_and_stream() self.update_macro_var() self.apply_bc() ## code fragment displaying vorticity is contributed by woclass vel = self.vel.to_numpy() ugrad = np.gradient(vel[:, :, 0]) vgrad = np.gradient(vel[:, :, 1]) vor = ugrad[1] - vgrad[0] vel_mag = (vel[:, :, 0]**2.0+vel[:, :, 1]**2.0)**0.5 ## color map colors = [(1, 1, 0), (0.953, 0.490, 0.016), (0, 0, 0), (0.176, 0.976, 0.529), (0, 1, 1)] my_cmap = matplotlib.colors.LinearSegmentedColormap.from_list( 'my_cmap', colors) vor_img = cm.ScalarMappable(norm=matplotlib.colors.Normalize( vmin=-0.02, vmax=0.02),cmap=my_cmap).to_rgba(vor) vel_img = cm.plasma(vel_mag / 0.15) img = np.concatenate((vor_img, vel_img), axis=1) gui.set_image(img) gui.show() if (i % 1000 == 0): print('Step: &#123;:&#125;'.format(i)) Reference$[1]$ Lattice Boltzmann methods From Wikipedia, the free encyclopedia $[2]$ 格子玻尔兹曼方法（LBM）的学习笔记 $[3]$ 计算流体力学视角的流体求解器 $[4]$ 大道至简的LBM算法","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Fluid  simulation","slug":"Fluid-simulation","permalink":"https://yangwc.com/categories/Fluid-simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Fluid simulation","slug":"Fluid-simulation","permalink":"https://yangwc.com/tags/Fluid-simulation/"}]},{"title":"Physical Simulation：Mass-spring systems","slug":"MassSpring","date":"2020-06-24T01:59:26.382Z","updated":"2020-06-24T02:07:36.051Z","comments":true,"path":"2020/06/24/MassSpring/","link":"","permalink":"https://yangwc.com/2020/06/24/MassSpring/","excerpt":"质点弹簧系统是一种经典的物理模型，简单方便，但稳定的求解器还是需要使用比较复杂的隐式欧拉积分方法，利用雅可比迭代法或者共轭梯度法求解大规模的线性方程组来求解速度矢量。这篇博客内容是学习了胡渊鸣大佬开的课程的笔记。","text":"质点弹簧系统是一种经典的物理模型，简单方便，但稳定的求解器还是需要使用比较复杂的隐式欧拉积分方法，利用雅可比迭代法或者共轭梯度法求解大规模的线性方程组来求解速度矢量。这篇博客内容是学习了胡渊鸣大佬开的课程的笔记。 &emsp;&emsp;质点弹簧系统是一种拉格朗日视角的物理模拟，把物体看成由一个个质点构成，质点之间通过弹簧相连，从而产生一定程度的弹性形变。质点弹簧系统适用于模拟布料软体、刚体等物理材质。 一、胡克定律&emsp;&emsp;对于物体中的任意两个不同的质点$x_i$和$x_j$，从$j$作用到$i$的作用力可由以下的胡克定律（Hooke’s law）给出： \\mathbf{f_{ij}}=-k(\\sqrt{\\mathbf{x_i} - \\mathbf{x_j}}-l_{ij})\\widehat{(\\mathbf{x_i} - \\mathbf{x_j})} \\tag {1}&emsp;&emsp;其中$k$是弹簧刚度系数（spring sitffness），$l_{ij}$是质点之间的弹簧静止长度，$\\widehat{\\mathbf{x_i}-\\mathbf{x_j}}$是从质点$i$指向质点$j$的单位方向向量。对于质点$x_i$，它收到的作用力就是与之相连的所有弹性力的合力： \\mathbf{f_i} = \\Sigma_j ^{j\\neq i}\\mathbf{f_{ij}} \\tag {2}&emsp;&emsp;求出了合力，再根据牛顿第二定律可求出质点的加速度和位置的变化梯度： \\frac{\\partial \\mathbf{v_i}}{\\partial t} = \\frac{1}{m_i}\\mathbf{f_i} \\tag {3} \\frac{\\partial \\mathbf{x_i}}{\\partial t}=\\mathbf{v_i}&emsp;&emsp;上面就是质点弹簧系统的核心公式。 二、显式积分求解&emsp;&emsp;一种最简单的求解方法就是采用前向欧拉积分（显式积分）： \\mathbf{v_{t+1}}= \\mathbf{v_t}+\\Delta t\\frac{\\mathbf{f_t}}{m}\\\\ \\mathbf{x_{t+1}}=\\mathbf{x_t}+\\Delta t \\mathbf{v_t} \\tag {4}&emsp;&emsp;但通常更常用的是下面的半隐式欧拉积分法（依旧是显式积分），区别在于更新$\\mathbf{x_{t+1}}$是用$\\mathbf{v_t}$还是$\\mathbf{v_{t+1}}$： \\mathbf{v_{t+1}}= \\mathbf{v_t}+\\Delta t\\frac{\\mathbf{f_t}}{m}\\\\ \\mathbf{x_{t+1}}=\\mathbf{x_t}+\\Delta t \\mathbf{v_{t+1}} \\tag {5}&emsp;&emsp;使用半隐式欧拉法的求解步骤可以简单地分成如下的步骤： Compute new velocity using $\\mathbf{v_{t+1}}= \\mathbf{v_t}+\\Delta t\\frac{\\mathbf{f_t}}{m}$ Collision detection and handling Compute new position using $\\mathbf{x_{t+1}}=\\mathbf{x_t}+\\Delta t \\mathbf{v_{t+1}}$ &emsp;&emsp;显式积分的求解比较简单，不难理解，这里不再细说。下面的代码实现了显式积分求解的质点弹簧系统。 12345678910111213141516171819202122@ti.kerneldef substep(): # Compute force and new velocity n = num_particles[None] for i in range(n): v[i] *= ti.exp(-dt * damping[None]) # damping total_force = ti.Vector(gravity) * particle_mass for j in range(n): if rest_length[i, j] != 0: x_ij = x[i] - x[j] total_force += -spring_stiffness[None] * (x_ij.norm() - rest_length[i, j]) * x_ij.normalized() v[i] += dt * total_force / particle_mass # Collide with ground for i in range(n): if x[i].y &lt; bottom_y: x[i].y = bottom_y v[i].y = 0 # Compute new position for i in range(num_particles[None]): x[i] += v[i] * dt &emsp;&emsp;显式积分求解的优点就是简单方便、计算量小，但它也有致命的缺点，就是很容易导致模拟的数值爆炸现象。为了防止数值爆炸，显式积分器对模拟的时间步长有如下的严格要求： \\Delta t\\leq c\\sqrt{\\frac{m}{k}}\\ \\ \\ \\ (c\\sim 1) \\tag {6}&emsp;&emsp;可以看到时间步长的上限与物体的刚度系数$k$密切相关，刚度系数越大，则时间步长应该越小，否则将不满足上述条件而爆炸。因此显式积分器对于高刚度系数的物体不友好，稳定性不够。 三、隐式积分求解&emsp;&emsp;隐式积分器从后向欧拉法展开，将原本显式积分的公式$(5)$换成下述形式： \\mathbf{x_{t+1}}=\\mathbf{x_t}+\\Delta t \\mathbf{v_{t+1}}\\\\ \\mathbf{v_{t+1}}= \\mathbf{v_t}+\\Delta t\\mathbf{M^{-1}}\\mathbf{f}(\\mathbf{x_{t+1}}) \\tag {7}&emsp;&emsp;其中$\\mathbf{M}^{-1}$是对角矩阵，对角线上的元素取值为质点质量的倒数。通俗地讲就是未来的物理量依旧取决于未来的状态，乍一看是一个鸡生蛋还是蛋生鸡的死循环问题。我们先把上面的$\\mathbf{x_{t+1}}$替换到下面的$\\mathbf{v_{t+1}}$公式中，有： \\mathbf{v_{t+1}}= \\mathbf{v_t}+\\Delta t\\mathbf{M^{-1}}\\mathbf{f}(\\mathbf{x_t}+\\Delta t \\mathbf{v_{t+1}})&emsp;&emsp;然后对$\\mathbf{f}(\\mathbf{x_t}+\\Delta t \\mathbf{v_{t+1}})$做一阶泰勒展开可得： \\mathbf{v_{t+1}}= \\mathbf{v_t}+\\Delta t\\mathbf{M^{-1}}[\\mathbf{f}(\\mathbf{x_t})+\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}(\\mathbf{x_t})\\Delta t \\mathbf{v_{t+1}}]&emsp;&emsp;再经过移项整理，就有如下的形式： [\\mathbf{I}-\\Delta t^2 \\mathbf{M}^{-1}\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{x}}(\\mathbf{x_t})]\\mathbf{v}_{t+1}=\\mathbf{v}_t+\\Delta t \\mathbf{M}^{-1}\\mathbf{f}(\\mathbf{x}_t) \\tag {8}&emsp;&emsp;对于每一个质点，都有上述的公式$(8)$。公式$(8)$中左边是一个未知量$\\mathbf{v_{t+1}}$乘上系数矩阵，右边是已知量，这是一个线性方程。把所有的质点的公式$(8)$组装起来，就得到了一个大规模的线性方程组。设质点数量为$n$，则左边的系数矩阵为$n\\times n$的矩阵，每个矩阵的元素又是一个小矩阵（对于二维情况，就是$2\\times 2$，而三维就是$3\\times 3$），未知量向量长度为$n$，每个向量的元素也是一个小向量（对于二维情况，就是$2$，而三维就是$3$），右边同理： \\begin{align} \\mathbf{A}&=[\\mathbf{I}-\\Delta t^2 \\mathbf{M}^{-1}\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}(\\mathbf{x}_t)]\\\\ \\mathbf{b}&=\\mathbf{v}_t+\\Delta t\\mathbf{M}^{-1}\\mathbf{f}(\\mathbf{x}_t)\\\\ \\mathbf{A}\\mathbf{v}_{t+1}&= \\mathbf{b} \\end{align}&emsp;&emsp;所以隐式积分器是先求解出上述方程中的$\\mathbf{v}_{t+1}$，然后再利用$\\mathbf{v}_{t+1}$去更新质点的位置向量。上述公式中还有一个比较难搞的就是$\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}$项，$\\mathbf{f}$是一个多元向量，$\\mathbf{x}$亦是一个多维向量，因此$\\mathbf{f}$关于$\\mathbf{x}$是的求导结果是一个雅可比矩阵（二维情况下该矩阵大小为$2\\times 2$，更高维依次类推）。关于雅可比矩阵，请看这个维基百科链接，这里不再赘述。 &emsp;&emsp;关于$\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}$的推导请看这里，最终的计算公式如下： \\begin{align} \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x_i}} &= -k[(1-\\frac{l_{ij}}{|x_{ij}|})(\\mathbf{I}-\\widehat x_{ij}\\cdot \\widehat x_{ij}^T)+\\widehat x_{ij}\\cdot \\widehat x_{ij}^T]\\\\ &=-k[\\mathbf{I}-\\frac{l_{ij}}{|x_{ij}|}(\\mathbf{I}+\\widehat x_{ij}\\cdot \\widehat x_{ij}^T)] \\tag {9} \\end{align}&emsp;&emsp;上面是关于$\\mathbf{x}_i$的求导，而关于$\\mathbf{x}_j$的求导则是取相反符号即可： \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x_j}}=-\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x_i}}&emsp;&emsp;对于质点$\\mathbf{x}_i$，它通常收到多个弹力，因此其最终的雅可比矩阵为多个导数的求和： \\Sigma_k^n\\frac{\\partial \\mathbf{f}_{ik}}{\\partial \\mathbf{x_i}}&emsp;&emsp;接下来就是如何求解线性方程组$\\mathbf{A}\\mathbf{v}_{t+1}= \\mathbf{b}$的问题，一种简单、容易并行的方法就是雅可比迭代，关于雅可比迭代的具体细节请看这里。雅可比迭代与高斯消元法不同，它采用迭代过程渐进逼近地方式来求方程的解，虽然要有一定的误差，但都在可接受的范围之内。雅可比迭代的核心公式如下： x_i^{k+1}=\\frac{1}{a_{ii}}(b_i-\\Sigma_{j\\neq i} a_{ij}x_j^{k}) \\tag {10}&emsp;&emsp;在我们的这个问题中（二维的质点弹簧系统），$x$和$b$是一个二维向量，$a_{ij}$和$a_{ii}$是一个二维的矩阵，因此需要做适当的高维扩展。$\\frac{1}{a_{ii}}$就变成了$2\\times 2$矩阵$a_{ii}$的逆矩阵。用隐式积分器求解质点弹簧系统的模拟步骤如下： 求解每个质点所受的弹簧合力$ \\mathbf{f}$ 构建线性方程组$\\mathbf{A}\\mathbf{v}_{t+1}= \\mathbf{b}$ 求解上述的方程组得到$\\mathbf{v}_{t+1}$，进行碰撞检测 最后根据$\\mathbf{v}_{t+1}$更新位置向量$\\mathbf{x}_{t+1}$ &emsp;&emsp;以二维为例，首先计算每个质点弹簧合力，然后保存起来： 1234567891011@ti.kerneldef calc_force(): # Compute force n = num_particles[None] for i in range(n): total_force = ti.Vector(gravity) * particle_mass for j in range(n): if rest_length[i, j] != 0: x_ij = x[i] - x[j] total_force += -spring_stiffness[None] * (x_ij.norm() - rest_length[i, j]) * x_ij.normalized() f[i] = total_force &emsp;&emsp;然后根据前面的公式构建矩阵$\\mathbf{A}$和$\\mathbf{b}$（这一步需要特别小心，很容易出错）： 123456789101112131415161718192021222324252627282930313233343536@ti.kerneldef build_linear_system(): n = num_particles[None] k = spring_stiffness[None] I = ti.Matrix([[1.0, 0.0], [0.0, 1.0]]) # initialization for i, j in A: A[i, j] = ti.Matrix([[0.0, 0.0], [0.0, 0.0]]) # build Ax=b to solve velocity implcitly for i in range(n): # Jacobi matrix for j in range(n): ij_length = rest_length[i, j] if ij_length != 0: x_ij = x[i] - x[j]; x_ij_norm = x_ij.normalized() ij_dist = x_ij.norm() mat = x_ij_norm @ (x_ij_norm.transpose()) # https://blog.mmacklin.com/2012/05/04/implicitsprings/ A[i, i] = A[i, i] - k * (I - ij_length / ij_dist * (I - mat)) if j != i: A[i, j] = A[i, j] + k * (I - ij_length / ij_dist * (I - mat)) cof = dt ** 2 for i, j in A: tmp = A[i, j] if i != j: A[i, j] = -cof * tmp else: A[i, j] = I - cof * tmp # calculate b for i in range(n): b[i] = v[i] + dt / particle_mass * f[i] &emsp;&emsp;然后采用雅可比迭代法求解线性方程组，下面给出了一次迭代的代码，通常需要迭代多次（例如20次）： 1234567891011121314@ti.kerneldef jacobi_iterate(): n = num_particles[None] # https://zh.wikipedia.org/wiki/%E9%9B%85%E5%8F%AF%E6%AF%94%E6%B3%95 for i in range(n): r = b[i] for j in range(n): if i != j and rest_length[i, j] != 0: r -= A[i, j] @ v[j] new_v[i] = A[i, i].inverse() @ r for i in range(n): v[i] = new_v[i] &emsp;&emsp;最后进行碰撞检测，并更新质点的位置（简单的时间积分即可）： 123456789101112131415@ti.kerneldef collision_and_damping(): n = num_particles[None] # velocity damping for i in range(n): v[i] *= (ti.exp(-dt * damping[None])) # Collide with ground if x[i].y &lt; bottom_y: x[i].y = bottom_y v[i].y = 0 # compute new position for i in range(n): x[i] += v[i] * dt &emsp;&emsp;这样就完成了一个模拟过程。隐式的欧拉积分确实要比显式的稳定一些，但把刚度系数拉到百万的数量级，依旧容易爆炸，不是无条件稳定。另外吐槽一句，目前taichi虽然好用，但还没有稳定下来，容易出现各种bug。我本来是打算用共轭梯度法求解线性方程组的，但调了一下午发现还是出现奇怪的编译错误，遂放弃了。 Reference$[1]$ GAMES201：高级物理引擎实战指南2020 $[2]$ 雅可比矩阵, 维基百科，自由的百科全书 $[3]$ 雅可比法, 维基百科，自由的百科全书","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Physical simulation","slug":"Physical-simulation","permalink":"https://yangwc.com/categories/Physical-simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Physical simulation","slug":"Physical-simulation","permalink":"https://yangwc.com/tags/Physical-simulation/"}]},{"title":"Physically Based Rendering：体积散射","slug":"VolumeScattering","date":"2020-04-19T13:54:26.174Z","updated":"2020-04-19T13:53:19.180Z","comments":true,"path":"2020/04/19/VolumeScattering/","link":"","permalink":"https://yangwc.com/2020/04/19/VolumeScattering/","excerpt":"在不考虑空气介质的散射效应时，我们假定光线在真空中传播，故光线的辐射率在传播过程不会发生变化。但真实地球世界却并非真空，大气散射、烟雾散射等丁达尔效应现象对渲染结果至关重要，这类光学效果涉及到散射介质，光线在此类介质中被吸收、散射，最终到达人眼的辐射率与未考虑介质散射的有非常明显的不同。","text":"在不考虑空气介质的散射效应时，我们假定光线在真空中传播，故光线的辐射率在传播过程不会发生变化。但真实地球世界却并非真空，大气散射、烟雾散射等丁达尔效应现象对渲染结果至关重要，这类光学效果涉及到散射介质，光线在此类介质中被吸收、散射，最终到达人眼的辐射率与未考虑介质散射的有非常明显的不同。 &emsp;&emsp;以下的内容大部分整理自pbrt第三版的第十章——VOLUME SCATTERING。 一、体积散射效应&emsp;&emsp;光线在参与介质（participating media）中传播的辐射率分布主要受以下三个方面的影响： 吸收（Absorption）：光线与介质中的粒子碰撞，直接将光能转换成其它形式的能量，例如热能，因此光线的辐射率被衰减了； 发光（Emission）：介质中的发光粒子产生的辐射率附加到光线上，因此光线的辐射率被增强了； 散射（Scattering）：光线与介质中的粒子碰撞，导致光线的传播方向发生了变化，对于给定的传播方向，既有该方向上的光线向外散射到其他方向（外散射），又有其他方向的光线被散射并融入到该方向上（内散射）。 &emsp;&emsp;这些体积散射效应按照介质的类别又可以分为均匀（homogeneous）散射和非均匀（inhomogeneous）散射。均匀散射指参与介质在其分布的空间中是均匀分布的（可以看成参与介质的粒子是均匀散落在空间中），此类介质的散射比较简单；非均匀散射则是指光线在非均匀分布的参与介质中传播，例如地球的大气层，从海平面逐渐升高空气的密度逐渐稀薄。 1、吸收&emsp;&emsp;阳光照射到空气中的烟雾，也会在地面上投射出一个阴影轮廓，这是因为光线经过烟雾介质时一部分的辐射率被吸收了，导致到达地面上的辐射率低了不少，从而形成阴影。烟雾越浓越厚，则阴影越明显。介质的吸收性质通常用介质的吸收截面$\\sigma_a$（absorption cross section）来表示，它描述了光透过介质中的单位距离时被吸收的分量，因此这是一个概率密度函数，单位为距离的倒数$m^{-1}$。吸收截面通常与空间中的位置$p$和介质的光谱有关，但在某些特殊情况下也与方向$\\omega$有关。值得注意的是，虽然$\\sigma_a$是概率密度，但它的取值并没有被限制于$0$到$1$之间，因此需要进一步取自然底数$e$的指数转换成吸收概率密度值（后面会看到）。 &emsp;&emsp;下图展示了光线在介质中经过的一段极短的距离（以$p$点为中心）被吸收的情况，设该极短距离为$dt$。光线从左边入射进来，其辐射率为$L_i(p,-\\omega)$，与介质中的粒子碰撞被吸收之后的出射辐射率记为$L_o(p,\\omega)$。现给定吸收截面$\\sigma_a$，则被吸收的微分辐射率为： L_o(p,\\omega)-L_i(p,-\\omega)=dL_o(p,\\omega)=-\\sigma_a(p,\\omega)L_i(p,-\\omega)dt \\tag {1.1}&emsp;&emsp;上式给出了光线被吸收的辐射率值，它表明被吸收的微分辐射率$dL_o$是其初始辐射率$L_i$的线性函数。 &emsp;&emsp;因此，给定光线的起始点$p$和入射方向$\\omega$，光线在介质中穿过$d$距离时，经过吸收衰减之后，剩余的辐射率占据原来的辐射率比值为如下的指数上的积分形式： e^{-\\int_0^d \\sigma_a(p+\\omega t,\\omega)dt} \\tag {1.2}&emsp;&emsp;即对$[0,d]$上的吸收截面$\\sigma_a$进行积分，得到总的吸收率，再取负就得到剩余的能量比例。 2、发光&emsp;&emsp;一方面光线透过介质被吸收导致衰减，但另一方面，介质中的发光粒子会产生光能，汇入到传播的光线当中。介质中的发光粒子的产生原因有很多，例如化学反应等。下图依旧以一个微分距离$dt$为例，记介质的发光辐射率为$L_e(p,\\omega)$，它是关于位置$p$和方向$\\omega$的函数，描述了单位距离上的发光辐射率值。 &emsp;&emsp;因此，经过上图的微分距离$dt$，新增的微分辐射率为： dL_o(p,\\omega)=L_e(p,\\omega) dt \\tag {1.3}&emsp;&emsp;在这里，我们假定发光辐射率$L_e(p,\\omega)$与入射辐射率$L_i$无关。 3、外散射&emsp;&emsp;对于给定的光线传播方向$\\omega$，根据散射方向的不同，可以分成内散射和外散射。对于给定的光线传播方向，因为与介质粒子碰撞导致光线的方向被弹射至其它方向，减弱了原本光线辐射率，这就是外散射；而又因为周围粒子的外散射导致其他方向的光线被弹射到当前的光线传播方向，增加了原本光线辐射率，这就是内散射。内散射源于外散射，两者不是排斥关系。这里先来看外散射效应。 &emsp;&emsp;每单位距离的外散射的概率密度用散射系数$\\sigma_s$。与吸收类似，外散射导致的减少的微分辐射率为： dL_o(p,\\omega)=-\\sigma_s(p,\\omega)L_i(p,-\\omega)dt &emsp;&emsp;通常，对于辐射率的衰减，我们把吸收和外散射两个因素综合起来一起计算，因此总的衰减比例为吸收截面加上散射系数：$\\sigma_a+\\sigma_s$。光线的吸收和外散射我们统一称为衰减（attenuation）或消光（extinction），用衰减系数$\\sigma_t$记为两者之和： \\sigma_t(p,\\omega)=\\sigma_a(p,\\omega)+\\sigma_s(p,\\omega) \\tag {1.4}&emsp;&emsp;给定了介质的衰减系数$\\sigma_t$，辐射率的衰减速率为$\\frac{dL_o(p,\\omega)}{dt}=-\\sigma_t(p,\\omega) L_i(p,\\omega)$。给定要通过的路径的两个端点$p$和$p’$（如下图所示），以下的积分公式计算光线通过这条路径时，除去被吸收和被外散射的，剩余的辐射率占比，我们称之为光束透射率（beam transmittance）： T_r(p\\to p')=e^{-\\int_0^d\\sigma_t(p+t\\omega,\\omega)dt} \\tag {1.5}&emsp;&emsp;其中$d=||p-p’||$是$p$和$p’$之间的直线距离，$\\omega$是$p\\to p’$的单位方向向量。 &emsp;&emsp;公式$(1.5)$计算得到的光束透射率取值介于$0$到$1$之间，本质上是个百分比。因此，经过衰减之后到达$p’$点的辐射率就是起始点$p$的辐射率再乘上光束透射率： T_r(p\\to p')L_o(p,\\omega)&emsp;&emsp;来观察观察一下光束透射率即公式$(1.5)$的一些性质。当介质为真空时，即$\\sigma_t=0$，则$T_r=1$；当$p=p’$时，$T_r=1$。符合物理常识。此外，若衰减系数$\\sigma_t$是关于方向对称的，即$\\sigma_t(\\omega)=\\sigma_t(-\\omega)$，或者$\\sigma_t$仅仅是关于位置的函数，则介质中两点之间的透射率亦符合如下的对称性： T_r(p\\to p')=T_r(p'\\to p)&emsp;&emsp;关于光束透射率的另一个重要的属性就是 ，对于任意的介质，有如下的性质： T_r(p\\to p'')=T_r(p\\to p')T_r(p'\\to p'')&emsp;&emsp;其中，$p’$是介于$p$和$p’’$之间的任意一点，如下图所示。这个性质非常有用，把原本$T_r(p\\to p’’)$的求解拆分成两段独立的$T_r(p\\to p’)$和$T_r(p’\\to p’’)$的求解，最后再通过相乘得到$T_r(p\\to p’’)$。 &emsp;&emsp;公式$(1.5)$中的指数取反就是两点之间的光学厚度（optical thickness），用符号$\\tau$表示： \\tau(p\\to p')=\\int_{o}^d \\sigma_t(p+t\\omega,-\\omega)dt \\tag {1.6}&emsp;&emsp;求解公式$(1.5)$的关键就是求解光学厚度$(1.6)$。对于均匀介质，衰减系数$\\sigma_t$是一个常量值，因此可以可以直接计算出光学厚度$\\tau=\\sigma_t d$，从而得出了Beer定律的光束透射率公式： T_r(p\\to p')=e^{-\\sigma_t d} \\tag {1.7}&emsp;&emsp;这里有两个非常重要的与衰减系数相关的概念。一个是参与介质的反照率（albedo），其公式为$\\rho=\\frac{\\sigma_s}{\\sigma_t}$，取值为$[0,1]$，它给出了散射比例；另一个是平均自由路径（mean free path），其公式为$1/\\sigma_t$，它给出了一束光线在介质中传播过程中与介质粒子碰撞时走过的平均路径。 4、内散射&emsp;&emsp;与外散射相反，内散射是其他方向的光线被弹射到当前的光线传播方向，增加了当前的光照辐射率值。对于内散射，这里做了一个假设，即介质粒子之间的间隔是粒子半径的数倍（即互不接触），因此在描述散射性质时可以忽略粒子之间的交互作用。基于该假设，给定空间中的一点和光线传播方向$\\omega$以及另外一个不同的方向$\\omega’$，相位函数（phase function）$p(\\omega,\\omega’)$描述了从$\\omega’$方向上的光线被散射到$\\omega$方向的概率密度值，类似于BSDF函数。因此相位函数有如下的归一化约束： \\int_{S^2}p(\\omega,\\omega')d\\omega' = 1&emsp;&emsp;即对整个球体方向的积分求和值应该为$1$。通过内散射和发光增加的微分辐射率为： dL_o(p,\\omega)=L_s(p,\\omega) dt &emsp;&emsp;而$L_s(p,\\omega)$综合了发光与内散射效应： L_s(p,\\omega)=L_e(p,\\omega)+\\sigma_s(p,\\omega)\\int_{S^2}p(p,\\omega_i,\\omega) L_i(p,\\omega_i)d\\omega{i} \\tag {1.8}二、相位函数&emsp;&emsp;BSDF模型对于给定的入射方向和出射方向，计算出射辐射率占入射辐照度的比值。而在体积散射中，相位函数也是一个类似的模型。在图形学中用到的相位函数中，有些是通过拟合得到的参数化模型，有些是根据介质形状和材质的散射辐射率分布推导出的解析模型。对于大部分的自然介质，其相位函数是关于$\\theta$的一维函数，入射方向$\\theta$是$\\omega_i$和出射方向$\\omega_o$的夹角，这些相位函数通常记为$p(cos\\theta)$。此类相位函数对应的参与介质是各向同性的，具有关于入射方向的局部旋转不变性。此外，因为$cos(-\\theta)=cos(\\theta)$，故该各向同性介质的相位函数具有可逆性，即$\\omega_i$和$\\omega_o$交换，但相位函数取值不变。对于各向异性的介质，其相位函数通常是关于$\\omega_i$和$\\omega_o$的四维函数，比较复杂。 &emsp;&emsp;这里需要注意的一点就是，相位函数本身亦可以是各向同性或者各向同性的，因此各向同性的介质可以有一个各向异性的相位函数。一个各向同性的相位函数描述的是向所有方向均等散射的情况，因此与$\\omega$和$\\omega’$这两个方向无关，各向同性的相位函数只有如下面的一个： p(\\omega_o,\\omega_i)=\\frac{1}{4\\pi}&emsp;&emsp;$\\omega_o$是原本的光线传播方向，$\\omega_i$是从其他方向弹射过来的入射方向。创建一个PhaseFunction的接口类如下，其中p(const Vector3f &amp;wo, const Vector3f &amp;wi)就是相位函数的接口，它的输入参数为$\\omega_i$和$\\omega_o$： 12345678class PhaseFunction &#123; public: // PhaseFunction Interface virtual ~PhaseFunction(); virtual Float p(const Vector3f &amp;wo, const Vector3f &amp;wi) const = 0; virtual Float Sample_p(const Vector3f &amp;wo, Vector3f *wi, const Point2f &amp;u) const = 0;&#125;; &emsp;&emsp;在这里，$\\omega_i$和$\\omega_o$的方向沿用BSDF的惯例，起始点为$p$，如下图所示。 &emsp;&emsp;Henyey和Greenstein提出一个现今广泛使用的相位函数，该相位函数的输入参数除了$cos\\theta$之外，还有非对称参数$g$。但应用到某个参与介质上时，非对称参数$g$就固定了： p_{HG}(cos\\theta)=\\frac{1}{4\\pi}\\frac{1-g^2}{(1+g^2+2g(cos\\theta))^{3/2}} \\tag {2.1}&emsp;&emsp;非对称参数$g$的取值范围为$[-1,1]$，$g$的不同取值则对应的相位函数分布亦不相同。下图分别展示了$g=-0.35$（实线）和$g=0.67$（虚线）时的Henyey-Greenstein相位函数分布。$g$取$[-1,0]$时光线大部分都从后半球方向散射进来，称之为后向散射（back-scattering）；$g$取$[0,1]$时光线大部分都从前半球方向散射进来，称之为前向散射（forward-scattering）。$g$的绝对值越大，则越偏向于$\\omega$（对于前向散射）或者$-\\omega$（对于后想散射）方向。 &emsp;&emsp;前向散射和后向散射产生的视觉效果差别非常大，下图给出了两者的渲染结果。左边的是后向散射的结果，因此可以模型后面的光线透过模型，抵达人眼，产生透射效果；右边的是前向散射的结果，模型倾向于反射其前方的光线，产生反射效果。 &emsp;&emsp;根据公式$(2.1)$，可以实现Henyey-Greenstein相位函数如下： 1234567891011121314151617181920inline Float PhaseHG(Float cosTheta, Float g) &#123; Float denom = 1 + g * g + 2 * g * cosTheta; return Inv4Pi * (1 - g * g) / (denom * std::sqrt(denom));&#125;class HenyeyGreenstein : public PhaseFunction &#123; public: // HenyeyGreenstein Public Methods HenyeyGreenstein(Float g) : g(g) &#123;&#125; Float p(const Vector3f &amp;wo, const Vector3f &amp;wi) const; Float Sample_p(const Vector3f &amp;wo, Vector3f *wi, const Point2f &amp;sample) const; private: const Float g;&#125;;Float HenyeyGreenstein::p(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; ProfilePhase _(Prof::PhaseFuncEvaluation); return PhaseHG(Dot(wo, wi), g);&#125; &emsp;&emsp;Henyey-Greenstein模型中的非对称参数$g$实际上有明确的数学意义，是相位函数$p$分布下的$cos\\theta$值的数学期望（或者说平均值），这里相位函数是概率密度函数，即： g=\\int_{S^2}p(-\\omega,\\omega')(\\omega \\cdot \\omega')d\\omega'=2\\pi \\int_0^\\pi p(-cos\\theta)cos\\theta sin\\theta d\\theta&emsp;&emsp;对于各向同性的相位函数（即前面的$\\frac{1}{4\\pi}$），上面计算得到$g=0$，符合预期结果。更复杂的相位函数可以通过一些简单的相位函数加权叠加得到： p(\\omega,\\omega')=\\sum_{i=1}^n \\omega_i p_i(\\omega\\to \\omega') \\tag {2.2}&emsp;&emsp;其中，权重的总和$\\sum_{i=1}^n\\omega_i$应该等于$1$，确保相位函数的归一化性质。pbrt并没有实现该类相位函数。 三、参与介质&emsp;&emsp;自然界中的散射介质形态各异，有些没有明显的物体边界（例如烟雾、大气层），而有些却有明显的物体边界（例如人体的皮肤）。尽管某些参与介质没有明确的边界，但通常拥有一个包围盒或者类似于包围盒的复杂几何体来表示介质涉及的最大范围，然后在渲染的时候并不渲染这个包围几何体即可。关于这方便的设定不再赘述。 &emsp;&emsp;有了边界，现在的问题就是如何描述边界之内的参与介质的分布情况。对于均匀的散射介质，介质粒子的密度为常量，根本不需要用额外的数据结构去存储参与介质的分布数据。而对于非均匀的散射介质，则要麻烦一些。这里首先创建一个Medium接口类，函数Tr是计算光束透射率的接口： 123456789class Medium &#123; public: // Medium Interface virtual ~Medium() &#123;&#125; virtual Spectrum Tr(const Ray &amp;ray, Sampler &amp;sampler) const = 0; virtual Spectrum Sample(const Ray &amp;ray, Sampler &amp;sampler, MemoryArena &amp;arena, MediumInteraction *mi) const = 0;&#125;; 1、均匀介质表示&emsp;&emsp;均匀介质（Homogeneous medium）是一类最简单的参与介质，也是一种最高效的介质模型，在图形领域亦有诸多应用。均匀介质其密度为某个常量值，因此其吸收截面$\\sigma_a$和散射系数$\\sigma_s$在整个介质区域内亦为常量。因此可以直接省去计算光学深度的积分公式，直接套用前面提到的Beer透射定律公式$(1.7)$，即： T_r(p\\to p')=e^{-\\sigma_t d}1234Spectrum HomogeneousMedium::Tr(const Ray &amp;ray, Sampler &amp;sampler) const &#123; ProfilePhase _(Prof::MediumTr); return Exp(-sigma_t * std::min(ray.tMax * ray.d.Length(), MaxFloat));&#125; 2、非均匀介质表示&emsp;&emsp;对于非均匀的散射介质，其密度随着空间位置的变化而变化（例如空气密度从海平面到高空逐渐稀薄），一般很难找到某种显式的数学解析式描述介质密度的空间分布情况。为此，一种表示非均匀介质的方法类似于离散采样，构建一个三维的均匀网格，在每个网格点上记录介质的密度值。则对于空间中的任意一点的密度值，可以通过其所在格子的八个顶点的密度值线性插值得到（三线性插值，当然也可以用B样条插值）。 &emsp;&emsp;此种表示方法在连续介质例如流体的物理模拟中使用非常多，思想比较简单，关键是非均匀介质的密度数据的获取，密度数据的获取一般亦是通过物理模拟得到，这里只负责利用这些数据渲染。对于非均匀介质，因其密度分布没有显式的数学公式表达，因此亦只能通过数值方法求解下面的光束透射率： T_r(p\\to p')=e^{-\\int_0^d\\sigma_t(p+t\\omega,\\omega)dt}&emsp;&emsp;求解的关键是$\\int_0^d \\sigma_t(p+t\\omega,\\omega)dt$即光学深度的积分，一种常用的方法就是利用梯度法求解该积分公式，还有一些则是基于随机采样的方法求解该积分（例如蒙特卡洛方法）。光束透射率的求解pbrt这里并没有提，暂时先跳过具体的细节。 四、BSSRDF&emsp;&emsp;类似于BSDF模型，BSSRDF（Bidirectional scattering-surface reflectance distribution function，双向散射表面反射分布函数）模型描述了在出射点$p_o$和出射方向上$\\omega_o$的出射辐射率占表面上另一个点$p_i$和入射方向$\\omega_i$的入射辐照度的比值，通常记为$S(p_o,\\omega_o,p_i,\\omega_i)$。由此，次表面散射的反射方程是对物体表面和半球方向的多重积分： L_o(p_o,\\omega_o)=\\int_A \\int_{H^2(n)} S(p_o,\\omega_o,p_i,\\omega_i)L_i(p_i,\\omega_i)|cos\\theta_i|d\\omega_i dA \\tag {4.1}&emsp;&emsp;其中$A$是介质的所有表面，这就是体积散射要求解的渲染方程。该方程虽然是物理准确的，但即便对于离线渲染，直接求解亦是不切实际。BSSRDF是一个高维函数，研究者们已经提出了一系列的简化模型来提高渲染效率。BSSRDF的计算与介质内部和外部的折射系数密切相关，因此通常需要内部和外部的相对折射指数作为其计算的参数。这里先创建一个BSSRDF接口类： 123456789101112131415class BSSRDF &#123; public: // BSSRDF Public Methods BSSRDF(const SurfaceInteraction &amp;po, Float eta) : po(po), eta(eta) &#123;&#125; virtual ~BSSRDF() &#123;&#125; // BSSRDF Interface virtual Spectrum S(const SurfaceInteraction &amp;pi, const Vector3f &amp;wi) = 0; virtual Spectrum Sample_S(const Scene &amp;scene, Float u1, const Point2f &amp;u2, MemoryArena &amp;arena, SurfaceInteraction *si, Float *pdf) const = 0; protected: // BSSRDF Protected Data const SurfaceInteraction &amp;po; Float eta;&#125;; &emsp;&emsp;其中，S就是BSSRDF函数接口。 1、可分离的BSSRDF&emsp;&emsp;相比于BSDF模型，BSSRDF模型复杂了不少，其中一个关键就是其对介质表面的依赖，很难找到一种通用的BSSRDF模型适用于任意形状的物体表面。这里来看一种简化的BSSRDF模型，它将$S(p_o,\\omega_o,p_i,\\omega_i)$拆分成如下的三种独立函数的乘积形式： S(p_o,\\omega_o,p_i,\\omega_i)\\approx (1-F_r(cos\\theta_o))S_p(p_o,p_i)S_{\\omega}(\\omega_i) \\tag {4.2}&emsp;&emsp;其中，$F_r$和$S_\\omega$是关于方向的函数，而$S_p$是关于空间的函数。 12345Spectrum S(const SurfaceInteraction &amp;pi, const Vector3f &amp;wi) &#123; ProfilePhase pp(Prof::BSSRDFEvaluation); Float Ft = FrDielectric(CosTheta(po.wo), 1, eta); return (1 - Ft) * Sp(pi) * Sw(wi);&#125; &emsp;&emsp;公式$(4.2)$中的$(1-F_r(cos\\theta))$不难理解，$F_r(cos\\theta)$给出了菲涅尔反射率，故剩余比例的为透射率，它给出了在$p_o$上向$\\omega_o$方向透射的光能辐射率的比例，确保符合能量守恒。而$S_\\omega(\\omega_i)$描述了在$p_i$上向$\\omega_i$入射的分布情况，其在菲涅尔方程的基础上再乘上一个缩放系数： S_\\omega(\\omega_i)=\\frac{1-F_r(cos\\theta_i)}{c\\pi} \\tag {4.3}&emsp;&emsp;上式中的缩放系数$c$是一个归一化因子，它使得： \\int_{H^2}S_\\omega(\\omega)cos\\theta d\\omega=1&emsp;&emsp;也就是说，$c$的计算公式为： \\begin{align} c=&\\int_{0}^{2\\pi}\\int_{0}^{\\frac{\\pi}{2}}\\frac{1-F_r(\\eta,cos\\theta)}{\\pi}sin\\theta cos\\theta d\\theta d\\phi\\\\ =&1-2\\int_o^{\\frac{\\pi}{2}}F_r(\\eta,cos\\theta)sin\\theta cos\\theta d\\theta \\end{align} \\tag {4.4}&emsp;&emsp;上面计算$c$的积分实际上只是关于菲涅尔方程的一阶矩（first moment），次表面散射中有多阶矩的$c$的应用，其区别在于上面公式中的$cos\\theta$的指数$i$，更通用的$i$阶菲涅尔矩的公式为： \\overline F_{r,i}(\\eta)=\\int_0^{\\frac{\\pi}{2}}F_r(\\eta,cos\\theta)sin\\theta cos^i\\theta d\\theta \\tag {4.5}&emsp;&emsp;但计算积分还是太麻烦了，因此实现的时候实际上用一个近似拟合上述积分公式的多项式去计算： 12345678910Float FresnelMoment1(Float eta) &#123; Float eta2 = eta * eta, eta3 = eta2 * eta, eta4 = eta3 * eta, eta5 = eta4 * eta; if (eta &lt; 1) return 0.45966f - 1.73965f * eta + 3.37668f * eta2 - 3.904945 * eta3 + 2.49277f * eta4 - 0.68441f * eta5; else return -4.61686f + 11.1136f * eta - 10.4646f * eta2 + 5.11455f * eta3 - 1.27198f * eta4 + 0.12746f * eta5;&#125; &emsp;&emsp;用公式$(4.4)$计算出缩放系数$c$，然后再代入公式$(4.3)$计算$S_\\omega$： 1234Spectrum Sw(const Vector3f &amp;w) const &#123; Float c = 1 - 2 * FresnelMoment1(1 / eta); return (1 - FrDielectric(CosTheta(w), 1, eta)) / (c * Pi);&#125; &emsp;&emsp;最后就是函数$S_p(p_i,p_o)$的计算。这里再次做了近似简化，注意到$p_i$和$p_o$相距越远，则从$p_i$入射进来的辐射率对$p_o$出射的辐射率贡献越小，因此假定物体形状是一个平面，将$S_p(p_i,p_o)$近似逼近成如下的一维公式： S_p(p_i,p_o)\\approx S_r(||p_o-p_i||) \\tag {4.6}&emsp;&emsp;$||p_o-p_i||$是该两点之间的直线距离。具体的$S_r(||p_o-p_i||)$公式取决于具体的BSSRDF函数，这里暂不展开。值得注意的是，将$S_p(p_i,p_o)$近似成上面的一维函数蕴含了一个假设，就是散射介质是相对比较均匀的。而且如果介质几何体越复杂（越不是一个平面），则上面的近似误差越大。 2、基于查找表的BSSRDF&emsp;&emsp;正如之前提到的傅里叶基下打表的BSDF模型，BSSRDF模型也可以通过真实实验测量的数据得到，并在渲染时查找这些数据并做相应的插值（线性插值或B样条插值）。在可分离的BSSRDF模型上，我们只需要存储前面提到的$S_r(||p_o-p_i||)$函数，其余两个函数均可以快速计算得到。 &emsp;&emsp;乍一看$S_r(||p_o-p_i||)$是一个一维函数，但其实不然。这里有隐性的关联，$S_r(||p_o-p_i||)$与散射材质息息相关，不同的散射介质应该有不同的$S_r$函数。因此，$S_r$还有额外的四个参数：折射指数$\\eta$、散射非对称系数$g$、反照率$\\rho$和衰减系数$\\sigma_t$。即$S_r(\\eta,g,\\rho,\\sigma_t,r)$，其中$r=||p_o-p_i||$，这是个五维函数，直接离散打表不太可能，因为这需要非常大的存储空间。因此需要适当地降维。 &emsp;&emsp;首先$\\sigma_t$和$r$可以合并成一个参数，$r_{optical}=\\sigma_t r$，称之为光学半径，这样所有的参数都是无量纲： S_r(\\eta,g,\\rho,\\sigma_t,r)=\\sigma_t^2S_r(\\eta,g,\\rho,1,r_{optical})&emsp;&emsp;转换成右边形式的还要乘上$\\sigma_t^2$补偿参数转变的损失。除此之外，令$\\eta$和$g$取值固定，最后$S_r$就是关于$\\rho$和$r$的二维函数，因此打表的BSSRDF只需存储反照率$\\rho$和光学半径$r$的离散值，分别对应下面的rhoSamples和radiusSamples： 1234567891011121314struct BSSRDFTable &#123; // BSSRDFTable Public Data const int nRhoSamples, nRadiusSamples; std::unique_ptr&lt;Float[]&gt; rhoSamples, radiusSamples; std::unique_ptr&lt;Float[]&gt; profile; std::unique_ptr&lt;Float[]&gt; rhoEff; std::unique_ptr&lt;Float[]&gt; profileCDF; // BSSRDFTable Public Methods BSSRDFTable(int nRhoSamples, int nRadiusSamples); inline Float EvalProfile(int rhoIndex, int radiusIndex) const &#123; return profile[rhoIndex * nRadiusSamples + radiusIndex]; &#125;&#125;; &emsp;&emsp;有了BSSRDFTable，则在计算$S_r$时就根据$r_{optical}$和$\\rho$做适当的B样条插值即可，因为返回的是一个光谱值，因此对光谱的每一个通道都进行插值和计算的过程： 12345678910111213141516171819202122232425262728293031323334Spectrum TabulatedBSSRDF::Sr(Float r) const &#123; Spectrum Sr(0.f); for (int ch = 0; ch &lt; Spectrum::nSamples; ++ch) &#123; // Convert $r$ into unitless optical radius $r_&#123;\\roman&#123;optical&#125;&#125;$ Float rOptical = r * sigma_t[ch]; // Compute spline weights to interpolate BSSRDF on channel _ch_ int rhoOffset, radiusOffset; Float rhoWeights[4], radiusWeights[4]; if (!CatmullRomWeights(table.nRhoSamples, table.rhoSamples.get(), rho[ch], &amp;rhoOffset, rhoWeights) || !CatmullRomWeights(table.nRadiusSamples, table.radiusSamples.get(), rOptical, &amp;radiusOffset, radiusWeights)) continue; // Set BSSRDF value _Sr[ch]_ using tensor spline interpolation Float sr = 0; for (int i = 0; i &lt; 4; ++i) &#123; for (int j = 0; j &lt; 4; ++j) &#123; Float weight = rhoWeights[i] * radiusWeights[j]; if (weight != 0) sr += weight * table.EvalProfile(rhoOffset + i, radiusOffset + j); &#125; &#125; // Cancel marginal PDF factor from tabulated BSSRDF profile if (rOptical != 0) sr /= 2 * Pi * rOptical; Sr[ch] = sr; &#125; // Transform BSSRDF value into world space units Sr *= sigma_t * sigma_t; return Sr.Clamp();&#125; &emsp;&emsp;这里提到了有效反照率（effective albedo），不是很明白为什么要这样做，先放着。 3、次表面散射材质&emsp;&emsp;pbrt实现了两类次表面散射材质，分别是SubsurfaceMaterial和KdSubsurfaceMaterial。SubsurfaceMaterial材质是一个有明确表面的材质，例如完美镜面透射或者gloosy镜面透射等。因此该材质除了BSSRDF模型，还有BSDF模型。关于BSDF模型，这里不再具体赘述。下面的scale、table、eta、sigma_a和sigma_s用于BSSRDF模型的计算： 123456789101112131415class SubsurfaceMaterial : public Material &#123; public: // SubsurfaceMaterial Public Methods ... private: // SubsurfaceMaterial Private Data const Float scale; std::shared_ptr&lt;Texture&lt;Spectrum&gt;&gt; Kr, Kt, sigma_a, sigma_s; std::shared_ptr&lt;Texture&lt;Float&gt;&gt; uRoughness, vRoughness; std::shared_ptr&lt;Texture&lt;Float&gt;&gt; bumpMap; const Float eta; const bool remapRoughness; BSSRDFTable table;&#125;; &emsp;&emsp;scale用于对吸收截面和散射系数的缩放（因此要求单位为$m^{-1}$）。在材质类的ComputeScatteringFunctions函数中初始化相关的bssrdf函数： 1234567891011121314void SubsurfaceMaterial::ComputeScatteringFunctions( SurfaceInteraction *si, MemoryArena &amp;arena, TransportMode mode, bool allowMultipleLobes) const &#123; // Perform bump mapping with _bumpMap_, if present if (bumpMap) Bump(bumpMap, si); // Initialize BSDF for SubsurfaceMaterial ... Spectrum sig_a = scale * sigma_a-&gt;Evaluate(*si).Clamp(); Spectrum sig_s = scale * sigma_s-&gt;Evaluate(*si).Clamp(); si-&gt;bssrdf = ARENA_ALLOC(arena, TabulatedBSSRDF)(*si, this, mode, eta, sig_a, sig_s, table);&#125; &emsp;&emsp;直接设置吸收截面和散射系数并不是非常直观，因此pbrt创建了KdSubsurfaceMaterial材质，方便直接根据漫反射反射率和自由平均路径$1/\\sigma_t$间接设置吸收截面和散射系数，使得这两个参数的设置变得直观起来。从反射率和自由平均路径计算吸收截面和散射系数SubsurfaceFromDiffuse将在后面实现路径积分时解释。 五、总结&emsp;&emsp;体积散射效果无论是理论上还是实现上相比于普通的表面散射效果复杂了不少，关于体积渲染的实现效果和效率至今仍是图形领域的重要话题。在阅读pbrt时有几个地方不是太理解，先放着等后面回过头再来看看。 Reference$[1]$ M, Jakob W, Humphreys G. Physically based rendering: From theory to implementation[M]. Morgan Kaufmann, 2016.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/tags/Ray-Tracer/"}]},{"title":"Physically Based Rendering：散射模型","slug":"Scattering","date":"2020-04-15T14:34:43.423Z","updated":"2020-04-19T13:53:33.475Z","comments":true,"path":"2020/04/15/Scattering/","link":"","permalink":"https://yangwc.com/2020/04/15/Scattering/","excerpt":"光线散射模型描述了光线碰撞到物体表面时以什么方式、什么方向进行反射、折射，在这里暂时不考虑次表面散射现象。光线的反射用BRDF函数描述，而透射则用BTDF函数描述，两者统一起来称为BSDF函数（双线散射分布函数）。","text":"光线散射模型描述了光线碰撞到物体表面时以什么方式、什么方向进行反射、折射，在这里暂时不考虑次表面散射现象。光线的反射用BRDF函数描述，而透射则用BTDF函数描述，两者统一起来称为BSDF函数（双线散射分布函数）。 &emsp;&emsp;以下的内容大部分整理自pbrt第三版的第八章——REFLECTION MODELS。 一、相关术语&emsp;&emsp;物体的表面反射模型主要来源以下几个： 测量数据模型：通过真实世界的实验测量得到，以数据库的形式使用； 现象学模型：用经验公式来拟合现实世界物体表面的定性特性； 模拟：有时候表面组成的底层信息可以获取到，例如我们知道颜料由带颜色的附着于某些介质的粒子组成，每种粒子的反射特性我们可以知道。这种情况下我们可以模拟微观层面的光散射来模拟产生反射数据。这个过程既可以在渲染过程完成也可以作为预处理； 波动光学模型：将光当作波来看待，求解麦克斯韦方程； 几何光学模型：如果知道了表面的底层散射和几何属性，反射模型可以直接从描述中构造出来。几何光学使得光和表面的交互可追踪，许多情况下都是非常好的选择。 &emsp;&emsp;目前大部分渲染算法都是基于几何光学，波动光学的复杂度相当高，其结果也未必优于几何光学。目前表面反射可以分成下图所示的四大类，从(a)到(d)分别是漫反射（diffuse）、粗糙镜面反射（glossy specular）、完美镜面反射（perfect specular）以及回归反射（retro-reflective）。大部分真实物体是这些反射类型的组合。这几种反射的区别主要在于它们的反射波瓣分布（下图中的网格描述的范围）。漫反射在半球方向上均匀地反射，这是一种理想的漫反射模型；粗糙镜面反射的反射波瓣在完美镜面反射基础上扩大了一些，用于描述金属等没那么光滑表面的反射；完美镜面反射即在非常光滑的表面上发生的反射；回归反射用于描述天鹅绒、月球等向入射方向反射的行为。 &emsp;&emsp;给定一类特定的反射，反射的分布函数还可以分成各向同性（isotropic）和各向异性（anisotropic）。大部分物体的表面反射都是各项同性的，这里所谓的各向同性就是指：对于物体表面上的一点和该点对应的法线，将表面绕着该法线进行旋转，光线反射的分布情况不变（即反射波瓣保持不变）。而如果反射波瓣发生了改变，则该物体表面就是反射各向异性的，织物、光盘和拉丝金属等都属于各向异性材质。 &emsp;&emsp;由于通常是具体到物体表面上的一个点对散射现象进行描述，因此我们的BRDF和BTDF的计算基本上都是在物体表面上的局部坐标系下进行，这个局部坐标系我们称之为着色坐标系。该坐标系如下图所示，以切线、副切线和法线向量分别作为$x$、$y$和$z$轴。 &emsp;&emsp;除了笛卡尔坐标系，有时还会用到球面坐标系。一个方向向量可以用球面坐标系$(\\theta,\\phi)$表示，如下图所示。笛卡尔坐标和球面坐标的相互转换比较简单，这里就不赘述了。在BRDF和BTDF函数中，用$\\omega_i$表示入射方向向量，而$\\omega_o$表示出射方向向量，默认均已经归一化为单位向量。而且$\\omega_i$和$\\omega_o$默认从着色点朝向入射、出射的方向。 二、散射函数接口&emsp;&emsp;首先定义一个接口类BxDF用于作为BRDF和BTDF的接口类： 12345678class BxDF &#123; public: // BxDF Interface ..... // BxDF Public Data const BxDFType type;&#125;; &emsp;&emsp;该接口类中的type用于子类指明当前是反射还是透射，同时又属于哪类反射模型： 123456789enum BxDFType &#123; BSDF_REFLECTION = 1 &lt;&lt; 0, BSDF_TRANSMISSION = 1 &lt;&lt; 1, BSDF_DIFFUSE = 1 &lt;&lt; 2, BSDF_GLOSSY = 1 &lt;&lt; 3, BSDF_SPECULAR = 1 &lt;&lt; 4, BSDF_ALL = BSDF_DIFFUSE | BSDF_GLOSSY | BSDF_SPECULAR | BSDF_REFLECTION | BSDF_TRANSMISSION,&#125;; &emsp;&emsp;下面的f函数是BSDF的核心，它要求输入入射方向和反射方向（默认着色点是着色坐标系下的原点），返回相应的BSDF函数值： 1virtual Spectrum f(const Vector3f &amp;wo, const Vector3f &amp;wi) const = 0; &emsp;&emsp;但有时我们想仅仅输入出射方向，然后根据出射方向计算入射方向并返回相应的BSDF函数值。这在完美镜面反射和粗糙镜面反射中非常有用，因为此时它们的反射波瓣很窄，只占整个半球方向很小一部分，我们不想盲目暴力地遍历所有的入射方向，而是直接根据向量的反射特性获取入射方向（对于完美镜面反射，除了此方向其他方向上的贡献均为$0$，这时的BRDF是一个狄拉克函数），省去了很大部分的计算。为此，亦声明了如下的接口： 123virtual Spectrum Sample_f(const Vector3f &amp;wo, Vector3f *wi, const Point2f &amp;sample, Float *pdf, BxDFType *sampledType = nullptr) const; &emsp;&emsp;函数的其他参数我们暂时先忽略。用球面坐标系$(\\theta,\\phi)$表示方向向量，BSDF函数本质上是关于入射方向和散射方向的4D函数，在某些特殊情况下，我们可以将其中的入射的两个维度砍掉。一种特殊情况就是半球-方向反射率（hemispherical-directional reflectance），此时它假定半球方向的入射光强度为一个常量，即入射辐射率是一个常量函数，因此可以将反射方程中的入射辐射率$L_i$提取到半球积分外面，剩下的部分就是半球-方向反射率（即下面的公式$(1)$）。对于给定的出射方向$\\omega_o$，可以直接计算入射辐照度在该方向上的反射比率如下： \\rho_{hd}(\\omega_o)=\\int_{H^2(n)} f_r(p,\\omega_o, \\omega_i)|cos\\theta_i|d\\omega_i \\tag {1}&emsp;&emsp;上述的公式也可以被解读为：来自某个给定方向上的入射光向整个半球方向反射的总反射比率（此时$\\omega_o$就变成了入射方向而非反射方向）。之所以能够这样解读，是因为基于物理的BRDF有个隐性的假设，即对称性，关于入射和反射的对称性。但需要注意BTDF并没有此类对称性。我们定义下面的rho函数接口用于计算半球-方向反射率，参数nSamples和samples用于辅助计算公式$(1)$的积分： 1virtual Spectrum rho(const Vector3f &amp;wo, int nSamples, const Point2f *samples) const; &emsp;&emsp;除了上面的半球-方向反射率之外，还有一种反射率叫做半球-半球反射率（hemispherical-hemispherical reflectance）。它计算的是：半球上所有的入射方向入射进来的辐射率都一样（即是一个常量）的情况下，表面接收到这些全部入射进来的辐照度，再向整个半球出射出去的辐照度比例（即表面出射的总能量/表面接收的总能量）： \\rho_{hh}=\\frac{1}{\\pi}\\int_{H^2(n)} \\int_{H^2(n)}f_r(p,\\omega_o,\\omega_i)|cos\\theta_o cos\\theta_i| d\\omega_o d\\omega_i \\tag {2}&emsp;&emsp;可以理解为在公式$(1)$的基础上，再削减出射方向的维度，因此上述的公式$(2)$没有输入参数。为此，定义下面的接口用以计算公式$(2)$： 12virtual Spectrum rho(int nSamples, const Point2f *samples1, const Point2f *samples2) const; &emsp;&emsp;在此基础上，我们再定义创建ScaledBxDF类，它本质上就是将BxDF函数的返回值再乘上一个光谱值（RGB或者基于采样的光谱向量），这很常见（例如反射方程中的入射辐射率$L_i$和$f_r$的相乘）： 1234567891011121314151617181920212223class ScaledBxDF : public BxDF &#123; public: // ScaledBxDF Public Methods ScaledBxDF(BxDF *bxdf, const Spectrum &amp;scale) : BxDF(BxDFType(bxdf-&gt;type)), bxdf(bxdf), scale(scale) &#123;&#125; Spectrum rho(const Vector3f &amp;w, int nSamples, const Point2f *samples) const &#123; return scale * bxdf-&gt;rho(w, nSamples, samples); &#125; Spectrum rho(int nSamples, const Point2f *samples1, const Point2f *samples2) const &#123; return scale * bxdf-&gt;rho(nSamples, samples1, samples2); &#125; Spectrum f(const Vector3f &amp;wo, const Vector3f &amp;wi) const; Spectrum Sample_f(const Vector3f &amp;wo, Vector3f *wi, const Point2f &amp;sample, Float *pdf, BxDFType *sampledType) const; Float Pdf(const Vector3f &amp;wo, const Vector3f &amp;wi) const; std::string ToString() const; private: BxDF *bxdf; Spectrum scale;&#125;; 三、镜面反射和透射&emsp;&emsp;我们首先来看完美光滑表面上发生的镜面反射和镜面透射。完美的镜面反射和透射非常特殊，给定一个入射方向$\\omega_i$，则该表面散射的方向有且仅有一个$\\omega_o$，而非遍布整个半球方向。给定入射方向$\\omega_i$，我们可以很容易地求出相应的完美镜面反射方向$\\omega_o$，而透射方向的计算则稍微要麻烦一点。Snell定律给出介质透射指数和透射角度的关系，透射指数描述了相对于真空情况下光线在介质中的速度降低了多少，用$\\eta$标记： \\eta_i sin\\theta_i = \\eta_t sin \\theta_t \\tag {3}&emsp;&emsp;实际情况下，折射系数通常与光的波长有关，随着光波的变化而变化。由此，入射光将在两种不同介质之间的边界上向多个方向发生透射，而非仅仅一个方向，这被称为色散现象（dispersion）。例如一束白光透过三棱镜将产生七彩光带，这七彩光带具有一定的范围。但在计算机图形学中，我们通常忽略这个现象，假定折射系数与波长无关，因为此种现象对于人类视觉来说不是很关键。 &emsp;&emsp;除了散射方向的计算，我们还要计算光能的反射比例和折射比例，这通过菲涅尔方程联系起来。 1、菲涅尔反射方程&emsp;&emsp;菲涅尔方程（Fresnel equations）描述了一束光照射到表面上时反射的比例。值得一提的是，在完美光滑的表面上，菲涅尔方程就是麦克斯韦方程的解。给定折射系数和入射方向与法线的夹角，菲涅尔方程给出了在物体表面上产生两种不同偏振状态的反射率，两种偏振状态分别是平行偏振光和垂直偏振光。但我们忽略光的偏振现象，假定光是无偏振的，因此菲涅尔反射率应该是两种偏振状态的反射率的平均值。 &emsp;&emsp;根据是否存在能够导电的自由电子，可以将物体的材质分成以下的三大类： 绝缘体（dielectric）：此类材质不导电，其介质的折射指数均为实数值，会透射一定量的入射光，此类材质有玻璃、矿物油、水和空气等； 导电体（conductor）：此类材质能够导电，因其具有自由移动的电子，此类材质不透明（一般透射进入内部的光被转换成了热能，厚度很薄的极端情况不考虑），仅发生反射。其介质的折射指数是复数值$\\overline \\eta=\\eta + ik$； 半导体（semiconductor）：例如硅或锗都属于此类，在这里我们不考虑。 &emsp;&emsp;绝缘体和导电体的反射率都有菲涅尔反射方程给出，但考虑到导电体的折射指数为复数的情况，有必要分类情况讨论。在此创建一个Fresnel接口类如下，Evaluate接口输入$cos\\theta_i$，返回菲涅尔反射率，反射率是$[0,1]$之间的一个浮点数值： 123456class Fresnel &#123; public: // Fresnel Interface virtual ~Fresnel(); virtual Spectrum Evaluate(Float cosI) const = 0;&#125;; &emsp;&emsp;首先来看入射介质与折射介质均为绝缘体的情况，此时折射指数均为实数值。设入射介质的折射指数为$\\eta_i$，折射介质的折射指数为$\\eta_t$，$\\theta_i$和$\\theta_t$分别是入射方向$\\omega_i$和折射方向$\\omega_t$与法线的夹角，则菲涅尔反射率计算如下： r_{||}=\\frac{\\eta_t cos\\theta_i-\\eta_icos\\theta_t}{\\eta_tcos\\theta_i+\\eta_icos\\theta_t} \\tag {4} r_{\\perp}=\\frac{\\eta_i cos\\theta_i-\\eta_tcos\\theta_t}{\\eta_icos\\theta_i+\\eta_tcos\\theta_t} \\tag {5}&emsp;&emsp;其中$r_{||}$是平行偏振光的反射率，$r_{\\perp}$是垂直偏振光的反射率。对于无偏振光，则菲涅尔反射率是两者的综合平均： F_r=\\frac12(r_{||}^2+r_{\\perp}^2) \\tag {6}&emsp;&emsp;因此，根据能量守恒定律，绝缘体的透射率为$1-F_r$。下图列出了一些常见的绝缘体的折射系数： &emsp;&emsp;根据以上讨论，可实现FrDielectric函数如下，输入参数分别为$\\eta_i$、$\\eta_t$和$cos\\theta_i$，返回绝缘体的反射率： 12345678910111213141516171819202122Float FrDielectric(Float cosThetaI, Float etaI, Float etaT) &#123; cosThetaI = Clamp(cosThetaI, -1, 1); // Potentially swap indices of refraction bool entering = cosThetaI &gt; 0.f; if (!entering) &#123; std::swap(etaI, etaT); cosThetaI = std::abs(cosThetaI); &#125; // Compute _cosThetaT_ using Snell's law Float sinThetaI = std::sqrt(std::max((Float)0, 1 - cosThetaI * cosThetaI)); Float sinThetaT = etaI / etaT * sinThetaI; // Handle total internal reflection if (sinThetaT &gt;= 1) return 1; Float cosThetaT = std::sqrt(std::max((Float)0, 1 - sinThetaT * sinThetaT)); Float Rparl = ((etaT * cosThetaI) - (etaI * cosThetaT)) / ((etaT * cosThetaI) + (etaI * cosThetaT)); Float Rperp = ((etaI * cosThetaI) - (etaT * cosThetaT)) / ((etaI * cosThetaI) + (etaT * cosThetaT)); return (Rparl * Rparl + Rperp * Rperp) / 2;&#125; &emsp;&emsp;在这里略提以下上面的代码，在计算之前我们首先根据$cos\\theta_i$的符号判断当前是处于介质内部还是外部，如果是在内部向外部透射，则应该交换一下折射系数。此外，我们还考虑了全反射的情况，根据Snell定律计算出来的$sin\\theta_t$是否大于等于$1$来判断，如果大于$1$，则返回$1.0$的反射率（即全部反射了）。由此，绝缘体的Fresnel类实现如下，直接调用FrDielectric函数： 12345678910111213class FresnelDielectric : public Fresnel &#123; public: // FresnelDielectric Public Methods Spectrum Evaluate(Float cosThetaI) const; FresnelDielectric(Float etaI, Float etaT) : etaI(etaI), etaT(etaT) &#123;&#125; private: Float etaI, etaT;&#125;;Spectrum FresnelDielectric::Evaluate(Float cosThetaI) const &#123; return FrDielectric(cosThetaI, etaI, etaT);&#125; &emsp;&emsp;紧接着来考虑导电体的介质反射率。我们仅考虑入射是绝缘体介质，透射是导电体的情况（从导电体到绝缘体的透射不考虑，因为光能被完全吸收了转化成热能，几乎很少见）。导电体的折射指数为复数值$\\overline \\eta=\\eta+ik$，虚数部分的$k$被称为吸收系数（absorption coefficient），导电体的折射系数和吸收系数均匀波长有关。绝缘体和导电体边界上的菲涅尔反射率计算公式如下： r_\\perp=\\frac{a^2+b^2-2a cos\\theta + cos^2\\theta}{a^2+b^2+2acos\\theta+cos^2\\theta} \\tag {7} r_{||}=r_{\\perp}\\frac{cos^2\\theta(a^2+b^2)-2acos\\theta sin^2\\theta + sin^4\\theta}{cos^2\\theta(a^2+b^2)+2acos\\theta sin^2\\theta+sin^4\\theta} \\tag {8}&emsp;&emsp;其中： a^2+b^2=\\sqrt{(\\eta^2-k^2-sin^2\\theta)^2+4\\eta^2k^2}&emsp;&emsp;而上式中的$\\eta+ik=\\overline \\eta_t/\\overline \\eta_i$。由此实现导电体的FrConductor，输入$cos\\theta_i$、$\\overline\\eta_i$、$\\overline \\eta_t = \\eta_t+ik$，返回导电体的菲涅尔反射率，输入折射指数是一个光谱值Spectrum，因为导体的折射指数与波长有关： 123456789101112131415161718192021222324Spectrum FrConductor(Float cosThetaI, const Spectrum &amp;etai, const Spectrum &amp;etat, const Spectrum &amp;k) &#123; cosThetaI = Clamp(cosThetaI, -1, 1); Spectrum eta = etat / etai; Spectrum etak = k / etai; Float cosThetaI2 = cosThetaI * cosThetaI; Float sinThetaI2 = 1. - cosThetaI2; Spectrum eta2 = eta * eta; Spectrum etak2 = etak * etak; Spectrum t0 = eta2 - etak2 - sinThetaI2; Spectrum a2plusb2 = Sqrt(t0 * t0 + 4 * eta2 * etak2); Spectrum t1 = a2plusb2 + cosThetaI2; Spectrum a = Sqrt(0.5f * (a2plusb2 + t0)); Spectrum t2 = (Float)2 * cosThetaI * a; Spectrum Rs = (t1 - t2) / (t1 + t2); Spectrum t3 = cosThetaI2 * a2plusb2 + sinThetaI2 * sinThetaI2; Spectrum t4 = t2 * sinThetaI2; Spectrum Rp = Rs * (t3 - t4) / (t3 + t4); return 0.5 * (Rp + Rs);&#125; &emsp;&emsp;因此，导电体的菲涅尔反射类FresnelConductor实现如下： 123456789101112131415class FresnelConductor : public Fresnel &#123; public: // FresnelConductor Public Methods Spectrum Evaluate(Float cosThetaI) const; FresnelConductor(const Spectrum &amp;etaI, const Spectrum &amp;etaT, const Spectrum &amp;k) : etaI(etaI), etaT(etaT), k(k) &#123;&#125; private: Spectrum etaI, etaT, k;&#125;;Spectrum FresnelConductor::Evaluate(Float cosThetaI) const &#123; return FrConductor(std::abs(cosThetaI), etaI, etaT, k);&#125; &emsp;&emsp;接下来就根据这些接口和函数实现完美的镜面反射和完美的镜面透射。 2、完美镜面反射&emsp;&emsp;对于完美的镜面反射，给定入射辐射率$L_i(\\omega_i)$，菲涅尔反射率已经给出了反射辐射率的比值，因此我们现在寻找一个完美镜面反射的BRDF函数（双向反射分布函数），使得： L_o(\\omega_o)=\\int f_r(\\omega_o, \\omega_i) L_i(\\omega_i)|cos\\theta_i|d\\omega_i =F_r(\\omega_r)L_i(\\omega_r)&emsp;&emsp;其中$\\omega_r$是$\\omega_o$关于表面法线$n$的反射向量。上式的意思就是，给定出射方向$\\omega_o$，反射方程计算的结果应当是$\\omega_o$的反射向量$\\omega_r$方向上入射进来的辐射率$L_i(\\omega_r)$再乘上菲涅尔反射率。这表明BRDF函数即$f_r$应当为一个狄拉克函数，仅在$\\omega_r$方向取值不为零。这里说的狄拉克函数就是采样理论的冲激函数$\\delta (x_0)$，$\\delta(x_0)$仅在$x=x_0$上取值不为零，而且具备取样特性$\\int f(x)\\delta(x-x_0)dx=f(x_0)$。经过一些简单的推导，可得完美镜面反射的BRDF函数如下： f_r(p,\\omega_o,\\omega_i)=F_r(\\omega_r)\\frac{\\delta(\\omega_i-\\omega_r)}{|cos\\theta_r|} \\tag {10}&emsp;&emsp;由此，可以实现镜面反射的BRDF函数如下： 12345678Spectrum SpecularReflection::Sample_f(const Vector3f &amp;wo, Vector3f *wi, const Point2f &amp;sample, Float *pdf, BxDFType *sampledType) const &#123; // Compute perfect specular reflection direction *wi = Vector3f(-wo.x, -wo.y, wo.z); *pdf = 1; return fresnel-&gt;Evaluate(CosTheta(*wi)) * R / AbsCosTheta(*wi);&#125; &emsp;&emsp;因为镜面反射的BRDF是一个狄拉克函数，因此仅使用Sample_f计算BRDF值，而不是使用f接口。SpecularReflection还接收一个光谱值R，这个是物体的反照率： 12345678910111213141516171819class SpecularReflection : public BxDF &#123; public: // SpecularReflection Public Methods SpecularReflection(const Spectrum &amp;R, Fresnel *fresnel) : BxDF(BxDFType(BSDF_REFLECTION | BSDF_SPECULAR)), R(R), fresnel(fresnel) &#123;&#125; Spectrum f(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; return Spectrum(0.f); &#125; Spectrum Sample_f(const Vector3f &amp;wo, Vector3f *wi, const Point2f &amp;sample, Float *pdf, BxDFType *sampledType) const; Float Pdf(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; return 0; &#125; private: // SpecularReflection Private Data const Spectrum R; const Fresnel *fresnel;&#125;; 3、完美镜面透射&emsp;&emsp;给定折射方向$\\omega_o$和入射方向$\\omega_i$，描述折射光的能量占据入射光的能量分布的函数就是BTDF函数（双向透射分布函数）。根据能量守恒，透射的能量占比应为菲涅尔反射之后剩余的能量占比，即$\\tau = 1-F_r(\\omega_i)$，透射的辐射功率为$d\\Phi_o=\\tau d\\Phi_i$，根据辐射率的定义，我们有： L_ocos\\theta_o dA d\\omega_o = \\tau(L_i cos\\theta_i dA d\\omega_i)&emsp;&emsp;将立体角$\\omega$转换成用球面坐标系$(\\theta,\\phi)$表示，上述公式可写成： L_o cos\\theta_o dA sin\\theta_o d\\theta_o d\\phi_o = \\tau(L_i cos\\theta_i dA sin\\theta_i d\\theta_i d\\phi_i) \\tag {11}&emsp;&emsp;然后对Snell定律即$\\eta_i sin\\theta_i = \\eta_o sin \\theta_o$得两边求关于$\\theta$的微分： \\eta_o cos\\theta_o d\\theta_o = \\eta_i cos\\theta_i d\\theta_i \\to \\frac{cos\\theta_o d\\theta_o}{cos\\theta_i d\\theta_i}=\\frac{\\eta_i}{\\eta_o} \\tag {12}&emsp;&emsp;联立$(11)$、$(12)$和Snell定律， 可得： L_o \\eta_i^2 d\\phi_o = \\tau L_i \\eta_o^2 d\\phi_i&emsp;&emsp;又因为$\\phi_i=\\phi_o+\\pi$，故$d\\phi_i=d\\phi_o$，可最终简化为： L_o=\\tau L_i \\frac{\\eta_o^2}{\\eta_i^2} \\tag {13}&emsp;&emsp;由此便给出了入射辐射率与出射辐射率之间的关系。正如完美镜面反射的BRDF一样，BTDF可以得到类似的狄拉克函数形式： f_t(\\omega_o,\\omega_i)=\\frac{\\eta_o^2}{\\eta_i^2}(1-F_r(\\omega_i))\\frac{\\delta(\\omega_i-T(\\omega_o,n))}{|cos\\theta_i|} \\tag {14}&emsp;&emsp;其中$T(\\omega_o,n)$表示$\\omega_o$对应的入射向量（经过完美透射）。因此实现完美镜面透射的BTDF函数如下，SpecularTransmission只考虑绝缘体的透射，因此需要借用绝缘体的菲涅尔函数FresnelDielectric，etaA和etaB分别保存了当前介质外部、内部的折射指数： 12345678910111213141516171819202122232425class SpecularTransmission : public BxDF &#123; public: // SpecularTransmission Public Methods SpecularTransmission(const Spectrum &amp;T, Float etaA, Float etaB, TransportMode mode) : BxDF(BxDFType(BSDF_TRANSMISSION | BSDF_SPECULAR)), T(T), etaA(etaA), etaB(etaB), fresnel(etaA, etaB), mode(mode) &#123;&#125; Spectrum f(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; return Spectrum(0.f); &#125; Spectrum Sample_f(const Vector3f &amp;wo, Vector3f *wi, const Point2f &amp;sample, Float *pdf, BxDFType *sampledType) const; Float Pdf(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; return 0; &#125; private: // SpecularTransmission Private Data const Spectrum T; const Float etaA, etaB; const FresnelDielectric fresnel; const TransportMode mode;&#125;; &emsp;&emsp;Sample_f的实现基本上对应的就是前面的公式$(14)$，但要注意是否发生了全反射以及判断是从里向外透射还是从外向里透射，下面中的Refract用于计算折射向量： 1234567891011121314151617Spectrum SpecularTransmission::Sample_f(const Vector3f &amp;wo, Vector3f *wi, const Point2f &amp;sample, Float *pdf, BxDFType *sampledType) const &#123; // Figure out which $\\eta$ is incident and which is transmitted bool entering = CosTheta(wo) &gt; 0; Float etaI = entering ? etaA : etaB; Float etaT = entering ? etaB : etaA; // Compute ray direction for specular transmission if (!Refract(wo, Faceforward(Normal3f(0, 0, 1), wo), etaI / etaT, wi)) return 0; *pdf = 1; Spectrum ft = T * (Spectrum(1.) - fresnel.Evaluate(CosTheta(*wi))); // Account for non-symmetry with transmission to different medium if (mode == TransportMode::Radiance) ft *= (etaI * etaI) / (etaT * etaT); return ft / AbsCosTheta(*wi);&#125; 4、综合反射与透射&emsp;&emsp;上面讨论的是仅发生完美镜面反射或仅发生完美镜面透射的情况，但现实中更多的是两者的综合作用，发生透射和发生反射的比例通过菲涅尔方程联系起来，$F_r(\\omega_i)$是反射率，则$1-F_r(\\omega_i)$是透射率。因此我们创建一个两者综合作用的BxDF类如下： 123456789101112131415161718192021222324class FresnelSpecular : public BxDF &#123; public: // FresnelSpecular Public Methods FresnelSpecular(const Spectrum &amp;R, const Spectrum &amp;T, Float etaA, Float etaB, TransportMode mode) : BxDF(BxDFType(BSDF_REFLECTION | BSDF_TRANSMISSION | BSDF_SPECULAR)), R(R), T(T), etaA(etaA), etaB(etaB), mode(mode) &#123;&#125; Spectrum f(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; return Spectrum(0.f); &#125; Spectrum Sample_f(const Vector3f &amp;wo, Vector3f *wi, const Point2f &amp;u, Float *pdf, BxDFType *sampledType) const; Float Pdf(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; return 0; &#125; private: // FresnelSpecular Private Data const Spectrum R, T; const Float etaA, etaB; const TransportMode mode;&#125;; &emsp;&emsp;实现的关键就是Sample_f函数，对于菲涅尔方程给定的反射率和透射率，我们以概率的形式进行采样，具体这里暂不展开，因为涉及到了后面的蒙特卡洛的渲染积分方法。 四、Lambertian漫反射&emsp;&emsp;Lambertian漫反射是一种理想的漫反射模型，在该光照模型下，入射光能向整个半球方向均匀地反射。尽管该模型现实生活中并不存在，但视觉上足以逼近真实了。Lambertian漫反射的推导非常简单，给定一束光，向整个半球反射的反射率为前面的半球-方向反射率（即前面的公式$(1)$），即： \\rho_{hd}(\\omega_o)=\\int_{H^2(n)} f_r(p,\\omega_o, \\omega_i)|cos\\theta_i|d\\omega_i&emsp;&emsp;Lambertian漫反射向这个半球方向均匀地反射，则其BRDF函数是跟$\\omega_o$、$\\omega_i$无关的常量函数，将$f_r$提出积分外部，则剩余的积分可直接求解： \\int_{H^2(n)} cos\\theta_i d\\omega_i = \\int_{H^2(n)} cos\\theta_i sin\\theta_i d\\theta_i d\\phi = \\pi&emsp;&emsp;理想情况下，不考虑能量损耗，反射到整个半球方向的总反射率$\\rho_{hd}=1$，因此有$1=\\pi \\times f_r$，得$f_r=\\frac{1}{\\pi}$，Lambertian的BRDF就是这么简单，但通常我们还要考虑物体表面的反照率（可以理解为表面颜色），故Lambertian的BRDF函数为： f_r(p)=\\frac{R}{\\pi} \\tag {15}&emsp;&emsp;其中$R$是物体的光谱反照率。因此实现LambertianReflection如下，f函数就是上面的公式$(15)$： 1234567891011121314151617class LambertianReflection : public BxDF &#123; public: // LambertianReflection Public Methods LambertianReflection(const Spectrum &amp;R) : BxDF(BxDFType(BSDF_REFLECTION | BSDF_DIFFUSE)), R(R) &#123;&#125; Spectrum f(const Vector3f &amp;wo, const Vector3f &amp;wi) const; Spectrum rho(const Vector3f &amp;, int, const Point2f *) const &#123; return R; &#125; Spectrum rho(int, const Point2f *, const Point2f *) const &#123; return R; &#125; private: // LambertianReflection Private Data const Spectrum R;&#125;;Spectrum LambertianReflection::f(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; return R * InvPi;&#125; &emsp;&emsp;值得一提的是rho接口，他直接返回了物体的反照率。因为向整个半球方向的总反射率为$1$，因此返回$1\\times R$。这是精准的解析解，没有必要用数值方法求解。 &emsp;&emsp;除了Lambertian反射，还有Lambertian透射，两者区别仅在于散射的方向。Lambertian透射向表面的下半球的方向进行均匀的透射，BTDF函数与前面的BRDF函数一样。pbrt中实现了此透射，非常简单，这里不再赘述。 五、基于微平面的散射模型&emsp;&emsp;上面讨论的几种BSDF模型都是基于理想状态的物理模型，真实物理世界很少或者可以说没有如此完美的物体表面，大部分物体的表面几乎或多或少地存在着不同程度的粗糙度。为此描述此种材质表面，基于几何光学的渲染方法提出了一种微平面（microfacet）模型，该模型基于这样的设定：任何一个物体的表面，在微观层面上都是由很多个不同朝向的光滑镜面组成，这些光滑的微平面对光的散射属性共同构成了宏观表面上的光线散射性质。 &emsp;&emsp;微平面的法线朝向越与宏观表面的法线一致，则表面越光滑；反之微平面的朝向越混乱，则表面越粗糙。由此，采用微平面模型对物体表面建模涉及到两个重要的点：微平面的分布描述函数、微平面的BSDF模型。这两点共同构成了宏观物体表面的BSDF函数。微平面的排列分布通常由宏观层面的统计概率描述。微平面的光学散射则稍微要麻烦一些，这是因为在微平面的局部模型下，涉及到下图所示的三类光学效应。首先是Masking即遮蔽，反射的光线被另外的微平面挡住，从而减弱了反射光线；其次是Shadowing即阴影，一个微平面被其他微平面遮挡了，处于阴影之中；最后是Interreflection即互反射，光线在微平面之间互相反射多次，最终才抵达人眼。 &emsp;&emsp;基于微平面的BSDF模型应当尽量考虑上述三种效应，同时也尽量使得数学表达式简洁，计算量尽可能地少。 1、Oren-Nayar漫反射&emsp;&emsp;Oren和Nayar观察到真实世界的物体表面上并不存在完美的Lambertian漫反射，他们提出了一种基于球形高斯分布的V形微平面模型，用以描述粗糙的表面。他们给出的近似拟合BRDF函数如下： f_r(\\omega_i,\\omega_o)=\\frac R\\pi(A+B\\ max(0,cos(\\phi_i-\\phi_o))sin\\ \\alpha\\ tan\\ \\beta) \\tag {16}&emsp;&emsp;其中： A=1-\\frac{\\sigma^2}{2(\\sigma^2+0.33)}\\\\ B=\\frac{0.45\\sigma^2}{\\sigma^2+0.09}\\\\ \\alpha = max(\\theta_i, \\theta_o)\\\\ \\beta = min(\\theta_i,\\theta_o)&emsp;&emsp;$\\sigma$是微平面法线朝向的标准差，弧度制。为此，实现Oren和Nayar的漫反射BRDF函数如下： 1234567891011121314151617class OrenNayar : public BxDF &#123; public: // OrenNayar Public Methods Spectrum f(const Vector3f &amp;wo, const Vector3f &amp;wi) const; OrenNayar(const Spectrum &amp;R, Float sigma) : BxDF(BxDFType(BSDF_REFLECTION | BSDF_DIFFUSE)), R(R) &#123; sigma = Radians(sigma); Float sigma2 = sigma * sigma; A = 1.f - (sigma2 / (2.f * (sigma2 + 0.33f))); B = 0.45f * sigma2 / (sigma2 + 0.09f); &#125; private: // OrenNayar Private Data const Spectrum R; Float A, B;&#125;; &emsp;&emsp;根据三角恒等式，公式$(16)$中的$cos(\\phi_i-\\phi_o)=cos\\phi_i cos\\phi_o+sin\\phi_i sin\\phi_o$，省去了一些角度的计算。而$\\alpha$和$\\beta$中角度的比较可以通过它们的$cos$值比较： 1234567891011121314151617181920212223Spectrum OrenNayar::f(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; Float sinThetaI = SinTheta(wi); Float sinThetaO = SinTheta(wo); // Compute cosine term of Oren-Nayar model Float maxCos = 0; if (sinThetaI &gt; 1e-4 &amp;&amp; sinThetaO &gt; 1e-4) &#123; Float sinPhiI = SinPhi(wi), cosPhiI = CosPhi(wi); Float sinPhiO = SinPhi(wo), cosPhiO = CosPhi(wo); Float dCos = cosPhiI * cosPhiO + sinPhiI * sinPhiO; maxCos = std::max((Float)0, dCos); &#125; // Compute sine and tangent terms of Oren-Nayar model Float sinAlpha, tanBeta; if (AbsCosTheta(wi) &gt; AbsCosTheta(wo)) &#123; sinAlpha = sinThetaO; tanBeta = sinThetaI / AbsCosTheta(wi); &#125; else &#123; sinAlpha = sinThetaI; tanBeta = sinThetaO / AbsCosTheta(wo); &#125; return R * InvPi * (A + B * maxCos * sinAlpha * tanBeta);&#125; &emsp;&emsp;该模型通过$\\sigma$控制表面的粗糙程度。 2、法线分布函数&emsp;&emsp;在对glossy specular等表面基于微平面的BSDF进行深入了解之前，我们先来看看描述微平面分布的相关函数。微平面的分布情况主要通过微平面的法线朝向来描述，因此微平面分布函数又被称为法线分布函数，记为$D(\\omega_h)$，其中输入的参数是微平面的法线向量$\\omega_h$（依旧在着色坐标系下），该函数返回具有$\\omega_h$朝向法线的微平面占比。法线分布函数的选取并不是任意的，需要符合物理逻辑。对于一个宏观表面上的微分面$dA$，在该微分面上的所有微平面投影到$dA$上的面积总和应该恰好等于$dA$，从数学上来讲，$D(\\omega_h)$应满足如下要求： \\int_{H^2(n)}D(\\omega_h)cos\\theta_h d\\omega_h = 1&emsp;&emsp;首先创建一个微平面函数的接口类如下，D就是微平面分布的接口函数： 12345678910class MicrofacetDistribution &#123; public: // MicrofacetDistribution Public Methods virtual ~MicrofacetDistribution(); virtual Float D(const Vector3f &amp;wh) const = 0; ... protected: ...&#125;; &emsp;&emsp;一个被广泛使用的微平面分布模型是由Beckmann和Spizzichino等人提出的微平面斜率的高斯分布函数，传统的Beckmann–Spizzichino法线分布函数定义为： D(\\omega_h)=\\frac{e^{-tan^2\\theta_h/\\alpha^2}}{\\pi \\alpha^2 cos^4\\theta_h}&emsp;&emsp;其中$\\sigma$是微平面的RMS斜率，而$\\alpha=\\sqrt2 \\sigma$。这是一个各向同性的法线分布函数。可将其扩展到各向异性，使得微平面的法线分布亦随着$\\omega_h$的方位角（也就是$\\phi_h$）变化而变化。令$\\alpha_x$为法线朝向垂直于$x$轴的微平面占比，$\\alpha_y$为法线朝向垂直于$y$轴的微平面占比，然后任何介于两者之间通过椭圆插值得到，故可以得到如下的各向异性微平面分布函数： D(\\omega_h)=\\frac{e^{-tan^2\\theta_h(cos^2\\phi_h/\\alpha_x^2+sin^2\\phi_h/\\alpha_y^2)}}{\\pi \\alpha_x \\alpha_y cos^4\\theta_h}&emsp;&emsp;当$\\alpha_x=\\alpha_y$时，上式就变成了各向同性函数。下面的函数实现了上述的公式，这里要特别注意$tan^2\\theta_h$趋于无穷的情况： 12345678Float BeckmannDistribution::D(const Vector3f &amp;wh) const &#123; Float tan2Theta = Tan2Theta(wh); if (std::isinf(tan2Theta)) return 0.; Float cos4Theta = Cos2Theta(wh) * Cos2Theta(wh); return std::exp(-tan2Theta * (Cos2Phi(wh) / (alphax * alphax) + Sin2Phi(wh) / (alphay * alphay))) / (Pi * alphax * alphay * cos4Theta);&#125; &emsp;&emsp;除了上面的法线分布函数，还有一个非常有用的微平面分布函数就是Trowbridge–Reitz法线分布函数。与Beckmann–Spizzichino函数相比，它两边趋于$0$的速度更加平缓，如下图所示： &emsp;&emsp;Trowbridge–Reitz法线分布函数定义为： D(\\omega_h)=\\frac{1}{\\pi \\alpha_x \\alpha_y cos^4 \\theta_h(1+tan^2\\theta_h(cos^2\\phi_h/\\alpha_x^2+sin^2\\phi_h/\\alpha^2_y))^2}123456789Float TrowbridgeReitzDistribution::D(const Vector3f &amp;wh) const &#123; Float tan2Theta = Tan2Theta(wh); if (std::isinf(tan2Theta)) return 0.; const Float cos4Theta = Cos2Theta(wh) * Cos2Theta(wh); Float e = (Cos2Phi(wh) / (alphax * alphax) + Sin2Phi(wh) / (alphay * alphay)) * tan2Theta; return 1 / (Pi * alphax * alphay * cos4Theta * (1 + e) * (1 + e));&#125; &emsp;&emsp;我们用$\\alpha_x$和$\\alpha_y$控制微平面的分布情况，这两个参数并不是很直观。为此尝试在粗糙度和这两个参数之间构建联系，下面RoughnessToAlpha就实现了该映射，用户只需输入$[0,1]$的粗糙度，$0$表示绝对光滑，$1$表示极度粗糙，非常直观方便。 123456inline Float TrowbridgeReitzDistribution::RoughnessToAlpha(Float roughness) &#123; roughness = std::max(roughness, (Float)1e-3); Float x = std::log(roughness); return 1.62142f + 0.819955f * x + 0.1734f * x * x + 0.0171201f * x * x * x + 0.000640711f * x * x * x * x;&#125; 3、几何遮蔽函数&emsp;&emsp;前面用法线分布函数对给定粗糙程度的表面的微平面分布进行了描述，对于微平面的Shadowing效应和Masking效应，我们将采用几何遮蔽函数对此进行建模。毫无疑问，几何遮蔽函数应该是跟观察方向$\\omega$相关的，给定观察方向，则因为Shadowing和Masking，只有一部分微平面能够被观察到。Smith’s masking-shadowing几何遮蔽函数$G_1(\\omega, \\omega_h)$给出了从$\\omega$方向能够观察到的法线为$\\omega_h$的微平面数量比例（因此，$0\\leq G_1(\\omega,\\omega_h)\\leq 1$）。通常情况下，几何遮蔽函数与微平面的法线$\\omega_h$无关，因此写成$G_1(\\omega)$的形式。 &emsp;&emsp;给定观察视角$\\omega$和一个微分面$dA$，则从视角$\\omega$观察到的$dA$的面积是投影面积$dA cos\\theta$，$\\theta$是$\\omega$与$dA$的法线的夹角。由此给出了下面的关于$G_1$的数学约束： cos \\theta=\\int_{H^2(n)} G_1(\\omega,\\omega_h) max(0,\\omega\\cdot \\omega_h) D(\\omega_h) d\\omega_h&emsp;&emsp;在观察方向$\\omega$上，有一些正向朝向的微平面被一些背向朝向的微平面遮挡了。记$A^+(\\omega)$是正向朝向$\\omega$的微平面的面积和，$A^-(\\omega)$是背向朝向$\\omega$的微平面面积和，则应该有$cos\\theta = A^+(\\omega)-A^-(\\omega)$，即在正向朝向的微平面中减去被遮挡的部分，则剩余部分应该就是从$\\omega$能够观察到的微平面（注意，这里采用高度场表示微平面，因此是可以这样直接减去）。那么根据定义，masking-shadowing函数就应该为可见的正向朝向面积和比上总的正向朝向面积和： G_1(\\omega)=\\frac{A^+(\\omega)-A^-(\\omega)}{A^+(\\omega)}&emsp;&emsp;但masking-shadowing函数的实现通常定义了如下的辅助函数： \\Lambda(\\omega)=\\frac{A^-(\\omega)}{A^+(\\omega)-A^-(\\omega)}=\\frac{A^-(\\omega)}{cos\\theta}&emsp;&emsp;在MicrofacetDistribution中我们定义如下的接口Lambda以计算$\\Lambda$这个辅助函数： 1virtual Float Lambda(const Vector3f &amp;w) const = 0; &emsp;&emsp;注意观察$\\Lambda$和$G_1$的差别，我们可以根据$\\Lambda$得到$G_1$： G_1(\\omega)=\\frac{1}{1+\\Lambda(\\omega)}123Float G1(const Vector3f &amp;w) const &#123; return 1 / (1 + Lambda(w));&#125; &emsp;&emsp;接下来的问题就是如何根据前面的法线分布函数$D(\\omega_h)$找到合适的符合数学约束的$\\Lambda$函数。但实际上满足前面数学约束的$\\Lambda$并不唯一，为此人们假定当前微平面的高度和周围微平面的高度不相关，这种假定虽然不符合现实，但也具备一定的准确度。 &emsp;&emsp;基于上述的假设，Beckmann–Spizzichino法线分布函数的各向同性$\\Lambda(\\omega)$函数为： \\Lambda(\\omega)=\\frac12(erf(a)-1+\\frac{e^{-a^2}}{a\\sqrt\\pi})&emsp;&emsp;其中$a=1/(\\alpha tan \\theta)$，$erf(x)=2/\\sqrt\\pi \\int_0^x e^{-x’^2}dx’$是误差函数。误差函数$erf$和$exp$函数计算量有点大，pbrt用一个近似的多项式拟合上面$\\Lambda(\\omega)$函数，虽然带来了一定的误差，但大大提升了计算效率： 12345678910Float BeckmannDistribution::Lambda(const Vector3f &amp;w) const &#123; Float absTanTheta = std::abs(TanTheta(w)); if (std::isinf(absTanTheta)) return 0.; // Compute _alpha_ for direction _w_ Float alpha = std::sqrt(Cos2Phi(w) * alphax * alphax + Sin2Phi(w) * alphay * alphay); Float a = 1 / (alpha * absTanTheta); if (a &gt;= 1.6f) return 0; return (1 - 1.259f * a + 0.396f * a * a) / (3.535f * a + 2.181f * a * a);&#125; &emsp;&emsp;这里同样用$\\alpha_x$和$\\alpha_y$使得$\\Lambda(\\omega)$为各向异性的形式。 &emsp;&emsp;而Trowbridge–Reitz法线分布函数的$\\Lambda(\\omega)$为： \\Lambda(\\omega)=\\frac{-1+\\sqrt{1+\\alpha^2 tan^2\\theta}}{2}123456789Float TrowbridgeReitzDistribution::Lambda(const Vector3f &amp;w) const &#123; Float absTanTheta = std::abs(TanTheta(w)); if (std::isinf(absTanTheta)) return 0.; // Compute _alpha_ for direction _w_ Float alpha = std::sqrt(Cos2Phi(w) * alphax * alphax + Sin2Phi(w) * alphay * alphay); Float alpha2Tan2Theta = (alpha * absTanTheta) * (alpha * absTanTheta); return (-1 + std::sqrt(1.f + alpha2Tan2Theta)) / 2;&#125; &emsp;&emsp;上面我们讨论的几何遮蔽函数输入一个参数，即观察方向$\\omega$。但要顾及的现象除了Masking，还有Shadowing。Masking是在出射方向$\\omega_o$出现了遮挡，而Shadowing则是在入射方向$\\omega_i$出现了遮挡。为此，我们定义$G(\\omega_o,\\omega_i)$函数综合考虑Masking和Shadowing的现象，返回经过Masking和Shadowing之后的可见微平面数量比例。 &emsp;&emsp;一种非常简单的$G(\\omega_o,\\omega_i)$直接利用前面的$G_1(\\omega)$函数，假定$G_1(\\omega_o)$和$G_1(\\omega_i)$互相独立、互不相关，因此有如下的$G(\\omega_o,\\omega_i)$： G(\\omega_o,\\omega_i)=G_1(\\omega_o)G_1(\\omega_i)&emsp;&emsp;上面的公式不难理解。但在实际情况中，$G_1(\\omega_o)$和$G_1(\\omega_i)$并不是互相独立的，而是具备一定的相关性。例如当$\\omega_i=\\omega_o$时，应该有$G(\\omega_o,\\omega_i)=G_1(\\omega_o)=G_1(\\omega_i)$，因为$G_1\\leq 1$，因此$G_1(\\omega_o)G_1(\\omega_i)$会造成计算结果偏小。一般情况，$\\omega_i$和$\\omega_o$越靠近，则相关性越大。 &emsp;&emsp;因此，一种更加准确的$G(\\omega_o,\\omega_i)$计算公式为： G(\\omega_o,\\omega_i)=\\frac{1}{1+\\Lambda(\\omega_o)+\\Lambda(\\omega_i)}123virtual Float G(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; return 1 / (1 + Lambda(wo) + Lambda(wi));&#125; 4、Torrance-Sparrow镜面散射&emsp;&emsp;Torrance和Sparrow提出了一种早期的、经典的BRDF模型，这类模型对金属表面进行建模。物体的表面是由大量的完美光滑的微平面组成，因此给定入射方向$\\omega_i$和反射方向$\\omega_o$，只有法线朝向为$\\omega_i+\\omega_o$的微平面会参与反射。我们称$\\omega_i+\\omega_o$为半角向量，记为$\\omega_h$。接下来就看看Torrance-Sparrow模型是如何推导的。 &emsp;&emsp;首先，根据辐射率的定义，入射到法线朝向为$\\omega_h$的微平面上的总微分辐射功率为： d\\Phi_h =L_i(\\omega_i)d\\omega_i dA^{\\perp}(\\omega_h)=L_i(\\omega_i)d\\omega_i cos\\theta_h dA(\\omega_h)&emsp;&emsp;$\\theta_h$是向量$\\omega_i$与$\\omega_h$的夹角。$dA(\\omega_h)$是法线朝向为$\\omega_h$的微平面总面积，又有$dA(\\omega_h)=D(\\omega_h)d\\omega_h dA$，因此： d\\Phi_h=L_i(\\omega_i)d\\omega_i cos\\theta_hD(\\omega_h)d\\omega_h dA \\tag {17}&emsp;&emsp;然后，根据菲涅尔反射率，可以得到出射的辐射功率： d\\Phi_o=F_r(\\omega_o)d\\Phi_h \\tag {18}&emsp;&emsp;再根据辐射率的定理，出射辐射率则为： L(\\omega_o)=\\frac{d\\Phi_o}{d\\omega_o cos\\theta_o dA}&emsp;&emsp;将公式$(18)$和$(17)$带入上面的方程，就有： L(\\omega_o)=\\frac{F_r(\\omega_o)L_i(\\omega_i)d\\omega_i D(\\omega_h) d\\omega_h dA cos\\theta_h}{d\\omega_o dA cos\\theta_o}&emsp;&emsp;$\\omega_h$和$\\omega_o$又存在着函数关系，有$d\\omega_h = \\frac{d\\omega_o}{4cos\\theta_h}$（pbrt并没有在此展开，实际上应该就是通过$\\omega_h=\\omega_i+\\omega_o$推导出来的）。带入上式有： L(\\omega_o)=\\frac{F_r(\\omega_o)L_i(\\omega_i)D(\\omega_h)d\\omega_i}{4cos\\theta_o}&emsp;&emsp;最后，根据BRDF的定义（即出射辐射率比上入射辐照度），加上几何遮蔽函数$G(\\omega_o,\\omega_i)$，我们可推得Torrance-Sparrow的BRDF公式为： f_r(\\omega_o,\\omega_i)=\\frac{D(\\omega_h)G(\\omega_o,\\omega_i)F_r(\\omega_o)}{4cos\\theta_o cos\\theta_i} \\tag {19}&emsp;&emsp;Torrance-Sparrow模型优势就是它的通用性，它并不依赖于某种特定的法线分布函数以及菲涅尔方程。因此对导电体和绝缘体均适用。下面定义了MicrofacetReflection作为此类BRDF函数的实现： 1234567891011121314151617181920class MicrofacetReflection : public BxDF &#123; public: // MicrofacetReflection Public Methods MicrofacetReflection(const Spectrum &amp;R, MicrofacetDistribution *distribution, Fresnel *fresnel) : BxDF(BxDFType(BSDF_REFLECTION | BSDF_GLOSSY)), R(R), distribution(distribution), fresnel(fresnel) &#123;&#125; Spectrum f(const Vector3f &amp;wo, const Vector3f &amp;wi) const; Spectrum Sample_f(const Vector3f &amp;wo, Vector3f *wi, const Point2f &amp;u, Float *pdf, BxDFType *sampledType) const; Float Pdf(const Vector3f &amp;wo, const Vector3f &amp;wi) const; private: // MicrofacetReflection Private Data const Spectrum R; const MicrofacetDistribution *distribution; const Fresnel *fresnel;&#125;; &emsp;&emsp;其中distribution和fresnel用于微平面相关分布函数的计算。下面的f实现了上面的公式$(19)$，这里要特别注意分母的$cos\\theta_i$和$cos\\theta_o$是否为零： 12345678910111213Spectrum MicrofacetReflection::f(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; Float cosThetaO = AbsCosTheta(wo), cosThetaI = AbsCosTheta(wi); Vector3f wh = wi + wo; // Handle degenerate cases for microfacet reflection if (cosThetaI == 0 || cosThetaO == 0) return Spectrum(0.); if (wh.x == 0 &amp;&amp; wh.y == 0 &amp;&amp; wh.z == 0) return Spectrum(0.); wh = Normalize(wh); // For the Fresnel call, make sure that wh is in the same hemisphere // as the surface normal, so that TIR is handled correctly. Spectrum F = fresnel-&gt;Evaluate(Dot(wi, Faceforward(wh, Vector3f(0,0,1)))); return R * distribution-&gt;D(wh) * distribution-&gt;G(wo, wi) * F / (4 * cosThetaI * cosThetaO);&#125; &emsp;&emsp;上面我们推导的是反射，即BRDF公式。扩展到BTDF，关键在于$d\\omega_o$和$d\\omega_h$之间的关系。在这里，我们考虑的是微平面上的镜面透射（而非前面的镜面反射）。给定入射向量$\\omega_i$和折射向量$\\omega_o$，我们同样可以得到一个半角向量$\\omega_h$，只不过这个半角向量是通过Snell定律获取（对于给定的$\\omega_i$和$\\omega_o$，只存在法向为$\\omega_h$的微平面发生了透射）。$\\omega_h$的计算公式如下： \\omega_h=\\omega_o + \\eta \\omega_i\\\\ \\eta=\\eta_i/\\eta_o&emsp;&emsp;因此，$d\\omega_h$与$d\\omega_o$的关系为： d\\omega_h=\\frac{\\eta_o^2|\\omega_o\\cdot \\omega_h|d\\omega_o}{(\\eta_i(\\omega_i\\cdot \\omega_h)+\\eta_o(\\omega_o\\cdot \\omega_h))^2}&emsp;&emsp;用这个替换调用上面BRDF推导过程中的$d\\omega_h$可得BTDF公式如下： f_t(\\omega_o,\\omega_i)=\\frac{\\eta^2 D(\\omega_h)G(\\omega_o,\\omega_i)(1-F_r(\\omega_o))}{((\\omega_o\\cdot \\omega_h)+\\eta(\\omega_i\\cdot \\omega_h))^2} \\frac{|\\omega_i\\cdot \\omega_h||\\omega_o\\cdot\\omega_h|}{cos\\theta_ocos\\theta_i} \\tag {20}&emsp;&emsp;因此，创建一个MicrofacetTransmission，实现公式$(20)$如下： 1234567891011121314151617181920212223Spectrum MicrofacetTransmission::f(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; if (SameHemisphere(wo, wi)) return 0; // transmission only Float cosThetaO = CosTheta(wo); Float cosThetaI = CosTheta(wi); if (cosThetaI == 0 || cosThetaO == 0) return Spectrum(0); // Compute $\\wh$ from $\\wo$ and $\\wi$ for microfacet transmission Float eta = CosTheta(wo) &gt; 0 ? (etaB / etaA) : (etaA / etaB); Vector3f wh = Normalize(wo + wi * eta); if (wh.z &lt; 0) wh = -wh; Spectrum F = fresnel.Evaluate(Dot(wo, wh)); Float sqrtDenom = Dot(wo, wh) + eta * Dot(wi, wh); Float factor = (mode == TransportMode::Radiance) ? (1 / eta) : 1; return (Spectrum(1.f) - F) * T * std::abs(distribution-&gt;D(wh) * distribution-&gt;G(wo, wi) * eta * eta * AbsDot(wi, wh) * AbsDot(wo, wh) * factor * factor / (cosThetaI * cosThetaO * sqrtDenom * sqrtDenom));&#125; &emsp;&emsp;通过微平面的完美镜面反射和透射，我们实现了glossy镜面反射和透射。 六、Ashikhmin-Shirley反射&emsp;&emsp;接下来要讨论的BRDF是由Ashikhmin和Shirley提出的一种综合了漫反射和镜面反射（glossy镜面反射）的反射模型。Ashikhmin-Shirley模型考虑如下图所示的两层表面，下面一层是漫反射材质，上面一层是glossy镜面反射材质（现实的这类物体有刷了光泽漆的墙、木桌等）。当观察方向接近于法线方向时，看到的主要下面一层的颜色；而当观察方向趋于与法向垂直时，观察道的更多是glossy镜面反射颜色。 &emsp;&emsp;这种表面的镜面反射和漫反射通过菲涅尔反射率联系起来。Ashikhmin-Shirley模型推导出了一种近似的Schlick菲涅尔方程如下： F_r(cos\\theta)=R+(1-R)(1-cos\\theta)^5&emsp;&emsp;其中，$\\theta$是观察方向与表面法线的夹角，$R$是镜面反照率光谱值。除了镜面反照率光谱值Rs，我们还有漫反射反照率光谱值Rd： 1234Spectrum SchlickFresnel(Float cosTheta) const &#123; auto pow5 = [](Float v) &#123; return (v * v) * (v * v) * v; &#125;; return Rs + pow5(1 - cosTheta) * (Spectrum(1.) - Rs);&#125; &emsp;&emsp;glossy镜面反射部分的BRDF如下： f_r(p,\\omega_o,\\omega_i)=\\frac{D(\\omega_h)F(\\omega_o)}{4(\\omega_h\\cdot \\omega_i)(max((n\\cdot \\omega_o),(n\\cdot \\omega_i)))} \\tag {21}&emsp;&emsp;而漫反射部分的BRDF为： f_r(p,\\omega_i,\\omega_o)=\\frac{28 R_d}{23\\pi}(1-R_s)(1-(1-\\frac{(n\\cdot\\omega_i)}{2})^5)(1-(1-\\frac{(n\\cdot\\omega_o)}{2})^5) \\tag {22}&emsp;&emsp;因此，Ashikhmin-Shirley模型就是上面公式$(21)$和$(22)$的综合，分别用各自的BRDF计算镜面反射辐射率、漫反射辐射率，最后总的辐射率就是两者的相加： 1234567891011121314Spectrum FresnelBlend::f(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; auto pow5 = [](Float v) &#123; return (v * v) * (v * v) * v; &#125;; Spectrum diffuse = (28.f / (23.f * Pi)) * Rd * (Spectrum(1.f) - Rs) * (1 - pow5(1 - .5f * AbsCosTheta(wi))) * (1 - pow5(1 - .5f * AbsCosTheta(wo))); Vector3f wh = wi + wo; if (wh.x == 0 &amp;&amp; wh.y == 0 &amp;&amp; wh.z == 0) return Spectrum(0); wh = Normalize(wh); Spectrum specular = distribution-&gt;D(wh) / (4 * AbsDot(wi, wh) * std::max(AbsCosTheta(wi), AbsCosTheta(wo))) * SchlickFresnel(Dot(wi, wh)); return diffuse + specular;&#125; 七、傅里叶基下的BSDF查找表&emsp;&emsp;上面讨论的BSDF模型足以渲染大部分的物体材质了，但是依旧存在有些材质的BSDF无法与上述的BSDF匹配（例如多层的金属材料）。对于此类材质，一种方法就是真实测量材质的BSDF数据，然后将结果保存到一个三维或四维的查找表上，计算时直接通过查找表查找即可。但直接暴力的存储需要极其庞大的存储空间。为此研究者们提出了一种压缩方法，在傅里叶频域空间压缩存储查找表，此种方法省去了大量的存储空间。 &emsp;&emsp;在这里我们仅考虑各向同性的BSDF模型。BSDF函数有两个输入参数即$\\omega_i$和$\\omega_o$，这两个参数转换成用球面坐标系$(\\theta,\\phi)$下表示，则BSDF可转变成如下形式： f(\\omega_i,\\omega_o)=f(\\mu_i,\\phi_i,\\mu_o,\\phi_o)&emsp;&emsp;其中$\\mu_i=cos\\theta_i$，$\\mu_o=cos\\theta_o$。又因为BSDF为各向同性，则BSDF的取值应该至于$\\phi=\\phi_i-\\phi_o$有关，因此： f(\\omega_i,\\omega_o)=f(\\mu_i,\\mu_o,\\phi_i-\\phi_o)=f(\\mu_i,\\mu_o,\\phi)&emsp;&emsp;各向同性的BSDF还应该是关于$\\phi$的偶函数，即$f(\\mu_i,\\mu_o,\\phi)=f(\\mu_i,\\mu_o,-\\phi)$。根据这些属性，BSDF函数展开成如下的傅里叶级数形式： f(\\mu_i,\\mu_o,\\phi_i-\\phi_o)|\\mu_i|=\\sum_{k=0}^{m-1}a_k(\\mu_i,\\mu_o)cos(k(\\phi_i-\\phi_o)) \\tag {23}&emsp;&emsp;其中$a_k(\\mu_i,\\mu_o)$是关于给定的$(\\mu_i,\\mu_o)$对应的傅里叶系数。为此我们只需存储傅里叶系数$a_k(\\mu_i,\\mu_o)$即可，对$\\mu_i$和$\\mu_o$做$n$个离散量化，只需$m$个存储$n\\times n$的傅里叶系数矩阵即可。但是对于给定的一对$(\\mu_i,\\mu_o)$，$m$的数量可以不相同，这是为了考虑空间的压缩。对于一些$(\\mu_i,\\mu_o)$，需要较大的$m$才能使得BSDF值达到指定的精度，而对于另一些则不需要那么大的$m$。因此我们实际上并不是存储的矩阵形式，而是对于每一对$(\\mu_i,\\mu_o)$和其对应的$m$，存储$a_0,a_1,…,a_{m-1}$。 &emsp;&emsp;创建如下的FourierBSDFTable保存一张BSDF数据表，这个数据表从指定外部文件中读取输入： 123456789101112131415struct FourierBSDFTable &#123; // FourierBSDFTable Public Data Float eta; int mMax; int nChannels; int nMu; Float *mu; int *m; int *aOffset; Float *a; // FourierBSDFTable Public Methods static bool Read(const std::string &amp;filename, FourierBSDFTable *table); ......&#125;; &emsp;&emsp;这里提一下上面的成员变量的作用： eta：材质内部折射指数和外部折射指数的比值； mMax：公式$(23)$中的$m$的最大值； nChannels：指明保存的傅里叶系数是几通道的，如果是$1$通道则为单色光谱，如果是$3$通道则$a_k$分别存储的是光亮度通道、红色通道和蓝色通道； nMu：$\\mu$的离散量化数量； mu：存储$\\mu$的nMu个离散量化的值，为数组，从小到大有序，因此对于给定的$u$，可以使用二分查找算法； m：一共有nMu*nMu对$(\\mu_i,\\mu_o)$，每一对都有各自的$m$值，用该数组保存，大小为nMu*nMu； aOffset：因为对于每一对$(\\mu_i,\\mu_o)$，$m$的大小不一，因此有必要指出当前$(\\mu_i,\\mu_o)$的第一个傅里叶系数$a$得起始下标，该数组就保存了对应的起始下标，大小为nMu*nMu； a：保存所有的傅里叶系数。 &emsp;&emsp;设$(\\mu_i,\\mu_o)$对应的离散量化下标为(offsetI,offsetO)，则获取对应的m和傅里叶系数的起始地址的代码如下，从该起始地址开始，后面的m个值均属于该$(\\mu_i,\\mu_o)$的傅里叶系数（这里说的是单色通道情况，而如果是三通道，则[0,m）为光亮度通道，[m,2m)为红色通道，[2m,3m)为蓝色通道）： 1234const Float *GetAk(int offsetI, int offsetO, int *mptr) const &#123; *mptr = m[offsetO * nMu + offsetI]; return a + aOffset[offsetO * nMu + offsetI];&#125; &emsp;&emsp;现在有个问题就是对于给定的$\\mu$，找到其所在的量化区间后，如何计算其对应的傅里叶系数值（因为$\\mu$很少刚刚好等于某个量化值）。这里采用Catmull-Rom样条插值，因此需要计算相应的插值权重，Catmull-Rom样条插值这里就不赘述了，下面的GetWeightsAndOffset实现了插值权重的计算，顺带计算相应的插值点的索引offset： 1234bool FourierBSDFTable::GetWeightsAndOffset(Float cosTheta, int *offset, Float weights[4]) const &#123; return CatmullRomWeights(nMu, mu, cosTheta, offset, weights);&#125; &emsp;&emsp;利用$\\mu_i$和$\\mu_o$的双Catmull-Rom样条插值，计算$a_k$如下，以$\\mu_i$为例，$o_i$就是上面计算得到的offset，$w_i$就是上面的weights： a_k=\\sum_{a=0}^3\\sum_{b=0}^3 a_k(o_i+a,o_o+b)w_i(a)w_o(b)&emsp;&emsp;有了以上的铺垫，我们就可以从BSDF查找表中计算傅里叶系数，然后再用公式$(23)$计算BSDF函数值，创建一个FourierBSDF实现该BSDF函数如下，bsdfTable就是BSDF查找表： 1234567891011121314151617class FourierBSDF : public BxDF &#123; public: // FourierBSDF Public Methods Spectrum f(const Vector3f &amp;wo, const Vector3f &amp;wi) const; FourierBSDF(const FourierBSDFTable &amp;bsdfTable, TransportMode mode) : BxDF(BxDFType(BSDF_REFLECTION | BSDF_TRANSMISSION | BSDF_GLOSSY)), bsdfTable(bsdfTable), mode(mode) &#123;&#125; Spectrum Sample_f(const Vector3f &amp;wo, Vector3f *wi, const Point2f &amp;u, Float *pdf, BxDFType *sampledType) const; Float Pdf(const Vector3f &amp;wo, const Vector3f &amp;wi) const; private: // FourierBSDF Private Data const FourierBSDFTable &amp;bsdfTable; const TransportMode mode;&#125;; &emsp;&emsp;在FourierBSDF::f接口中，我们根据输入的wo和wi查找相应的傅里叶系数，然后根据这些系数计算并返回BSDF值。f的实现主要分成以下几步，首先计算$\\mu_i$和$\\mu_o$以及$\\phi=\\phi_i-\\phi_o$，这里为了效率，仅计算了$cos\\phi$： 12345678910111213141516171819202122Spectrum FourierBSDF::f(const Vector3f &amp;wo, const Vector3f &amp;wi) const &#123; // Find the zenith angle cosines and azimuth difference angle Float muI = CosTheta(-wi), muO = CosTheta(wo); Float cosPhi = CosDPhi(-wi, wo); // Compute Fourier coefficients $a_k$ for $(\\mui, \\muo)$ // Determine offsets and weights for $\\mui$ and $\\muo$ ... // Allocate storage to accumulate _ak_ coefficients ... // Accumulate weighted sums of nearby $a_k$ coefficients ... // Evaluate Fourier expansion for angle $\\phi$ ... // Update _scale_ to account for adjoint light transport ...&#125; &emsp;&emsp;然后，分别查找$\\mu_i$和$\\mu_o$对应的量化下标offsetI和offsetO以及样条插值权重weightsI和weights： 123456// Determine offsets and weights for $\\mui$ and $\\muo$int offsetI, offsetO;Float weightsI[4], weightsO[4];if (!bsdfTable.GetWeightsAndOffset(muI, &amp;offsetI, weightsI) || !bsdfTable.GetWeightsAndOffset(muO, &amp;offsetO, weightsO)) return Spectrum(0.f); &emsp;&emsp;分配好空间，以保存接下来计算得到的$a_k$系数： 123// Allocate storage to accumulate _ak_ coefficientsFloat *ak = ALLOCA(Float, bsdfTable.mMax * bsdfTable.nChannels);memset(ak, 0, bsdfTable.mMax * bsdfTable.nChannels * sizeof(Float)); &emsp;&emsp;使用前面计算好的样条插值权重计算$a_0,a_1,…,a_{m-1}$： 12345678910111213141516// Accumulate weighted sums of nearby $a_k$ coefficientsint mMax = 0;for (int b = 0; b &lt; 4; ++b) &#123; for (int a = 0; a &lt; 4; ++a) &#123; // Add contribution of _(a, b)_ to $a_k$ values Float weight = weightsI[a] * weightsO[b]; if (weight != 0) &#123; int m; const Float *ap = bsdfTable.GetAk(offsetI + a, offsetO + b, &amp;m); mMax = std::max(mMax, m); for (int c = 0; c &lt; bsdfTable.nChannels; ++c) for (int k = 0; k &lt; m; ++k) ak[c * bsdfTable.mMax + k] += weight * ap[c * m + k]; &#125; &#125;&#125; &emsp;&emsp;根据$a_0,a_1,…,a_{m-1}$计算公式$\\sum_{k=0}^{m-1}a_k(\\mu_i,\\mu_o)cos(k(\\phi_i-\\phi_o))$（即下面的Fourier方法），并乘上一个$1/|\\mu_i|$复原BSDF函数值，如果是三通道则要多调用两次Fourier： 12345678910111213141516171819 // Evaluate Fourier expansion for angle $\\phi$ Float Y = std::max((Float)0, Fourier(ak, mMax, cosPhi)); Float scale = muI != 0 ? (1 / std::abs(muI)) : (Float)0;// Update _scale_ to account for adjoint light transport if (mode == TransportMode::Radiance &amp;&amp; muI * muO &gt; 0) &#123; float eta = muI &gt; 0 ? 1 / bsdfTable.eta : bsdfTable.eta; scale *= eta * eta; &#125; if (bsdfTable.nChannels == 1) return Spectrum(Y * scale); else &#123; // Compute and return RGB colors for tabulated BSDF Float R = Fourier(ak + 1 * bsdfTable.mMax, mMax, cosPhi); Float B = Fourier(ak + 2 * bsdfTable.mMax, mMax, cosPhi); Float G = 1.39829f * Y - 0.100913f * B - 0.297375f * R; Float rgb[3] = &#123;R * scale, G * scale, B * scale&#125;; return Spectrum::FromRGB(rgb).Clamp(); &#125; &emsp;&emsp;这里略提一下Fourier的实现，它本质上就是计算$\\sum_{k=0}^m a_kcos(k\\phi)$，但是考虑到cos函数的计算量，这里做了一些优化。注意到$cos(k\\phi)=(2cos\\phi)cos((k-1)\\phi)-cos((k-2)\\phi)$，那么可以通过动态规划的形式从k=0开始循环，保存上两次的$cos((k-1)\\phi)$和$cos((k-2)\\phi)$，避免了多次调用cos函数： 1234567891011121314Float Fourier(const Float *a, int m, double cosPhi) &#123; double value = 0.0; // Initialize cosine iterates double cosKMinusOnePhi = cosPhi; double cosKPhi = 1; for (int k = 0; k &lt; m; ++k) &#123; // Add the current summand and update the cosine iterates value += a[k] * cosKPhi; double cosKPlusOnePhi = 2 * cosPhi * cosKPhi - cosKMinusOnePhi; cosKMinusOnePhi = cosKPhi; cosKPhi = cosKPlusOnePhi; &#125; return value;&#125; 八、总结&emsp;&emsp;在这里我们讨论了完美镜面反射和透射的BSDF模型、Lambertian漫反射BSDF模型、Oren-Nayar漫反射模型、Torrance-Sparrow镜面散射模型、Ashikhmin-Shirley反射模型和基于查找表思想的傅里叶基形式的BSDF模型，其中Oren-Nayar漫反射模型、Torrance-Sparrow镜面散射模型和Ashikhmin-Shirley反射模型都基于微平面理论，实现了诸如glossy镜面反射、更加真实的漫反射等效果。涵盖了绝大部分的BSDF，物体的材质大都是这些BSDF模型的一个或多个的组合。 Reference$[1]$ M, Jakob W, Humphreys G. Physically based rendering: From theory to implementation[M]. Morgan Kaufmann, 2016.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/tags/Ray-Tracer/"}]},{"title":"Physically Based Rendering：采样和重建（三）","slug":"Sampling3","date":"2020-04-11T14:52:19.700Z","updated":"2020-04-15T14:34:09.453Z","comments":true,"path":"2020/04/11/Sampling3/","link":"","permalink":"https://yangwc.com/2020/04/11/Sampling3/","excerpt":"本篇承接上一篇《采样和重建（二）》，主要包含了在对场景的辐射率采样之后的图像重建方面的内容。","text":"本篇承接上一篇《采样和重建（二）》，主要包含了在对场景的辐射率采样之后的图像重建方面的内容。 &emsp;&emsp;以下的内容大部分整理自pbrt第三版的第七章——SAMPLING AND RECONSTRUCTION。 九、图像重建&emsp;&emsp;在对场景的辐射率进行采样之后得到一系列的采样值，我们紧接着需要对这些采样值处理以显示或保存到图片中，这个就是图像的重建。根据信号处理理论，图像重建需要做以下三件事： 根据采样值重建原始的连续图像的辐射率函数$\\hat L$。 对$\\hat L$进行滤波，以过滤掉超出当前采样频率对应的奈奎斯特极限频率。 再对滤波之后的$\\hat L$进行采样以计算每个离散像素的最终的辐射率值。 &emsp;&emsp;实践中我们并不需要$\\hat L$的显式函数，因此前两步可以合成一步，用一个滤波函数表达。理想情况下，均匀采样的频率不低于奈奎斯特极限频率就可以完好无损地重建出原始的函数信号。但现实很残酷，完好无损的重建是不可能的。而且场景的辐射率函数通常拥有很高的频率范围，均匀的采样频率追不上，为此通常都是使用随机采样方法而非均匀采样。因为随机采样将走样现象转换成了噪声，更容易被人眼接受。 &emsp;&emsp;计算给定的像素$(x,y)$的最终的辐射率值$I(x,y)$，可以用如下的邻域采样值的加权平均表示： I(x,y)= \\frac{\\Sigma_i f(x-x_i,y-y_i)\\omega (x_i,y_i) L(x_i,y_i)}{\\Sigma_i f(x-x_i, y-y_i)}&emsp;&emsp;其中，$L(x_i,y_i)$是采样点$(x_i,y_i)$处采样得到的辐射率，$\\omega(x_i,y_i)$是该采样点的贡献权重（由摄像机类返回，对于投影式摄像机，这个权重就是$1$），$f(x-x_i,y-y_i)$就是一个滤波器函数，因此这本质上就是一个滤波的过程。接下来实现各种滤波器函数。在此之前，首先创建一个Filter接口类对滤波函数$f$进行封装： 1234567891011class Filter &#123; public: // Filter Interface virtual ~Filter(); Filter(const Vector2f &amp;radius) : radius(radius), invRadius(Vector2f(1 / radius.x, 1 / radius.y)) &#123;&#125; virtual Float Evaluate(const Point2f &amp;p) const = 0; // Filter Public Data const Vector2f radius, invRadius;&#125;; &emsp;&emsp;其中radius指定$x$方向$y$方向的滤波半径，Evaluate方法输入一个点p，返回这个点p出的滤波函数值f（而非滤波结果）。注意这里滤波核的中心是原点。 1、Box滤波器&emsp;&emsp;图形学中最常用的滤波器就是Box滤波器，或者说均值滤波器，尽管这个滤波器的滤波效果可以说是最糟糕。Box滤波器可能会导致后走样现象的产生，因为它将高频的采样值融入到了重建值中。下图显式了一个Box滤波器，它就是一个常量函数。 &emsp;&emsp;Box滤波器实现起来及其简单，而且高效。对于所有在滤波核范围内的点，它都返回一个常量1.0： 1234567class BoxFilter : public Filter &#123; public: BoxFilter(const Vector2f &amp;radius) : Filter(radius) &#123;&#125; Float Evaluate(const Point2f &amp;p) const;&#125;;Float BoxFilter::Evaluate(const Point2f &amp;p) const &#123; return 1.; &#125; 2、Triangle滤波器&emsp;&emsp;Triangle滤波器顾名思义，它的滤波函数形状就是一个三角形。如下图所示，在中心处权值为$1.0$，然后两边向外在滤波核半径内逐渐线性递减，因此Triangle滤波器本质上是实现了线性插值。 12345class TriangleFilter : public Filter &#123; public: TriangleFilter(const Vector2f &amp;radius) : Filter(radius) &#123;&#125; Float Evaluate(const Point2f &amp;p) const;&#125;; &emsp;&emsp;实现Triangle滤波器也非常简单，以x轴为例，这里实现的是中心处函数值为radius.x，因此x方向的f=radius.x - std::abs(p.x)。y轴同理： 1234Float TriangleFilter::Evaluate(const Point2f &amp;p) const &#123; return std::max((Float)0, radius.x - std::abs(p.x)) * std::max((Float)0, radius.y - std::abs(p.y));&#125; 3、Gaussian滤波器&emsp;&emsp;Gaussian滤波使用高斯函数（或者说正太分布函数）作为其滤波的权重函数，它的滤波效果平滑，但不可避免地带来了模糊效果。下图展示了一个一维的高斯函数，函数从中心向两边平滑递减： &emsp;&emsp;一个一维的高斯滤波函数定义如下： f(x)=e^{-\\alpha x^2}-e^{-\\alpha r^2}&emsp;&emsp;其中$\\alpha$决定了高斯曲线的陡峭程度（即向两边的降低速率），$r$是滤波核半径。原本的高斯函数是$f(x)=e^{-\\alpha x^2}$而非上述的公式，再减去$e^{-\\alpha r^2}$是确保在滤波核的边缘上高斯函数降低至零。$e^{-\\alpha r^2}$可以提前计算保存好。创建一个Gaussian滤波器类如下： 1234567891011121314151617181920class GaussianFilter : public Filter &#123; public: // GaussianFilter Public Methods GaussianFilter(const Vector2f &amp;radius, Float alpha) : Filter(radius), alpha(alpha), expX(std::exp(-alpha * radius.x * radius.x)), expY(std::exp(-alpha * radius.y * radius.y)) &#123;&#125; Float Evaluate(const Point2f &amp;p) const; private: // GaussianFilter Private Data const Float alpha; const Float expX, expY; // GaussianFilter Utility Functions Float Gaussian(Float d, Float expv) const &#123; return std::max((Float)0, Float(std::exp(-alpha * d * d) - expv)); &#125;&#125;; &emsp;&emsp;二维的Gaussian滤波函数就是由两个一维的Gaussian滤波函数相乘得到（这是因为高斯函数的可分性）： 123Float GaussianFilter::Evaluate(const Point2f &amp;p) const &#123; return Gaussian(p.x, expX) * Gaussian(p.y, expY);&#125; 4、Mitchell滤波器&emsp;&emsp;注意到Gaussian滤波无法消除震鸣现象和模糊现象，Mitchell等人提出了一种更加优质的滤波器——Mitchell滤波器。Mitchell滤波函数如下图所示，与Gaussian函数相比，一个明显的区别就是它有两部分的函数值为负值——即负值波瓣。这负值波瓣起到了关键性的作用，它减弱了模糊现象，使得图像的高频部分变得更加锐利而不是模糊。但是负值波瓣部分不是越大越好，超过一定大小则震鸣现象将再次出现。而且因为存在负值部分，因此滤波时要注意滤波结果的值是否为负（为负值则clamp至$0$）。 ​ &emsp;&emsp;Mitchell有两个参数，分别记为$B$和$C$。Mitchell等人建议$B$和$C$的参数设置满足线性关系$B+2C=1$。一维的Mitchell滤波函数表达式如下，它的定义域范围为$[-2,2]$。因为是偶函数，我们仅讨论$[0,2]$。Mitchell滤波函数可以分成两部分，分别是$[0,1]$和$[1,2]$，这两部分分别由两个不同的三次多项式指定： f(x)=\\frac16 \\begin{cases} (12-9B-6C)|x|^3+(-18+12B+6C)|x|^2+(6-2B)& |x|","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/tags/Ray-Tracer/"}]},{"title":"Physically Based Rendering：采样和重建（二）","slug":"Sampling2","date":"2020-04-11T14:51:56.551Z","updated":"2020-04-11T14:51:39.732Z","comments":true,"path":"2020/04/11/Sampling2/","link":"","permalink":"https://yangwc.com/2020/04/11/Sampling2/","excerpt":"本篇承接上一篇《采样和重建（一）》，主要包含了分层抖动采样、Halton采样、(0,2)序列采样、最大最短距离采样和Sobol’采样方面的内容。","text":"本篇承接上一篇《采样和重建（一）》，主要包含了分层抖动采样、Halton采样、(0,2)序列采样、最大最短距离采样和Sobol’采样方面的内容。 &emsp;&emsp;以下的内容大部分整理自pbrt第三版的第七章——SAMPLING AND RECONSTRUCTION。 四、分层抖动采样&emsp;&emsp;分层抖动采样就是在分层采样的基础上做随机抖动。分层采样（Stratified Sampling）本质上就是一维均匀采样的高维扩展，也就是高维的均匀采样方法。在此基础上，我们使得采样点做一些单个采样间隔内的随机偏移，从而达到即随机又总体上比较均匀分布的效果。以二维的采样为例，下图从左到右分别展示了随机采样、分层均匀采样、分层抖动采样的采样点分布情况。可以看到随机采样某些地方聚集、某些地方稀疏，分布得极其不理想；均匀采样均匀有规律地分布；在均匀采样的基础上做单个采样间隔内的随机抖动就得到了最右边的效果，相对于单纯的随机采样来说，采样点的分布均匀得多。 &emsp;&emsp;因此，分层抖动采样本质上就是前面提到的反走样技术中的非均匀采样方法，它尝试把走样的现象转换成噪声。对采样空间$[0,1)^n$的每一维做均匀的划分（即分层），每一维的一个划分相互组合得到采样空间中的均匀子空间（上图中就是一个小格子），这些子空间互不重叠，在每个这样的子空间做随机采样，从而完成整个分层抖动采样的过程，这就是分层抖动采样的大致思想。 &emsp;&emsp;然后，理想的分层抖动采样很容易陷入维数灾难（curse of dimensionality）的窘况。以五维采样空间为例（采样向量就是个五维向量），如果对每一维划分四层，则总共产生的子空间有$4^5=1024$，即我们要做$1024$次的采样。所以理想的分层抖动采样对于高维的采样空间不是很友好，因为维数越高，则划分的子空间数量呈指数增长。为此，人们想出了一种方法来解决这个问题，这种方法就是用一维和二维的分层抖动采样组合成高维的采样点。例如五维的采样空间，可以拆分成两维的采样、一维的采样和两维的采样。以摄像机需要的五个采样值$(x,y,t,u,v)$为例，如下图所示，我们分别在两维空间对$(x,y)$和$(u,v)$做分层抖动采样，在一维空间对$t$做分层抖动采样，分别得到四个低维的采样点，然后再将这些低维的采样点随机串起来得到一个五维的采样向量，例如$(x_2,y_2,t_1,u_1,v_1)$。注意这里的随机串联很重要。 &emsp;&emsp;随机串联的方法就是对每个低维的采样点做打乱顺序的洗牌过程，然后再逐个串起来即可。声明一个StratifiedSampler的采样器类如下： 1234567891011121314151617// StratifiedSampler Declarationsclass StratifiedSampler : public PixelSampler &#123; public: // StratifiedSampler Public Methods StratifiedSampler(int xPixelSamples, int yPixelSamples, bool jitterSamples, int nSampledDimensions) : PixelSampler(xPixelSamples * yPixelSamples, nSampledDimensions), xPixelSamples(xPixelSamples), yPixelSamples(yPixelSamples), jitterSamples(jitterSamples) &#123;&#125; void StartPixel(const Point2i &amp;); private: // StratifiedSampler Private Data const int xPixelSamples, yPixelSamples; const bool jitterSamples;&#125;; &emsp;&emsp;jitterSamples用于控制是否要做随机抖动，如果不抖动则就是分层的均匀采样。在StartPixel中一次性生成所有的采样点，StratifiedSample1D生成一个维度的随机抖动采样点，然后使用Shuffle对这些采样点进行随机打乱（这里的洗牌算法使用的是Knuth-Durstenfeld Shuffle算法）： 123456789101112void StratifiedSampler::StartPixel(const Point2i &amp;p) &#123; // Generate single stratified samples for the pixel for (size_t i = 0; i &lt; samples1D.size(); ++i) &#123; StratifiedSample1D(&amp;samples1D[i][0], xPixelSamples * yPixelSamples, rng, jitterSamples); Shuffle(&amp;samples1D[i][0], xPixelSamples * yPixelSamples, 1, rng); &#125; // ... // Generate arrays of stratified samples for the pixel // ...&#125; &emsp;&emsp;StratifiedSample1D的定义如下，二维同理： 12345678// Sampling Function Definitionsvoid StratifiedSample1D(Float *samp, int nSamples, RNG &amp;rng, bool jitter) &#123; Float invNSamples = (Float)1 / nSamples; for (int i = 0; i &lt; nSamples; ++i) &#123; Float delta = jitter ? rng.UniformFloat() : 0.5f; samp[i] = std::min((i + delta) * invNSamples, OneMinusEpsilon); &#125;&#125; &emsp;&emsp;上面仅仅处理了一次只请求一个采样值的数据生成，接下来要处理一次性请求多个采样值的数据生成。例如某个积分器要求一次性获取$64$个二维采样向量，用这$64$个采样点对光源做重要性采样。这时我们要求这$64$个采样点在二维平面$[0,1)^2$上有良好的分布（分成$8\\times 8=64$个子空间）。我们姑且称之为一个块，一个块要求的采样值数量不一，取决于使用场合。这个数量有时可能不太好对二维进行划分（例如$7$个）。一种方法就是强制使得数量为两个整数的相乘，例如令$7$变为$9=3\\times 3$。 &emsp;&emsp;但pbrt提出了另一种采样方法解决这个问题，这个采样叫做拉丁超立方采样（Latin hypercube sampling，简称为LHS，亦被称为n-rook采样）。给定任意的待采样数量，它都能生成比较良好分布的采样点。以二维$[0,1)^2$为例，我们要在上面采样$4$个点，则将二维$[0,1)^2$空间划分成$4\\times 4$个子空间，然后仅在那些对角线上的子空间中做随机采样，最后再在每一维度上对采样点随机打乱顺序。下图展示了这个过程： &emsp;&emsp;因此，可以实现LatinHypercube方法如下： 12345678910111213141516void LatinHypercube(Float *samples, int nSamples, int nDim, RNG &amp;rng) &#123; // Generate LHS samples along diagonal Float invNSamples = (Float)1 / nSamples; for (int i = 0; i &lt; nSamples; ++i) for (int j = 0; j &lt; nDim; ++j) &#123; Float sj = (i + (rng.UniformFloat())) * invNSamples; samples[nDim * i + j] = std::min(sj, OneMinusEpsilon); &#125; // Permute LHS samples in each dimension for (int i = 0; i &lt; nDim; ++i) &#123; for (int j = 0; j &lt; nSamples; ++j) &#123; int other = j + rng.UniformUInt32(nSamples - j); std::swap(samples[nDim * j + i], samples[nDim * other + i]); &#125; &#125;&#125; &emsp;&emsp;最后，分块获取的采样值生成中，一维的不需要借助于LatinHypercube（但需要按块生成采样，而不是像上面那样一次性生成一个维度的全部采样值），仅二维需要使用LatinHypercube方法。 12345678910111213141516171819void StratifiedSampler::StartPixel(const Point2i &amp;p) &#123; // Generate single stratified samples for the pixel ... // Generate arrays of stratified samples for the pixel for (size_t i = 0; i &lt; samples1DArraySizes.size(); ++i) for (int64_t j = 0; j &lt; samplesPerPixel; ++j) &#123; int count = samples1DArraySizes[i]; StratifiedSample1D(&amp;sampleArray1D[i][j * count], count, rng, jitterSamples); Shuffle(&amp;sampleArray1D[i][j * count], count, 1, rng); &#125; for (size_t i = 0; i &lt; samples2DArraySizes.size(); ++i) for (int64_t j = 0; j &lt; samplesPerPixel; ++j) &#123; int count = samples2DArraySizes[i]; LatinHypercube(&amp;sampleArray2D[i][j * count].x, count, 2, rng); &#125; ...&#125; &emsp;&emsp;分层抖动采样方法产生的采样点分布比较均匀，不会出现过度的聚集现象，也不会有某些采样空间只拥有极少的采样点的情况。但它也有缺点，它仅考虑了整个采样空间的分布均匀情况，而忽略了单个维度上的采样点分布情况，因此有可能出现在某一维度上聚集的极端的情况，如下图所示： 五、Halton采样&emsp;&emsp;Halton采样算法基于低差异性的点集合，在确保生成的点集整体上不会太过聚集的同时，又考虑采样点在单个维度上的分布情况。我们将借助Hammersley序列和Halton序列实现Halton采样器。 1、Hammersley序列和Halton序列&emsp;&emsp;Hammersley序列和Halton序列是两个密切相关的低差异性点集。这两个序列的生成都依赖于逆根（radical inverse）函数实现，它基于这样的一个事实：给定一个正整数$a$和基数$b$，$a$在基数$b$下的位序列记为$d_m(a)…d_2(a)d_1(a)$，$0\\leq d_i(a)\\leq b-1$则有： a=\\sum_{i=1}^m d_i(a)b^{i-1}&emsp;&emsp;逆根函数$\\Phi$根据给定的非负整数$a$和基数$b$，通过将$a$在基数$b$下的位序列以小数点为界镜像映射到$[0,1)$的浮点数： \\Phi_b(a)=0.d_1(a)d_2(a)...d_m(a)&emsp;&emsp;一个最简单的低差异性序列就是van der Corput序列，它是由基数为$2$的逆根函数生成的一维序列$x_a=\\Phi_2(a)$，下图展示了这样的一个序列。 &emsp;&emsp;在此基础上，我们定义$n$维的Halton序列，为了生成一个$n$维的采样向量，分别使用$n$个互素的素数$(p_1,…,p_n)$，每一维使用其对应一个素数$p_i$作为基数调用逆根函数$\\Phi_{p_i}(a)$生成相应的采样值： x_a=(\\Phi_2(a),\\Phi_3(a),\\Phi_5(a),...,\\Phi_{p_n}(a))&emsp;&emsp;对于一个$n$维的低差异性序列，其差异性值满足$D_N^* (x_a)=O(\\frac{(logN)^n}{N})$，逼近最优。设总b_的采样数量为$N$，$b_i$是第$i$个素数，则Hammersley点集被定义为： x_a=(\\frac aN,\\Phi_{b_1}(a),\\Phi_{b_2}(a),...,\\Phi_{b_n}(a))&emsp;&emsp;给定基数base和非负整数a，可以很容易地用模板函数实现逆根函数如下所示： 1234567891011121314template &lt;int base&gt;PBRT_NOINLINE static Float RadicalInverseSpecialized(uint64_t a) &#123; const Float invBase = (Float)1 / (Float)base; uint64_t reversedDigits = 0; Float invBaseN = 1; while (a) &#123; uint64_t next = a / base; uint64_t digit = a - next * base; reversedDigits = reversedDigits * base + digit; invBaseN *= invBase; a = next; &#125; return std::min(reversedDigits * invBaseN, OneMinusEpsilon);&#125; &emsp;&emsp;我们使用自然数中前$1000$个素数（$2,3,5,7,…$）作为基数实例化模板函数，由于二进制的特殊性，这里对基数为$2$的情况做了优化，用ReverseBits64实现了专门用于基数为$2$的逆根函数计算： 12345678910111213141516171819// Low Discrepancy Function DefinitionsFloat RadicalInverse(int baseIndex, uint64_t a) &#123; switch (baseIndex) &#123; case 0: // Compute base-2 radical inverse#ifndef PBRT_HAVE_HEX_FP_CONSTANTS return ReverseBits64(a) * 5.4210108624275222e-20;#else return ReverseBits64(a) * 0x1p-64;#endif case 1: return RadicalInverseSpecialized&lt;3&gt;(a); case 2: return RadicalInverseSpecialized&lt;5&gt;(a); case 3: return RadicalInverseSpecialized&lt;7&gt;(a); ...... &#125;&#125; &emsp;&emsp;这样调用RadicalInverse就可以很方便地生成Hammersley序列和Halton序列。Hammersley序列和Halton序列的一个缺点就是随着基数$b$的增大，生成的采样点逐渐变得有规律起来，如下图左边所示。 &emsp;&emsp;为此，我们考虑在生成过程中对每一位进行一个随机扰动（上图右边是扰动的结果），记扰动算子为$p$，则添加了扰动算子的逆根函数返回值为： \\Psi_b(a)=0.p(d_1(a))p(d_2(a))...p(d_m(a))&emsp;&emsp;每一位的数字位取值为$(0,1,…,b-1)$，扰动操作将$d_i(a)$映射到另一个位于$(0,1,…,b-1)$的随机一个。为了实现扰动，我们预先生成一个扰动表perms，perms的前两个用于基数为$2$的扰动映射，紧接着的三个用于基数为$3$的扰动映射，依次类推。 123456789101112131415std::vector&lt;uint16_t&gt; ComputeRadicalInversePermutations(RNG &amp;rng) &#123; std::vector&lt;uint16_t&gt; perms; // Allocate space in _perms_ for radical inverse permutations int permArraySize = 0; for (int i = 0; i &lt; PrimeTableSize; ++i) permArraySize += Primes[i]; perms.resize(permArraySize); uint16_t *p = &amp;perms[0]; for (int i = 0; i &lt; PrimeTableSize; ++i) &#123; // Generate random permutation for $i$th prime base for (int j = 0; j &lt; Primes[i]; ++j) p[j] = j; Shuffle(p, Primes[i], 1, rng); p += Primes[i]; &#125; return perms;&#125; &emsp;&emsp;因此，有了随机扰动表，则对于每一位数字$d_i$，$perms[start+d_i]$（对应下面代码中的perm[digit]）就是该位扰动映射的结果。最后的invBase * perm[0] / (1 - invBase))考虑了最高位前面的零。 1234567891011121314151617181920template &lt;int base&gt;PBRT_NOINLINE static FloatScrambledRadicalInverseSpecialized(const uint16_t *perm, uint64_t a) &#123; const Float invBase = (Float)1 / (Float)base; uint64_t reversedDigits = 0; Float invBaseN = 1; while (a) &#123; uint64_t next = a / base; uint64_t digit = a - next * base; CHECK_LT(perm[digit], base); reversedDigits = reversedDigits * base + perm[digit]; invBaseN *= invBase; a = next; &#125; DCHECK_LT(invBaseN * (reversedDigits + invBase * perm[0] / (1 - invBase)), 1.00001); return std::min( invBaseN * (reversedDigits + invBase * perm[0] / (1 - invBase)), OneMinusEpsilon);&#125; 2、Halton采样器的实现&emsp;&emsp;Halton采样器基于Halton序列实现，相比于分层抖动采样，它的采样操作没有使用伪随机数。因此Halton采样的方法有可能出现走样现象而不是噪声。Halton序列的采样是一种全局采样器，设总共的像素数为$m$，每个像素的采样数量为$k$，每个采样向量维数为$d$，则它利用Halton序列生成$m\\times k$个采样。每个采样都有一个全局索引$0,1,…,m\\times k-1$，第$j$个采样向量的第$i$个维度使用扰动的Halton序列生成采样值$p(\\Phi_{b_i}(j))$，注意根逆函数的输入是采样向量编号$j$和第$i$个素数。大致的实现思路就是这样。 &emsp;&emsp;因此我们要构建像素坐标$(x,y)$和该像素获取的采样向量的全局索引$s$的对应关系。对于每一个采样向量，该向量的前两维用于构建这种对应关系。以第一个采样向量为例，其全局索引$s=0$，注意到前两维的采样值分别由$\\Phi_2(s=0)$和$\\Phi_3(s=0)$生成（使用的基数分别为$2$和$3$）。假设像素的宽$w=2^j$、高$h=3^k$，则总共的像素数量为$2^j\\times 3^k$，想办法把$x$和$\\Phi_2(s=0)$关联起来，$y$同理。设$\\Phi_2(s=0)=0.d_1(0)d_2(0)…d_j(0)…d_m(0)$，则将$\\Phi_2(0)$和$2^j$相乘得$d_1(0)d_2(0)…d_j(0).d_{j+1}(0)…d_m(0)$，取这里的整数部分$d_1(0)d_2(0)…d_j(0)$与像素坐标的$x$对应起来，因为$d_1(0)d_2(0)…d_j(0)$取值在$[0,2^j-1]$，刚好对应图像的宽度（即横向的像素数量）。 &emsp;&emsp;总共生成$2^j\\times 3^k\\times spp$，$spp$是每个像素的采样数量，这些采样数量每隔$2^j\\times 3^k$作为一组全部像素的单个采样点集合。例如某个像素的初始的采样点索引为$2$，则下一个应该是$2^j\\times 3^k+2$，再下一个应该是$2^j\\times 3^k+3$，依次类推下去。创建HaltonSampler类如下： 123456789101112131415161718192021222324252627282930class HaltonSampler : public GlobalSampler &#123; public: // HaltonSampler Public Methods HaltonSampler(int nsamp, const Bounds2i &amp;sampleBounds, bool sampleAtCenter = false); int64_t GetIndexForSample(int64_t sampleNum) const; Float SampleDimension(int64_t index, int dimension) const; std::unique_ptr&lt;Sampler&gt; Clone(int seed); private: // HaltonSampler Private Data static std::vector&lt;uint16_t&gt; radicalInversePermutations; Point2i baseScales, baseExponents; int sampleStride; int multInverse[2]; mutable Point2i pixelForOffset = Point2i(std::numeric_limits&lt;int&gt;::max(), std::numeric_limits&lt;int&gt;::max()); mutable int64_t offsetForCurrentPixel; // Added after book publication: force all image samples to be at the // center of the pixel area. bool sampleAtPixelCenter; // HaltonSampler Private Methods const uint16_t *PermutationForDimension(int dim) const &#123; if (dim &gt;= PrimeTableSize) LOG(FATAL) &lt;&lt; StringPrintf(\"HaltonSampler can only sample %d \" \"dimensions.\", PrimeTableSize); return &amp;radicalInversePermutations[PrimeSums[dim]]; &#125;&#125;; &emsp;&emsp;radicalInversePermutations是前面提到的扰动映射表，baseScales存储$(2^i,3^k)$，baseExponents存储$(i,k)$，sampleStride存储$2^j\\times 3^k$。offsetForCurrentPixel存储当前像素的第一个采样点的全局索引值，后续该像素的第i个采样点的索引就可以通过offsetForCurrentPixel+sampleStride*i得到。采样点的全局索引用于输入根逆函数获取采样值。如果图像的宽度不是$2$的幂次方，我们就寻找第一个大于宽度的$2$的幂次方值，作为$2^i$，高度同理。SampleDimension获取采样点的全局索引，返回采样向量对应维度dim的采样值： 12345678910Float HaltonSampler::SampleDimension(int64_t index, int dim) const &#123; if (sampleAtPixelCenter &amp;&amp; (dim == 0 || dim == 1)) return 0.5f; if (dim == 0) return RadicalInverse(dim, index &gt;&gt; baseExponents[0]); else if (dim == 1) return RadicalInverse(dim, index / baseScales[1]); else return ScrambledRadicalInverse(dim, index, PermutationForDimension(dim));&#125; 六、(0,2)序列采样&emsp;&emsp;(0,2)序列采样也是一种特殊的低差异性序列，与分层抖动采样相比，它生成的采样分布情况更加优良。以基数$2$对二维空间$[0,1)^2$进行分割（其实就是二分），取两个非负整数$l_1$和$l_2$作为分割深度，则分割的区间集合为： E=\\{[\\frac{a_1}{2^{l_1}},\\frac{a_1+1}{2^{l_1}})\\times [\\frac{a_2}{2^{l_2}},\\frac{a_2+1}{2^{l_2}})\\}&emsp;&emsp;其中$a_i=0,1,2,4,…,2^{l_i-1}$。(0,2)序列生成的$2^{l_1+l_2}$个采样中，上述的区间都将包含一个采样点，分布情况非常优秀。每生成$2^{l_1+l_2}$个 采样点都符合上述情况，因此以$2^{l_1+l_2}$个采样点作为一个批次，逐个批次为不同的像素生成采样点。(0,2)序列本质上是Sobol序列的前两维，它也用到了前面的根逆函数。但是考虑到根逆函数为不同基数（大于$2$的基数）的计算的计算量有点过于庞大，考虑将大于$2$的基数的逆根计算转换成基数为$2$的逆根计算。一种高效的方法就是使用生成矩阵（generator matrices）将其他不同基数的逆根计算转换到同一个基数下的逆根计算。 &emsp;&emsp;设基数为$b$，非负整数$a$的位数为$n$，第$i$位为$d_i(a)$，我们有一个$n\\times n$的生成矩阵$C$，则可以生成$x_a\\in [0,1)$的采样点如下： x_a=[b^{-1} b^{-2} ... b^n] \\left[ \\begin{matrix} c_{1,1} & c_{1,2} & ... & c_{1,n}\\\\ c_{2,1} & ... & ... & c_{2,n}\\\\ ... & ... & ... & ...\\\\ c_{n,1} & ... & ... & c_{n,n} \\end{matrix} \\right] \\left[ \\begin{matrix} d_1(a)\\\\ d_2(a) \\\\ ... \\\\ d_3(a) \\end{matrix} \\right]&emsp;&emsp;若生成矩阵$C$为单位矩阵，则上述的公式等价于前面我们提到的常规的根逆计算。在这里，我们仅将实现基数$b=2$和$n=32$的生成矩阵。基数为$2$时，生成矩阵的元素取值为$0$或者$1$，因此生成矩阵的一列可以用一个$32$为的非负整数表示。根据矩阵向量乘法定义，则上面的矩阵向量相乘$C[d_i(a)]^T$可写成： \\left[ \\begin{matrix} c_{1,1} & c_{1,2} & ... & c_{1,n}\\\\ c_{2,1} & ... & ... & c_{2,n}\\\\ ... & ... & ... & ...\\\\ c_{n,1} & ... & ... & c_{n,n} \\end{matrix} \\right] \\left[ \\begin{matrix} d_1(a)\\\\ d_2(a) \\\\ ... \\\\ d_3(a) \\end{matrix} \\right] = d_1 \\left[ \\begin{matrix} c_{1,1}\\\\ c_{2,1} \\\\ ... \\\\ c_{n,1} \\end{matrix} \\right] + ... + d_n \\left[ \\begin{matrix} c_{1,n}\\\\ c_{2,n} \\\\ ... \\\\ c_{n,n} \\end{matrix} \\right]&emsp;&emsp;$d_i$也是取值为$0$或者$1$，因此上面的矩阵向量相乘等价于将$d_i$为$1$的对应的矩阵的那一列全部加起来，因为我们用一个$32$位整数表示矩阵的一列，所以这些列的相加等价于这些整数的异或。由此可以非常高效实现矩阵向量相乘的过程： 123456inline uint32_t MultiplyGenerator(const uint32_t *C, uint32_t a) &#123; uint32_t v = 0; for (int i = 0; a != 0; ++i, a &gt;&gt;= 1) if (a &amp; 1) v ^= C[i]; return v;&#125; &emsp;&emsp;矩阵向量相乘MultiplyGenerator的结果$v$再与$[b^{-1} b^{-2} … b^n]$相乘等价于将$v$做二进制位的镜像翻转再除以$2^{32}$，这样就得到了我们所需的采样值$x_a$。为了进一步减少位翻转的开销，pbrt里在传入MultiplyGenerator之前先将生成矩阵的每一列做了位翻转，因为生成矩阵是预计算好的，这样后面就不用再翻转了。整个过程只涉及到一些移位运算和异或操作，非常高效。 &emsp;&emsp;同样为了加入随机性，我们给MultiplyGenerator的返回值做一个位扰动。扰动算法的设计需要非常小心，以防丢失了(0,2)序列的低差异性属性。具体的扰动算法这里不说了，扰动的实现可以直接用一个预先生成好的扰动序列scramble（用$32$位整数表示）与返回值v进行异或，如下所示，Float(0x1p-32)是$2^{-32}$： 12345inline Float SampleGeneratorMatrix(const uint32_t *C, uint32_t a, uint32_t scramble = 0) &#123; return std::min((MultiplyGenerator(C, a) ^ scramble) * Float(0x1p-32), OneMinusEpsilon);&#125; &emsp;&emsp;上面给出了一种高效的采样值计算过程。但pbrt指出，我们可以实现更高效的采样值生成过程。更高效的实现是从生成采样值的过程中以格雷码（gray code）的顺序执行入手。对于$n$位的$a$，$a$的取值范围为$0$到$2^n-1$，格雷码以一种特殊的顺序枚举$0$到$2^n-1$之间的数字，这种特殊的顺序表现在两个相邻的格雷码之间仅有一个二进制位不同，如下图的$g(n)$所示： &emsp;&emsp;由此，假设对于$a$，我们已经计算得到了$v=C[d_i(a)]^T$，那么对于与$a$只有一个二进制位不同的$a’$的$v’=C[d_I(a’)]^T$的计算，我们只需在$v$的基础上加上或减去（通过异或实现）该不同二进制位对应的矩阵的那一列。所以关键是找到不同的那一个二进制位，这个比较技巧性，就不赘述了。下面是GrayCodeSample方法，与SampleGeneratorMatrix只生成给定的a对应的采样值不同，它输入$n$，生成$0$到$2^n-1$之间所有整数值对应的采样值，结果保存到$p$数组中，通过以格雷码的顺序枚举加速整个生成过程。 12345678inline void GrayCodeSample(const uint32_t *C, uint32_t n, uint32_t scramble, Float *p) &#123; uint32_t v = scramble; for (uint32_t i = 0; i &lt; n; ++i) &#123; p[i] = std::min(v * Float(0x1p-32) /* 1/2^32 */, OneMinusEpsilon); v ^= C[CountTrailingZeros(i + 1)]; &#125;&#125; &emsp;&emsp;上面讨论的都是一维的采样值生成过程，二维就是多加了一个生成矩阵C1，差别不大。 12345678910inline void GrayCodeSample(const uint32_t *C0, const uint32_t *C1, uint32_t n, const Point2i &amp;scramble, Point2f *p) &#123; uint32_t v[2] = &#123;(uint32_t)scramble.x, (uint32_t)scramble.y&#125;; for (uint32_t i = 0; i &lt; n; ++i) &#123; p[i].x = std::min(v[0] * Float(0x1p-32), OneMinusEpsilon); p[i].y = std::min(v[1] * Float(0x1p-32), OneMinusEpsilon); v[0] ^= C0[CountTrailingZeros(i + 1)]; v[1] ^= C1[CountTrailingZeros(i + 1)]; &#125;&#125; &emsp;&emsp;紧接着用上述的相关函数创建ZeroTwoSequenceSampler的(0,2)序列采样器，这个采样器对于摄像机的五个采样值（即$(x,y,t,u,v)$）以及其他二维的采样值使用扰动的(0,2)序列生成，剩下的一维采样值使用扰动的van der Corput序列生成（就是基数为$2$的根逆函数生成的序列）。 12345678class ZeroTwoSequenceSampler : public PixelSampler &#123; public: // ZeroTwoSequenceSampler Public Methods ZeroTwoSequenceSampler(int64_t samplesPerPixel, int nSampledDimensions = 4); void StartPixel(const Point2i &amp;); std::unique_ptr&lt;Sampler&gt; Clone(int seed); int RoundCount(int count) const &#123; return RoundUpPow2(count); &#125;&#125;; &emsp;&emsp;(0,2)序列采样器需要确保每个像素的采样数量为$2$的幂次方，如果不是就使得采样数量为第一个大于当前采样数量的$2$幂次方，即下面的RoundUpPow2函数，因为这样能够使得(0,2)序列生成得采样点分布更加优良： 123456789ZeroTwoSequenceSampler::ZeroTwoSequenceSampler(int64_t samplesPerPixel, int nSampledDimensions) : PixelSampler(RoundUpPow2(samplesPerPixel), nSampledDimensions) &#123; if (!IsPowerOf2(samplesPerPixel)) Warning( \"Pixel samples being rounded up to power of 2 \" \"(from %\" PRId64 \" to %\" PRId64 \").\", samplesPerPixel, RoundUpPow2(samplesPerPixel));&#125; &emsp;&emsp;对于一维的采样点，将使用前面的GrayCodeSample生成van der Corput序列，van der Corput序列本身就是由基数为$2$的根逆函数生成，因此生成矩阵就是一个单位矩阵。下面的CVanDerCorput数组保存了生成矩阵的每一列，它的每一列都预先翻转了。 123456789101112131415161718192021inline void VanDerCorput(int nSamplesPerPixelSample, int nPixelSamples, Float *samples, RNG &amp;rng) &#123; uint32_t scramble = rng.UniformUInt32(); // Define _CVanDerCorput_ Generator Matrix const uint32_t CVanDerCorput[32] = &#123; // clang-format off 0b10000000000000000000000000000000, 0b1000000000000000000000000000000, 0b100000000000000000000000000000, 0b10000000000000000000000000000, // Remainder of Van Der Corput generator matrix entries ... &#125;; int totalSamples = nSamplesPerPixelSample * nPixelSamples; GrayCodeSample(CVanDerCorput, totalSamples, scramble, samples); // Randomly shuffle 1D sample points for (int i = 0; i &lt; nPixelSamples; ++i) Shuffle(samples + i * nSamplesPerPixelSample, nSamplesPerPixelSample, 1, rng); Shuffle(samples, nPixelSamples, nSamplesPerPixelSample, rng);&#125; &emsp;&emsp;为了防止生成的采样点之间存在某种意想不到的关联，生成的采样序列需要被Shuffle进行一个随机洗牌过程。对于二维的采样值，这里将使用Sobol序列的生成矩阵调用GrayCodeSample。如下所示： 12345678910111213141516171819202122232425inline void Sobol2D(int nSamplesPerPixelSample, int nPixelSamples, Point2f *samples, RNG &amp;rng) &#123; Point2i scramble; scramble[0] = rng.UniformUInt32(); scramble[1] = rng.UniformUInt32(); // Define 2D Sobol$'$ generator matrices _CSobol[2]_ const uint32_t CSobol[2][32] = &#123; &#123;0x80000000, 0x40000000, 0x20000000, 0x10000000, 0x8000000, 0x4000000, 0x2000000, 0x1000000, 0x800000, 0x400000, 0x200000, 0x100000, 0x80000, 0x40000, 0x20000, 0x10000, 0x8000, 0x4000, 0x2000, 0x1000, 0x800, 0x400, 0x200, 0x100, 0x80, 0x40, 0x20, 0x10, 0x8, 0x4, 0x2, 0x1&#125;, &#123;0x80000000, 0xc0000000, 0xa0000000, 0xf0000000, 0x88000000, 0xcc000000, 0xaa000000, 0xff000000, 0x80800000, 0xc0c00000, 0xa0a00000, 0xf0f00000, 0x88880000, 0xcccc0000, 0xaaaa0000, 0xffff0000, 0x80008000, 0xc000c000, 0xa000a000, 0xf000f000, 0x88008800, 0xcc00cc00, 0xaa00aa00, 0xff00ff00, 0x80808080, 0xc0c0c0c0, 0xa0a0a0a0, 0xf0f0f0f0, 0x88888888, 0xcccccccc, 0xaaaaaaaa, 0xffffffff&#125;&#125;; GrayCodeSample(CSobol[0], CSobol[1], nSamplesPerPixelSample * nPixelSamples, scramble, samples); for (int i = 0; i &lt; nPixelSamples; ++i) Shuffle(samples + i * nSamplesPerPixelSample, nSamplesPerPixelSample, 1, rng); Shuffle(samples, nPixelSamples, nSamplesPerPixelSample, rng);&#125; &emsp;&emsp;最后，在StartPixel中使用这里两个函数生成所需的采样点。 1234567891011121314151617void ZeroTwoSequenceSampler::StartPixel(const Point2i &amp;p) &#123; ProfilePhase _(Prof::StartPixel); // Generate 1D and 2D pixel sample components using $(0,2)$-sequence for (size_t i = 0; i &lt; samples1D.size(); ++i) VanDerCorput(1, samplesPerPixel, &amp;samples1D[i][0], rng); for (size_t i = 0; i &lt; samples2D.size(); ++i) Sobol2D(1, samplesPerPixel, &amp;samples2D[i][0], rng); // Generate 1D and 2D array samples using $(0,2)$-sequence for (size_t i = 0; i &lt; samples1DArraySizes.size(); ++i) VanDerCorput(samples1DArraySizes[i], samplesPerPixel, &amp;sampleArray1D[i][0], rng); for (size_t i = 0; i &lt; samples2DArraySizes.size(); ++i) Sobol2D(samples2DArraySizes[i], samplesPerPixel, &amp;sampleArray2D[i][0], rng); PixelSampler::StartPixel(p);&#125; 七、最大最短距离采样&emsp;&emsp;(0,2)序列有时生成的采样点也会过于聚集，一种可行的解决方案就是使用一对不同的生成矩阵，使得既可以生成良好分布的(0,2)序列，又可以最大化采样点之间的最短距离。下面将实现一个最大最短距离采样器MaxMinDistSampler： 123456789class MaxMinDistSampler : public PixelSampler &#123; public: // MaxMinDistSampler Public Methods ... private: // MaxMinDistSampler Private Data const uint32_t *CPixel;&#125;; &emsp;&emsp;这里实现的最大最短距离采样与(0,2)序列采样区别不是很大，主要的区别在于使用了一个特殊的生成器矩阵。这些生成器矩阵的获取比较复杂，直接从相关的paper中拿出来保存到CMaxMinDist中。下面的变量保存了17个生成矩阵，分别对应着像素采样数量为$2^0,2^1,2^2,…,2^{15},2^{16}$，例如单个像素的采样数量为$2^5$，则获取CMaxMinDist[5]作为特殊的生成矩阵保存到CPixel中。 1extern uint32_t CMaxMinDist[17][32]; &emsp;&emsp;对于非$2$幂次数量的情况，则用Log2Int方法计算其矩阵的下标，并用RoundUpPow2变成大于当前数量的第一个$2$幂次方。（注意，采样数量上限为$2^{16}$）。 12345678910111213MaxMinDistSampler(int64_t samplesPerPixel, int nSampledDimensions) : PixelSampler([](int64_t spp) &#123; if (!IsPowerOf2(spp)) &#123; spp = RoundUpPow2(spp); Warning(\"Non power-of-two sample count rounded up to %\" PRId64 \" for MaxMinDistSampler.\", spp); &#125; return spp; &#125;(samplesPerPixel), nSampledDimensions) &#123; int Cindex = Log2Int(samplesPerPixel); CPixel = CMaxMinDist[Cindex];&#125; &emsp;&emsp;它生成采样点的过程是这样的。对于第一个维度的二维的采样值，用Point2f(i * invSPP, SampleGeneratorMatrix(CPixel, i))生成，剩余的照搬前面的(0,2)序列采样。（应该是为了像素内的偏移$(x,y)$量身定做的，因为第一个维度一般用于像素内偏移的采样） 1234567891011121314151617181920212223void MaxMinDistSampler::StartPixel(const Point2i &amp;p) &#123; Float invSPP = (Float)1 / samplesPerPixel; for (int i = 0; i &lt; samplesPerPixel; ++i) samples2D[0][i] = Point2f(i * invSPP, SampleGeneratorMatrix(CPixel, i)); Shuffle(&amp;samples2D[0][0], samplesPerPixel, 1, rng); // Generate remaining samples for _MaxMinDistSampler_ for (size_t i = 0; i &lt; samples1D.size(); ++i) VanDerCorput(1, samplesPerPixel, &amp;samples1D[i][0], rng); for (size_t i = 1; i &lt; samples2D.size(); ++i) Sobol2D(1, samplesPerPixel, &amp;samples2D[i][0], rng); for (size_t i = 0; i &lt; samples1DArraySizes.size(); ++i) &#123; int count = samples1DArraySizes[i]; VanDerCorput(count, samplesPerPixel, &amp;sampleArray1D[i][0], rng); &#125; for (size_t i = 0; i &lt; samples2DArraySizes.size(); ++i) &#123; int count = samples2DArraySizes[i]; Sobol2D(count, samplesPerPixel, &amp;sampleArray2D[i][0], rng); &#125; PixelSampler::StartPixel(p);&#125; 八、Sobol’ 采样&emsp;&emsp;(0,2)序列采样在Halton采样的基础上改进，仅使用基数为$2$的根逆函数，而且使用不同的生成矩阵，但它仅考虑一维和两维的情况。由此，考虑全部维度使用不同的生成矩阵，就是Sobol序列的采样方法。Sobol序列也只考虑基数为$2$的根逆函数，采样向量的每个维度都有格子不同的生成矩阵$C$。生成矩阵的获取由特殊的算法得到，这里给出了Sobol序列的预计算好的生成矩阵，可以直接拿来用。下面实现了Sobol采样，它的关键点就是生成矩阵的SobolMatrices32，其他和前面的Halton采样一样： 123456inline float SobolSampleFloat(int64_t a, int dimension, uint32_t scramble) &#123; uint32_t v = scramble; for (int i = dimension * SobolMatrixSize; a != 0; a &gt;&gt;= 1, i++) if (a &amp; 1) v ^= SobolMatrices32[i]; return std::min(v * 0x1p-32f /* 1/2^32 */, FloatOneMinusEpsilon);&#125; &emsp;&emsp;Sobol’采样在各个维度上使用不同的生成矩阵，因此具备优良的分布。但是它的缺点就是在收敛之前容易出现网格结构状的失真现象。下面实现一个Sobol’采样器，注意它是一个全局采样器： 1234567891011121314151617181920212223class SobolSampler : public GlobalSampler &#123; public: // SobolSampler Public Methods std::unique_ptr&lt;Sampler&gt; Clone(int seed); SobolSampler(int64_t samplesPerPixel, const Bounds2i &amp;sampleBounds) : GlobalSampler(RoundUpPow2(samplesPerPixel)), sampleBounds(sampleBounds) &#123; if (!IsPowerOf2(samplesPerPixel)) Warning(\"Non power-of-two sample count rounded up to %\" PRId64 \" for SobolSampler.\", this-&gt;samplesPerPixel); resolution = RoundUpPow2( std::max(sampleBounds.Diagonal().x, sampleBounds.Diagonal().y)); log2Resolution = Log2Int(resolution); &#125; int64_t GetIndexForSample(int64_t sampleNum) const; Float SampleDimension(int64_t index, int dimension) const; private: // SobolSampler Private Data const Bounds2i sampleBounds; int resolution, log2Resolution;&#125;; &emsp;&emsp;这里的一个实现难点依旧跟Halton采样一样，就是如何将像素坐标与采样向量的全局索引的映射建立起来。依旧用采样向量的前两维$[0,1)^2$与像素坐标联系起来。设图像的宽高$(w,h)=(2^i,2^j)$，如果不是$2$的幂次方则做一个RoundUpPow2，令log2Resolution=max(i,j)，sampleBounds是像素的坐标范围。理论上，将前两维$[0,1)^2$乘上$2^{log2Resolution}$就可以得到采样向量与像素坐标的映射关系。但通常情况下我们是已知像素坐标要获取采样向量，因此要实现的是逆映射，具体细节这里不展开。 &emsp;&emsp;因为前两维用于像素坐标与采样向量的映射，因此获取时要做一些额外的处理（返回相对于当前像素的偏移量）。 123456789Float SobolSampler::SampleDimension(int64_t index, int dim) const &#123; Float s = SobolSample(index, dim); // Remap Sobol$'$ dimensions used for pixel samples if (dim == 0 || dim == 1) &#123; s = s * resolution + sampleBounds.pMin[dim]; s = Clamp(s - currentPixel[dim], (Float)0, OneMinusEpsilon); &#125; return s;&#125; Reference$[1]$ M, Jakob W, Humphreys G. Physically based rendering: From theory to implementation[M]. Morgan Kaufmann, 2016.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/tags/Ray-Tracer/"}]},{"title":"Physically Based Rendering：采样和重建（一）","slug":"Sampling1","date":"2020-04-11T14:50:47.647Z","updated":"2020-09-25T08:49:24.184Z","comments":true,"path":"2020/04/11/Sampling1/","link":"","permalink":"https://yangwc.com/2020/04/11/Sampling1/","excerpt":"计算机作为离散数据的计算机器，只能使用采样的手段处理连续的数据。计算图形学本质上是采用虚拟摄像机处理虚拟空间的信号数据，因此在图形领域采样的身影无所不在，如何使用采样和重建的技术合成尽可能真实、无噪声、无走样的图像是图形学的一个重要话题。","text":"计算机作为离散数据的计算机器，只能使用采样的手段处理连续的数据。计算图形学本质上是采用虚拟摄像机处理虚拟空间的信号数据，因此在图形领域采样的身影无所不在，如何使用采样和重建的技术合成尽可能真实、无噪声、无走样的图像是图形学的一个重要话题。 &emsp;&emsp;以下的内容大部分整理自pbrt第三版的第七章——SAMPLING AND RECONSTRUCTION。 一、采样理论&emsp;&emsp;在图形渲染领域，二维的像素栅格阵列本质上就是在虚拟摄像机的成像平面上对入射辐射率做离散的采样，因此相关的采样技术可以说是图形学的重要基础技术之一。对于连续的函数或信号，采样是我们的计算机捕获这个函数或信号的方法。除此之外，对于采样得到的数据我们还需要进行重建以尽可能地还原出原始的函数或信号分布。在基于光线追踪的渲染算法中，从视角发射的一条射线就是一个采样的过程，它返回入射到这个视角方向的出射辐射率，但这仅仅是采样，如何利用采样得到的辐射率构建与原始图像尽可能相符的图片是重建的范畴（更多的涉及到图像空间的滤波技术）。 &emsp;&emsp;上图给出了原始信号（左）与采样重建得到的信号（右）对比，黑色的点通过采样得到，然后在采样点之间利用线性插值的重建方法得到捕获的信号分布。可以看到，重建的结果与原始信号存在着差异，这是由采样的离散性和线性的重建引入的误差导致，这些误差在图形学中表现为锯齿和闪烁。分析和减小此类误差的手段在相关的信号处理课程中已有详细的介绍，这里只摘取了一些重点。 1、傅里叶分析&emsp;&emsp;傅里叶分析是信号处理技术的基础，大致方法就是将空间域（或时空域）的信号和函数通过傅里叶变换转变到频率域，在频率域分析相关的频谱特征。下面给出了一维的时空域和频率域互相转换的公式，它们互相构成了傅里叶变换对： F(\\omega)=\\int_{-\\infty}^{+\\infty}f(x)e^{-i2\\pi \\omega x}dx\\\\ f(x)=\\int_{-\\infty}^{+\\infty}F(\\omega)e^{i2\\pi\\omega x}d\\omega \\tag {1}&emsp;&emsp;傅里叶变换是工科的基础，相关的细节就不赘述了。傅里叶分析中常用的一个定理就是卷积定理，卷积定理指出：空间域的卷积等价于频率域的乘积，反之亦然。即空间域的两个函数乘积等价于这两个函数的傅里叶变换形式的卷积（如下公式$(2)$），而频率域的两个函数乘积等价于这两个函数在空间域的卷积（如下公式$(3)$）： \\zeta \\{f(x)g(x)\\}=F(\\omega)\\otimes G(\\omega) \\tag {2} \\zeta(f(x)\\otimes g(x))=F(\\omega)G(\\omega) \\tag {3}2、采样与重建&emsp;&emsp;常用的采样的手段是等间隔的均匀采样，即根据每隔一定间隔采样一次。这种形式的采样可以表示成一个脉冲序列与原始信号的乘积。脉冲序列由一个个单位脉冲函数构成，离散变量的单位脉冲函数$\\delta (x)$定义如下： \\delta (x)= \\begin{cases} 1, x=0\\\\ 0, x\\neq 0 \\end{cases}&emsp;&emsp;对于这样的单位脉冲函数，它具备如下的取样特性，它在$x=x_0$处取一次$f(x)$的函数值： \\displaystyle \\sum_{x=-\\infty}^{+\\infty}f(x-x_0)\\delta (x)=f(x_0)&emsp;&emsp;由此，脉冲序列就是由很多个$\\delta (x)$组成，其数学形式如下： III_T(x)=T\\sum_{i=-\\infty}^{\\infty}\\delta (x-iT) \\tag {4}&emsp;&emsp;其中$T$是采样周期或者说采样间隔，数学形式看着有点复杂，但其实很简单。下图(b)展示了离散的脉冲序列，其采样周期为$T=1$。 &emsp;&emsp;因此，采样的过程就可以表达成采样的脉冲序列和原始信号的乘积： III_T(x) f(x)=T\\sum_i \\delta(x-iT)f(iT) \\tag {5}&emsp;&emsp;公式$(5)$给出了采样得到的函数值序列，这是一个一个离散的点（如上图(c)所示）。根据这些离散点，我们进行一个重建的过程以还原原始的函数分布，在空间域我们对这些采样点进行卷积得到重建的结果（其实就是滤波），记重建的函数分布为$\\hat f(x)$： \\hat f(x)=(III_I(x)f(x))\\otimes r(x) \\tag {6}&emsp;&emsp;$r(x)$是卷积权重函数。事实上，根据卷积定理，我们可以知道采样的信号在频率的分布情况，公式$(5)$可以转换成频率域的卷积形式，卷积的函数分别是$F(\\omega)$和脉冲序列函数的傅里叶形式，脉冲序列函数在频率域依旧是一个脉冲序列，只不过它的采样周期变成了$1/T$。在频率域，$F(\\omega)$和脉冲序列做卷积，得到的结果就是$F(\\omega)$在每个采样脉冲点上都有一份相同的拷贝。下图(c)是原始信号的傅里叶形式（即$F(\\omega)$，图(a)是$F(\\omega)$与频率域的脉冲序列做卷积的结果，每一个脉冲点上均有$F(\\omega)$的一份拷贝，他们之间的间隔就是脉冲间隔$1/T$。 &emsp;&emsp;在理想的情况下，在频率域，我们只需要获取一份$F(\\omega)$即可，因此设计一个滤波器$\\prod_T(\\omega)$如上图(b)所示，将其与采样得到的傅里叶形式（即上图(a)）相乘，试图得到原始信号的傅里叶分布（即上图(c)）。这个过程可以描述为如下的形式： \\hat F(\\omega)=(F(\\omega)\\otimes III_{1/T}(\\omega)) \\prod_T(\\omega) \\tag {7}&emsp;&emsp;上述的公式$(7)$与公式$(6)$殊途同归、遥相呼应，公式$(6)$给出了空间域下的采样重建过程，得到$\\hat f(x)$，而公式$(7)$则给出了频率域的表达形式，得到$\\hat F(\\omega)$。$\\hat f(x)$和$\\hat F(\\omega)$是采样信号分别在空间域和频率域的不同形式，它们通过卷积定理联系起来。因此，理想的采样和重建的过程可以总结为，在空间域中，对信号做一个均匀的采样，然后对采样得到的数据序列做一个卷积（在图形学中，一个常用的卷积核就是box滤波，即对周围的邻域采样值取平均）。 &emsp;&emsp;目前我们的讨论都是假设$F(\\omega)$是一个带限（band limit）函数，即它在频率的自变量（也就是频率）是有上界的，即定义域范围是有限的（通常由$[-u_{max},+u_{nax}]$表示）。超出该定义域范围的$F(\\omega)$取值为$0$，代表$f(x)$没有该频率的分量。从现实意义上说，即一个信号的频率是有最大频率的。 &emsp;&emsp;值得一提的是，空间域中采样间隔为$T$的脉冲序列，对应到频率域，其采样间隔变成了$1/T$。这是一个反比关系，即如果空间域采样间隔越小，采样越密集，则频域下的采样间隔越大（因此越能扩展到更高的频率范围，从而捕获更多的高频信息）。 3、走样现象（Aliasing）&emsp;&emsp;前面我们假设待采样的信号是带限的，但在图形学中仅有很少函数分布是带限的，大部分都拥有无穷的频率范围。对一个非带限函数采样或着使用过低的采样频率（对应就是使用较宽的采样间隔）进行采样时，将不可能避免地使得重建出来的函数分布与原始的函数分布相差甚远，这种现象被称为走样（aliasing）。 &emsp;&emsp;这种现象的产生原因不难理解，以过低的采样频率为例子。采样频率过低意味着使用了很宽的采样间隔$T$，转换到频率域，则其对应的频率域周期$1/T$过低，使得原本互不重叠的采样分布重叠了，正如下图图(a)所示。 &emsp;&emsp;由此重叠产生的效应我们称之为混淆。我们再对上图(a)的频谱分布进行一个带通滤波，将得到上图$(b)$的结果，这个就是重建得到的结果。相比于原始的频谱分布，它的两端被切掉了，丢失了高频部分的信息，从而产生了走样现象。由此为了避免产生明显的走样现象，一种暴力的方法就是提高我们的采样频率，降低采样间隔，但盲目地提高采样率不可取。 &emsp;&emsp;理想情况下，我们的目标就是使得上图(a)中的函数刚刚好互不重叠即可。奈奎斯特采样定理给出了理想采样频率与待采样的原始信号频率之间的关系，即采样的频率至少应该为采样信号中的最大频率的两倍才不至于丢失采样信号中的信息，才能够完美地重建出原始的信号分布。这个定理我们可以从两个方面看：一是给定一个采样频率，则我们能够完美捕获的最大信号频率是采样频率的一半；另一方面则是若已知信号的最大频率，则可以以最大频率的两倍作为采样频率进行采样以完美重建原始的信号。采样频率低于定理给出的频率则是欠采样，超出了则是过采样，最理想的情况就是刚刚好，即临采样。 4、反走样技术&emsp;&emsp;针对走样现象，目前已经有诸多的反走样技术，下面介绍三大类反走样技术。在采样阶段产生的失真我们称之为前走样（prealiasing），在重建阶段产生的失真被称为后走样（postaliasing）。 （1）、非均匀采样&emsp;&emsp;顾名思义，在均匀采样的基础上对采样的间隔做一个随机扰动，使得采样的间隔不再是均匀分布。记$\\zeta$为$[0,1]$的随机数，则一个非均匀的脉冲采样序列定义为： \\sum_{i=-\\infty}^{\\infty} \\delta(x-(i+\\frac12-\\zeta)T)&emsp;&emsp;上面的公式就是在原来的基础上加了$\\frac12-\\zeta$，它的取值范围是$[-\\frac12,+\\frac12]$，即在一个周期内随机扰动。对于采样频率过低的情况，均匀采样和非均匀采样均会产生错误的结果，只不过非均匀采样产生的噪声而非走样现象。对于人类视觉系统，噪声更容易被接收一些。 （2）、自适应采样&emsp;&emsp;这里的自适应针对的是原始信号频域的适应，注意到一般信号有高频和低频部分。对于低频部分，没必要使用很高的采样率去采样；而对于高频分布，则应该提高采样频率以防走样的产生。这项技术的初衷非常好，但比较难适用到现实中，因为通常情况下待采样的信号分布我们并不知晓。一些技术通过上两次的采样值对当前的信号频率进行评估，如果值差异过大则适当地提高采样频率。这种方法效果依旧不是很好。 （3）、预滤波技术&emsp;&emsp;预滤波技术从产生问题的根源着手。既然在较低采样率的情况下难以捕获高频信息导致了走样的产生，那么直接在采样之前想办法去掉高频信息。此类技术直接在采样之前对信号执行一个滤波操作，这个滤波采用的滤波核通常是一个低通滤波器，目的是去掉高频、保留低频。这种方法迫使原始的信号分布在一个较低频的范围之内，避免了走样的产生。但附带的结果就是原始信号中的高频信息丢失，信号更加平滑、更加模糊了。相对于走样来说，模糊更能够被人接受一些。 5、渲染中的采样&emsp;&emsp;基于光线追钟的渲染算法本质上就是对场景的辐射率进行采样和重建的过程。场景中的辐射率是一个关于像素位置$(x,y)$、时间$t$、透镜采样点$(u,v)$和光线采样值$i_1,i_2,…$的高维函数： f(x,y,t,u,v,i_1,i_2,...)\\to L \\tag {8}&emsp;&emsp;如何对这些维度进行高效、高质量的采样和重建是光线追踪渲染中永恒不变的话题。在图形渲染中，产生走样的源头主要有场景的几何体（边缘锯齿现象）、纹理和材质（摩尔纹和闪烁现象）和阴影等。这些源头是渲染的重要组成成分，因此不可能直接丢弃。最后需要提的一点就是二维阵列中的像素本质上是一个离散的采样点，它并没有面积这个概念。 二、采样器接口&emsp;&emsp;前面已经提到，光线追踪渲染器通常需要对一个$n$维的空间进行采样，因此一个采样点可以表示成$n$维向量。每个维度在$[0,1)$上随机采样，采样空间序列也可以写成$[0,1)^n$。具体多少维，取决于渲染的积分器。一种最简单的随机采样方法就是随机均匀采样，但这种方法往往需要很多的采样数量才能使得渲染积分收敛到正确的结果。在相同采样数量下，我们寻求尽可能好和高效的采样方法。 &emsp;&emsp;对于生成的采样向量的用途，我们规定，采样向量的前$5$维用于摄像机（分别是单个像素内的偏移量$(x,y)$、摄像机时间$t$和透镜光圈上的随机采样$(u,v)$）。采样向量剩余维度的数据用于光线追踪过程中的采样。 1、采样质量评估：差异性&emsp;&emsp;对于高维的采样向量的质量评估，傅里叶分析已经满足不了。为此，数学家们提出了一种高维采样质量的评估策略——差异性（discrepancy）。对于一个分布良好的采样序列，它应该具备低差异性的评判标准。$n$维空间$[0,1)^n$的采样序列的差异性评估方法为：寻找$[0,1)^n$的任意一个子空间，使得散落在这个子空间内的采样点数量占总采样点数量的比值与这个子空间占总空间的比值的差最大。理想情况下，最优的采样策略应该是这两个比值相等，即差异性为零。因此，我们的目的就是寻找使得差异性尽可能小的采样模式。 &emsp;&emsp;差异性的计算定义如下，首先从采样空间的$[0,1)^n$的一个子空间$B$开始，例如下面这个： B=\\{[0,v_1]\\times[0,v_2]\\times...\\times[0,v_n]\\}&emsp;&emsp;其中$0\\leq v_i &lt; 1$。对于给定的采样点序列$P=x_1,…,x_N$，则该采样点集相对于子空间$B$的差异性为： D_N(B,P)=sup_{b\\in B}|\\frac{\\#\\{x_i\\in b\\}}{N}-V(b)|&emsp;&emsp;其中$#\\{x_i\\in b\\}$为采样点散落到子空间$b$的采样点数量，$V(b)$是子空间的体积。上面的符号$sup$应该是$max$的意思。差异性的计算一般需要数值方法来近似求解，很少能够直接求解析解。差异性的主旨思想就是衡量采样点的分布情况，如果分布得不是很均匀，那么其差异性将会很高。 &emsp;&emsp;当然差异性得评估标准也有缺点：一些低差异性的点集内部可能会出现一些点过于聚集。这是低差异性的评估方法的固有缺陷导致。由此衍生了另一种评估方法，计算采样点集中任意两个采样点之间的最小距离，这个最小距离一般是越大越好。泊松圆盘采样（Poisson disk sampling）就是就是基于此评判标准进行采样的。但在实际的用途中，尽管泊松圆盘采样得到的采样点分布质量很高，但是其效率远低于低差异性的采样方法，因此用的比较少。 2、Sampler接口&emsp;&emsp;在光追渲染器中，我们首先创建一个基本的采样器基类，声明一些通用的接口以供不同的采样策略进行复用。采样器基类如下所示： 1234567891011121314151617181920212223242526272829303132333435// Sampler Declarationsclass Sampler &#123; public: // Sampler Interface virtual ~Sampler(); Sampler(int64_t samplesPerPixel); virtual void StartPixel(const Point2i &amp;p); virtual Float Get1D() = 0; virtual Point2f Get2D() = 0; CameraSample GetCameraSample(const Point2i &amp;pRaster); void Request1DArray(int n); void Request2DArray(int n); virtual int RoundCount(int n) const &#123; return n; &#125; const Float *Get1DArray(int n); const Point2f *Get2DArray(int n); virtual bool StartNextSample(); virtual std::unique_ptr&lt;Sampler&gt; Clone(int seed) = 0; virtual bool SetSampleNumber(int64_t sampleNum); int64_t CurrentSampleNumber() const &#123; return currentPixelSampleIndex; &#125; // Sampler Public Data const int64_t samplesPerPixel; protected: // Sampler Protected Data Point2i currentPixel; int64_t currentPixelSampleIndex; std::vector&lt;int&gt; samples1DArraySizes, samples2DArraySizes; std::vector&lt;std::vector&lt;Float&gt;&gt; sampleArray1D; std::vector&lt;std::vector&lt;Point2f&gt;&gt; sampleArray2D; private: // Sampler Private Data size_t array1DOffset, array2DOffset;&#125;; &emsp;&emsp;samplesPerPixel存储每个像素需要的采样数量，currentPixel存储当前正在进行采样的像素坐标，而currentPixelSampleIndex指明当前进行到第几个采样。某一些应用场景可能会一次性要求获取当前采样向量中的$m$个一维或两维向量，由此，我们声明下面的几个变量： 1234std::vector&lt;int&gt; samples1DArraySizes, samples2DArraySizes;std::vector&lt;std::vector&lt;Float&gt;&gt; sampleArray1D;std::vector&lt;std::vector&lt;Point2f&gt;&gt; sampleArray2D;size_t array1DOffset, array2DOffset; &emsp;&emsp;这里以1D为例。samples1DArraySizes存储每个$m$的大小（因为可能有多个不同数量的一次性请求），然后sampleArray1D存储相应数量的采样点，array1DOffset辅助采样点的获取。然后使用下面的函数进行初始化： 12345void Sampler::Request1DArray(int n) &#123; CHECK_EQ(RoundCount(n), n); samples1DArraySizes.push_back(n); sampleArray1D.push_back(std::vector&lt;Float&gt;(n * samplesPerPixel));&#125; &emsp;&emsp;上面的RoundCount用于某些特殊的采样算法对$n$进行调整，例如要求$n$是$2$的幂次方。每次获取sampleArray1D对应的采样序列，并使其加一： 123456const Float *Sampler::Get1DArray(int n) &#123; if (array1DOffset == sampleArray1D.size()) return nullptr; CHECK_EQ(samples1DArraySizes[array1DOffset], n); CHECK_LT(currentPixelSampleIndex, samplesPerPixel); return &amp;sampleArray1D[array1DOffset++][currentPixelSampleIndex * n];&#125; &emsp;&emsp;提供一个GetCameraSample方便直接获取摄像机要用到的采样值： 1234567CameraSample Sampler::GetCameraSample(const Point2i &amp;pRaster) &#123; CameraSample cs; cs.pFilm = (Point2f)pRaster + Get2D(); cs.time = Get1D(); cs.pLens = Get2D(); return cs;&#125; &emsp;&emsp;每使用完一个采样点，则调用StartNextSample转换到下一个采样点： 12345bool Sampler::StartNextSample() &#123; // Reset array offsets for next pixel sample array1DOffset = array2DOffset = 0; return ++currentPixelSampleIndex &lt; samplesPerPixel;&#125; &emsp;&emsp;而SetSampleNumber允许我们根据给定的参数sampleNum跳到第sampleNum采样点： 123456bool Sampler::SetSampleNumber(int64_t sampleNum) &#123; // Reset array offsets for next pixel sample array1DOffset = array2DOffset = 0; currentPixelSampleIndex = sampleNum; return currentPixelSampleIndex &lt; samplesPerPixel;&#125; &emsp;&emsp;最后，使用采样器的接口进行采样的过程大致如下： 123456sampler-&gt;StartPixel(p);do &#123; Float v = a(sampler-&gt;Get1D()); v += b(sampler-&gt;Get1D()); v += c(sampler-&gt;Get1D());&#125; while (sampler-&gt;StartNextSample()); 3、PixelSampler接口&emsp;&emsp;我们首先继承采样器接口类实现一个PixelSampler，这个PixelSampler就是逐像素的采样器。每一次在指定的像素位置上，一次性生成指定数量的采样点，然后再逐个获取生成的采样点进行相关的采样计算过程。 12345678910111213141516class PixelSampler : public Sampler &#123; public: // PixelSampler Public Methods PixelSampler(int64_t samplesPerPixel, int nSampledDimensions); bool StartNextSample(); bool SetSampleNumber(int64_t); Float Get1D(); Point2f Get2D(); protected: // PixelSampler Protected Data std::vector&lt;std::vector&lt;Float&gt;&gt; samples1D; std::vector&lt;std::vector&lt;Point2f&gt;&gt; samples2D; int current1DDimension = 0, current2DDimension = 0; RNG rng;&#125;; &emsp;&emsp;构造函数中的nSampledDimensions用于指定采样向量的最大维度。samples1D和samples2D分别存储采样点，每一维度存储samplesPerPixel个生成好的采样数据。以1D为例，索引方式为samples1D[current1DDimension][currentPixelSampleIndex]： 1234567PixelSampler::PixelSampler(int64_t samplesPerPixel, int nSampledDimensions) : Sampler(samplesPerPixel) &#123; for (int i = 0; i &lt; nSampledDimensions; ++i) &#123; samples1D.push_back(std::vector&lt;Float&gt;(samplesPerPixel)); samples2D.push_back(std::vector&lt;Point2f&gt;(samplesPerPixel)); &#125;&#125; &emsp;&emsp;因此Get1D函数定义如下，它每次获取一个一维的采样数据，超出预设的最大维度下标，则返回均匀随机数，成员变量rng是随机数生成器（random number generator）： 1234567Float PixelSampler::Get1D() &#123; CHECK_LT(currentPixelSampleIndex, samplesPerPixel); if (current1DDimension &lt; samples1D.size()) return samples1D[current1DDimension++][currentPixelSampleIndex]; else return rng.UniformFloat();&#125; 4、GlobalSampler接口&emsp;&emsp;除了逐像素一次性生成的采样方法，还有一些其他的采样算法是在整个图像空间生成采样，然后将采样点播撒到图像中的像素上。也就是连续生成的采样不再是全部由单个像素占有并使用，而是一个个地分发到不同的像素上。以下图一类采样算法生成的采样序列为例，中间一列给出了生成的前二维采样序列，考虑$2\\times 3$大小的图像，这些前二维采样值乘上图像的分辨率得到相应的像素采样坐标（向下取整）。 &emsp;&emsp;可以看到在该采样算法中，采样索引为$0$、$6$、$12$的采样点对应着$(0,0)$这个像素坐标，其他依次类推。GlobalSampler的一些接口和通用函数声明如下所示： 12345678910111213141516171819class GlobalSampler : public Sampler &#123; public: // GlobalSampler Public Methods bool StartNextSample(); void StartPixel(const Point2i &amp;); bool SetSampleNumber(int64_t sampleNum); Float Get1D(); Point2f Get2D(); GlobalSampler(int64_t samplesPerPixel) : Sampler(samplesPerPixel) &#123;&#125; virtual int64_t GetIndexForSample(int64_t sampleNum) const = 0; virtual Float SampleDimension(int64_t index, int dimension) const = 0; private: // GlobalSampler Private Data int dimension; int64_t intervalSampleIndex; static const int arrayStartDim = 5; int arrayEndDim;&#125;; &emsp;&emsp;这里有两个关键的接口辅助此类采样器的实现： 12virtual int64_t GetIndexForSample(int64_t sampleNum) const = 0;virtual Float SampleDimension(int64_t index, int dimension) const = 0; &emsp;&emsp;GetIndexForSample输入采样点的索引sampleNum，根据当前的像素坐标currentPixel返回全局采样索引值（即上图中的sample index）。以上图为例，若当前像素坐标为(0,2)，sampleNum=0，则该接口应该返回第一个出现的对应的像素坐标值为(0,2)的采样的全局索引即2。同理GetIndexForSample(1)应返回8。这里sampleNum是相对于当前像素的采样索引，即当前像素的第一个采样、第二个采样，依次类推。 &emsp;&emsp;而SampleDimension获取全局采样索引index（即上面接口的输出）和要查询的第几维度dimension，返回对应的采样值相对于其所在像素的偏移量。依旧以上图为例，SampleDimension(4,1)返回值为0.333333，首先根据全局采样索引4找到对应的采样向量(0.125000, 0.444444)，乘以图像分辨率得到(0.250000, 1.33333)，减去其对应的像素坐标(0,1)，得到偏移量(0.250000, 0.33333)，然后因为dimension为1，所以返回0.33333。 &emsp;&emsp;尽管是全局生成了所有像素的所有采样点，但是在使用的时候还是逐个像素使用该像素应有的采样点，这需要一些技巧找到给定像素的所有采样点。dimension用于记录当前像素将要获取的当前采样向量中的下一个维度的索引，intervalSampleIndex记录当前采样向量的全局索引。在初始时，我们需要从全局的采样点中获取当前像素应有的采样点，保存到基类的sampleArray1D和sampleArray2D当中： 1234567891011121314151617181920void GlobalSampler::StartPixel(const Point2i &amp;p) &#123; Sampler::StartPixel(p); dimension = 0; intervalSampleIndex = GetIndexForSample(0); // Compute _arrayEndDim_ for dimensions used for array samples arrayEndDim = arrayStartDim + sampleArray1D.size() + 2 * sampleArray2D.size(); // Compute 1D array samples for _GlobalSampler_ for (size_t i = 0; i &lt; samples1DArraySizes.size(); ++i) &#123; int nSamples = samples1DArraySizes[i] * samplesPerPixel; for (int j = 0; j &lt; nSamples; ++j) &#123; int64_t index = GetIndexForSample(j); sampleArray1D[i][j] = SampleDimension(index, arrayStartDim + i); &#125; &#125; // Compute 2D array samples for _GlobalSampler_ // ...&#125; &emsp;&emsp;arrayStartDim和arrayEndDim记录那些需要一次性获取多个一维或二维采样值的下标起始，例如sampleArray1D[arrayStartDim]到arrayStartDim[arrayEndDim]之间的数组保存着这样的采样值。sampleArray1D[0]到sampleArray1D[arrayStartDim]之间保存着用于摄像机的五个维度的采样。而arrayStartDim[arrayEndDim]之后的保存着一次只获取一个一维的采样值。 &emsp;&emsp;值得注意的是sampleArray1D依旧保存的是那些需要一次性获取多个一维采样值，而对于那些一次只获取一个采样值的，我们依旧从全局采样点集中动态获取，2D同理： 12345678910111213141516Float GlobalSampler::Get1D() &#123; ProfilePhase _(Prof::GetSample); if (dimension &gt;= arrayStartDim &amp;&amp; dimension &lt; arrayEndDim) dimension = arrayEndDim; return SampleDimension(intervalSampleIndex, dimension++);&#125;Point2f GlobalSampler::Get2D() &#123; ProfilePhase _(Prof::GetSample); if (dimension + 1 &gt;= arrayStartDim &amp;&amp; dimension &lt; arrayEndDim) dimension = arrayEndDim; Point2f p(SampleDimension(intervalSampleIndex, dimension), SampleDimension(intervalSampleIndex, dimension + 1)); dimension += 2; return p;&#125; 三、完全随机采样&emsp;&emsp;完全随机采样是一种基本的采样方法之一，虽然效果不佳，但是通常被用于与其他的采样方法进行比较，因此有必要创建一个完全随机的采样器。完全随机的采样器很简单，使用随机数生成器直接生成即可： 1234567891011121314151617181920212223242526272829303132333435class RandomSampler : public Sampler &#123; public: RandomSampler(int ns, int seed = 0); void StartPixel(const Point2i &amp;); Float Get1D(); Point2f Get2D(); std::unique_ptr&lt;Sampler&gt; Clone(int seed); private: RNG rng;&#125;;RandomSampler::RandomSampler(int ns, int seed) : Sampler(ns), rng(seed) &#123;&#125;Float RandomSampler::Get1D() &#123; ProfilePhase _(Prof::GetSample); CHECK_LT(currentPixelSampleIndex, samplesPerPixel); return rng.UniformFloat();&#125;Point2f RandomSampler::Get2D() &#123; // ...&#125;void RandomSampler::StartPixel(const Point2i &amp;p) &#123; ProfilePhase _(Prof::StartPixel); for (size_t i = 0; i &lt; sampleArray1D.size(); ++i) for (size_t j = 0; j &lt; sampleArray1D[i].size(); ++j) sampleArray1D[i][j] = rng.UniformFloat(); for (size_t i = 0; i &lt; sampleArray2D.size(); ++i) for (size_t j = 0; j &lt; sampleArray2D[i].size(); ++j) sampleArray2D[i][j] = &#123;rng.UniformFloat(), rng.UniformFloat()&#125;; Sampler::StartPixel(p);&#125; &emsp;&emsp;完全随机的采样方法拥有简单、高效的优点，但其缺点也十分明显。完全随机生成的采样点很容易出现某些采样空间播撒的采样点很少（欠采样）、而另外一些采样空间播撒的采样点很密集（过采样），因此为了能够收敛到正确的结果往往需要生成大量的采样点。 Reference$[1]$ M, Jakob W, Humphreys G. Physically based rendering: From theory to implementation[M]. Morgan Kaufmann, 2016.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/tags/Ray-Tracer/"}]},{"title":"Physically Based Rendering：光谱与辐射度量","slug":"Spectral","date":"2020-04-06T07:20:09.030Z","updated":"2020-04-07T12:38:48.492Z","comments":true,"path":"2020/04/06/Spectral/","link":"","permalink":"https://yangwc.com/2020/04/06/Spectral/","excerpt":"最近捡起了pbrt这本书开始阅读，之前虽然对光线追踪有所得了解，但感觉比较凌乱、不成体系，于是打算深入阅读pbrt，对离线渲染做一个深入的了解。","text":"最近捡起了pbrt这本书开始阅读，之前虽然对光线追踪有所得了解，但感觉比较凌乱、不成体系，于是打算深入阅读pbrt，对离线渲染做一个深入的了解。 &emsp;&emsp;以下的内容大部分整理自pbrt第三版的第五章——COLOR AND RADIOMETRY。 一、Spectral&emsp;&emsp;光谱能量分布（Spectral Power Distribution，简称为SPD）描述了光波波长的辐射功率分布情况，通常采用直角坐标系下的分布曲线来表示，横轴表示波长$\\lambda$，相应地纵坐标表示单位波长间隔内的辐射功率。在计算机图形学中我们通常只关注人眼可见的光波，因而波长取值范围为$400nm-700nm$。现实世界的物体光谱能量分布可以非常复杂，为了高效、准确地描述物体的SPD，人们着手于寻找SPD的低维映射，即寻找一组基，使得可以通过这组基的系数加权来近似表示光谱能量分布情况。 &emsp;&emsp;目前关于光谱表示已经有非常多的研究成果，在这里我们重点关注两个常用的光谱表示方法：基于三原色的光谱表示法和基于采样的光谱表示法。基于三原色的光谱表示法我们非常熟悉，它的表示方法是在红、绿、蓝三原色的基础上作系数加权和，这里的三原色充当着“基”的角色。此方法简单、方便、搞笑，但准确度略低。而基于采样的光谱表示法则是直接对物体的光谱能量分布曲线进行离散地采样，由于采样定理的限制，采样数量通常远多于三个，因而性能方面不如三原色法，但在物理上它更为准确。 &emsp;&emsp;上面提到的两种光谱表示法都可以抽象为基于系数的光谱表示法，具体实现的区别在于“基”的不同和系数数量的不同（即维度的不同）。三原色法有三个系数，表示为一个三维向量。基于采样的光谱表示法的系数数量取决于采样数量，设采样数量为$n$，则它表现为一个$n$维向量。所以创建声明一个CoefficientSpectrum类将基于系数的光谱表示法中的公共函数等实现。 123456789// Spectrum Declarationstemplate &lt;int nSpectrumSamples&gt;class CoefficientSpectrum &#123; // CoefficientSpectrum Public Methods // CoefficientSpectrum Public Dataprotected: // CoefficientSpectrum Protected Data Float c[nSpectrumSamples];&#125;; &emsp;&emsp;关于该类的实现主要是算术运算和其他的辅助运算，即加、减、乘、除、相等判断、开方、指数、对数等等，这些运算都是对向量的逐个分量做的运算，例如开放是对向量的每一个分量开发，得到的结果依旧是一个向量。比较简单，不再细说。 1、基于采样的光谱表示法&emsp;&emsp;基于采样的光谱表示法顾名思义，就是直接对SPD曲线进行采样。只考虑人眼最敏感的波长范围，我们对$[400nm,700nm]$波长范围内的光谱能量分布进行离散采样，采样得到的是一个二元组$(\\lambda_i,v_i)$，即波长$\\lambda_i$对应的辐射功率为$v_i$。在渲染邻域，我们取$60$为采样数量已经足够准确地描述复杂物体的光谱能量分布了，因而基于采样的光谱表示法通常是一个$60$维的向量，采样间隔为$5nm$。 123456789static const int sampledLambdaStart = 400;static const int sampledLambdaEnd = 700;static const int nSpectralSamples = 60;class SampledSpectrum : public CoefficientSpectrum&lt;nSpectralSamples&gt; &#123; public: // SampledSpectrum Public Methods private: // SampledSpectrum Private Data&#125;; &emsp;&emsp;这里我们是等间距地对波长范围进行采样。基于采样的光谱表示法首先要考虑数据的获取，通常从真实的采样数据转换而来。真实的采样数据不一定是等间距地采样，为此我们进行相应的转换。转换的思想其实很简单，例如我们需要获取$[400nm, 405nm)$对应的辐射功率值，则获取落到这个波长范围内的采样数据，计算平均值。 1234567891011121314151617181920static SampledSpectrum FromSampled(const Float *lambda, const Float *v, int n) &#123; // Sort samples if unordered, use sorted for returned spectrum if (!SpectrumSamplesSorted(lambda, v, n)) &#123; std::vector&lt;Float&gt; slambda(&amp;lambda[0], &amp;lambda[n]); std::vector&lt;Float&gt; sv(&amp;v[0], &amp;v[n]); SortSpectrumSamples(&amp;slambda[0], &amp;sv[0], n); return FromSampled(&amp;slambda[0], &amp;sv[0], n); &#125; SampledSpectrum r; for (int i = 0; i &lt; nSpectralSamples; ++i) &#123; // Compute average value of given SPD over $i$th sample's range Float lambda0 = Lerp(Float(i) / Float(nSpectralSamples), sampledLambdaStart, sampledLambdaEnd); Float lambda1 = Lerp(Float(i + 1) / Float(nSpectralSamples), sampledLambdaStart, sampledLambdaEnd); r.c[i] = AverageSpectrumSamples(lambda, v, n, lambda0, lambda1); &#125; return r;&#125; &emsp;&emsp;计算平均值的方法也很简单，以下图为例，我们要计算$[500nm,600nm)$对应的辐射功率值，则计算落到这个范围内的样本值与横坐标构成的面积，将所有的面积加起来再除以横轴范围（这里就是$100nm$）即可。这里要注意就是边界处理情况。 （1）转换到XYZ表色&emsp;&emsp;基于采样的光谱表示法太过耗费空间，人们发现人类视觉系统的特殊性使得仅使用三个浮点数就可以表示人类肉眼感知到的大部分颜色，这就是颜色感知三刺激理论。三刺激理论的三色理论表明，几乎所有的人类可感知的SPD能够仅仅利用三个值$x_{\\lambda}$、$y_{\\lambda}$和$z_{\\lambda}$来表示，这个三个值分别对应着红原色刺激量、绿颜色刺激量和蓝原色刺激量，色的感觉是由于三种原色光刺激的综合结果。 &emsp;&emsp;给定光谱能量分布函数$S(\\lambda)$，三刺激的刺激量通过如下的公式计算： x_{\\lambda}=\\frac{1}{\\int Y(\\lambda)d\\lambda}\\int_\\lambda S(\\lambda)X(\\lambda)d\\lambda \\tag{1} y_{\\lambda}=\\frac{1}{\\int Y(\\lambda)d\\lambda}\\int_\\lambda S(\\lambda)Y(\\lambda)d\\lambda \\tag{2} z_{\\lambda}=\\frac{1}{\\int Y(\\lambda)d\\lambda}\\int_\\lambda S(\\lambda)Z(\\lambda)d\\lambda \\tag{3}&emsp;&emsp;其中$X(\\lambda)$、$Y(\\lambda)$和$Z(\\lambda)$是光谱匹配曲线，如下图所示，由CIE（International Commission on illumination，CIE是法语名的简称）经过实验得到。这些曲线描述了人类视网膜中的三类视锥细胞对三种原色的刺激响应。值得注意的是，不同的光谱能量分布可能会映射到一个非常相似的三刺激值，也就是说人眼观察到不同光谱能量分布的同色光（metamers）。此外，$XYZ$表色法并不是很适用于光谱计算。 &emsp;&emsp;上图中的光谱匹配曲线并没有解析的数学表达式，而是同样地通过离散采样记录保存，这里的采样间隔为$1nm$，因而采样数量为$471$。 12345static const int nCIESamples = 471;extern const Float CIE_X[nCIESamples];extern const Float CIE_Y[nCIESamples];extern const Float CIE_Z[nCIESamples];extern const Float CIE_lambda[nCIESamples]; &emsp;&emsp;这些采样的匹配曲线同样可以看成是具体的SPD，用这些采样值初始化，得到$XYZ$匹配曲线对应的SPD，用以后续的颜色表示方法的转换。 123456789for (int i = 0; i &lt; nSpectralSamples; ++i) &#123; Float wl0 = Lerp(Float(i) / Float(nSpectralSamples), sampledLambdaStart, sampledLambdaEnd); Float wl1 = Lerp(Float(i + 1) / Float(nSpectralSamples), sampledLambdaStart, sampledLambdaEnd); X.c[i] = AverageSpectrumSamples(CIE_lambda, CIE_X, nCIESamples, wl0, wl1); Y.c[i] = AverageSpectrumSamples(CIE_lambda, CIE_Y, nCIESamples, wl0, wl1); Z.c[i] = AverageSpectrumSamples(CIE_lambda, CIE_Z, nCIESamples, wl0, wl1);&#125; &emsp;&emsp;然后直接套用上面的转换公式即可，将积分转换成黎曼和的形式求解。注意到$\\int Y(\\lambda)d\\lambda$与要转换的SPD无关，所以可以提前求解出来然后直接使用。 x_\\lambda=\\frac{1}{\\int Y(\\lambda)d\\lambda}\\frac{\\lambda_{end}-\\lambda_{start}}{N}\\Sigma_{i=0}^{N-1}X_i c_i1234567891011121314static const Float CIE_Y_integral = 106.856895;void ToXYZ(Float xyz[3]) const &#123; xyz[0] = xyz[1] = xyz[2] = 0.f; for (int i = 0; i &lt; nSpectralSamples; ++i) &#123; xyz[0] += X.c[i] * c[i]; xyz[1] += Y.c[i] * c[i]; xyz[2] += Z.c[i] * c[i]; &#125; Float scale = Float(sampledLambdaEnd - sampledLambdaStart) / Float(CIE_Y_integral * nSpectralSamples); xyz[0] *= scale; xyz[1] *= scale; xyz[2] *= scale;&#125; &emsp;&emsp;$XYZ$表色有个明显的作用就是它的$Y$分量，$Y$分量衡量人眼感知的颜色亮度，因而在某些场合被使用。 （2）转换到RGB表色&emsp;&emsp;上述提到的$XYZ$表色并不与我们熟悉的$RGB$表色划上等号，$XYZ$表色是完全基于人类视觉感知建立的颜色空间，而$RGB$表色则不是。$XYZ$表色系统在$RGB$系统的基础上，选用三个理想的原色来代替实际的三原色，从而将$RGB$表色系统中的光谱三刺激值$r$、$g$和$b$均变为正值。$RGB$表色系统的三原色与显示设备密切相关，在不同的显示设备之间存在差异，因而相同$rgb$在不同设备上的显示效果也存在着一定程度的差异。SPD转换到$RGB$表色并不是直接转换，而是先将其转换到$XYZ$表色，然后再转换到$RGB$表色系统。 &emsp;&emsp;转换到$RGB$同样需要借助光谱响应曲线$R(\\lambda)$、$G(\\lambda)$和$B(\\lambda)$，这些光谱响应曲线实际就是与显示设备相关的三原色光谱能量分布曲线，不同的显示设备具有不同的响应曲线： r=\\int R(\\lambda)S(\\lambda)d\\lambda=\\int R(\\lambda)(x_\\lambda X(\\lambda)+y_\\lambda Y(\\lambda)+z_\\lambda Z(\\lambda))d\\lambda \\\\ =x_\\lambda \\int R(\\lambda)X(\\lambda)d\\lambda+y_\\lambda \\int R(\\lambda)Y(\\lambda)d\\lambda+z_\\lambda\\int R(\\lambda)Z(\\lambda)d\\lambda \\tag {4}&emsp;&emsp;公式$(4)$给出了转换到$r$分量的公式，可以看到最终积分里面的被积公式都是可以提前知道，因此可以提前计算好然后直接使用。其他分量的计算公式类似，只需替换相应的光谱响应曲线即可，从$XYZ$到$RGB$的转换可以写成下面的矩阵向量相乘形式表示： \\left[\\begin{matrix} r \\\\ g \\\\ b \\end{matrix} \\right] = \\left( \\begin{matrix} \\int R(\\lambda)X(\\lambda)d\\lambda & \\int R(\\lambda)Y(\\lambda)d\\lambda & \\int R(\\lambda)Z(\\lambda)d\\lambda\\\\ \\int G(\\lambda)X(\\lambda)d\\lambda & \\int G(\\lambda)Y(\\lambda)d\\lambda & \\int G(\\lambda)Z(\\lambda)d\\lambda\\\\ \\int B(\\lambda)X(\\lambda)d\\lambda & \\int B(\\lambda)Y(\\lambda)d\\lambda & \\int B(\\lambda)Z(\\lambda)d\\lambda \\end{matrix} \\right) \\left[ \\begin{matrix} x_\\lambda \\\\ y_\\lambda \\\\ z_\\lambda \\end{matrix} \\right] \\tag {5}&emsp;&emsp;矩阵内的积分值都可以提前计算好，省去极大的转换开销。这里实现了转换到高清电视的$RGB$表色系统： 12345inline void XYZToRGB(const Float xyz[3], Float rgb[3]) &#123; rgb[0] = 3.240479f * xyz[0] - 1.537150f * xyz[1] - 0.498535f * xyz[2]; rgb[1] = -0.969256f * xyz[0] + 1.875991f * xyz[1] + 0.041556f * xyz[2]; rgb[2] = 0.055648f * xyz[0] - 0.204043f * xyz[1] + 1.057311f * xyz[2];&#125; &emsp;&emsp;相应的从$RGB$转换到$XYZ$则直接使用上述矩阵的逆矩阵即可： 12345inline void RGBToXYZ(const Float rgb[3], Float xyz[3]) &#123; xyz[0] = 0.412453f * rgb[0] + 0.357580f * rgb[1] + 0.180423f * rgb[2]; xyz[1] = 0.212671f * rgb[0] + 0.715160f * rgb[1] + 0.072169f * rgb[2]; xyz[2] = 0.019334f * rgb[0] + 0.119193f * rgb[1] + 0.950227f * rgb[2];&#125; &emsp;&emsp;因此，从SPD转换到$RGB$表色，首先转换到$XYZ$表色，最后转换到$RGB$表色系统： 12345void ToRGB(Float rgb[3]) const &#123; Float xyz[3]; ToXYZ(xyz); XYZToRGB(xyz, rgb);&#125; 2、基于三原色的光谱表示法&emsp;&emsp;这里说的光谱表示法就是我们最为熟悉、最为常用的$RGB$表色系统，三原色分别是红、绿、蓝，$RGB$向量本质上是这三种原色的叠加权重。$RGB$表色系统比较简单，诸多细节不再赘述。这里提一下从$SPD$转换到$RGB$，就是先转换到$XYZ$，然后再转换到$RGB$： 12345678910111213141516171819202122static RGBSpectrum FromSampled(const Float *lambda, const Float *v, int n) &#123; // Sort samples if unordered, use sorted for returned spectrum if (!SpectrumSamplesSorted(lambda, v, n)) &#123; std::vector&lt;Float&gt; slambda(&amp;lambda[0], &amp;lambda[n]); std::vector&lt;Float&gt; sv(&amp;v[0], &amp;v[n]); SortSpectrumSamples(&amp;slambda[0], &amp;sv[0], n); return FromSampled(&amp;slambda[0], &amp;sv[0], n); &#125; Float xyz[3] = &#123;0, 0, 0&#125;; for (int i = 0; i &lt; nCIESamples; ++i) &#123; Float val = InterpolateSpectrumSamples(lambda, v, n, CIE_lambda[i]); xyz[0] += val * CIE_X[i]; xyz[1] += val * CIE_Y[i]; xyz[2] += val * CIE_Z[i]; &#125; Float scale = Float(CIE_lambda[nCIESamples - 1] - CIE_lambda[0]) / Float(CIE_Y_integral * nCIESamples); xyz[0] *= scale; xyz[1] *= scale; xyz[2] *= scale; return FromXYZ(xyz);&#125; （1）转换到SPD&emsp;&emsp;既然能够将$SPD$转换到$RGB$，人们也希望能够实现从$RGB$转换到$SPD$，但这并非那么容易。前面我们提到了不同能量分布的同色光，即不同能量分布的$SPD$转换到相近甚至相同的$RGB$，这通常是一个多对一的映射关系。所以一个$RGB$对应着无穷多个$SPD$。为此设定了几个标准用于转换： 如果$RGB$的全部系数均是相同的值，则转换得到的$SPD$应该是一个常量； 转换得到的SPD应该尽可能地光滑，因为真实世界大多数物体都具有相对光滑的$SPD$。 &emsp;&emsp;一种想法是直接根据$RGB$系数对光谱响应曲线$R(\\lambda)$、$G(\\lambda)$和$B(\\lambda)$加权叠加得到相应的$SPD$，但得到的$SPD$往往不是很光滑，因此通常并不这么做。关于这方面的内容pbrt讲述得不是很清楚，大概的做法就是为红、绿、蓝以及三原色的混合色（白、青、黄、紫红）各自计算单独的光滑SPD（可以提前计算并保存好），然后再加权混合得到。这些原色及混合色的SPD本身还可以分成反射颜色和照明颜色，这是因为照明光谱和反射光谱存在着差异。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162static const int nRGB2SpectSamples = 32;extern const Float RGB2SpectLambda[nRGB2SpectSamples];extern const Float RGBRefl2SpectWhite[nRGB2SpectSamples];extern const Float RGBRefl2SpectCyan[nRGB2SpectSamples];extern const Float RGBRefl2SpectMagenta[nRGB2SpectSamples];extern const Float RGBRefl2SpectYellow[nRGB2SpectSamples];extern const Float RGBRefl2SpectRed[nRGB2SpectSamples];extern const Float RGBRefl2SpectGreen[nRGB2SpectSamples];extern const Float RGBRefl2SpectBlue[nRGB2SpectSamples];extern const Float RGBIllum2SpectWhite[nRGB2SpectSamples];extern const Float RGBIllum2SpectCyan[nRGB2SpectSamples];extern const Float RGBIllum2SpectMagenta[nRGB2SpectSamples];extern const Float RGBIllum2SpectYellow[nRGB2SpectSamples];extern const Float RGBIllum2SpectRed[nRGB2SpectSamples];extern const Float RGBIllum2SpectGreen[nRGB2SpectSamples];extern const Float RGBIllum2SpectBlue[nRGB2SpectSamples];//...SampledSpectrum SampledSpectrum::FromRGB(const Float rgb[3], SpectrumType type) &#123; SampledSpectrum r; if (type == SpectrumType::Reflectance) &#123; // Convert reflectance spectrum to RGB if (rgb[0] &lt;= rgb[1] &amp;&amp; rgb[0] &lt;= rgb[2]) &#123; // Compute reflectance _SampledSpectrum_ with _rgb[0]_ as minimum r += rgb[0] * rgbRefl2SpectWhite; if (rgb[1] &lt;= rgb[2]) &#123; r += (rgb[1] - rgb[0]) * rgbRefl2SpectCyan; r += (rgb[2] - rgb[1]) * rgbRefl2SpectBlue; &#125; else &#123; r += (rgb[2] - rgb[0]) * rgbRefl2SpectCyan; r += (rgb[1] - rgb[2]) * rgbRefl2SpectGreen; &#125; &#125; else if (rgb[1] &lt;= rgb[0] &amp;&amp; rgb[1] &lt;= rgb[2]) &#123; // Compute reflectance _SampledSpectrum_ with _rgb[1]_ as minimum r += rgb[1] * rgbRefl2SpectWhite; if (rgb[0] &lt;= rgb[2]) &#123; r += (rgb[0] - rgb[1]) * rgbRefl2SpectMagenta; r += (rgb[2] - rgb[0]) * rgbRefl2SpectBlue; &#125; else &#123; r += (rgb[2] - rgb[1]) * rgbRefl2SpectMagenta; r += (rgb[0] - rgb[2]) * rgbRefl2SpectRed; &#125; &#125; else &#123; // Compute reflectance _SampledSpectrum_ with _rgb[2]_ as minimum r += rgb[2] * rgbRefl2SpectWhite; if (rgb[0] &lt;= rgb[1]) &#123; r += (rgb[0] - rgb[2]) * rgbRefl2SpectYellow; r += (rgb[1] - rgb[0]) * rgbRefl2SpectGreen; &#125; else &#123; r += (rgb[1] - rgb[2]) * rgbRefl2SpectYellow; r += (rgb[0] - rgb[1]) * rgbRefl2SpectRed; &#125; &#125; r *= .94; &#125; else &#123; // Convert illuminant spectrum to RGB // .... &#125; return r.Clamp();&#125; 二、Radiometry&emsp;&emsp;辐射度量学（Radiometry）是研究光线传播、反射等光学属性的数学工具，通过它人们构建了图形学中最为重要的渲染方程。辐射度量学是基于辐射测量原理的学科，它本质上属于几何光学的范畴（对应的是波动光学）。使用几何光学模型描述光线散射行为通常有如下的前提假设： 线性：光学系统中两个效果的组合等价于各自单独效果的叠加； 能量守恒：顾名思义，不解释； 无偏振：忽略电磁场的偏振效果，因此光的唯一相关属性就是波长分布； 不考虑荧光和磷光：不同波长的光波之间互不相干； 状态稳定：光在传播的过程中已经达到平衡状态，其辐射率分布不会随着时间发生变化。 &emsp;&emsp;几何光学模型不能用于描述光的衍射和干涉，因此无力对此类的物理现象进行建模，相应地也就无法物理准确地实现此类现象。在图形渲染领域，用到的辐射度量学物理量主要有四个，分别是辐射通量（flux）、辐照度（irradiance）、辐射强度（intensity）和辐射率（radiance）。需要注意的是，这些辐射物理量通常是波长相关的。 1、辐射能量（Energy）&emsp;&emsp;首先从辐射能量（通常用$Q$表示）开始说起，辐射能量的单位就是焦耳（joules，简称J）。能量的定义初中物理就已经学过。光源向外辐射光子，这些光子有各自的波长取值并携带一定的能量。所有的基础辐射物理量都在用不同的方式描述辐射出来的光子。 2、辐射通量（Flux）&emsp;&emsp;辐射能量描述的仅仅是某一时间段内辐射出来的总能量，我们并不关心。我们更关注的是单位时间的辐射能量，也就是功率（Power），在这里我们也称之为辐射通量（通常用$\\Phi$标记），单位为瓦特（watt，简称w），其数学定义为： \\Phi=lim_{\\Delta t \\to 0}\\frac{\\Delta Q}{\\Delta t} = \\frac{dQ}{dt} \\tag {6}&emsp;&emsp;对于一个发光体，我们更倾向于使用功率也就是通量来描述它的发光亮度。例如$10W$的灯泡亮度明显不如$30W$的灯泡，这里就是用辐射通量来衡量。 3、辐照度（Irradiance）&emsp;&emsp;有了辐射通量之后，我们还需要描述在物体单位面积上的辐射通量，这是因为有时我们仅仅关注某一个固定区域的辐射行为。由此衍生了辐照度的定义，即单位面积上的辐射通量，符号记为$E$： E(p)=lim_{\\Delta A\\to 0}\\frac{\\Delta \\Phi(p)}{\\Delta A}=\\frac{d\\Phi(p)}{dA} \\tag {7}&emsp;&emsp;辐照度可以分成两类，分别是入射辐照度和出射辐照度，两者分别对应着入和出。入射辐照度，顾名思义就是对于一个给定区域，单位时间接收到的辐射能量；而出射辐照度就是在辐射体上的一个给定区域，单位时间辐射出去的能量。引入辐照度的概念对光照计算有非常重要的意义，以下图为例，设光源辐射通量为$\\phi$，光源向四周均匀地辐射能量，因此辐射到给定半径$r$的圆上的辐照度为$E=\\frac{\\Phi}{4\\pi r^2}$。从这个辐照度公式可以看出，半径越大则辐照度越低，这是因为光源的辐射通量是固定的，接收面积越大则平摊到单位面积上的能量越少。这个正好解释了点光源的衰减公式是距离平方的反比。 &emsp;&emsp;此外，辐照度公式也解释了渲染方程中的Lambert定律（或者说余弦定律），Lambert定律指出，一定数量的光能到达物体表面的比例正比于入射方向与表面法线夹角的余弦值。这是因为辐照度的定义中的面积是必须与入射方向垂直，当表面与入射方向垂直时，入射辐照度就为$\\frac{\\Phi}{A}$。而当表面与入射方向不垂直的时候，就需要将表面投影到与入射方向垂直$Acos\\theta$，此时辐照度为$\\frac{\\Phi}{Acos\\theta}$。辐照度是方向无关的，可以理解成所有方向作用的结果，即从所有方向入射和向所有方向出射。 4、辐射强度（Intensity）&emsp;&emsp;在三维空间中，我们通常采用立体角来表示一定范围的方向。立体角（单位为立体弧度，简称sr）就是二维弧度角的三维推广，这里不再赘述。为什么用立体角而不是直接用方向向量？这是因为三维空间的方向有无穷多个，用立体角更加方便。辐照度衡量单位面积上的辐射通量，而相应地，辐射强度就是衡量单位立体角上的辐射通量，通常标记为$I$： I=lim_{\\Delta \\omega\\to 0}\\frac{\\Delta \\phi}{\\Delta \\omega}=\\frac{d\\Phi}{d\\omega} \\tag {8}&emsp;&emsp;从物理意义上理解，就是在单位方向上的辐射功率。对于一个向外均匀辐射的点光源，其辐射强度是$I=\\frac{\\Phi}{4\\pi}$。辐射强度是方向相关的，我们通常设定的点光源的辐射能量实际上是辐射强度而不是辐射通量。 5、辐射率（Radiance）&emsp;&emsp;辐照度和辐射强度分别衡量了在面积、在方向上的辐射通量分布情况，但我们需要一个物理量即衡量辐射通量在面积上的分布情况又衡量在方向上的分布情况，这就是辐射率。辐射率就是单位面积、单位立体角上的辐射通量，标记为$L$，其定义为： L=\\frac{d\\phi}{d\\omega dA} \\tag {9}&emsp;&emsp;注意这里的$dA$是投影面积。根据入射和出射两个不同的方向，辐射率可以分别地被解读为入射辐射率和出射辐射率。解读为入射辐射率时，它可以被理解成指定在入射方向（这里说的方向通常时立体角）上的辐照度： L=\\frac{d\\phi}{d\\omega dA}=\\frac{dE}{d\\omega} \\tag {19}&emsp;&emsp;如下图所示，这时$\\omega$是光线入射方向，我们在入射辐照度的基础上限定了方向，可以理解为仅仅在$\\omega$这个方向入射进来的辐照度，标记为$L_i(p,\\omega)$。 &emsp;&emsp;被解读为出射辐射率的时候，可以理解成向指定的出射方向上的出射辐照度，标记为$L_o(p,\\omega)$,此时$\\omega$是光线的出射方向。我们计算渲染方程时，本质上就是在计算着色点在观察方向上的出射辐射率。 三、辐射积分形式的转换&emsp;&emsp;渲染方程本质上就是围绕着相关的辐射物理量做积分计算，因而其积分形式根据积分变量的不同有着不同的形式，但本质上都是一样。下面以计算辐照度的积分方程为例（注意，这并不是渲染方程）： E(p,n)=\\int _{\\Omega} L_i(p,\\omega)|cos\\theta| d\\omega \\tag {11}&emsp;&emsp;计算给定着色点$p$上的辐照度就是对以法线为中心轴的半球方向的入射辐射率进行积分，$\\theta$是入射方向与法线方向的夹角。这个积分公式中的积分变量是立体角$\\Omega$。给定一个微分立体角$d\\omega$，我们能将其转换到球面坐标系下的表示形式（如下图所示）$d\\omega=sin\\theta d\\theta d\\phi$。 &emsp;&emsp;故公式$(11)$对立体角的积分可以转换成对球面坐标$(\\theta,\\phi)$的双重积分形式： E(p,n)=\\int_{0}^{2\\pi}\\int_0^{\\pi/2}L_i(p,\\theta,\\phi)cos\\theta sin\\theta d\\theta d\\phi \\tag {12}&emsp;&emsp;公式$(11)$和$(12)$本质上是对所有的入射方向积分，但有时对方向积分不是很方便。对所有入射方向的积分等价于对所有辐射入射光的表面积分，因此有时亦转换成对面积微元的积分形式。如下图所示，考虑转换成$dA$的形式。注意到微分立体角的定义，我们有$d\\omega=dA cos\\theta/r^2$，这里$\\theta$是$dA$表面上的法线与$p$点到$dA$向量的夹角，$r$是点$p$到$dA$的直线距离，本质上就是将$dA$投影到垂直方向，然后除以距离的平方。 E(p,n)=\\int_A L_i cos\\theta_i\\frac{cos\\theta_o dA}{r^2} \\tag {13}四、表面散射与次表面散射&emsp;&emsp;当一束光照射到物体表面时，会产生一系列的光线散射行为。在图形渲染领域，我们从两个主要的方面来对光线散射进行建模：散射光线的光谱分布和散射光线的方向分布。散射光线的光谱分布描述了散射光的光谱能量分布的变化，例如一道白光打到橘子上面，则白光中的蓝色光波大部分被吸收，而红色和绿色光波大部分被反射，从而反射出橘红色的外观。散射光线的方向分布描述了散射的光线在空间中的方向分布性，给定一个方向，它评估朝向这个方向散射的光线比例。这里说的散射主要包含反射、折射和次表面散射，其中反射和折射可以统称为散射，而次表面散射要复杂得多。 &emsp;&emsp;针对光线的散射和次表面散射，目前主要两类数学函数对此进行描述：BSDF和BSSRDF。BSDF描述了物体表面的散射特性，仅考虑光线的反射与折射，忽略次表面散射。对于那些次表面散射效果不明显的物体来说是个非常明显的优化。而BSSRDF是一个更为通用的光线散射模型，它包含了BSDF，除此之外还考虑次表面散射的复杂效果。 1、BSDF函数&emsp;&emsp;BSDF的全称是Bidirectional Scattering Distribution Function，即双向散射分布函数。根据反射和折射的不同，BSDF又可以分为BRDF和BTDF。BRDF全称为Bidirectional Reflectance Distribution Function，即双向反射分布函数，BRDF在图形渲染领域用的最多，它描述了反射光线占据入射光线的比例。在着色点$p$上，给定入射方向$\\omega_i$及入射辐射率$L_i(p,\\omega_i)$，我们要计算在观察方向$\\omega_o$上的出射辐射率。 &emsp;&emsp;首先可以计算$p$上的微分辐照度： dE(p,\\omega_i)=L_i(p,\\omega_i)cos\\theta_id\\omega_i&emsp;&emsp;几何光学的线性前提指出，反射的微分辐射率应该正比于此微分辐照度： dL_o(p,\\omega_o)∝ dE(p,\\omega_i)&emsp;&emsp;事实上，这个正比关系的比例就是BRDF的定义，即反射辐射率占据入射辐照度的比例，通常记为$f_r(p,\\omega_o,\\omega_i)$： f_r(p,\\omega_o,\\omega_i)=\\frac{dL_o(p,\\omega_o)}{dE(p,\\omega_i)} \\tag {14}&emsp;&emsp;一个基于物理的BRDF应该具有如下两个重要的属性： 互逆性：即$f_r(p,\\omega_i,\\omega_o)=f_r(p,\\omega_o,\\omega_i)$； 能量守恒：反射的总能量应该不能超过入射的总能量，即要求（$H^2(n)$是半球方向）： \\int_{H^2(n)} f_r(p,\\omega_o,\\omega_i) cos\\theta_i d\\omega_i \\leq 1&emsp;&emsp;而BTDF全称为Bidirectional Transmittance Distribution Function，即双向透射分布函数。它的定义与BRDF类似，只不过描述的是光线透射（折射）占据入射能量的比例，通常记为$f_t(p,\\omega_o,\\omega_i)$。将$f_r(p,\\omega_o,\\omega_i)$和$f_t(p,\\omega_o,\\omega_i)$综合起来就是BSDF，记为$f(p,\\omega_o,\\omega_i)$。有了光线的散射比例值，我们就可以计算光线散射辐射率： dL_o(p,\\omega_o)=f(p,\\omega_o,\\omega_i)L_i(p,\\omega_i)|cos\\theta_i|d\\omega_i \\tag {15}&emsp;&emsp;上述公式不难理解，就是在入射辐照度的基础上再乘以比例系数BSDF，得到散射的辐射率。上面公式中我们取$cos\\theta_i$的绝对值，这是因为法线向量不一定与散射方向在表面的同一侧上（例如折射的时候）。综合考虑反射与透射，对着色点上的整个球体方向的出射辐射率进行积分，就可以得到最终的出射辐射率： L_o(p,\\omega_o)=\\int_{S^2}f(p,\\omega_o,\\omega_i)L_i(p,\\omega_i)|cos\\theta_i|d\\omega_i \\tag {16}&emsp;&emsp;上述的公式就是渲染领域的最基础、最重要的方程——散射方程，它的积分定义域是整个球体方向（而非半球，因为这里综合考虑了透射）。当散射方程$(16)$仅仅考虑表面上的半球方向$H(n^2)$时，也就是仅考虑反射时，它就变成了反射方程。 2、BSSRDF函数&emsp;&emsp;可以看到BSDF函数仅考虑一个点，这个点就是着色点$p$，光线的入射、折射和反射都是直接在$p$上进行。但真实物理世界还有一个更为复杂的现象——次表面散射，简单来说就是光线从一个表面的点上进入，经过内部的散射，最终从另外一个点射出（如下图所示）。BSSRDF全称为Bidirectional Scattering Surface Reflectance Distribution Function，即双向散射表面反射分布函数，它的输入参数有四个，分别是入射方向$\\omega_i$、入射点$p_i$、出射方向$\\omega_o$和出射点$p_o$，通常记为$S(p_o,\\omega_o,p_i,\\omega_i)$。 &emsp;&emsp;BSSRDF定义为点出射到$p_o$的$\\omega_o$方向上的微分辐射率占从$\\omega_i$入射到$p_i$点的微分辐射通量的比值： S(p_o,\\omega_o,p_i,\\omega_i)=\\frac{dL_o(p_o,\\omega_o)}{d\\Phi(p_i,\\omega_i)} \\tag {17}&emsp;&emsp;由此可得到BSSRDF对应的渲染方程，这个渲染方程出了考虑整个半球方向，还要考虑物体的整个表面，因而是关于半球方向和表面区域的四重积分，多了两重对物体表面的积分： L_o(p_o,\\omega_o)=\\int_A \\int_{H^2(n)}S(p_o,\\omega_o,p_i,\\omega_i)L_i(p_i,\\omega_i)|cos\\theta_i|d\\omega_i dA \\tag {18}&emsp;&emsp;上述给出了物理准确的通用的渲染方程，但四重积分真的难顶。注意到随着$p_i$和$p_o$的距离增大，$S$的取值逐渐减小，因此这是一个可行优化方向。 Reference$[1]$ M, Jakob W, Humphreys G. Physically based rendering: From theory to implementation[M]. Morgan Kaufmann, 2016.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/tags/Ray-Tracer/"}]},{"title":"Physically Based Rendering：摄像机模型","slug":"Camera","date":"2020-04-04T07:44:51.185Z","updated":"2020-04-11T14:44:05.083Z","comments":true,"path":"2020/04/04/Camera/","link":"","permalink":"https://yangwc.com/2020/04/04/Camera/","excerpt":"pbrt专门有一章讲述了目前离线渲染的几种摄像机模型，图形学中的摄像机大多数为针孔摄像机模型，然而现实生活中并不存在这类的摄像机，它忽略了真实摄像头中的光穿过透镜的一些效应。为了实现景深和运动模糊的效果，需要做一些额外的设定。","text":"pbrt专门有一章讲述了目前离线渲染的几种摄像机模型，图形学中的摄像机大多数为针孔摄像机模型，然而现实生活中并不存在这类的摄像机，它忽略了真实摄像头中的光穿过透镜的一些效应。为了实现景深和运动模糊的效果，需要做一些额外的设定。 &emsp;&emsp;以下的内容大部分整理自pbrt第三版的第六章——CAMERA MODELS。 一、摄像机概述&emsp;&emsp;在光线追踪的渲染算法中，摄像机的主要作用就是根据采样点生成相应的采样光线，这通常涉及到几个空间的变换（通常是从成像空间变换到世界空间，与实时渲染管线相反）。在这里我们首先看看几个空间坐标系的划分，pbrt这里的坐标划分与实时渲染略有不同，但本质上都是一样，仅仅是为了方便生成采样光线而做的一些命名： 物体空间：即物体自身的局部空间，不解释； 世界空间：所有物体摆放的一个统一的空间，不解释； 相机空间：pbrt采用了左手坐标系，因此摄像机朝向$z$轴正向，摄像机位于原点； 屏幕空间：这里说的屏幕空间仅仅是投影平面空间，一般来说近平面就是投影平面，但这里不是，近平面到远平面的$z$值取值$[0,1]$，$xy$值是投影后的取值；（与实时渲染中的屏幕空间不同概念） 标准空间：在屏幕空间的基础上将$xy$规范化到$[0,1]$，深度值保持不变； 光栅空间：在标准空间的基础上将$xy$映射到成像分辨率范围内，即$(0,0)$到$(resolution.x, resolution.y)$，这里对应着实时渲染中的屏幕空间的概念。 &emsp;&emsp;这里特别提一下一些与实时渲染的不同点，投影平面不一定就是近平面（这是合法的）。屏幕空间依旧是一个三维坐标系，因为它蕴含着深度值，而$x$和$y$是投影值。标准空间这里只是过渡，实现时并没有额外地抽离开来。摄像机主要用于生成追踪光线，通常是先将光栅空间的采样点变换到世界空间，然后在世界空间执行光线追踪算法。光栅空间的一个采样如下所示： 12345struct CameraSample &#123; Point2f pFilm; Point2f pLens; Float time;&#125;; &emsp;&emsp;pFilm就是光栅空间的采样点，其余两个变量用于实现景深和运动模糊，这里先忽略。创建一个摄像机的基类，对于一个摄像机来说，最重要的就是从世界空间到相机空间的变换矩阵，这里我们保存它的逆矩阵CameraToWorld。而film即成像胶卷，本质上就是一个二维图像类。shutterOpen和shutterClose保存相机快门的开启和关闭时间，用于模拟运动模糊效果。medium保存环境的介质类型，例如雾。 123456789101112131415161718// Camera Declarationsclass Camera &#123; public: // Camera Interface Camera(const AnimatedTransform &amp;CameraToWorld, Float shutterOpen, Float shutterClose, Film *film, const Medium *medium); virtual ~Camera(); virtual Float GenerateRay(const CameraSample &amp;sample, Ray *ray) const = 0; virtual Float GenerateRayDifferential(const CameraSample &amp;sample, RayDifferential *rd) const; // ...other // Camera Public Data AnimatedTransform CameraToWorld; const Float shutterOpen, shutterClose; Film *film; const Medium *medium;&#125;; &emsp;&emsp;这里摄像机的核心就是GenerateRay，它接收一个光栅空间的采样点，并生成相应的世界空间的采样光线，函数的返回值返回该采样光线对最终辐射率值的贡献权重。除此之外，还有GenerateRayDifferential，它除了生成采样光线，还会对光线执行一些微分操作，它把生成的光线的起始点和方向看作是光栅空间的坐标$x$和$y$的函数，对光线的起始点和方向分别在$x$和$y$方向根据偏微分的值做一定的偏移，这些偏移量蕴含了单个像素间距的采样范围，用以后续的一些抗锯齿操作（例如纹理反走样）。 &emsp;&emsp;上面的摄像机仅仅包含了世界空间与相机空间的变换，我们进一步继承这个基类，实现一个投影摄像机类，它负责将三维世界投影到二维平面（或者相反的过程）。 1234567891011121314151617181920212223242526272829303132class ProjectiveCamera : public Camera &#123; public: // ProjectiveCamera Public Methods ProjectiveCamera(const AnimatedTransform &amp;CameraToWorld, const Transform &amp;CameraToScreen, const Bounds2f &amp;screenWindow, Float shutterOpen, Float shutterClose, Float lensr, Float focald, Film *film, const Medium *medium) : Camera(CameraToWorld, shutterOpen, shutterClose, film, medium), CameraToScreen(CameraToScreen) &#123; // Initialize depth of field parameters lensRadius = lensr; focalDistance = focald; // Compute projective camera transformations // Compute projective camera screen transformations ScreenToRaster = Scale(film-&gt;fullResolution.x, film-&gt;fullResolution.y, 1) * Scale(1 / (screenWindow.pMax.x - screenWindow.pMin.x), 1 / (screenWindow.pMin.y - screenWindow.pMax.y), 1) * Translate(Vector3f(-screenWindow.pMin.x, -screenWindow.pMax.y, 0)); RasterToScreen = Inverse(ScreenToRaster); RasterToCamera = Inverse(CameraToScreen) * RasterToScreen; &#125; protected: // ProjectiveCamera Protected Data Transform CameraToScreen, RasterToCamera; Transform ScreenToRaster, RasterToScreen; Float lensRadius, focalDistance;&#125;; &emsp;&emsp;这里保存了四个矩阵变换变量，名字对应着各自的空间变换，不解释。lensRadius和focalDistance用于实现景深效果，先忽略。这里我们要注意把screenWindow和film区别开来，screenWindow本质上是投影平面上的视口范围，取该平面上的最大点和最小点之间的范围；而film是光栅空间（或者说图像空间），它的取值是$(0,0)$到$(resolution.x, resolution.y)$。film和screenWindow的大小不一定相等，因此需要做一些映射，我们来看看ScreenToRaster的计算： 12345ScreenToRaster = Scale(film-&gt;fullResolution.x, film-&gt;fullResolution.y, 1) * Scale(1 / (screenWindow.pMax.x - screenWindow.pMin.x), 1 / (screenWindow.pMin.y - screenWindow.pMax.y), 1) * Translate(Vector3f(-screenWindow.pMin.x, -screenWindow.pMax.y, 0)); &emsp;&emsp;这里可以拆解成三个部分解读（从右到左），分别是一次平移变换和两次缩放变换。平移变换负责将窗口的左上角变换到原点。然后执行一个缩放变换，负责将投影窗口上的$x$和$y$分别映射到$[0,1]$，然后再缩放到光栅空间的取值范围内。这里注意，屏幕空间的$y$轴依旧是向上的，但光栅空间的$y$轴是朝下的，因此需要取$y$的相反数，这包含在了第一次缩放当中。 二、正交摄像机&emsp;&emsp;首先来看基于正交投影的摄像机，正交投影的相关概念非常简单，不解释。正交投影本质上相当于没有投影，因此$x$和$y$值保持不变，但这里将$z$映射到$[0,1]$。 123Transform Orthographic(Float zNear, Float zFar) &#123; return Scale(1, 1, 1 / (zFar - zNear)) * Translate(Vector3f(0, 0, -zNear));&#125; &emsp;&emsp;对于离线光线追踪来说，投影后的深度值没有太大的意义（不像实时渲染的z-buffer），这里之所以将$z$值做了一个映射是为了使得投影矩阵可逆，如果直接丢弃$z$那么投影矩阵将不可逆。使投影矩阵可逆是因为在这里我们基本上是使用它的逆矩阵而不是正向投影。既然深度值意义不大，所以远近平面的设置可以很随意（因为也不依靠视锥体做裁剪），pbrt这里直接将近平面和远平面设置为$z=0$和$z=1$，对超出这个范围的$z$值做投影也不会有任何的问题。 1234567891011121314151617181920212223// OrthographicCamera Declarationsclass OrthographicCamera : public ProjectiveCamera &#123; public: // OrthographicCamera Public Methods OrthographicCamera(const AnimatedTransform &amp;CameraToWorld, const Bounds2f &amp;screenWindow, Float shutterOpen, Float shutterClose, Float lensRadius, Float focalDistance, Film *film, const Medium *medium) : ProjectiveCamera(CameraToWorld, Orthographic(0, 1), screenWindow, shutterOpen, shutterClose, lensRadius, focalDistance, film, medium) &#123; // Compute differential changes in origin for orthographic camera rays dxCamera = RasterToCamera(Vector3f(1, 0, 0)); dyCamera = RasterToCamera(Vector3f(0, 1, 0)); &#125; Float GenerateRay(const CameraSample &amp;sample, Ray *) const; Float GenerateRayDifferential(const CameraSample &amp;sample, RayDifferential *) const; private: // OrthographicCamera Private Data Vector3f dxCamera, dyCamera;&#125;; &emsp;&emsp;这里提一下dxCamera和dxCamera，这两个变量分别保存了光栅空间的$dx$和$dy$变换到相机空间的值，$dx$在光栅空间就是$x$轴方向的一个像素的偏移值，即Vector3f(1, 0, 0)。将这个偏移值变换到摄像机空间方便我们直接对射出的光线做一个可微偏移（GenerateRayDifferential），dy同理（之所以能够直接对偏移量做变换是因为正交投影不会改变间距和方向）。 &emsp;&emsp;对于正交投影相机来说，它发射的光线的方向都是一样的，为$(0,0,1)$，区别仅在于光线的起始点不同。对光栅空间的采样点做逆变换到摄像机空间就得到了光线的起始点。最后再将光栅的起始点和方向变换到世界空间。 12345678910111213141516// OrthographicCamera DefinitionsFloat OrthographicCamera::GenerateRay(const CameraSample &amp;sample, Ray *ray) const &#123; // Compute raster and camera sample positions Point3f pFilm = Point3f(sample.pFilm.x, sample.pFilm.y, 0); Point3f pCamera = RasterToCamera(pFilm); *ray = Ray(pCamera, Vector3f(0, 0, 1)); // Modify ray for depth of field if (lensRadius &gt; 0) &#123; // ... &#125; ray-&gt;time = Lerp(sample.time, shutterOpen, shutterClose); ray-&gt;medium = medium; *ray = CameraToWorld(*ray); return 1;&#125; 三、透视摄像机&emsp;&emsp;对于三维图形学，透射摄像机用得更广泛一些。透视投影的相关概念这里不再赘述。首先来看下透射投影矩阵的构造，pbrt这里的透视投影固定了投影平面在$z=1$上而不是视锥的近平面，由此简化了很多东西。投影视锥由视域fovy、near和far决定。投影矩阵分为两部分，一个是投影过程，一个是缩放过程。 &emsp;&emsp;首先是投影过程，根据三角形相似的原理就可以得到，注意成像平面是$z=1$而不是近平面。相应的还将$z$值根据近平面和远平面做缩放（意图与正交投影的一样）： x'=x/z\\\\ y'=y/z\\\\ z'=\\frac{f(z-n)}{z(f-n)}&emsp;&emsp;同样地，写成齐次坐标系下的矩阵形式： \\left[ \\begin{matrix} 1 & 0 & 0 & 0\\\\ 0 & 1 & 0 & 0\\\\ 0 & 0 & \\frac{f}{f-n} & -\\frac{fn}{f-n}\\\\ 0 & 0 & 1 & 0 \\end{matrix} \\right]&emsp;&emsp;投影到投影平面上的点再进行缩放，将$x$和$y$缩放到$[-1,1]$。因为成像平面为$z=1$，所以可以直接求出成像平面的宽度的一半为$tan(fov/2)$。所以透视投影矩阵的实现代码如下： 12345678Transform Perspective(Float fov, Float n, Float f) &#123; // Perform projective divide for perspective projection Matrix4x4 persp(1, 0, 0, 0, 0, 1, 0, 0, 0, 0, f / (f - n), -f * n / (f - n), 0, 0, 1, 0); // Scale canonical perspective view to specified field of view Float invTanAng = 1 / std::tan(Radians(fov) / 2); return Scale(invTanAng, invTanAng, 1) * Transform(persp);&#125; &emsp;&emsp;可以看到，最后缩放的时候我们直接忽略了投影平面上的屏幕宽高的比例，对于正方形屏幕来说没有问题。而对于长方形的屏幕来说，较短的那条边被映射到$[-1,1]$，而较长的那条边会根据比例进行适当地延长（pbrt默认screenWindow的较短边长范围$[-1,1]$，较长边长按照图像的宽高等比例放大）。相应地，为了对光线微分，我们提前计算好dxCamera和dyCamera： 12345// Compute differential changes in origin for perspective camera raysdxCamera = (RasterToCamera(Point3f(1, 0, 0)) - RasterToCamera(Point3f(0, 0, 0)));dyCamera = (RasterToCamera(Point3f(0, 1, 0)) - RasterToCamera(Point3f(0, 0, 0))); &emsp;&emsp;这里跟正交投影部分略有不同。我们是先将光栅空间的两个相邻点做逆变换到相机空间，然后再相减，而不是直接将光栅空间的间隔做逆变换，这是因为透视投影对间隔做了扭曲了，直接对间隔变换将得到的错误的结果。然后就是根据光栅空间的采样点生成相应的光线，与正交投影相反，透射相机生成的光线方向各不相同，但起始点都一样，均为摄像机的位置（即$(0,0,0)$。 123456789101112131415Float PerspectiveCamera::GenerateRay(const CameraSample &amp;sample, Ray *ray) const &#123; // Compute raster and camera sample positions Point3f pFilm = Point3f(sample.pFilm.x, sample.pFilm.y, 0); Point3f pCamera = RasterToCamera(pFilm); *ray = Ray(Point3f(0, 0, 0), Normalize(Vector3f(pCamera))); // Modify ray for depth of field if (lensRadius &gt; 0) &#123; // ... &#125; ray-&gt;time = Lerp(sample.time, shutterOpen, shutterClose); ray-&gt;medium = medium; *ray = CameraToWorld(*ray); return 1;&#125; 四、环境摄像机&emsp;&emsp;除了用于观察的摄像机，还有一种摄像机模型类似于真实世界中的广角镜头。如下图所示，这种图片展现了周围$360$度所有方向的镜像，因此有一定的扭曲，此类摄像机我们称之为环境摄像机。类似于一个天空盒，把周围六个面的图像组合到一张图像中。 &emsp;&emsp;实现此类摄像机不难，我们直接朝摄像机周围的所有发射追踪的光线。但是我们最终成像平面是二维的，所以需要做一定的转换。注意到所有的方向可以用中心在原点的单位球体上的所有点表示，这些方向向量可以转换成球面坐标$(\\theta,\\phi)$的形式，$\\theta$取值范围为$[0,\\pi]$，$\\phi$取值$[0,2\\pi]$。所以对于光栅空间的$(x,y)$，我们可以将$x$与$\\phi$对应，将$y$与$\\theta$对应，由此对所有的方向进行采样。 &emsp;&emsp;三维方向向量和球面坐标向量的互相转换不难，这里不再赘述。值得一提的是，此类摄像机渲染出来的图像通常用于IBL，即Image Basde Lighting，基于图像的光照。 123456789101112131415// EnvironmentCamera Method DefinitionsFloat EnvironmentCamera::GenerateRay(const CameraSample &amp;sample, Ray *ray) const &#123; ProfilePhase prof(Prof::GenerateCameraRay); // Compute environment camera ray direction Float theta = Pi * sample.pFilm.y / film-&gt;fullResolution.y; Float phi = 2 * Pi * sample.pFilm.x / film-&gt;fullResolution.x; Vector3f dir(std::sin(theta) * std::cos(phi), std::cos(theta), std::sin(theta) * std::sin(phi)); *ray = Ray(Point3f(0, 0, 0), dir, Infinity, Lerp(sample.time, shutterOpen, shutterClose)); ray-&gt;medium = medium; *ray = CameraToWorld(*ray); return 1;&#125; 五、薄透镜近似模型&emsp;&emsp;真实的摄像机往往涉及到光圈的概念，这个所谓的光圈就是摄像机中的透镜。一般情况下，光圈越大则越多光线可以进入摄像机到达成像胶卷，因此需要的曝光时间比较短。真实的针孔摄像机并不是没有光圈，只是它的光圈非常小，这意味着它需要的曝光时间比较长，因此如果在曝光时间内拍摄的物体在移动或相机在移动，则得到的照片会出现运动模糊（motion blur）的效果。但光圈越大，则越容易出现景深（depth of field）的模糊效果，此时相机聚焦在一个平面上，其他的距离这个聚焦平面越远的物体将越模糊（失焦）。 &emsp;&emsp;为了实现真实摄像机的聚焦和失焦（即景深）效果，针对图形渲染领域的投影类相机，人们提出了薄透镜近似模型（thin lens approximation）。在薄透镜近似模型中，我们直接忽略透镜的厚度，因为相对于半径它的厚度可以直接忽略。以下全部的内容都是关于薄透镜近似模型。 &emsp;&emsp;平行的光线穿过透镜，将在透镜后面聚焦到一个点上，这个点被称为焦点（focal point）。焦点到透镜的直线距离被称为焦长（focal length），注意不是焦距。将一个成像平面放置在焦点处，则无穷远处的物体在成像平面上处于聚焦状态，因为无穷远处的物体发出或散射的光线无限逼近于平行光线。 &emsp;&emsp;现假设透镜处于$z$轴原点处，其焦长为$f$，则对于场景中深度为$z$的聚焦平面的距离$z’$的计算可采用高斯透镜方程（如下所示），这表示如果要让$z$处的物体聚焦，则成像平面应放置$z’$处： \\frac{1}{z'}-\\frac{1}{z}=\\frac{1}{f} \\to z'=\\frac{fz}{f+z} &emsp;&emsp;对于那些不在$z$处的物体，其聚焦的焦点并不在$z’$处，因此处于失焦状态，它聚焦到$z’$处成像平面上的是一个圆盘而非一个聚焦点，这个圆盘我们称之为弥散圆（circle of confusion）。弥散圆的大小取决于光圈的半径、焦距（focal distance，这里就是$z’$）和物体到透镜的距离。聚焦物体到透镜的距离我们称之为景深（depth of field），透镜到成像平面的距离我们称之为焦距（而非焦长）。 &emsp;&emsp;假设，景深为$z_f$，相应的成像平面在$z_f’$。则对于$z$处的点，其弥散圆如何确定？如下图(a)所示，$z’$是$z$处的焦点，透镜后方的两条虚线与成像平面$z_f’$的相交构成了弥散圆。 &emsp;&emsp;由此，我们可以根据相似三角形原理计算弥散圆的直径。如上图(b)所示，设光圈直径为$d_1$，而弥散圆直径为$d_c$，其计算公式如下： \\frac{d_1}{z'}=\\frac{d_c}{|z'-z_f'|} \\to d_c=|\\frac{d_1(z'-z_f')}{z'}|&emsp;&emsp;将前面的高斯透镜方程带入可得： d_c=|\\frac{d_1 f(z-z_f)}{z(f+z_f)}|&emsp;&emsp;上述公式中，$d_1$是光圈直径，$f$焦长，$z_f$景深，$z$场景的物体。这个公式表明，聚焦的前与后的模糊不是对称的。一般在聚焦前面的物体模糊速度更快。光圈越大，则弥散圆越大，相应地更模糊。说了这么多，其实在光线追踪里面实现薄透镜模型非常简单。 &emsp;&emsp;对于没有实现薄透镜模型的投影摄像机，我们可以将其透镜看成一个点，这个点就是摄像机的眼睛。那么对于要实现薄透镜模型的摄像机，其透镜不再是一个点，而是一个具有一定大小的圆盘，这个圆盘就是光圈。在这里，成像平面介于光圈和场景物体之间。如下图所示，我们在这个光圈上随机采样一个点作为射线的起始点，连接成像平面上的采样点作为射线方向。 &emsp;&emsp;我们有两个参数，分别是光圈半径lensRadius和焦距focalDistance，焦距就是光圈到成像平面的距离。在没有实现薄透镜近似模型时我们默认成像平面在$z=1$处。现在加入了焦距参数，我们要重新计算成像平面上的点，方法就是计算原来的射线与z=focalDistance的交点，在摄像机空间做这些很简单，不解释，$t=focalDistance/d_z$，$d_z$是原来射线的方向向量的$z$分量： 1234567891011121314151617181920Float PerspectiveCamera::GenerateRay(const CameraSample &amp;sample, Ray *ray) const &#123; // Compute raster and camera sample positions // ... // Modify ray for depth of field if (lensRadius &gt; 0) &#123; // Sample point on lens Point2f pLens = lensRadius * ConcentricSampleDisk(sample.pLens); // Compute point on plane of focus Float ft = focalDistance / ray-&gt;d.z; Point3f pFocus = (*ray)(ft); // Update ray for effect of lens ray-&gt;o = Point3f(pLens.x, pLens.y, 0); ray-&gt;d = Normalize(pFocus - ray-&gt;o); &#125; // ... return 1;&#125; &emsp;&emsp;ConcentricSampleDisk负责在圆盘上随机采样一个点。我们用随机采样来近似实现真实相机的光圈效应，这使得我们必须提升每个像素发射的采样光线数量，否则将出现严重的噪声。 Reference$[1]$ M, Jakob W, Humphreys G. Physically based rendering: From theory to implementation[M]. Morgan Kaufmann, 2016.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/tags/Ray-Tracer/"}]},{"title":"空间管理Space management：四叉树 & 八叉树","slug":"Octree","date":"2020-01-10T01:32:18.682Z","updated":"2020-06-25T10:04:00.528Z","comments":true,"path":"2020/01/10/Octree/","link":"","permalink":"https://yangwc.com/2020/01/10/Octree/","excerpt":"又是好久没更新博客了，最近在写算法分析与设计课程的期末作业，作业的题目随意，我就随兴写了烟花粒子的四叉树可视化程序和光追渲染器的八叉树求交优化。之前写的光追渲染器对每个三角网格模型的求交都是暴力遍历所有的三角形，对于三角形数量很多的模型来说效率非常低，所以我捡起了这个渲染器并为每个三角网格模型构建一颗八叉树加快射线与三角形的求交速度。还真别说，性能提升巨大。所以这篇博客本质上是一个期末作业。最后，新年快乐！","text":"又是好久没更新博客了，最近在写算法分析与设计课程的期末作业，作业的题目随意，我就随兴写了烟花粒子的四叉树可视化程序和光追渲染器的八叉树求交优化。之前写的光追渲染器对每个三角网格模型的求交都是暴力遍历所有的三角形，对于三角形数量很多的模型来说效率非常低，所以我捡起了这个渲染器并为每个三角网格模型构建一颗八叉树加快射线与三角形的求交速度。还真别说，性能提升巨大。所以这篇博客本质上是一个期末作业。最后，新年快乐！ &emsp;&emsp;分治算法因其巧妙的算法思想能够将算法的时间复杂度降低到一个非常低的程度，例如广为流传的二分搜索算法将线性搜索时间复杂度$O(n)$降低到了$O(log\\ n)$，这是一个极其恐怖的性能提升。诸多的实际应用领域例如计算机图形学、计算机视觉都有着分治算法的身影。在这里我们将重点关注计算机图形学领域非常实用、好用、高效的分治算法——四叉树空间分割算法和八叉树空间分割算法，二分搜索算法本质上处理的是一维的数据，但在图形学领域我们通常面临的是二维或者三维的点和向量，而且在实际应用中这些数据的量都非常庞大（几百万个点的点云、高精度的网格模型等等），因此为了加速这些更高维数据的搜索，一些高效的数据结构算法被提出，其中四叉树和八叉树算法就是其中之一。（当然也有高维的二叉树，例如k-d树） 一、背景介绍&emsp;&emsp;我们先来看一下算法的应用背景，计算机图形学领域研究的主要内容就是关于二维三维空间的图形绘制、物理模拟、几何建模、电脑动画等等可视化课题，目前最为流行的模型表示方法就是采用三角形网格模型，即每个网格面采用一个三角形来表示，如下图1所示： 图1 三角网格模型 &emsp;&emsp;这种表示方法就是采用了有限元的思想，利用很多个三角形去逼近一个曲面，三角形越多，网格精度越高，模型越接近目标物体。在一个大型的场景中，通常有很多个这样的物体模型，因此采用暴力的方法遍历每一个物体送入绘制管线、碰撞检测等等是非常不可行的，特别是对于游戏、虚拟现实等等实时性要求比较高的应用来说这是灾难性的做法，暴力搜索策略的碰撞检测将耗费大部分的时间。除了碰撞检测之外，还有射线与物体的求交运算，即发射一条射线，找到射线与物体相交的一点，这个在射击游戏中非常常见，此外在基于光线追踪的图形渲染技术中亦是如此。暴力、简单的做法就是将射线方程与场景中的每个三角形进行数学上的交点求取运算（其实就是解一个方程），遍历完所有的三角形，取一个最近的交点就是最终结果，算法复杂度为$O(n)$。这种方法不可取，复杂场景中的三角形数量几千万甚至上亿，在每一帧进行这样求交运算带来的后果就是帧率的急剧降低，用户看到的画面将非常卡顿，毫无流畅的游戏体验感。 &emsp;&emsp;事实上，可以很容易理解，射线并不会与场景中所有的三角形相交，暴力遍历的算法有$99.9\\%$的计算量都在做无用功，因为在所有的三角形中只有一个三角形才是我们要寻找的会相交的、距离最最近的那个三角形。注意到这些，学者们提出了一些基于分治的空间分割算法思想，将二维空间、三维空间做一个划分，排除点那些不可能相交的三角形，加速整个搜索过程。在众多的空间分割算法中，四叉树和八叉树的算法是其中的一种简单、高效、好用的空间分割算法，其中四叉树对应的是二维的空间划分，而八叉树对应的是三维的空间划分，算法的思想并不难理解，本质上属于二分搜索的高维扩展。 四叉树和八叉树就是$2D$和$3D$的“二分法”，搜索过程与二叉树搜索也类似，二叉树中是将数组排序后存入二叉树中，从而在查找中实现时间复杂度为$log\\ n$。而四叉树/八叉树是按平面/空间范围划分有序节点，将所有点/面片/网格模型放入所属节点中，达到类似于排序的结果，进而在搜索时可以快速排除掉那些不符合条件的 点/面片/网格模型。 ## 二、四叉树分割算法 &emsp;&emsp;四叉树或四元树也被称为Q树（QuadTree）。四叉树广泛应用于图像处理、空间数据索引、2D中的快速碰撞检测、存储稀疏数据等，而八叉树（Octree）主要应用于3D图形处理。 #### 1、四叉树分割算法——原理 &emsp;&emsp;四叉树索引的基本思想是将地理空间递归划分为不同层次的树结构。它将已知范围的空间等分成四个相等的子空间，如此递归下去，直至树的层次达到一定深度或者满足某种要求（例如数据对象数量少于一定的阈值）后停止分割。四叉树的结构比较简单，并且当空间数据对象分布比较均匀时，具有比较高的空间数据插入和查询效率，因此四叉树是GIS中常用的空间索引之一。常规四叉树的结构如图所示。 图2 一颗构建好的四叉树 &emsp;&emsp;这里划分空间通常是一个轴向包围盒，这个包围盒可以用它的最低点和最高点来表示。总的来说，四叉树的定义是：它的每个节点下至多可以有四个子节点，通常把一部分二维空间细分为四个象限或区域并把该区域里的相关信息存入到四叉树节点中。 四叉树的每一个节点代表一个矩形区域，每一个矩形区域又可划分为四个小矩形区域，这四个小矩形区域作为四个子节点所代表的矩形区域。 &emsp;&emsp;这里的四叉树只有叶子节点才存储数据对象（如点、三角面片、网格模型等等），内部节点不存储数据对象，因而访问数据对象都要根据内部节点走到叶子节点去访问。一般点是没有大小的，因此数据对象是点的时候没有必要考虑跨越了多个区域的情况。而如果数据对象是面片、网格模型等有大小的时，四叉树的构建就需要小心一点。如图2所示，数据对象是一个矩形，每个矩形有一定的大小，图中的$2$、$4$、$8$这三个数据对象跨越了两个区域，为了防止遗漏，在构建四叉树的时候最好把这些跨越了多个区域的数据对象均放入它所涉及到的叶子节点上。一般情况下叶子节点不会直接存储原始的数据对象，而是将原始的数据对象用一个线性表存储，然后四叉树中的叶子节点存储的是数据对象在线性表中的索引，这是为了防止程序代码耦合度过高。 &emsp;&emsp;说了这么多，接下来我们以构建烟花粒子系统的四叉树为例展开相关的算法介绍和实现。烟花粒子系统中的数据对象是粒子点云，即点集，我们为这个系统构建一颗四叉树，并将其可视化出来。首先定义四叉树中的一个节点对象，代码如下所示。一个节点对象可能是内部节点，也可能是外部节点。对于内部节点，它应该有四个子节点的指针；对于叶子节点，它应该有一个存储数据对象的表。对于每一个节点，都应该有一个包围盒指定当前节点所覆盖的矩形空间范围，我们用矩形空间范围的最小点和最大点来表示。 1234567891011121314151617181920212223struct TreeNode&#123; //! 子节点 TreeNode *children[4]; //! 包围盒 glm::vec2 bMin, bMax; //! 叶节点的物体列表 std::vector&lt;glm::vec2&gt; objects; //! 是否是叶节点 bool isLeaf; TreeNode() : isLeaf(false), bMin(glm::vec2(0.0f)), bMax(glm::vec2(0.0f)) &#123; children[0] = children[1] = children[2] = children[3] = nullptr; &#125; TreeNode(glm::vec2 min, glm::vec2 max) : isLeaf(false), bMin(min), bMax(max) &#123; children[0] = children[1] = children[2] = children[3] = nullptr; &#125;&#125;; #### 2、四叉树分割算法——构建 &emsp;&emsp;从一颗空树开始，给定粒子系统中所有的粒子点云的线性表，我们采用自顶向下的方式构建一颗四叉树。从根节点往下划分，直到到达给定的树的深度或者当前节点覆盖的数据对象少于一定的数量时不再往下划分。根据数据对象数量、当前节点的深度，自顶向下构建四叉树的过程大致可以分成三类： - 如果当前数据对象数量为零，代表当前节点覆盖的区域不包含任何数据对象，则返回空指针，表示当前节点是一个不包含任何对象的空指针节点； - 如果当前数据对象不为零且小于一定的数量，亦或者当前节点的深度达到最大深度，则不再往下划分，将当前的节点构建为叶子节点，并将数据对象添加到叶子节点的存储表中； - 出去上面的两种情况，剩下的情况就是当前节点为内部节点的情况，对于每一个内部节点，需要往下划分四个区域节点，我们根据当前区域的范围分割成上、下、左、右四个子区域，根据数据对象的位置将数据对象表分成四类，递归调用构建函数，返回子节点的指针，最后再返回当前内部节点的指针。 &emsp;&emsp;以上的步骤递归嵌套地执行，即可完成自顶向下的四叉树构建。详细的代码实现如下所示，这个函数的输入是当前节点的深度，当前节点的矩形范围以及数据对象列表，返回构建的节点指针。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566TreeNode * QuadTree::recursiveBuild(unsigned int depth, glm::vec2 min, glm::vec2 max, const std::vector&lt;glm::vec2&gt;&amp; objects)&#123; //! if there is no object at all, just return nullptr. if (objects.empty()) return nullptr; //! if the number of objects is less than 10 or reach the maxDepth, //! just create the node as leaf and return it. if (objects.size() &lt; 4 || depth == mMaxDepth) &#123; TreeNode *cur = new TreeNode(min, max); for (auto &amp;point : objects) &#123; if (isContain(point, min, max)) cur-&gt;objects.push_back(point); &#125; cur-&gt;isLeaf = true; return cur; &#125; //! otherwise just subdivied into four sub nodes. glm::vec2 center = (min + max) * 0.5f; float length = std::max(max.x - min.x, max.y - min.y); // --------- // | 3 | 2 | // --------- // | 0 | 1 | // --------- glm::vec2 subMin[4]; glm::vec2 subMax[4]; //! get the four subnodes' region. subMin[0] = min; subMax[0] = center; subMin[1] = center - glm::vec2(0.0f, length / 2); subMax[1] = center + glm::vec2(length / 2, 0.0f); subMin[2] = center; subMax[2] = max; subMin[3] = min + glm::vec2(0.0f, length / 2); subMax[3] = center + glm::vec2(0.0f, length / 2); //! subdivide the objects into four classes according to their positions. std::vector&lt;glm::vec2&gt; classes[4]; for (auto &amp;point : objects) &#123;s if (isContain(point, subMin[0], subMax[0])) classes[0].push_back(point); else if (isContain(point, subMin[1], subMax[1])) classes[1].push_back(point); else if (isContain(point, subMin[2], subMax[2])) classes[2].push_back(point); else if (isContain(point, subMin[3], subMax[3])) classes[3].push_back(point); &#125; //! allocate memory for current node. TreeNode *cur = new TreeNode(min, max); cur-&gt;children[0] = recursiveBuild(depth + 1, subMin[0], subMax[0], classes[0]); cur-&gt;children[1] = recursiveBuild(depth + 1, subMin[1], subMax[1], classes[1]); cur-&gt;children[2] = recursiveBuild(depth + 1, subMin[2], subMax[2], classes[2]); cur-&gt;children[3] = recursiveBuild(depth + 1, subMin[3], subMax[3], classes[3]); return cur;&#125; &emsp;&emsp;这里需要提一点的是上面代码中的$isContain$函数，它输入一个点和一个矩形包围盒范围，判断这个点是否在这个矩形包围盒之内，这个判断很简单，不再赘述。我们来看一下这种自顶向下构建方法的时间复杂度。四叉树的**每一层**都会处理$O(N)$个数据对象，若数据分布的比较均匀，则树高为$O(log_4 N)$，因此构建的时间复杂度为$O(N log_4 N)$，乍一看比暴力方法的时间复杂度还要高，但我们对每一个数据对象没有做很复杂的数据运算（如射线求交、碰撞检测等等），而且构建只需一次，对于那些检索密集型的应用来说非常划算。 #### 3、四叉树分割算法——销毁 &emsp;&emsp;我们采用四叉链表结构作为四叉树的实现方式，涉及到大量的动态指针，在销毁时需要手动释放这些指针占用堆空间。销毁四叉树并不难，直接采用**后序遍历**方法即可，后序遍历即先销毁子节点，将子节点指针置空，然后再销毁父节点，如此递归下去。 12345678910111213void QuadTree::recursiveDestory(TreeNode *node)&#123; if (node == nullptr) return; recursiveDestory(node-&gt;children[0]); recursiveDestory(node-&gt;children[1]); recursiveDestory(node-&gt;children[2]); recursiveDestory(node-&gt;children[3]); delete node; node = nullptr;&#125; #### 4、四叉树分割算法——插入 &emsp;&emsp;除了前面提到的给定数据对象列表直接自顶向下构建整颗四叉树，还有一种构建方法是动态插入法。即给一个数据对象，将其插入到四叉树的一个叶子节点上，可以称为逐个插入法。逐个插入法就是要找到输入的数据对象所在的四叉树叶子节点，然后将该数据对象添加到该叶子节点的数据对象列表上。插入过程要靠的情况稍微多一点，如果插入的叶子节点的列表长度超过了一定数量，则应该将当前的叶子节点分裂，这时它不再是叶子节点而是内部节点了，递归插入到该内部节点的叶子节点中。总的来说，插入过程分成以下几种情况： - 若输入的数据对象不在当前节点的覆盖范围之内，直接停止插入过程； - 如果走到了四叉树的最大深度，则一定不会再分裂，直接将该数据对象添加到节点的数据对象列表当中； - 如果当前节点是叶子节点，将数据对象添加到该节点的数据对象列表中。然后若数据对象表列长度超过了一定数量，则需要将该叶子节点分裂，往下递归划分，将数据对象列表分散到子节点中，自己不再是叶子节点； - 如果当前节点是内部节点，则需要确定当前的数据对象落到它的四个子节点中的哪一个，然后递归调用插入过程。 &emsp;&emsp;插入过程也不是很复杂，理清了思路就好，实现的代码如下所示，代码的输入是深度、当前节点指针、当前节点覆盖的范围以及要插入的数据对象： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071bool QuadTree::recursiveInsert(unsigned int depth, TreeNode * node, glm::vec2 min, glm::vec2 max, glm::vec2 object)&#123; if (!isContain(object, min, max)) return false; glm::vec2 center = (max + min) * 0.5f; float length = std::max(max.x - min.x, max.y - min.y); //! get the four sub-nodes' region. glm::vec2 subMin[4]; glm::vec2 subMax[4]; subMin[0] = min; subMax[0] = center; subMin[1] = center - glm::vec2(0.0f, length / 2); subMax[1] = center + glm::vec2(length / 2, 0.0f); subMin[2] = center; subMax[2] = max; subMin[3] = min + glm::vec2(0.0f, length / 2); subMax[3] = center + glm::vec2(0.0f, length / 2); //! reach the max depth. if (depth == mMaxDepth) &#123; node-&gt;objects.push_back(object); return true; &#125; if (node-&gt;isLeaf) &#123; node-&gt;objects.push_back(object); if (node-&gt;objects.size() &gt; 4) &#123; //! 超过四个就分裂，自己不再是叶子节点 node-&gt;children[0] = new TreeNode(subMin[0], subMax[0]); node-&gt;children[1] = new TreeNode(subMin[1], subMax[1]); node-&gt;children[2] = new TreeNode(subMin[2], subMax[2]); node-&gt;children[3] = new TreeNode(subMin[3], subMax[3]); node-&gt;isLeaf = false; node-&gt;children[0]-&gt;isLeaf = true; node-&gt;children[1]-&gt;isLeaf = true; node-&gt;children[2]-&gt;isLeaf = true; node-&gt;children[3]-&gt;isLeaf = true; for (auto &amp;point : node-&gt;objects) &#123; if (isContain(point, subMin[0], subMax[0])) node-&gt;children[0]-&gt;objects.push_back(point); else if (isContain(point, subMin[1], subMax[1])) node-&gt;children[1]-&gt;objects.push_back(point); else if (isContain(point, subMin[2], subMax[2])) node-&gt;children[2]-&gt;objects.push_back(point); else if (isContain(point, subMin[3], subMax[3])) node-&gt;children[3]-&gt;objects.push_back(point); &#125; std::vector&lt;glm::vec2&gt;().swap(node-&gt;objects); &#125; return true; &#125; //! 若为内部节点，往下深入搜索 if (isContain(object, subMin[0], subMax[0])) return recursiveInsert(depth + 1, node-&gt;children[0], subMin[0], subMax[0], object); if (isContain(object, subMin[1], subMax[1])) return recursiveInsert(depth + 1, node-&gt;children[1], subMin[1], subMax[1], object); if (isContain(object, subMin[2], subMax[2])) return recursiveInsert(depth + 1, node-&gt;children[2], subMin[2], subMax[2], object); if (isContain(object, subMin[3], subMax[3])) return recursiveInsert(depth + 1, node-&gt;children[3], subMin[3], subMax[3], object); return false;&#125; &emsp;&emsp;插入一个数据对象的时间复杂就是四叉树的高度，理想情况下为$O(log_4 N)$。 #### 5、四叉树分割算法——删除 &emsp;&emsp;有插入就有删除，给定一个数据对象，我们要找到包含这个数据对象的节点，并将其从数据对象列表中删除。与插入的情况相反，删除对象的操作可能导致当前节点的数据对象列表为空，此时需要将当前节点从整颗四叉树中删除，因为它不再包含任何有效的数据。叶子节点的删除可能导致父节点的删除（全部子节点变为空指针），如此递归下去，因此需要仔细斟酌整个删除的过程。总的来说，删除过程需要考虑的情况有如下几种： - 当前节点为空或者当前节点覆盖的范围不包含输入的数据对象，直接终止删除过程； - 当前节点为叶子节点或者走到最大四叉树深度了，遍历当前节点的数据对象列表，找到需要删除对象的数组下标，如果没有则不执行删除操作，否则将其从数组中移除。如果移除一个对象之后的数据列表为空，则需要将其删除，返回一个标志告诉父节点将其删除； - 当前节点为内部节点，找到包含该数据对象的子节点，递归调用删除程序，如果程序返回一个删除标志，则释放该子节点的内存。如果内部节点的四个子节点指针均为空指针，则告诉该内部节点的父节点将其删除，如果递归下去。 &emsp;&emsp;将数据对象从数组中删除一个小小的技巧就是将数组尾部的内容覆盖到该数据对象所在的位置，然后将尾部的数据删除即可，避免数据的大规模移动。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566bool QuadTree::recursiveRemove(unsigned int depth, TreeNode * node, glm::vec2 min, glm::vec2 max, glm::vec2 object)&#123; if (node == nullptr) return false; //! 确保在当前节点的范围之内 if (!isContain(object, min, max)) return false; //! 达到最大深度或者走到叶子节点了. if (depth == mMaxDepth || node-&gt;isLeaf) &#123; //! 找到要删除元素的位置 int index = -1; for (size_t i = 0; i &lt; node-&gt;objects.size(); ++i) &#123; if ((std::pow(node-&gt;objects[i].x - object.x, 2) + std::pow(node-&gt;objects[i].y - object.y, 2)) &lt; 0.001f) &#123; index = i; break; &#125; &#125; if (index != -1) &#123; //! 与最后一个元素交换，方便删除 node-&gt;objects[index] = node-&gt;objects.back(); node-&gt;objects.pop_back(); &#125; //! 如果叶子节点空了，则告诉父节点需要将其删除掉. if (node-&gt;objects.empty()) return true; return false; &#125; //! 非叶子节点 if (recursiveRemove(depth + 1, node-&gt;children[0], node-&gt;children[0]-&gt;bMin, node-&gt;children[0]-&gt;bMax, object)) &#123; delete node-&gt;children[0]; node-&gt;children[0] = nullptr; &#125; else if (recursiveRemove(depth + 1, node-&gt;children[1], node-&gt;children[1]-&gt;bMin, node-&gt;children[1]-&gt;bMax, object)) &#123; delete node-&gt;children[1]; node-&gt;children[1] = nullptr; return false; &#125; else if (recursiveRemove(depth + 1, node-&gt;children[2], node-&gt;children[2]-&gt;bMin, node-&gt;children[2]-&gt;bMax, object)) &#123; delete node-&gt;children[2]; node-&gt;children[2] = nullptr; &#125; else if (recursiveRemove(depth + 1, node-&gt;children[3], node-&gt;children[3]-&gt;bMin, node-&gt;children[3]-&gt;bMax, object)) &#123; delete node-&gt;children[3]; node-&gt;children[3] = nullptr; &#125; //! 父节点的全部子节点为空，自己也将被删除了 if (node-&gt;children[0] == nullptr &amp;&amp; node-&gt;children[1] == nullptr &amp;&amp; node-&gt;children[2] == nullptr &amp;&amp; node-&gt;children[3] == nullptr) &#123; return true; &#125; return false;&#125; #### 6、四叉树分割算法——访问 &emsp;&emsp;最后，为了将四叉树的空间划分结果可视化出来，需要对整颗四叉树做一个遍历。树形数据结构遍历都是一些很基础的基本功，这里不再赘述，每遍历到一个节点，将该节点的矩形区域范围绘制出来。 12345678910111213141516171819202122232425262728293031323334353637383940414243void QuadTree::recursiveTraverse(TreeNode *node, glm::vec2 min, glm::vec2 max, std::vector&lt;glm::vec2&gt;&amp; lines)&#123; glm::vec2 center = (max + min) * 0.5f; float length = std::max(max.x - min.x, max.y - min.y); glm::vec2 corners[4]; corners[0] = min; corners[1] = min + glm::vec2(length, 0.0f); corners[2] = max; corners[3] = min + glm::vec2(0.0f, length); //! get the bounding box to draw. lines.push_back(corners[0]); lines.push_back(corners[1]); lines.push_back(corners[1]); lines.push_back(corners[2]); lines.push_back(corners[2]); lines.push_back(corners[3]); lines.push_back(corners[3]); lines.push_back(corners[0]); if (node == nullptr || node-&gt;isLeaf) return; //! get the four sub-nodes' region. glm::vec2 subMin[4]; glm::vec2 subMax[4]; subMin[0] = min; subMax[0] = center; subMin[1] = center - glm::vec2(0.0f, length / 2); subMax[1] = center + glm::vec2(length / 2, 0.0f); subMin[2] = center; subMax[2] = max; subMin[3] = min + glm::vec2(0.0f, length / 2); subMax[3] = center + glm::vec2(0.0f, length / 2); recursiveTraverse(node-&gt;children[0], subMin[0], subMax[0], lines); recursiveTraverse(node-&gt;children[1], subMin[1], subMax[1], lines); recursiveTraverse(node-&gt;children[2], subMin[2], subMax[2], lines); recursiveTraverse(node-&gt;children[3], subMin[3], subMax[3], lines);&#125; #### 7、四叉树可视化实验结果 &emsp;&emsp;采用OpenGL图形API将烟花粒子系统和它的四叉树结构可视化出来如下图所示，烟花粒子系统的实现和OpenGL渲染API的使用超出了本课程的范围，因此不再赘述。这里构建的四叉树数据对象是粒子点云，下面一系列图片中左边的图展示是烟花粒子效果，右边的图则展示了构建出来的粒子点云四叉树的可视化结果（绿色部分）。可以看到空间分割的情况符合我们的预期，在粒子点密集的区域四叉树往下分割了不少层，而点云稀疏甚至没有的部分不会往下继续分割，这个就是二维分割的四叉树结构，当我们要搜索某一个区域周围的粒子点时就可以通过四叉树迅速地进行邻域查询。 &emsp;&emsp;关于四叉树的可视化部分就到这里，下面来看看三维八叉树在计算机图形学光线追踪渲染技术中的实际应用和性能提升。 ## 三、利用八叉树算法加速射线求交 &emsp;&emsp;四叉树和八叉树从本质上来讲没有区别，算法的思想都是一样的，只不过四叉树是针对二维的情况，而八叉树针对的是三维的情况。在三维空间均匀划分的是一个长方体，每一个长方体分割成八个相同的子长方体，因而三维情况的空间分割树有八个子节点，被称为八叉树。Wiki的八叉树定义为： 八叉树（Octree）是一种用于描述三维空间的树状数据结构。八叉树的每个节点表示一个正方体的体积元素，每个节点有八个子节点，这八个子节点所表示的体积元素加在一起就等于父节点的体积。一般中心点作为节点的分叉中心。 图3 三维空间的八叉树 &emsp;&emsp;想象一个立方体，我们最少可以切成多少个相同等分的小立方体？答案就是8个。再想象 我们有一个房间，房间里某个角落藏着一枚金币，我们想很快的把金币找出来。 我们可以把房间当成一个立方体，先切成八个小立方体，然后排除掉没有放任何东西的小立方体，再把有可能藏金币的小立方体继续切八等份…. 如此下去，平均在$Log_8$(房间内的所有物品数)的时间内就可找到金币。八叉树就是用在3D空间中的场景管理，可以加速我们的物体搜索、碰撞检测、射线求交、邻域搜索等等空间查找操作。 &emsp;&emsp;一般情况下，八叉树的构建过程如下： (1).设定最大递归深度; (2).找出场景的最大尺寸，并以此尺寸建立第一个立方体; (3).依序将单位元元素丢入能被包含且没有子节点的立方体; (4).若没有达到最大递归深度，就进行细分八等份，再将该立方体所装的单位元元素全部分担给八个子立方体; (5).若发现子立方体所分配到的单位元元素数量不为零且跟父立方体是一样的，则该子立方体停止细分，因为跟据空间分割理论，细分的空间所得到的分配必定较少，若是一样数目，则再怎么切数目还是一样，会造成无穷切割的情形； (6).重复步骤(3)，直到达到最大递归深度。 &emsp;&emsp;光线追踪是一个递归的过程。发射一束光线到场景，求出光线和几何图形间最近的交点，如果该交点的材质是反射性或折射性的，可以在该交点向反射方向或折射方向继续追踪，如此递归下去，直到设定的最大递归深度或者射线追踪到光源处（或者背景色），如此便计算处一个像素的着色值。光线追踪算法涉及到大量的光线与几何体的求交点运算，因而与渲染效率息息相关。复杂物体大多采用三角形网格表示，因而进一步来说是射线与三角形的求交运算。下面我将为每个三角网格模型构建一颗八叉树，加速射线与三角网格模型的求交。 1、八叉树分割算法——构建与销毁&emsp;&emsp;在开始构建之前，我们首先要弄清楚一点的就是在这里数据对象是一个个三角形而非一个个点，三角形是由一定的大小的，它有可能刚好处在分割平面上，导致三角形跨越了多个子节点。这时为了防止遗漏，应该把三角形都存储到由交集的节点中。同时，每个叶子节点中并不会直接存储三角形图元数据，而是存储三角形面片的索引。定义八叉树节点如下所示： 12345678910111213141516171819202122232425struct Node&#123; //! it's leaf or not. bool m_isLeaf; //! the eight children. Node *m_children[8]; //! extent. AABB m_boundingBox; //! objects' id list. std::vector&lt;std::tuple&lt;unsigned int, unsigned int, unsigned int&gt;&gt; m_objectIds; Node() : m_isLeaf(true), m_boundingBox(AABB(Vector3D(0.0f, 0.0f, 0.0f), Vector3D(0.0f, 0.0f, 0.0f))) &#123; m_children[0] = m_children[1] = m_children[2] = m_children[3] = nullptr; m_children[4] = m_children[5] = m_children[6] = m_children[7] = nullptr; &#125; Node(Vector3D min, Vector3D max, bool isLeaf) : m_isLeaf(isLeaf), m_boundingBox(AABB(min, max)) &#123; m_children[0] = m_children[1] = m_children[2] = m_children[3] = nullptr; m_children[4] = m_children[5] = m_children[6] = m_children[7] = nullptr; &#125;&#125;; &emsp;&emsp;上面的$m_objectIds$s是一个三元组的vector，三元组的每个元素存储一个三角形面片的三个顶点索引。然后输入模型的三角形面片列表，我们自顶向下构建八叉树。构建的过程其实跟四叉树的构建过程差别不大，区别在于子区域的划分： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849Node *Octree::recursiveBuild(unsigned int depth, Vector3D min, Vector3D max, const MeshHitable * target, std::vector&lt;unsigned int&gt; ids)&#123; //! reach the max depth or there are just a few objects, we dont' need to go further. if (depth == m_maxDepth || ids.size() &lt;= 10 * 3) &#123; Node *cur = new Node(min, max, true); const auto &amp;vertices = target-&gt;m_vertices; for (size_t i = 0; i &lt; ids.size(); i += 3) &#123; if (isContain(cur-&gt;m_boundingBox, vertices[ids[i + 0]].m_position, vertices[ids[i + 1]].m_position, vertices[ids[i + 2]].m_position)) cur-&gt;m_objectIds.push_back(std::make_tuple(ids[i + 0], ids[i + 1], ids[i + 2])); &#125; return cur; &#125; //! otherwise, just divide into 8 sub-nodes. Node *cur = new Node(min, max, false); std::vector&lt;unsigned int&gt; subIds[8]; const auto &amp;vertices = target-&gt;m_vertices; std::vector&lt;AABB&gt; subRegions = cur-&gt;m_boundingBox.getEightSubAABB(); for (size_t i = 0; i &lt; ids.size(); i += 3) &#123; const auto &amp;p1 = vertices[ids[i + 0]].m_position; const auto &amp;p2 = vertices[ids[i + 1]].m_position; const auto &amp;p3 = vertices[ids[i + 2]].m_position; for (size_t j = 0; j &lt; 8; ++j) &#123; if (isContain(subRegions[j], p1, p2, p3)) &#123; subIds[j].push_back(ids[i + 0]); subIds[j].push_back(ids[i + 1]); subIds[j].push_back(ids[i + 2]); &#125; &#125; &#125; cur-&gt;m_children[0] = recursiveBuild(depth + 1, subRegions[0].getMin(), subRegions[0].getMax(), target, subIds[0]); cur-&gt;m_children[1] = recursiveBuild(depth + 1, subRegions[1].getMin(), subRegions[1].getMax(), target, subIds[1]); cur-&gt;m_children[2] = recursiveBuild(depth + 1, subRegions[2].getMin(), subRegions[2].getMax(), target, subIds[2]); cur-&gt;m_children[3] = recursiveBuild(depth + 1, subRegions[3].getMin(), subRegions[3].getMax(), target, subIds[3]); cur-&gt;m_children[4] = recursiveBuild(depth + 1, subRegions[4].getMin(), subRegions[4].getMax(), target, subIds[4]); cur-&gt;m_children[5] = recursiveBuild(depth + 1, subRegions[5].getMin(), subRegions[5].getMax(), target, subIds[5]); cur-&gt;m_children[6] = recursiveBuild(depth + 1, subRegions[6].getMin(), subRegions[6].getMax(), target, subIds[6]); cur-&gt;m_children[7] = recursiveBuild(depth + 1, subRegions[7].getMin(), subRegions[7].getMax(), target, subIds[7]); return cur;&#125; &emsp;&emsp;构建过程的时间复杂度为$Nlog_8\\ N$。同样的，八叉树销毁过程采用后序遍历的方式： 123456789101112131415161718void Octree::recursiveDestory(Node * node)&#123; //! backward traverse for deletion. if (node == nullptr) return; recursiveDestory(node-&gt;m_children[0]); recursiveDestory(node-&gt;m_children[1]); recursiveDestory(node-&gt;m_children[2]); recursiveDestory(node-&gt;m_children[3]); recursiveDestory(node-&gt;m_children[4]); recursiveDestory(node-&gt;m_children[5]); recursiveDestory(node-&gt;m_children[6]); recursiveDestory(node-&gt;m_children[7]); delete node; node = nullptr;&#125; &emsp;&emsp;这里有一点需要特别注意的就是需要判断一个立方体区域是否与三角形相交或包含，我采用的方法是首先判断三角形的三个点中是否至少有一个在立方体内部，如果是则返回真。而如果三角形的三个顶点均不在立方体区域内部，但仍有可能三角形与立方体区域存在交集，此时可以通过依次判断三角形的三条边是否与立方体相交。 12345678910111213141516171819bool Octree::isContain(const AABB &amp; box, const Vector3D &amp; p1, const Vector3D &amp; p2, const Vector3D &amp; p3)&#123; //! at least one point of the triangle is inside the box, return true. if (box.isInside(p1) || box.isInside(p2) || box.isInside(p3)) return true; //! otherwise testing for each edge. Ray edge1(p1, p2 - p1); Ray edge2(p2, p3 - p2); Ray edge3(p3, p1 - p3); float length1 = (p2 - p1).getLength(); float length2 = (p3 - p2).getLength(); float length3 = (p1 - p3).getLength(); if (box.hit(edge1, 0.0f, length1) || box.hit(edge2, 0.0f, length2) || box.hit(edge3, 0.0f, length3)) return true; //! if there is no intersection at all, just return false. return false;&#125; 2、八叉树分割算法——访问&emsp;&emsp;构建了八叉树之后，八叉树的优势就体现出来了，在光线追踪过程每发射一条射线我们就要检测该射线与场景中的物体是否发生了碰撞，如果发生了碰撞则需要将碰撞的点计算出来。有了八叉树结构，我们首先检测射线是否与当前节点的区域发生了碰撞，如果不发生碰撞则直接退出；而如果发生了碰撞且当前节点是叶子节点的话，则遍历叶子节点存储的三角形，计算射线与三角形的交点，取距离射线源最近的结果；而当前节点是内部节点的话，则递归调用遍历程序，判断射线是否与该区域的八个子区域碰撞，有碰撞才会继续往下走，无碰撞则不会往下深入。 &emsp;&emsp;这个算法的实现并不复杂，实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445bool Octree::recursiveTraveling(Node * node, const Ray &amp; ray, float &amp; t_min, float &amp; t_max, std::function&lt;bool(unsigned int i1, unsigned int i2, unsigned int i3, float &amp;t)&gt; func)&#123; //! just return false if node is nullptr. if (node == nullptr) return false; //! if the ray doesn't intersect with the aabb box, just return false. if (!node-&gt;m_boundingBox.hit(ray, t_min, t_max)) return false; //! if it's a leaf, travels every objects of this node. if (node-&gt;m_isLeaf) &#123; bool ret = false; for (size_t i = 0; i &lt; node-&gt;m_objectIds.size(); ++i) &#123; const auto &amp;face = node-&gt;m_objectIds[i]; ret |= func(std::get&lt;0&gt;(face), std::get&lt;1&gt;(face), std::get&lt;2&gt;(face), t_max); &#125; return ret; &#125; //! otherwise, divide into 8 sub-nodes. bool ret = false; std::vector&lt;AABB&gt; subRegions = node-&gt;m_boundingBox.getEightSubAABB(); if (subRegions[0].hit(ray, t_min, t_max)) ret |= recursiveTraveling(node-&gt;m_children[0], ray, t_min, t_max, func); if (subRegions[1].hit(ray, t_min, t_max)) ret |= recursiveTraveling(node-&gt;m_children[1], ray, t_min, t_max, func); if (subRegions[2].hit(ray, t_min, t_max)) ret |= recursiveTraveling(node-&gt;m_children[2], ray, t_min, t_max, func); if (subRegions[3].hit(ray, t_min, t_max)) ret |= recursiveTraveling(node-&gt;m_children[3], ray, t_min, t_max, func); if (subRegions[4].hit(ray, t_min, t_max)) ret |= recursiveTraveling(node-&gt;m_children[4], ray, t_min, t_max, func); if (subRegions[5].hit(ray, t_min, t_max)) ret |= recursiveTraveling(node-&gt;m_children[5], ray, t_min, t_max, func); if (subRegions[6].hit(ray, t_min, t_max)) ret |= recursiveTraveling(node-&gt;m_children[6], ray, t_min, t_max, func); if (subRegions[7].hit(ray, t_min, t_max)) ret |= recursiveTraveling(node-&gt;m_children[7], ray, t_min, t_max, func); return ret;&#125; &emsp;&emsp;可以看到，通过八叉树的结构我们可以避免很多无用功，大大加速整个求交的过程，有了八叉树后射线与三角形求交的时间复杂度就变成了$O(log_8 N)$，其中$N$是输入规模，相比于原来的$O(N)$复杂度提升非常明显。下面就通过实验比较有无八叉树的算法效率。至于光线追踪方面的算法，与本门课的主题关系比较小，而且涉及的内容实在太多，因此不再赘述。 3、八叉树加速实验结果&emsp;&emsp;我们将对比两个场景渲染的实验，两个场中均涉及到复杂的三角网格模型。控制的变量有每个像素发射的采样光线个数、有无八叉树加速。一般情况下，采样光线数量越多越好，数量过少会导致渲染出来的图片噪声过多，质量太低。场景中三角网格比较多的模型如下图5所示： 图5 从左到右三角形面片的数量依次为100000个、69666个和5022个 &emsp;&emsp;下面给出了渲染绘制出来的两个场景，从左到右、从上到下的光线采样数量依次为16、64、128和512，可以看到低采样数时渲染出来的图片噪声明显，质量很低。因此为了渲染出高质量的图片，采样数不能太低，这意味着求交数量也将大大增加。我们记下面的两个场景分别为dragonSquare和dragonBox。下面的图片可以看到利用八叉树并没有损失光线追踪的渲染质量。 图6 场景一：dragonSquare 图7 场景二：dragonBox &emsp;&emsp;下面的表1给出了两个场景在有无八叉树时的渲染时间对比，表格中的s是second的缩写，即单位为秒。表格的最后一列有两个“&gt;3小时“是因为等不下去了，就不再等了，花费的时间实在太长了。通过表格可以看到，有了八叉树的加速，时间效率提升极其明显，特别是采样数量越多的时候，提升倍数越多，完全是几个数量级的提升。这是因为采样数量呈指数增值时，射线与三角形的求交数量也呈指数增长。 表1 渲染时间比较 scene 16次采样 64次采样 128次采样 512次采样 dragonSquare（无八叉树） 427.776s 2195.918s 8715.48s >3小时 dragonSquare（有八叉树） 17.824s 70.382s 147.72s 614.562s dragonBox（无八叉树） 380.944s 1818.491s 6548.502 >3小时 dragonBox（有八叉树） 13.136s 58.661s 105.642s 422.652s 四、总结&emsp;&emsp;分治算法思想是一个非常有用的算法思想，我们围绕分治算法这一主题展开其在计算机图形学中应用——四叉树和八叉树空间分割算法实践。抛开诸多细节不说，四叉树和八叉树在本质上与二分查找类似。与二分查找的排序预处理一样，四叉树和八叉树都需要在最开始构建，构建的复杂度与排序的时间复杂度一致，对于那些查找密集型的应用场景来说非常划算（提升的效率极其恐怖）。当然，四叉树和八叉树并不完美的，它们也有缺点，一个比较大的缺点就是它要求空间物体对象分布比较均匀，如果分布不均匀，那么构建出来的树将不会很平衡，这将影响后续的访问效率。针对这一问题，后续的学者已经有比较多的算法变种，限于篇幅这里不再详细展开。","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/tags/Ray-Tracer/"}]},{"title":"流体模拟Fluid Simulation：基于窄带的粒子流体表面重建","slug":"reviewOfSR","date":"2019-11-10T09:03:11.165Z","updated":"2020-01-10T02:44:37.732Z","comments":true,"path":"2019/11/10/reviewOfSR/","link":"","permalink":"https://yangwc.com/2019/11/10/reviewOfSR/","excerpt":"在基于粒子的流体模拟中，Marching Cubes$^{[1]}$是常用的、流行的表面重建算法。但在Marching Cubes算法构建表面网格之前需要获得整个流体区域的标量场，这个标量场需要我们根据流体的粒子计算得到，这个标量场的质量好坏与最终重建得到的网格质量好坏息息相关。针对采用Marching Cubes算法重建粒子流体表面网格的方法，学者们都着重关注如何提高重建出来的流体网格质量、如何提高表面重建过程的时间效率和内存效率，因此我们从表面网格质量、重建算法效率两方面着手基于粒子的流体表面重建的综述。","text":"在基于粒子的流体模拟中，Marching Cubes$^{[1]}$是常用的、流行的表面重建算法。但在Marching Cubes算法构建表面网格之前需要获得整个流体区域的标量场，这个标量场需要我们根据流体的粒子计算得到，这个标量场的质量好坏与最终重建得到的网格质量好坏息息相关。针对采用Marching Cubes算法重建粒子流体表面网格的方法，学者们都着重关注如何提高重建出来的流体网格质量、如何提高表面重建过程的时间效率和内存效率，因此我们从表面网格质量、重建算法效率两方面着手基于粒子的流体表面重建的综述。 &emsp;&emsp;在基于粒子的流体模拟中，Marching Cubes$^{[1]}$是常用的、流行的表面重建算法。但在Marching Cubes算法构建表面网格之前需要获得整个流体区域的标量场，这个标量场需要我们根据流体的粒子计算得到，这个标量场的质量好坏与最终重建得到的网格质量好坏息息相关。针对采用Marching Cubes算法重建粒子流体表面网格的方法，学者们都着重关注如何提高重建出来的流体网格质量、如何提高表面重建过程的时间效率和内存效率，因此我们从表面网格质量、重建算法效率两方面着手基于粒子的流体表面重建的综述。 一、表面网格质量&emsp;&emsp;表面网格的质量主要取决于标量场，因为表面网格本质上是从标量场提取出来的，标量场的好坏对表面网格的质量影响重大。这里标量场就是符号距离场，它隐式地表示了流体表面网格，我们必须从这个标量场获取显式的三角形网格用以后续的渲染。为了提高重建出来的表面网格质量，一方面我们可以提高标量场网格的分辨率，随着分辨率的提高我们重建出来的网格能够更多地捕获流体细节，网格更加细致，但随之而来的代价就是庞大的计算量和内存开销；另一方面就是从流体表面的标量场定义函数入手，因为粒子凹凸不平的原因，如果表面函数定义得不好，那么很难重建出平滑、自然的表面网格。 &emsp;&emsp;针对表面定义函数这一部分，已有不少学者提出了不同的表面函数。 1、Blinn&emsp;&emsp;Blinn等人最早提出了一种表面定义函数$^{[2]}$，其数学定义为： r_i=\\sqrt{(x-x_i)^2+(y-y_i)^2+(z-z_i)^2}\\\\ D(x,y,z)=\\Sigma_i b_i e^{-a_i r^2_i} \\tag {1}&emsp;&emsp;上式计算了某一点$(x,y,z)$的密度值，然后表面就被定义为密度值等于某个阈值$T$的等值面： F(x,y,z)=D(x,y,z)-T \\tag {2}&emsp;&emsp;该方法计算简单，缺点就是不能够产生平滑的表面网格，尤其对于那些具有尖锐特征的流体粒子。 2、Müller&emsp;&emsp;接着Müller等人提出了采用周围邻域粒子的加权叠加来计算标量场的值$^{[3]}$，标量场的值依旧是密度值减去给定的阈值，密度值计算采用下面的公式： \\rho(r)=\\Sigma_j W(r-r_j,h) \\tag {3}&emsp;&emsp;其中，$r$是标量场中的要计算标量值的一点，$h$是光滑核半径，$W$是权重函数。这种计算标量场的方法虽然在一定程度上优于了Blin$^{[2]}$等人提出的方法，但重建出来的网格依旧不够平滑。 3、Zhu and Bridson&emsp;&emsp;为了进一步提升平滑性，Zhu和Bridson等人提出了采用粒子的符号距离场来定义流体的表面标量场$^{[4]}$，其数学定义如下所示： \\Phi(x) = |x-\\overline x| - \\overline x\\\\ \\overline x = \\Sigma_jw_jx_j\\\\ \\overline r = \\Sigma_j w_jr_j\\\\ w_j=\\frac{k(|x-x_j|/R)}{\\Sigma_j k(|x-x_j|/R)}\\\\ k(s)=max(0,(1-s^2)^3) \\tag {4}&emsp;&emsp;其中的$\\Phi(x)$就是流体的标量场函数，$\\overline x$和$\\overline r$是平均位置和平均半径。该方法构建出来的网格已经很平滑了，但是有个致命的缺点，那就是在高曲率的地方质心容易被甩出流体区域，造成了失真。 4、Adams&emsp;&emsp;Adams等人采用了一种完全不同的策略来重建流体表面$^{[5]}$，他们提出在模拟初始时为每个粒子计算其到表面的距离并保存，在后续的模拟过程中更新这个距离（redistancing）。给定每个粒子$i$的到表面的距离$d_i$，流体表面就定义为下面水平集函数$\\phi(x)$的零等值面： \\phi(x)=d(x)-||a(x)-x|| \\\\ a(x)=\\Sigma_i w_i(x)x_i/\\Sigma_i w_i(x)\\\\ d(x)=\\Sigma_i w_i(x)d_i/\\Sigma_i w_i(x) \\tag {5}&emsp;&emsp;其中的$w_i(x)$是光滑的对称权重函数。该方法采用了新奇的思路来重建流体表面，与Zhu Bridson等人的方法相比，能够重建出更加平滑的流体表面，但算法比较复杂，计算量也比较大，更适用于自适应采样的粒子流体模拟方法。 5、Solenthaler&emsp;&emsp;针对Zhu Bridson等人的缺点，Solenthaler等人在他们的基础上做了改进$^{[6]}$。Solenthaler等人将前面的公式$(4)$改成如下的形式： \\Phi(x)=|x-\\overline x|-\\overline rf \\tag {6}&emsp;&emsp;与原来的公式相比，多了一个因子$f$，其计算公式如下： \\lambda=\\frac{t_{high}-EV_{max}}{t_{high}-t_{low}} \\\\ f= \\begin{cases} 1\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ EV_{max}< t_{low}\\\\ \\lambda^3-3\\lambda^2+3\\lambda \\ \\ \\ \\ otherwise \\end{cases} \\tag {7}&emsp;&emsp;其中的$t_{high}$和$t_{low}$分别是设定的参数，取值分别为$3.5$和$0.4$。而公式$(7)$中的$EV_{max}$是$3\\times 3$雅可比矩阵$\\nabla_x(\\overline x(x))$的最大特征值。这种方法考虑了高曲率区域质心被甩出流体外的情况，显著改善了Zhu Bridson等人的方法存在的缺点。 6、Yu and Turk&emsp;&emsp;目前效果最好的方法当属Yu和Turk等人提出的基于各向异性核的粒子流体表面重建方法$^{[7]}$。他们利用了加权主成分分析法，为了每个流体粒子计算一个协方差矩阵$C_{i}$并对其做奇异值分解，根据得到的特征值和特征向量计算变换矩阵$G_i$。最后通过这个变换矩阵将原本球形的光滑核变换成有偏向性的椭球形。 x_i^{\\omega}=\\frac{\\Sigma_j \\omega_{ij}x_j}{\\Sigma_j \\omega_{ij}}\\\\ C_i=\\Sigma_j \\omega_{ij}(x_j-x_i^\\omega)(x_j-x_i^\\omega)^T/\\Sigma_j \\omega_{ij} \\tag {8}&emsp;&emsp;对协方差矩阵$C_i$做奇异值分解（简称SVD），得到特征值和特征向量： C=R\\Sigma R^T\\\\ \\Sigma=diag(\\sigma_1, ...,\\sigma_d) \\tag {9}&emsp;&emsp;每个粒子的各向异性变换矩阵$G_i$就为： G_i=\\frac1h_i R\\overline \\Sigma^{-1}R^T \\tag {10}&emsp;&emsp;最后标量场的计算公式如下： \\phi(x)=\\Sigma_j \\frac{m_j}{\\rho_j}W(x-\\overline x_j, G_j) \\tag {11}&emsp;&emsp;在标量场计算之前他们还对流体粒子做了一个拉普拉斯平滑处理，使得流体粒子整体上向内部收缩了一些。可以看到，该方法计算量极大，相比先前的方法多了不少数学运算，但不得不说目前这种方法重建出来的表面网格质量最佳。 &emsp;&emsp;总的来说，由于表面函数与重建网格的质量关系密切，因此不少学者围绕这方面的问题做了不少的研究，目前最好的方法已经可以重建出来质量极高的表面网格了，但高质量的方法普遍的缺点就是计算量大，算法相比之下实现起来也比较复杂。因此，如何在保证质量的前提下减少算法的计算量或许是个值得探究的课题。 二、重建算法效率&emsp;&emsp;表面重建这一步介于流体模拟和流体渲染之间，是尤为重要的一步，与后续的渲染质量息息相关。但是由于表面重建过程不仅涉及到MC三角化，还涉及到标量场的计算，其中标量场的计算是整个重建过程最为耗时的部分。目前已有不少学者围绕重建效率展开相关的研究，提出了一些高速并行的高效算法。 1、Müller&emsp;&emsp;Müller等人早之前已经提出了窄带的思想$^{[3]}$，他们指出可以先找出在流体表面区域附近的标量场格子，然后仅仅在这些格子上执行Marching Cubes算法。然而Müller等人只是顺带一提，并没有给出具体的解决可能存在的”双层“问题和可行的并行方案。 2、Bridson&emsp;&emsp;针对传统八叉树的效率问题，Bridson提出了一种稀疏块网格结构$^{[8]}$，只有在窄带区域这些块才存在并可以细分成更加精细的均匀网格。他们的方法本质上就是采用了两重不同分辨率的空间网格，先用较为粗糙的稀疏网格找出窄带区域，然后在窄带区域做进一步的细分，以此大大节省了内存空间占用。 3、Houston&emsp;&emsp;Houston等人设计了一种新颖的稀疏水平集结构$^{[9]}$，他们的方法采用了游程编码（run-length encoding），根据到窄带区域的距离对流体区域进行编码，可扩展性强。算法效率有所提升，但很难设计成高速并行的程序结构。 4、Nielsen&emsp;&emsp;Nielsen等人提出了一种动态管网的网格数据结构（DT-grid）$^{[10]}$，与八叉树不同，这种结构是非层次的，能够大大提升内存效率。之后为了提高高分辨率下的效率，Nielsen等人又提出了一种不同的结合了压缩策略的技术$^{[11]}$。同样地，他们的方法都难以高速并行化，因为这些方法被设计出来时就没有考虑过并行。 5、Akinci&emsp;&emsp;注意到先前的方法在高速并行化方面做的工作较少，Akinci等人开始着手重建过程的GPU并行化工作，围绕基于窄带的表面重建这个主题，他们设计了一种高速并行化的表面重建管线$^{[12]}$，整个管线大致分成两个阶段，分别是标量场的计算和MC三角化。相比之前串行的方法，他们的算法获得非常高的加速比，但是整个重建过程并没有被完全并行化，为了避免可能潜在的线程竞争有一些阶段还是需要串行执行。此外，他们的表面粒子提取方法不是很好。 6、Wu&emsp;&emsp;继Akinci等人之后，Wu等人针对Akinci等人方法的内存效率问题做了一些改进$^{[13]}$，他们利用一种并行高效的布谷鸟哈希表替换原本的均匀网格，从而减少无用的内存空间耗费，提升了内存效率。但在并行管线上与原来相比并没有太大的改变，依旧没有实现完全并行化。 7、Kanai&emsp;&emsp;在追求高速并行化的同时，也有研究者研究如何减少重建出来网格的三角形数量，这是因为在一些平坦的区域我们没有必要生成那么多的三角形，在曲率比较高的区域可以生成更多的三角形以捕获更多的细节。但是由于我们目前采用的方案都是均匀的空间网格，因此无论是平坦区域还是复杂区域我们都生成了同样数量的三角形。Kanai 等人综合考虑了并行化和自适应网格，他们将Akinci等人提出的三层网格自适应表面重建的方案$^{[15]}$进行了并行化，并针对不同分辨率的网格之间存在的裂缝问题进行了修补$^{[14]}$。但该方法因为需要考虑如何进行修补裂缝，算法比较复杂，并行方案相对于基于窄带的并行算法速度要慢一些。 &emsp;&emsp;总的来说，目前GPU并行化还存在一些问题。基于窄带的方案应该是目前最佳的，但是受限于一些潜在的竞争条件不能完全地将整个过程并行化，表面窄带的构建算法也值得探讨是否能够重新调整一下。 参考文献$[1]$ Lorensen W E, Cline H E. Marching cubes: A high resolution 3D surface construction algorithm[C]//ACM siggraph computer graphics. ACM, 1987, 21(4): 163-169. $[2]$ Blinn J F. A generalization of algebraic surface drawing[J]. ACM transactions on graphics (TOG), 1982, 1(3): 235-256. $[3]$ Müller M, Charypar D, Gross M. Particle-based fluid simulation for interactive applications[C]//Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation. Eurographics Association, 2003: 154-159. $[4]$ Zhu Y, Bridson R. Animating sand as a fluid[J]. ACM Transactions on Graphics (TOG), 2005, 24(3): 965-972. $[5]$ Adams B, Pauly M, Keiser R, et al. Adaptively sampled particle fluids[C]//ACM Transactions on Graphics (TOG). Acm, 2007, 26(3): 48. $[6]$ Solenthaler B, Schläfli J, Pajarola R. A unified particle model for fluid–solid interactions[J]. Computer Animation and Virtual Worlds, 2007, 18(1): 69-82. $[7]$ Yu J, Turk G. Reconstructing surfaces of particle-based fluids using anisotropic kernels[J]. ACM Transactions on Graphics (TOG), 2013, 32(1): 5. $[8]$ Bridson R E, Fedkiw R. Computational aspects of dynamic surfaces[D]. stanford university, 2003. $[9]$ Houston B, Wiebe M, Batty C. RLE sparse level sets[C]//ACM SIGGRAPH 2004 Sketches. ACM, 2004: 137. $[10]$ Nielsen M B, Museth K. Dynamic Tubular Grid: An efficient data structure and algorithms for high resolution level sets[J]. Journal of Scientific Computing, 2006, 26(3): 261-299. $[11]$ Nielsen M B, Nilsson O, Söderström A, et al. Out-of-core and compressed level set methods[J]. ACM Transactions on Graphics (TOG), 2007, 26(4): 16. $[12]$ Akinci G, Ihmsen M, Akinci N, et al. Parallel surface reconstruction for particle‐based fluids[C]//Computer Graphics Forum. Oxford, UK: Blackwell Publishing Ltd, 2012, 31(6): 1797-1809. $[13]$ Wu W, Li H, Su T, et al. GPU-accelerated SPH fluids surface reconstruction using two-level spatial uniform grids[J]. The Visual Computer, 2017, 33(11): 1429-1442. $[14]$ Du S, Kanai T. Gpu-based adaptive surface reconstruction for real-time sph fluids[J]. 2014. $[15]$ Akinci G, Akinci N, Oswald E, et al. Adaptive surface reconstruction for SPH using 3-level uniform grids[J]. 2013.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/tags/Fluid-Simulation/"}]},{"title":"频域处理Frequency domain processing：傅里叶变换","slug":"FFT","date":"2019-10-24T09:27:00.046Z","updated":"2019-11-10T09:01:16.634Z","comments":true,"path":"2019/10/24/FFT/","link":"","permalink":"https://yangwc.com/2019/10/24/FFT/","excerpt":"研一上学期课程比较繁重，加上在实验室要做的事情也多，所以最近博客都没更新了。最近数图课上到频域变换处理相关章节，其中的傅里叶变换其实在图形学领域也有颇多的应用（例如大规模的海水模拟），遂写下这篇博客整理了一些关于傅里叶变换的内容。","text":"研一上学期课程比较繁重，加上在实验室要做的事情也多，所以最近博客都没更新了。最近数图课上到频域变换处理相关章节，其中的傅里叶变换其实在图形学领域也有颇多的应用（例如大规模的海水模拟），遂写下这篇博客整理了一些关于傅里叶变换的内容。 复数域 傅里叶变换 傅里叶变换的实现 参考资料 &emsp;&emsp;傅里叶变换在高等数学中早有涉及，只不过当时叫傅里叶级数，即给定一个连续函数，我们可以将其展开成一系列不同系数的正弦函数和余弦函数的叠加（与泰勒级数具有相似的形式）。傅里叶变换涉及到了的复数域，我们先来看一些复数域的内容。 一、复数域&emsp;&emsp;复数的内容在高中已经学过，这里只是将一些复数域的定义和性质罗列出来。复数$C$的定义如下： C=R+jI \\tag {1}&emsp;&emsp;其中$R$和$I$均为实数，$j$是$-1$的平方根，即$j=\\sqrt{-1}$或者说$j^2=-1$。$R$是复数的实部，$I$是复数的虚部。因此实数域是虚部$I=0$的复数域子集。给定复数$C$，其共轭的定义$C’$为: C'=R-jI \\tag {2}&emsp;&emsp;复数的实部$R$和虚部$I$可以看成复平面上的一个二维点$(R,I)$，该复平面的横坐标是实数轴，纵坐标是虚数轴，如下图1所示。因此复数$R+jI$等价于复平面直角坐标系中的点$(R,I)$。 图1 复平面直角坐标系 &emsp;&emsp;定义复数的模就是在复平面上的复数点到原点的距离$|C|=\\sqrt{R^2+I^2}$。$\\theta$是点向量与实数轴的夹角，因此$\\tan\\theta=\\frac IR$，在已知$I$和$R$的情况下，可以计算$\\theta=arctan(I/R)$。有了模长和夹角，我们可以将其复数域从直角坐标系转成极坐标系下的形式： C=|C|(cos\\theta+jsin\\theta) \\tag {3}&emsp;&emsp;上式等价于公式$(1)$，因为$|C|cos\\theta=R$、$|C|sin\\theta=I$。又有欧拉公式$e^{j\\theta}=cos\\theta+jsin\\theta$，因此可以将公式$(3)$用欧拉公式替换成如下形式： C=|C|e^{j\\theta} \\tag {4}&emsp;&emsp;复数的四则运算与实数类似，我们牢牢记住$j^2=-1$即可。这里只看复数的加法和乘法，给定两个复数$C_1=a+jbj$和$C_2=c+jd$，则其相加和相乘的结果为： C_1+C_2=(a+c)+j(b+d)\\\\ C_1\\cdot C_2=(ac-bd)+j(ad+bc) \\tag {5}&emsp;&emsp;复数与实数运算更加简单，直接实数与实部运算、实数与复部运算即可。傅里叶变换要用到的复数域性质就这些，我们可以定义实现一个简单的复数类，将上面的求模、取共轭、四则运算实现，方便后面的傅里叶变换。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990//！ complex number class.class Complex&#123;public: Complex() : real(0.0), imaginary(0.0) &#123;&#125; Complex(double r, double i = 0.0) : real(r), imaginary(i) &#123;&#125; Complex(const Complex &amp;other) &#123; real = other.real; imaginary = other.imaginary; &#125; //! opeartor overloading. Complex operator+(const Complex &amp;other) const; Complex operator-(const Complex &amp;other) const; Complex operator*(const Complex &amp;other) const; Complex operator*(const double &amp;value) const; Complex &amp;operator+=(const Complex &amp;other); Complex &amp;operator-=(const Complex &amp;other); Complex &amp;operator*=(const Complex &amp;other); Complex &amp;operator*=(const double &amp;value); Complex conjugate() const; double length() const; double getReal() const &#123; return real; &#125; double getImaginary() const &#123; return imaginary; &#125; void setReal(const double &amp;value) &#123; real = value; &#125; void setImaginary(const double &amp;value) &#123; imaginary = value; &#125;private: double real; double imaginary;&#125;;Complex Complex::operator+(const Complex &amp; other) const&#123; return Complex(this-&gt;real + other.real, this-&gt;imaginary + other.imaginary);&#125;Complex Complex::operator-(const Complex &amp; other) const&#123; return Complex(this-&gt;real - other.real, this-&gt;imaginary - other.imaginary);&#125;Complex Complex::operator*(const Complex &amp; other) const&#123; return Complex( this-&gt;real * other.real - this-&gt;imaginary * other.imaginary, this-&gt;real * other.imaginary + this-&gt;imaginary * other.real);&#125;Complex Complex::operator*(const double &amp; value) const&#123; return Complex(this-&gt;real * value, this-&gt;imaginary * value);&#125;Complex &amp; Complex::operator+=(const Complex &amp; other)&#123; this-&gt;real += other.real; this-&gt;imaginary += other.imaginary; return *this;&#125;Complex &amp; Complex::operator-=(const Complex &amp; other)&#123; this-&gt;real -= other.real; this-&gt;imaginary -= other.imaginary; return *this;&#125;Complex &amp; Complex::operator*=(const Complex &amp; other)&#123; *this = (*this) * other; return *this;&#125;Complex &amp; Complex::operator*=(const double &amp; value)&#123; *this = (*this) * value; return *this;&#125;Complex Complex::conjugate() const&#123; return Complex(this-&gt;real, -this-&gt;imaginary);&#125;double Complex::length() const&#123; return sqrt(this-&gt;real * this-&gt;real + this-&gt;imaginary * this-&gt;imaginary);&#125; 二、傅里叶变换&emsp;&emsp;接下来就是傅里叶变换的一些基本内容，包含连续域的傅里叶变换和离散域的傅里叶变换形式。 1、傅里叶级数&emsp;&emsp;首先来回顾一下高数中的傅里叶级数。给定一个具有周期$T$的连续变量$t$的周期函数$f(t)$，该函数可以被展开成一系列适当系数的正弦和余弦的系数加权和，具体形式如下： f(t)=\\Sigma_{n=-\\infty}^{+\\infty}c_n e^{j\\frac{2\\pi n}{T}t} \\tag {6}&emsp;&emsp;$c_n$则是相应的权重系数，它被称为傅里叶系数，是一个具体的值而非函数，它的具体计算公式如下： c_n=\\frac1T\\int_{-T/2}^{T/2}f(t)e^{-j\\frac{2\\pi n}{T}t}dt,n =0,+1,-1,+2,-2,... \\tag {7}&emsp;&emsp;根据欧拉公式$e^{j\\theta}=cos\\theta+jsin\\theta$，其中$e^{j\\frac{2\\pi n}{T}t}=cos(\\frac{2\\pi n}{T}t)+jsin(\\frac{2\\pi n}{T}t)$。因此公式$(6)$可以写成包含正弦和余弦函数的形式： f(t)=\\Sigma_{-\\infty}^{+\\infty}c_n[cos(\\frac{2\\pi n}{T}t)+jsin(\\frac{2\\pi n}{T}t)] \\tag {8}&emsp;&emsp;公式$(8)$的意思就是$f(t)$可以展开成$n$个正弦和余弦的叠加。设三角函数的通式为$Asin(\\omega x+q)。$对于每一个$n$，都对应一个振幅为$A=c_n$、周期为$\\frac{2\\pi}{\\omega}=\\frac{2\\pi}{\\frac{2\\pi n}{T}}=\\frac Tn$、频率为$n/T$的正弦和余弦，这些正弦和余弦是$f(t)$的一个分量。因此所有（$n$个）不同振幅、不同周期、不同频率的正弦和余弦构成了最初原来的那个函数$f(t)$。 图2 不同振幅、不同频率的波 &emsp;&emsp;&emsp;一个恰当的比喻就是将傅里叶变换比作一个玻璃棱镜。棱镜是可以将光分成不同颜色成分的物理仪器，每个成分的颜色由波长（或频率）决定。傅里叶变换可以看作是“数学的棱镜”，将函数基于频率分成不同的成分。当我们考虑光时，主要讨论它的光谱和频率谱线。同样，傅里叶变换使我们能够通过频率成分来分析一个函数。 2、连续变量的傅里叶变换&emsp;&emsp;连续变量$t$的连续函数$f(t)$的傅里叶变换由下式定义： F(\\mu)=\\int_{-\\infty}^{+\\infty}f(t)e^{-j2\\pi \\mu t}dt \\tag {9}&emsp;&emsp;公式$(9)$其实对应于前面的公式$(7)$，即求傅里叶展开中的傅里叶系数，用$F(\\mu)$表示，$\\mu$也是一个连续变量（对应前面的$n/T$）。用欧拉公式可以把公式$(9)$表示为： F(\\mu)=\\int_{-\\infty}^{+\\infty}f(t)[cos(2\\pi\\mu t)-jsin(2\\pi\\mu t)]dt \\tag {10}&emsp;&emsp;即便$f(t)$是实数，其傅里叶变换得到的系数通常是复数。变量$\\mu$决定了相应的正弦波、余弦波的频率，因此傅里叶变换域是频率域。公式$(9)$是傅里叶正变换，即将时空域的值变换到频率域。同样地，我们可以将其变换回来（因为傅里叶变换是可逆的），这就是傅里叶反变换，其数学形式如下（对应前面的公式$(6)$）： f(t)=\\int_{-\\infty}^{+\\infty}F(\\mu)e^{j2\\pi\\mu t}d\\mu \\tag {11}&emsp;&emsp;可以看到，傅里叶正变换和反变换非常类似，在形式上仅在一些符号以及系数上有点差别。傅里叶的正变换即公式$(9)$和傅里叶反变换即公式$(11)$构成了傅里叶变换对。在应用领域通常正变换和反变换都需要用到。目前讨论的都是一元函数，进一步地我们可以把它扩展到二元函数，二维的连续傅里叶变换对由以下两个表达式给出： F(\\mu,v)=\\int_{-\\infty}^{+\\infty}\\int_{-\\infty}^{+\\infty}f(t,z)e^{-j2\\pi (\\mu t+vz)}dtdz \\\\ f(t,z)=\\int_{-\\infty}^{+\\infty}\\int_{-\\infty}^{+\\infty}F(\\mu,v)e^{j2\\pi (\\mu t+vz)}d\\mu dv \\tag {12}&emsp;&emsp;其中的$u$、$v$是频率变量。 3、离散形式的傅里叶变换&emsp;&emsp;在现实的工程实现中，通常我们只能对连续函数$f(t)$进行一个离散的采样，得到离散的$f(t)$值（如下图3左边所示）。我们的图像就是对真实光照的离散采样，每一个像素对应一个采样点，像素值就是采样得到的函数值。相应地，我们将在这些离散采样的点上进行傅里叶变换，得到相应的傅里叶系数。 图3 离散傅里叶变换 &emsp;&emsp;由于$f(t)$是一个周期函数，因此通常只需采样一个周期的样本即可。对于一个$M\\times N$大小的二维图像，它可以看成周期为$M\\times N$的离散采样。设$x=0,1,2,3,…,N-1$是采样点，采样间隔为$\\Delta x$，相应的$f(x)$是采样得到的离散信号，则其离散傅里叶正变换和反变换换定义如下： F(\\mu)=\\frac{1}{N}\\Sigma_{x=0}^{N-1}f(x)e^{-j2\\pi \\mu x/N}\\\\ f(x)=\\Sigma_{\\mu=0}^{N-1}F(\\mu)e^{j2\\pi \\mu x/N} \\tag {13}&emsp;&emsp;离散傅里叶变换就是把积分符号换成了累加符号，同时积分区间换成了一个周期（这里一个周期就是$N$）。注意傅里叶变换后得到的函数是在复数域内的，即$F(\\mu)=R(\\mu)+jI(\\mu)=|F(\\mu)|e^{j\\phi(\\mu)}$。下面是关于傅里叶变换的基本概念，这些概念在前面复数域已经提到过了，只不过换了一个名字。 傅里叶谱：$|F(\\mu)|=[R^2(\\mu)+I^2(\\mu)]^{1/2}$，就是傅里叶系数的模长（因为变换得到的是复数）。 相角：$\\phi(\\mu)=arctan(\\frac{I(\\mu)}{R(\\mu)})$，就是复平面上的$\\theta$角。 能量谱：$P(\\mu)=|F(\\mu)|^2=R^2(\\mu)+I^2(\\mu)$，模长的平方。 &emsp;&emsp;离散的傅里叶变换后的函数$F(\\mu)$也是离散的，其采样的点分布在$u=0,\\Delta \\mu,2\\Delta \\mu, …,(N-1)\\Delta \\mu$，$\\Delta x$和$\\Delta \\mu$有如下的反比关系： \\Delta \\mu=\\frac{1}{N\\Delta x} \\tag {14}&emsp;&emsp;同样地，推广到二维。二维离散傅里叶变换本质上是一维情况向两个方向的简单扩展： F(\\mu,v)=\\frac{1}{MN}\\Sigma_{x=0}^{M-1}\\Sigma_{y=0}^{N-1}f(x,y)e^{-j2\\pi (\\mu x/M+vy/N)}\\\\ f(x,y)=\\Sigma_{\\mu=0}^{M-1}\\Sigma_{v=0}^{N-1}F(\\mu,v)e^{j2\\pi(\\mu x/M+vy/N)} \\tag {15}&emsp;&emsp;这里可以发现一个非常有趣的地方，当$\\mu = 0,v=0$时，傅里叶变换得到的本质上就是原离散函数的均值： F(0,0)=\\frac{1}{MN}\\Sigma_{x=0}^{M-1}\\Sigma_{y=0}^{N-1}f(x,y) \\tag {16}4、傅里叶变换的性质&emsp;&emsp;傅里叶变换有许多非常有用的性质，这里只记录一些关键和重要的性质。 周期性：正变换和反变换均有的性质，$F(\\mu,v)=F(\\mu + M,v)=F(u,v+N)=F(\\mu+M,v+N)$，$f(x,y)=f(x+M,y)=f(x,y+N)=f(x+M,y+N)$，可以看到这里$f(x,y)$也是有周期性的。具体到二维的图像处理就是我们将一张图像重复地平铺（像贴地板瓷块一样），所以在$x$和$y$方向上的周期就是图像的宽和高。周期性不难理解，具体证明也不难。 共轭对称性：若$f(x,y)$时实函数，则$F(\\mu,v)=F’(-\\mu,-v)$，$|F(\\mu,v)|=|F’(-\\mu,-v)|$。注意这里必须要求时$f(x,y)$是实函数，否则将是共轭反对称性。 平移性：空间域的平移，$f(x-x_0,y-y_0)\\iff F(\\mu,v)e^{-j2\\pi (\\mu_0x/M+v_0y/N)}$；频率域的平移，$f(x,y)e^{j2\\pi(\\mu_0x/M+v_0y/N)}\\iff F(\\mu-\\mu_0,v-v_0)$。这个性质告诉我们，在一个领域的平移等价于另一个领域的函数值乘上一个指数系数。在图像处理中，频率域的平移很常用，当$\\mu_0=M/2,v_0=N/2$时，$e^{j2\\pi(\\mu_0x/M+v_0y/N)}=(-1)^{x+y}$，将频率域的原点平移到了图像的中心点$(M/2,N/2)$。 5、图像频率域的滤波步骤&emsp;&emsp;图像频率域的滤波和空间域的滤波有一一对应的关系。二维卷积定理表明，在空间域的循环卷积滤波等价于频率域的点乘： f(x,y)\\star h(x,y)\\iff F(\\mu,v)H(\\mu,v) \\tag {17}&emsp;&emsp;上式中，$f(x,y)$就是待滤波的图像，$h(x,y)$是卷积核，相应的$F(\\mu,v)$是由$f(x,y)$做傅里叶正变换得到的，$H(\\mu,v)$是由卷积核$h(x,y)$做傅里叶变换得到的。$H(\\mu,v)$和$F(\\mu,v)$做点乘并不是向量的点乘，它们都是一个二维的矩阵，所谓的点乘就是两个矩阵中相同位置的元素相乘，得到的结果依旧是一个矩阵，记为$G(\\mu,v)=F(\\mu,v)H(\\mu,v)$。最后将$G(\\mu,v)$通过傅里叶反变换就得到了滤波后的图像，记为$g(x,y)$。 &emsp;&emsp;因此，二维的图像频率域滤波步骤如下： 将原图像的像素值$f(x,y)$乘上$(-1)^{x+y}$，得到$f(x,y)(-1)^{x+y}$，由平移性可知，这一步相当于将频率域的原点平移到了图像的中心点$(M/2,N/2)$； 将$f(x,y)(-1)^{x+y}$做傅里叶正变换，得到$F(\\mu-M/2,v-N/2)$，注意不是$F(\\mu,v)$； 在频率域执行点乘，得到滤波后的矩阵$G(\\mu-M/2,v-N/2)=H(\\mu- M/2,v-N/2)F(\\mu-M/2,v-N/2)$； 对$G(\\mu-M/2,v-N/2)$执行傅里叶反变换，得到$g(x,y)(-1)^{x+y}$； 最后将$g(x,y)(-1)^{x+y}$再乘上$(-1)^{x+y}$得到$g(x,y)$，即滤波后的图像。 &emsp;&emsp;滤波矩阵$H$并不一定要通过空间域的卷积核做傅里叶正变换得到，可以直接在频率域设计相关的滤波器。这里略提一下傅里叶变换得到的频谱图与空间域原图的联系，傅里叶变换得到的频谱图显示的每个正弦波的振幅，振幅越大则代表对应频率的正弦波在原函数中的成分占比越多。经过中心化之后，在图像中心的是低频正弦波的振幅，从中心向四周频率越高，相应的四周频谱值代表高频正弦波的振幅。低频成分代表了变化较为缓慢的部分，相应的就是在空间域的图像比较平滑的部分；高频成分代表了变化剧烈的部分，相应的就是在空间域的图像灰度值变化比较迅速的部分（例如边缘）。频谱的$uv$坐标并不和空间域的图像$xy$坐标一一对应，因而相应的值也不是一一对应的。频谱$(u,v)$处的值取决于空间域图像的所有像素值，是一个从空间域到频率域的多对一的映射关系，坐标$(u,v)$蕴含了平面波的频率和角度，因而坐标$(u,v)$和该坐标处的频谱值能够代表一个平面波（当然这个平面波还不包含相位信息，这个相位信息蕴含在相角当中）。 三、傅里叶变换的实现&emsp;&emsp;上面讨论了一些关于傅里叶变换的理论内容，接下就是关于傅里叶变换的具体实现。我们把目光放到下面的二维离散傅里叶变换公式： F(\\mu,v)=\\frac{1}{MN}\\Sigma_{x=0}^{M-1}\\Sigma_{y=0}^{N-1}f(x,y)e^{-j2\\pi (\\mu x/M+vy/N)}&emsp;&emsp;其中的$\\mu$取值范围为$0,1,…,M-1$，$v$的取值范围为$0,1,…,N-1$。$M$和$N$是图像的宽度和高度。因此$F(\\mu,v)$是一个$M\\times N$大小的傅里叶系数矩阵，与空间域的图像大小一致。对$M\\times N$的图像做傅里叶变换，就是要计算$M\\times N$个傅里叶系数，每个傅里叶系数的计算公式上面已经给出。 &emsp;&emsp;一个傅里叶系数的计算需要两重循环，遍历空间域图像的所有值，时间复杂度为$O(M\\times N)$。然后一共需要计算$M\\times N$个傅里叶系数，因而对于$M\\times N$大小的图像我们直接计算其傅里叶变换的时间复杂度是$O((M\\times N))^2$。一般情况下$M$和$N$不会相差很远，因而这个时间复杂度可以说是四次方级别的。常见的图像大小$M=N=1024$，则需要执行万亿次的基础计算。可以看到，这个时间复杂度实在是太高了。 &emsp;&emsp;快速傅里叶变换算法的提出使得傅里叶变换的时间复杂度降到了$O(MNlog_2(MN))$，耗费的时间大大减少，从而使得傅里叶变换开始在工业界得到了广泛的应用。下面是关于傅里叶变换和快速傅里叶变换的实现理论。 1、二维DFT的可分性&emsp;&emsp;这里的DFT就是Discrete Fourier Transform即离散傅里叶变换的简称。二维的DFT可以分成两个方向上的一维变换，我们可以把二维离散傅里叶变换写成： F(x,v)=\\Sigma_{y=0}^{N-1}f(x,y)e^{-j2\\pi vy/N} \\tag {18} F(u,v)=\\Sigma_{x=0}^{M-1}e^{-j2\\pi \\mu x/M}\\Sigma_{y=0}^{N-1}f(x,y)e^{-j2\\pi vy/N} =\\Sigma_{x=0}^{M-1}F(x,v)e^{-j2\\pi \\mu x/M} \\tag {19}&emsp;&emsp;公式$(18)$首先对图像的每一行执行一维的傅里叶变换，得到中间的结果$F(x,v)$。然后公式$(19)$对公式$(18)$得到的结果$F(x,v)$执行逐列的一维傅里叶变换。即$f(x,y)$的二维DFT可以通过计算$f(x,y)$的每一行的一维变换，得到中间结果，然后沿着中间结果的每一列计算一维变换得到最终的结果。 &emsp;&emsp;注意上面的公式少了系数$1/(MN)$，这个系数$1/(MN)$可以灵活放置，可以放到傅里叶正变换的公式中，也可以放到傅里叶反变换的公式中，或者更为对称一点的做法就是正变换和反变换的系数均为$1/\\sqrt{(MN)}$，只要保证正变换和反变换前面的常量系数乘起来等于$1/MN$即可。但是灵活放置不意味着可以直接去掉。 2、用DFT算法计算IDFT&emsp;&emsp;这里的IDFT就是Inverse Discrete Fourier Transform即离散傅里叶反变换的简称。二维的IDFT计算公式的形式如下： f(x,y)=\\Sigma_{\\mu=0}^{M-1}\\Sigma_{v=0}^{N-1}F(\\mu,v)e^{j2\\pi(\\mu x/M+vy/N)}&emsp;&emsp;仔细观察傅里叶正变换和反变换的差别，可以发现，我们可以采用傅里叶正变换的形式来表示反变换，对上面的公式两边取复共轭，并乘上$MN$，有： MNf'(x,y)=\\Sigma_{\\mu=0}^{M-1}\\Sigma_{v=0}^{N-1}F'(\\mu,v)e^{-j2\\pi(\\mu x/M+vy/N)} \\tag {20}&emsp;&emsp;此时，在公式$(20)$的右边，我们具有与正变换相同的形式，与正变换的区别就是正变换的输入是$f(x,y)$，而这里的输入是$F(\\mu,v)$的复共轭$F’(\\mu,v)$。通过公式$(20)$我们可以得到$MNf’(x,y)$，然后将其除以$MN$再取复共轭就得到了$f(x,y)$（如果$f(x,y)$本身就是实函数的话，例如我们的图像都是实函数，取复共轭其实就是它自身）。因此，我们可以通过将$F’(\\mu,v)$输入到正变换的算法中，得到$MNf’(x,y)$，再除以$MN$并取复共轭就得到了时空域的函数。即我们用用DFT算法来计算IDFT。 3、快速傅里叶变换&emsp;&emsp;前面我们已知二维的DFT可划分成一维的DFT计算，因此接下来的快速傅里叶变换我们只考虑一维的DFT。快速傅里叶变换的英文名是Fast Fourier Transform，因此简称为FFT。由前面可知，一维的DFT计算公式如下： F(\\mu)=\\Sigma_{x=0}^{M-1}f(x)e^{-j2\\pi \\mu/M}=\\Sigma_{x=0}^{M-1}f(x)W_M^{\\mu x} \\tag {21}&emsp;&emsp;为了便于讨论，我们记$W_M^{\\mu x}$为： W_M^{\\mu x}=e^{-j2\\pi \\mu x/M} \\tag {22}&emsp;&emsp;设$M$是$2$的幂次方，即存在正整数$n$，使得$M=2^n$。因此可以将$M$分成两半，即$M=2K$，$K$也是正整数。将$M=2K$代入公式$(21)$有： F(\\mu) =\\Sigma_{x=0}^{2K-1}f(x)W_{2K}^{\\mu x} =\\Sigma_{x=0}^{K-1}f(2x)W_{2K}^{\\mu (2x)} +\\Sigma_{x=0}^{K-1}f(2x+1)W_{2K}^{\\mu (2x+1)} \\tag {23}&emsp;&emsp;根据公式$(22)$易证$W^{2\\mu x}_{2K}=W^{ux}_K$，所以$(23)$可以表示成： F(\\mu)= \\Sigma_{x=0}^{K-1}f(2x)W_{K}^{\\mu x} +\\Sigma_{x=0}^{K-1}f(2x+1)W_{K}^{\\mu x}W_{2K}^\\mu \\tag {24}&emsp;&emsp;定义$F_{even}(\\mu)$和$F_{odd}(\\mu)$如下： F_{even}(\\mu)=\\Sigma_{x=0}^{K-1}f(2x)W_{K}^{\\mu x} \\\\ F_{odd}(\\mu)=\\Sigma_{x=0}^{K-1}f(2x+1)W_{K}^{\\mu x} \\tag {25}&emsp;&emsp;其中的$\\mu=0,1,2,…,K-1$，注意这里不是$0,1,2…,M-1$了。公式$(24)$可以简写如下： F(\\mu)=F_{even}(\\mu)+F_{odd}(\\mu)W_{2K}^\\mu \\tag {26}&emsp;&emsp;公式$(26)$给出了前半部分的计算公式。又因为$W_M^{\\mu + M}=W_M^\\mu$和$W_{2M}^{\\mu + M}=-W_{2M}^\\mu$（这两个可以仔细地根据公式$(22)$推导得到），我们可以得到后半部分的计算公式： F(\\mu + K)=F_{even}(\\mu)-F_{odd}(\\mu)W_{2K}^\\mu \\tag {27}&emsp;&emsp;可以看到，我们将原$\\mu=0,1,2,…,M-1$的函数值划分成偶数部分和奇数部分，分别对偶数部分和奇数部分做傅里叶变换得到公式$(25)$，最后的结果是根据公式$(25)$的结果做一些合并得到（即公式$(26)$和公式$(27)$）。原来一维的傅里叶变换计算量为$M^2=4K^2$，采用上面的方法我们仅需要计算两个长度为$K$的傅里叶变换，因而计算量就变为$K^2+K^2=2K^2$，显然易见，计算量降低为原来的一半。这是一个非常大的优化。 &emsp;&emsp;将一个大的问题划分成两个规模一样、问题相似的子问题，这个其实就是分治算法的策略。上面仅仅讨论了一次划分，我们还可以继续对$F_{even}(\\mu)$和$F_{odd}(\\mu)$做一样的分割问题操作。这样一直递归下去，直到问题的规模变为$2$，这时直接采用公式$(21)$计算，然后自底向上逐次合并成原问题，最后的合并得到最终解。 &emsp;&emsp;只要规模不是小于一定的程度，则每个子问题可以继续划分成两个更小的子问题，如此递归划分下去，构成了一颗满二叉树，该满二叉树的高度为$log_2(M)$。二叉树树一层的所有的节点合并需要的时间复杂是$O(M)$，因而最终总共需要的时间就是树的高度乘上每一层合并需要的时间，即$O(Mlog_2(M))$。采用FFT算法的时间复杂度由原来的$O(M^2)$降低到$O(Mlog_2 M)$。 4、实现代码&emsp;&emsp;理清了整个算法思路之后，我们就可以很容易地实现FFT算法了。 &emsp;&emsp;首先是算法的输入，为了将DFT和IDFT结合起来，我们将算法的输入设为复数元素的矩阵。进行DFT正变换时将图像的像素值赋值到复数元素的实部，虚部为$0$，构成实部为灰度值、虚部为$0$的复数元素矩阵；进行反变换时输入傅里叶系数矩阵，做相应的操作。因此，我们首先定义一个复数元素矩阵： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139//! complex number matrix class.class ComplexMatrix&#123;public: ComplexMatrix() = default; ComplexMatrix(const Image &amp;image); ComplexMatrix(int row, int col, const Complex &amp;value = Complex()); ComplexMatrix(const ComplexMatrix &amp;other); int getRow() const; int getCol() const; Complex *getRawData(); Complex operator()(int r, int c) const; std::vector&lt;Complex&gt; getRowData(int r) const; std::vector&lt;Complex&gt; getColData(int c) const; void multiply(const double &amp;value); void conjugated(); void set(int r, int c, const Complex &amp;value); void setRowData(int r, const std::vector&lt;Complex&gt; &amp;value); void setColData(int c, const std::vector&lt;Complex&gt; &amp;value);private: std::vector&lt;std::vector&lt;Complex&gt;&gt; data;&#125;;ComplexMatrix::ComplexMatrix(const Image&amp; image)&#123; int row = image.height(); int col = image.width(); data.resize(row, std::vector&lt;Complex&gt;(col)); //! (-1)^(x+y). for (size_t r = 0; r &lt; row; ++r) for (size_t c = 0; c &lt; col; ++c) this-&gt;set(r, c, Complex(image(c, r).r * pow(-1.0, r + c), 0.0));&#125;ComplexMatrix::ComplexMatrix(int row, int col, const Complex &amp; value)&#123; assert(row &gt;= 0 &amp;&amp; col &gt;= 0); data.resize(row, std::vector&lt;Complex&gt;(col, value));&#125;ComplexMatrix::ComplexMatrix(const ComplexMatrix &amp; other)&#123; int row = other.getRow(); int col = other.getCol(); data.resize(row, std::vector&lt;Complex&gt;(col)); for (size_t r = 0; r &lt; row; ++r) for (size_t c = 0; c &lt; col; ++c) this-&gt;set(r, c, other(r, c));&#125;Complex ComplexMatrix::operator()(int r, int c) const&#123; if (r &lt; 0 || r &gt;= getRow()) return Complex(); if (c &lt; 0 || c &gt;= getCol()) return Complex(); return data[r][c];&#125;std::vector&lt;Complex&gt; ComplexMatrix::getRowData(int r) const&#123; int col = getCol(); std::vector&lt;Complex&gt; ret(col, Complex()); if (r &lt; 0 || r &gt;= getRow()) return ret; return data[r];&#125;std::vector&lt;Complex&gt; ComplexMatrix::getColData(int c) const&#123; int row = getRow(); std::vector&lt;Complex&gt; ret(row, Complex()); if (c &lt; 0 || c &gt;= getCol()) return ret; for (size_t r = 0; r &lt; row; ++r) ret[r] = (*this)(r, c); return ret;&#125;void ComplexMatrix::multiply(const double &amp; value)&#123; int row = getRow(); int col = getCol(); for (size_t r = 0; r &lt; row; ++r) for (size_t c = 0; c &lt; col; ++c) this-&gt;set(r, c, (*this)(r, c) * value);&#125;void ComplexMatrix::conjugated()&#123; int row = getRow(); int col = getCol(); for (size_t r = 0; r &lt; row; ++r) for (size_t c = 0; c &lt; col; ++c) this-&gt;set(r, c, (*this)(r, c).conjugate());&#125;int ComplexMatrix::getRow() const &#123; return data.size(); &#125;int ComplexMatrix::getCol() const&#123; if (data.size() &gt; 0) return data[0].size(); else return 0;&#125;Complex * ComplexMatrix::getRawData() &#123; return data.data()-&gt;data(); &#125;void ComplexMatrix::set(int r, int c, const Complex &amp; value)&#123; if (r &lt; 0 || r &gt;= getRow()) return; if (c &lt; 0 || c &gt;= getCol()) return; data[r][c] = value;&#125;void ComplexMatrix::setRowData(int r, const std::vector&lt;Complex&gt;&amp; value)&#123; if (r &lt; 0 || r &gt;= getRow()) return; assert(value.size() == getCol()); for (size_t c = 0; c &lt; getCol(); ++c) this-&gt;set(r, c, value[c]);&#125;void ComplexMatrix::setColData(int c, const std::vector&lt;Complex&gt;&amp; value)&#123; if (c &lt; 0 || c &gt;= getCol()) return; assert(value.size() == getRow()); for (size_t r = 0; r &lt; getRow(); ++r) this-&gt;set(r, c, value[r]);&#125; &emsp;&emsp;然后实现一维的傅里叶变换，根据分治策略写好即可，这里目前我只实现了递归版本。递归虽然实现简单、简洁，但对性能不太友好，因为涉及到太多的函数调用和跳转，因此性能最佳的应该是将递归转成迭代版本（这里涉及到蝶形算法，暂不展开），自底向上逐步计算而非自顶向下展开。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Complex FFT::W(const double &amp; u, const double &amp; x, const double &amp; k)&#123; //! euler equation -&gt; exp(-j*2*pi*u*x/k) = cos(-2*pi*u*x/k) + jsin(-2*pi*u*x/k) double theta = -M_2_PI * u * x / k; return Complex(cos(theta), sin(theta));&#125;std::vector&lt;Complex&gt; FFT::fourierTransform1D(const std::vector&lt;Complex&gt;&amp; samples)&#123; std::vector&lt;Complex&gt; result; //! at the bottom of recursion. if (samples.size() == 2) &#123; result.resize(2); result[0] = samples[0] + samples[1]; result[1] = samples[0] - samples[1]; return result; &#125; //! divide and conquer. int M = samples.size(); int K = M / 2; std::vector&lt;Complex&gt; oddSamples(K); std::vector&lt;Complex&gt; evenSamples(K); for (size_t i = 0; i &lt; K; ++i) &#123; evenSamples[i] = samples[2 * i]; oddSamples[i] = samples[2 * i + 1]; &#125; //! calculation of sub problems. auto evenResult = fourierTransform1D(evenSamples); auto oddResult = fourierTransform1D(oddSamples); //! merge the odd result and the event result. result.resize(M); for (size_t u = 0; u &lt; K; ++u) &#123; auto W_u_2K = W(static_cast&lt;double&gt;(u), 1.0, static_cast&lt;double&gt;(M)); W_u_2K *= oddResult[u]; result[u] = evenResult[u] + W_u_2K; result[u + K] = evenResult[u] - W_u_2K; &#125; return result;&#125; &emsp;&emsp;最后的二维就是在一维的基础上进行： 1234567891011121314151617181920212223242526ComplexMatrix FFT::fourierTransform2D(const ComplexMatrix &amp; target)&#123; ComplexMatrix result(target); size_t row = result.getRow(); size_t col = result.getCol(); //! do fft per row. for (size_t i = 0; i &lt; row; ++i) &#123; std::vector&lt;Complex&gt; data = result.getRowData(i); auto value = fourierTransform1D(data); result.setRowData(i, value); &#125; //! do fft per col. for (size_t i = 0; i &lt; col; ++i) &#123; std::vector&lt;Complex&gt; data = result.getColData(i); auto value = fourierTransform1D(data); result.setColData(i, value); &#125; result.multiply(1.0 / sqrt(row * col)); return result;&#125; &emsp;&emsp;傅里叶的正变换和反变换都根据上面的代码实现，只不过反变换需要额外的一些操作，但都不难： 123456789101112131415161718ComplexMatrix Image::fft2D(const ComplexMatrix &amp; target, bool flag)&#123; // discrete fourier transform. if (flag) &#123; ComplexMatrix ret = FFT::fourierTransform2D(target); return ret; &#125; //! inverse discrete fourier transform. else &#123; ComplexMatrix tmp = target; tmp.conjugated(); ComplexMatrix ret = FFT::fourierTransform2D(tmp); ret.conjugated(); return ret; &#125;&#125; &emsp;&emsp;最后还需要特别注意的是FFT假定输入的矩阵大小都是$2$的幂次方，但输入的图像并不一定是$2$的幂次方，因此对于不是$2$的幂次方的图像，需要做适当的填充： 1234567891011121314151617181920212223242526272829303132Image Image::expandToPowerOf2(const Image &amp; target)&#123; int width = target.width(); int height = target.height(); int basis = 2; while (basis &lt; width) basis *= 2; width = basis; basis = 2; while (basis &lt; height) basis *= 2; height = basis; Image result; result.initialize(width, height, target.channel()); for (size_t row = 0; row &lt; height; ++row) &#123; for (size_t col = 0; col &lt; width; ++col) &#123; if (row &lt; target.height() &amp;&amp; col &lt; target.width()) &#123; result.set(col, row, target(col, row)); &#125; else result.set(col, row, Pixel(0, 0, 0, 255)); &#125; &#125; return result;&#125; &emsp;&emsp;最后的最后，可能需要将图像的频谱输出，简单计算傅里叶系数矩阵的谱，并做一些适当的标定（这个很重要，不然输出的频谱图看不出频率域的特征）。 12345678910111213141516171819202122232425262728293031Image Image::spectrumOfcomplexMatrix(const ComplexMatrix &amp; target)&#123; //! get the spectrum image double tMax = -1; double tMin = FLT_MAX; Image output; output.initialize(target.getCol(), target.getRow(), 1); for (size_t row = 0; row &lt; target.getRow(); ++row) &#123; for (size_t col = 0; col &lt; target.getCol(); ++col) &#123; double gray = log2(target(row, col).length() + 1); tMax = std::max(tMax, gray); tMin = std::min(tMin, gray); &#125; &#125; for (size_t row = 0; row &lt; target.getRow(); ++row) &#123; for (size_t col = 0; col &lt; target.getCol(); ++col) &#123; Pixel tmp; tmp.a = 255; double gray = log2(target(row, col).length() + 1); gray = (gray - tMin) / (tMax - tMin) * 255; tmp.r = tmp.g = tmp.b = static_cast&lt;unsigned char&gt;(gray); output.set(col, row, tmp); &#125; &#125; return output;&#125; &emsp;&emsp;下面是一些程序的运行结果，左边是原图，中间是频谱图，右边是相角图。一般频谱图中心是最亮的，这是因为中心周围的值代表了低频分量，图像一般都是低频分量（平滑部分）比较多，高频分量（尖锐部分）比较少，因而四周比较暗。频谱值代表了原图的灰度值分布，而相角则蕴含了原图的形状内容，但是仅从频谱和相角无法从肉眼上知道原图的模样。 参考资料：$[1]$ （美）拉斐尔 C.冈萨雷斯（Rafael C.Gonzalez），理查德 E.伍兹（Richard E.Woods）.数字图像处理 第3版 英文版[M].北京：电子工业出版社.2017.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Image processing","slug":"Image-processing","permalink":"https://yangwc.com/categories/Image-processing/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Image processing","slug":"Image-processing","permalink":"https://yangwc.com/tags/Image-processing/"}]},{"title":"流体模拟Fluid Simulation：Free-Surface Flow","slug":"FreeSurfaceFlow","date":"2019-09-22T13:58:59.573Z","updated":"2019-10-14T11:34:08.534Z","comments":true,"path":"2019/09/22/FreeSurfaceFlow/","link":"","permalink":"https://yangwc.com/2019/09/22/FreeSurfaceFlow/","excerpt":"在基于欧拉网格的烟雾模拟中，流体与自由面的接触面是一个模糊的边界，因此无需对烟雾的边界进行对流。但在液体模拟中，流体与自由面之间是有一个明确的边界，因此我们需要显式地跟踪这个流体边界来模拟液体的物理行为，基于水平集的自由表面流（Free-Surface Flow）是目前的主流方法。","text":"在基于欧拉网格的烟雾模拟中，流体与自由面的接触面是一个模糊的边界，因此无需对烟雾的边界进行对流。但在液体模拟中，流体与自由面之间是有一个明确的边界，因此我们需要显式地跟踪这个流体边界来模拟液体的物理行为，基于水平集的自由表面流（Free-Surface Flow）是目前的主流方法。 流体表面的定义 流体的水平集对流 流体水平集的扭曲矫正 参考资料 Free-Surface Flow &emsp;&emsp;在基于欧拉网格的烟雾模拟中，流体与自由面的接触面是一个模糊的边界，因此无需对烟雾的边界进行对流。但在液体模拟中，流体与自由面之间是有一个明确的边界，因此我们需要显式地跟踪这个流体边界来模拟液体的物理行为，基于水平集（Level set）的方法是目前的主流方法。 一、流体表面的定义&emsp;&emsp;在基于粒子的流体模拟中，我们无需显式地定义液体与自由面的边界，流体粒子所占据的区域就是液体所在的区域。通过周围的流体粒子，我们能够确定给定的区域是否处于流体内部还是外部。但是在基于欧拉网格的流体模拟中，我们很难直接确定区域是处于流体内部还是外部，这是因为流体表面是连续可微的。如果直接给欧拉网格一个二值标量场，$0$表示流体外部，$1$表示流体内部，那么在流体的边界处该标量场不是连续的，因而也不是可微的。保证流体边界处连续可微对于模拟高质量的流体来说非常重要，这是因为流体表面张力或者计算法线向量的计算都需要在流体表面做一些微分算子。 &emsp;&emsp;目前最为主流的方法就是采用隐式表面也就是符号距离场来表示流体的边界。隐式表面具有许多非常良好的性质。给定一个隐式表面的符号距离场函数$\\phi(x)$，那么我们可以通过符号来判断一个区域是处于流体内部还是外部，隐式表面被定义在$\\phi(x)=0$的零等值面上（如图1中的虚线）。$\\phi(x)$的梯度满足如下方程（程函方程）： |\\nabla \\phi(x)|= 1\\tag {1} 图1 符号距离场 &emsp;&emsp;符号距离场在表面附近的函数值是连续可微的，因而它能够表示连续平滑的流体表面。采用符号距离场来对物体表面进行建模的方法通常被称为水平集法（Level set method），目前水平集在各个领域都有涉及，流体模拟只是其中之一。通过将一个表面转换成符号距离场，我们可以非常容易地计算表面的一些几何性质。 &emsp;&emsp;水平法一个非常有用的点就是它处理拓扑结构变化的能力。如下图2所示，这张图分别展示了显式表面和隐式表面在处理圆形表面扩张时的差别。圆形表面向外扩张时，显式表面需要将每一个网格点沿着法线向量向外挪动得到扩张后的几何体，而隐式表面只需将其相应的符号距离场值减去扩张距离即可。当几何体大到一定的程度时，两个圆形会产生相交的区域（在流体中表现为液滴相融），显式表面法需要逐个计算相交点，非常复杂且很容易出错；而隐式表面无需额外的特殊处理。这就是隐式表面的优势所在。 图2 (a)显式以及(b)隐式的圆形表面扩张 &emsp;&emsp;水平集法尽管在这里有众多优势，但是实现起来也没那么容易。采用水平集法跟踪流体表面需要解决两个主要问题。一个就是如何正确地追踪流体表面，这个其实就是一个流体对流问题；另外一个就是如何解决符号距离场对流带来的扭曲问题，这是因为在对流时我们不能够很好地保持符号距离场的几何性质。 二、流体的水平集对流&emsp;&emsp;在对流步骤，流体的水平集与流体其他的标量场属性别无二致，因此我们只需将流体的水平集传入半拉格朗日对流解算器即可。但是因为流体的符号距离场并不是流体本身的物理属性，因此采用半拉格朗日法对符号距离场对流会在距离流体表面较远的地方带来一些扭曲的问题，这个我们稍后再看。 &emsp;&emsp;在常见的液体模拟中，实际上还有一个流体被我们忽略了，这个流体就是空气。普遍情况下，空气对液体的影响比较小，几乎可以忽略，因此一种简化的液体模拟模型——自由表面流（Free-Surface Flow）被提出。这种流体模拟假设空气对液体的压强、粘性作用很小，通常空气的压强被设置为某一个给定的常量值。这种液体模型是最简单的，但是也能够模拟出非常真实的液体。此外，还有一些模拟例如泡状流（Bubby Flow）能够模拟空气与液体的交互现象，这类流体模拟属于多相流模型（Multiphase Fluid Flow）。多相流在同一个系统中采用不同的密度、粘度系数等来模拟流体模拟多种不同的流体，因而较为复杂。 &emsp;&emsp;这里目前讨论的还是属于自由表面流，我们通过构建额外的流体水平集标量场来追踪流体的表面，流体的水平集和其他可对流的标量场一样在对流阶段采用半拉格朗日对流法进行对流。自由表面流直接忽略空气的流体量场，因此在对流时我们需要处理空气区域的速度场，因为自由表面流对空气区域的速度场是没有定义的，如下图3(a)所示，圆形区域是流体区域，在圆形区域外速度场无定义。但在半拉格朗日对流法中，由于其后向追踪的特性，我们有可能会追踪流体区域外部的空气区域去获取流体的速度场，因此我们需要做一个速度场外推（extrapolation）操作。 图3 (a)流体无外推的速度场,(b)外推后的速度场 &emsp;&emsp;将速度场从流体区域外推到空气区域与从流体区域外推到固体墙区域的过程类似，我们简单地采用诺伊曼边界条件，使速度场在法线方向上的变化率为零： \\frac{\\partial v}{\\partial n}=0 \\tag {2}&emsp;&emsp;我们通过在法线方向外推流体的速度场来求解上述的边界条件。最大的外推距离取决于流体的CFL距离，超出CFL距离的外推没有意义，因为即便在边界处逆向追踪，其追踪的距离也不会超过CFL距离。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960void LevelSetLiquidSolver3::extrapolateVelocityToAir(double currentCfl)&#123; auto sdf = signedDistanceField(); auto vel = gridSystemData()-&gt;velocity(); auto u = vel-&gt;uAccessor(); auto v = vel-&gt;vAccessor(); auto w = vel-&gt;wAccessor(); auto uPos = vel-&gt;uPosition(); auto vPos = vel-&gt;vPosition(); auto wPos = vel-&gt;wPosition(); Array3&lt;char&gt; uMarker(u.size()); Array3&lt;char&gt; vMarker(v.size()); Array3&lt;char&gt; wMarker(w.size()); uMarker.parallelForEachIndex([&amp;](size_t i, size_t j, size_t k) &#123; if (isInsideSdf(sdf-&gt;sample(uPos(i, j, k)))) uMarker(i, j, k) = 1; else &#123; uMarker(i, j, k) = 0; u(i, j, k) = 0; &#125; &#125;); vMarker.parallelForEachIndex([&amp;](size_t i, size_t j, size_t k) &#123; if (isInsideSdf(sdf-&gt;sample(vPos(i, j, k)))) vMarker(i, j, k) = 1; else &#123; vMarker(i, j, k) = 0; v(i, j, k) = 0; &#125; &#125;); wMarker.parallelForEachIndex([&amp;](size_t i, size_t j, size_t k) &#123; if (isInsideSdf(sdf-&gt;sample(wPos(i, j, k)))) wMarker(i, j, k) = 1; else &#123; wMarker(i, j, k) = 0; w(i, j, k) = 0; &#125; &#125;); const Vector3D gridSpacing = sdf-&gt;gridSpacing(); const double h = max3(gridSpacing.x, gridSpacing.y, gridSpacing.z); const double maxDist = std::max(2.0 * currentCfl, _minReinitializeDistance) * h; std::cout &lt;&lt; \"Max velocity extrapolation distance: \" &lt;&lt; maxDist &lt;&lt; std::endl; FmmLevelSetSolver3 fmmSolver; fmmSolver.extrapolate(*vel, *sdf, maxDist, vel.get()); applyBoundaryCondition();&#125; &emsp;&emsp;有了外推值，我们就可以应用半拉格朗日对流解算器了。 12345678910void LevelSetLiquidSolver3::computeAdvection(double timeIntervalInSeconds)&#123; double currentCfl = cfl(timeIntervalInSeconds); Timer timer; extrapolateVelocityToAir(currentCfl); std::cout &lt;&lt; \"velocity extrapolation took \" &lt;&lt; timer.durationInSeconds() &lt;&lt; \" seconds\\n\"; GridFluidSolver3::computeAdvection(timeIntervalInSeconds);&#125; 三、流体水平集的扭曲矫正&emsp;&emsp;前面我们提到，由于流体的水平集并不是流体自身的物理属性，因而在进行半拉格朗日对流时我们没有办法很好地保证水平集的优良几何属性在对流过程中不被破坏。事实上，在经过对流之后，水平集在流体表面附近的几何属性依旧保持得很好，只是在距离表面比较远的地方它的符号距离场值不再代表当前的点到表面的最近距离（如下图4(c)所示），当然符号没有被破坏。注意到在流体表面处的符号距离场值被保持得很好，因此我们可以通过表面附近的值去修正远离表面的被扭曲的值（如下图4(d)所示）。 图4 (a)初始距离场,涡流(b)旋转导致距离场扭曲(c),复原后的距离场(d) &emsp;&emsp;在经过对流之后，流体的符号距离场被扭曲的现象在数学上被解释为不再满足程函方程$|\\nabla \\phi(x)|=1$，即前面的公式$(1)$。但我们知道$\\phi(x)$的符号依然是正确的，而且在流体表面附近$\\phi(x)$依然是一个正确的值。因此为了恢复流体的符号距离场，一种暴力的方法就是遍历所有的欧拉网格点并找寻找其到表面的最近点，显然这种方法太过复杂暴力且非常耗时。我们可以采用一个相反的策略。 图5 (a)复原后的距离场,(b)距离场修复过程 &emsp;&emsp;以图5(b)的一维情况为例（图中的水平线是零等值线），因为在零等值线上的距离场是没有被扭曲的（图5(b)中的虚线与实线在零等高线处的梯度相等，虚线是修复的距离场），因此我们可以从零等值线附近的欧拉网格点出发，逐步扩散到其他网格点，在扩散的过程中累加距离场。在一维的情况中，扩散的方向就是左和右两个方向，通过这个扩散过程逐步矫正经过的网格点的符号距离场，最终得到正确的符号距离场（如图5(a)所示）。 &emsp;&emsp;这个矫正的过程实际上跟波在空间中的传播很类似，因此我们可以借鉴流体的对流方程并做一些修改： \\frac{\\partial \\phi}{\\partial t}+u\\cdot \\nabla \\phi=1 \\tag {3}&emsp;&emsp;其中$u$是速度场（注意这里的速度场不再是流体的速度场），$t$是一个伪时间轴（因为这并不是物理模拟，它属于几何处理）。与原对流方程不同，方程右边是$1$而不是$0$。当方程右边是$0$时，它表明$\\phi$仅在速度场$u$的作用下变化；当方程右边是某个不为零的常量$c$时，它表明$\\phi$在速度场$u$的作用下经过一个单位距离时$\\phi$的增量为$c$。因此令方程右边的$c=1$就是使得$\\phi$在速度场$u$的作用下经过一个单位距离时$\\phi$的增量就是一个单位的距离。 &emsp;&emsp;公式$(3)$中的速度场就是传播速度，我们令其沿着表面的法线方向传播。我们根据隐式表面的符号距离场计算表面的单位法线向量： n=\\frac{\\nabla \\phi(x)}{|\\nabla \\phi(x)|} \\tag {4}&emsp;&emsp;在远离表面被扭曲的地方采用公式$(4)$计算法线向量显然会带来一些误差，这些暂且不管。将公式$(3)$中的$u$替换为公式$(4)$的法线场： \\frac{\\partial \\phi}{\\partial t}+\\frac{\\nabla \\phi}{|\\nabla \\phi|}\\cdot \\nabla \\phi=1 \\tag {5}&emsp;&emsp;然后公式$(5)$可以进一步简化为： \\frac{\\partial \\phi}{\\partial t}+(|\\nabla \\phi|-1)=0 \\tag {6}&emsp;&emsp;需要注意公式$(6)$仅适用于正距离场区域（也就是流体外部区域），对于负距离场区域，我们将采用的是下面的公式： \\frac{\\partial \\phi}{\\partial t}-(|\\nabla \\phi|-1)=0 \\tag {7}&emsp;&emsp;把公式$(6)$和公式$(7)$合并一起，可以写成如下的形式： \\frac{\\partial \\phi}{\\partial t}+sign(\\phi)(|\\nabla \\phi|-1)=0 \\tag {8}&emsp;&emsp;公式$(8)$其实不难理解。如果我们有一个正确的符号距离场，那么其满足方程$|\\nabla \\phi|=1$即$|\\nabla \\phi|-1=0$，此时由公式$(8)$可知$\\frac{\\partial \\phi}{\\partial t}=0$，这意味着$\\phi$不会发生变化，这也正是我们想要的（因为不需要矫正了）。而若$\\phi$产生了一下扭曲，导致一些区域的$|\\nabla \\phi|\\neq 1$，这就是说$|\\nabla\\phi|-1\\neq 0$，这个相当于一个误差项使得我们的距离场朝着误差项减小的方向变化，最终达到$|\\nabla \\phi|=1$。 &emsp;&emsp;最后就是关于公式$(8)$的实现了，我们通过迭代的方式来实现公式$(8)$，设置一个时间步长和迭代次数，在每一次的迭代中计算当前的$\\frac{\\partial \\phi}{\\partial t}$，然后采用一个显式欧拉积分更新流体的水平集。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758void IterativeLevelSetSolver3::reinitialize( const ScalarGrid3&amp; inputSdf, double maxDistance, ScalarGrid3* outputSdf) &#123; const Size3 size = inputSdf.dataSize(); const Vector3D gridSpacing = inputSdf.gridSpacing(); ArrayAccessor3&lt;double&gt; outputAcc = outputSdf-&gt;dataAccessor(); const double dtau = pseudoTimeStep(inputSdf.constDataAccessor(), gridSpacing); const unsigned int numberOfIterations = distanceToNumberOfIterations(maxDistance, dtau); copyRange3(inputSdf.constDataAccessor(), size.x, size.y, size.z, &amp;outputAcc); Array3&lt;double&gt; temp(size); ArrayAccessor3&lt;double&gt; tempAcc = temp.accessor(); std::cout &lt;&lt; \"Reinitializing with pseudoTimeStep: \" &lt;&lt; dtau &lt;&lt; \" numberOfIterations: \" &lt;&lt; numberOfIterations &lt;&lt; std::endl; for (unsigned int n = 0; n &lt; numberOfIterations; ++n) &#123; inputSdf.parallelForEachDataPointIndex( [&amp;](size_t i, size_t j, size_t k) &#123; double s = sign(outputAcc, gridSpacing, i, j, k); std::array&lt;double, 2&gt; dx, dy, dz; getDerivatives(outputAcc, gridSpacing, i, j, k, &amp;dx, &amp;dy, &amp;dz); // Explicit Euler step double val = outputAcc(i, j, k) - dtau * std::max(s, 0.0) * (std::sqrt( square(std::max(dx[0], 0.0)) + square(std::min(dx[1], 0.0)) + square(std::max(dy[0], 0.0)) + square(std::min(dy[1], 0.0)) + square(std::max(dz[0], 0.0)) + square(std::min(dz[1], 0.0))) - 1.0) - dtau * std::min(s, 0.0) * (std::sqrt(square(std::min(dx[0], 0.0)) + square(std::max(dx[1], 0.0)) + square(std::min(dy[0], 0.0)) + square(std::max(dy[1], 0.0)) + square(std::min(dz[0], 0.0)) + square(std::max(dz[1], 0.0))) - 1.0); tempAcc(i, j, k) = val; &#125;); std::swap(tempAcc, outputAcc); &#125; auto outputSdfAcc = outputSdf-&gt;dataAccessor(); copyRange3(outputAcc, size.x, size.y, size.z, &amp;outputSdfAcc);&#125; &emsp;&emsp;在计算符号的函数中，为了获取更加平滑的效果，我们采用一个更加平滑的符号计算方法，如下所示，其中$h$是网格单元的边长： sign=\\frac{\\phi}{\\sqrt{\\phi^2+h^2}} \\tag {9}1234567891011double IterativeLevelSetSolver3::sign( const ConstArrayAccessor3&lt;double&gt;&amp; sdf, const Vector3D&amp; gridSpacing, size_t i, size_t j, size_t k) &#123; double d = sdf(i, j, k); double e = min3(gridSpacing.x, gridSpacing.y, gridSpacing.z); return d / std::sqrt(d * d + e * e);&#125; &emsp;&emsp;然后紧接着我们用getDerivatives计算$\\phi$的梯度$\\nabla \\phi$，一种逆风法就是计算两个单边的差分，然后根据符号选择采用两个差分结果中的一个。最后根据符号以及梯度，在给定的时间步长下，我们更新每一个网格点的符号距离场值： \\phi=\\phi_{old}-\\Delta t\\cdot sign(\\phi)(|\\nabla \\phi|-1) \\tag {10}&emsp;&emsp;最后还有个时间步长的选取，如下所示： 1234567891011121314151617181920212223242526272829303132333435double IterativeLevelSetSolver3::pseudoTimeStep( ConstArrayAccessor3&lt;double&gt; sdf, const Vector3D&amp; gridSpacing) &#123; const Size3 size = sdf.size(); const double h = max3(gridSpacing.x, gridSpacing.y, gridSpacing.z); double maxS = -std::numeric_limits&lt;double&gt;::max(); double dtau = _maxCfl * h; for (size_t k = 0; k &lt; size.z; ++k) &#123; for (size_t j = 0; j &lt; size.y; ++j) &#123; for (size_t i = 0; i &lt; size.x; ++i) &#123; double s = sign(sdf, gridSpacing, i, j, k); maxS = std::max(s, maxS); &#125; &#125; &#125; while (dtau * maxS / h &gt; _maxCfl) dtau *= 0.5; return dtau;&#125;unsigned int IterativeLevelSetSolver3::distanceToNumberOfIterations( double distance, double dtau) &#123; return static_cast&lt;unsigned int&gt;(std::ceil(distance / dtau));&#125; &emsp;&emsp;这种迭代的方法非常有意思，在一层一层的迭代中，距离场的函数逐渐地被恢复原样、不再扭曲。符号距离场的这个恢复过程被称为重初始化（reinitialization）。在每一帧的对流之后，我们都要调用这个过程来复原真正的符号距离场，重初始化的范围我们取当前的CFL距离，因为超出CFL距离的重初始化没有意义。 12345678910void LevelSetLiquidSolver3::onEndAdvanceTimeStep(double timeIntervalInSeconds)&#123; double currentCfl = cfl(timeIntervalInSeconds); Timer timer; reinitialize(currentCfl); std::cout &lt;&lt; \"reinitializing level set field took \" &lt;&lt; timer.durationInSeconds() &lt;&lt; \" seconds\\n\"; ......&#125; 参考资料：$[1]$ Kim, D. (2017). Fluid engine development. Boca Raton: Taylor &amp; Francis, a CRC Press, Taylor &amp; Francis Group.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/tags/Fluid-Simulation/"}]},{"title":"流体模拟Fluid Simulation：基于欧拉网格的流体模拟","slug":"Smoke","date":"2019-09-12T08:52:08.033Z","updated":"2019-09-23T11:14:07.801Z","comments":true,"path":"2019/09/12/Smoke/","link":"","permalink":"https://yangwc.com/2019/09/12/Smoke/","excerpt":"基于粒子与基于网格的流体模拟算法是计算机图形学中的流体模拟界的两大算法框架，这两种算法分别对应了物理模拟中的两个视角：拉格朗日视角和欧拉视角。与基于粒子的拉格朗日流体模拟解然不同，基于欧拉网格的流体模拟采用了完全不同的算法策略，它不再关注随着物理规律运动的流体粒子，而将目光转向空间中的一组固定的网格点。","text":"基于粒子与基于网格的流体模拟算法是计算机图形学中的流体模拟界的两大算法框架，这两种算法分别对应了物理模拟中的两个视角：拉格朗日视角和欧拉视角。与基于粒子的拉格朗日流体模拟解然不同，基于欧拉网格的流体模拟采用了完全不同的算法策略，它不再关注随着物理规律运动的流体粒子，而将目光转向空间中的一组固定的网格点。 基于欧拉网格的流体模拟 MAC网格数据结构 边界条件处理 体积力项 半拉格朗日对流 流体粘度项 流体压强梯度项 烟雾流体模拟 参考资料 基于欧拉网格的流体模拟 &emsp;&emsp;基于粒子与基于网格的流体模拟算法是计算机图形学中的流体模拟界的两大算法框架，这两种算法分别对应了物理模拟中的两个视角：拉格朗日视角和欧拉视角。与基于粒子的拉格朗日流体模拟解然不同，基于欧拉网格的流体模拟采用了完全不同的算法策略，它不再关注随着物理规律运动的流体粒子，而将目光转向空间中的一组固定的网格点。流体模拟的这两种算法各有利弊，其中基于欧拉网格的模拟方法可以实现无条件稳定，这是基于粒子的模拟方法所不能及的。目前已经有人提出了两种方法结合的模拟策略，这里暂且不说。**相比于基于粒子的模拟方法，无论是从理论上还是从实现上，基于欧拉网格的模拟方法要复杂得多**。 ## 一、基于欧拉网格的流体模拟 &emsp;&emsp;首先来看基于欧拉网格的流体模拟算法的整体框架，主要参考了Jos Stam$^{[1]}$的极为经典的无条件稳定的流体模拟论文。基于欧拉网格的流体模拟算法同样是围绕Navier-Stokes流体力学方程展开，只不过采用了不同的偏微分方程求解方式： $$ \\nabla \\cdot u=0 \\tag {1} $$ $$ \\frac{D u}{Dt}=\\frac{\\partial u}{\\partial t}+u\\cdot \\nabla u=-\\frac1\\rho\\nabla p+\\mu \\nabla ^2u+f \\tag {2} $$ &emsp;&emsp;还是熟悉的流体力学方程，其中$u$是流体的速度场，$t$为时间，$\\rho$是流体密度，$\\mu$是流体的粘度系数，$f$是体积力项（如重力）。$\\frac{Du}{Dt}$是流体关于速度场的物质导数，在基于粒子的流体模拟中，每个粒子的加速度就是关于速度的物质导数$\\frac{Du}{Dt}$，这是因为物质导数的定义就是从拉格朗日视角展开的。拉格朗日视角下的物质导数可以转换成欧拉视角下的一个形式，就是公式$(2)$中的左边$\\frac{Du}{Dt}=\\frac{\\partial u}{\\partial t}+u\\cdot \\nabla u$，这是因为欧拉视角下我们关注的是空间中的一个固定点，因而物质导数就变成了给定点上的速度随时间的变化率$\\frac{\\partial u}{\\partial t}$与在流体先前的速度场作用下的变化率$u\\cdot \\nabla u$之和。因此，在基于欧拉网格的流体模拟方法中，我们需要计算固定点上的加速度$\\frac{\\partial u}{\\partial t}$，将$u\\cdot \\nabla u$移至右边： $$ \\frac{\\partial u}{\\partial t}=-u\\cdot \\nabla u-\\frac1\\rho\\nabla p+\\mu \\nabla^2u+f \\tag {3} $$ &emsp;&emsp;所以，相比于基于粒子的模拟方法，基于欧拉网格的模拟方法多了一项需要计算，即$-u\\cdot \\nabla u$，后面将会提到这一项就是**对流项**，这是粒子方法与网格方法最大的不同之处。 &emsp;&emsp;公式$(3)$中的压强梯度项$-\\frac1\\rho \\nabla p$与保持流体的不可压缩性（即散度为零，公式$(1)$）密切相关，计算压强项这一步我们通常称之为投影，即将速度场投影到散度为零的空间中，从而使得流体表现出不可压缩的特性。根据Helmholtz-Hodge分解定理，给定的一个速度场$w$，可将其分解成一个散度为零的速度矢量$u$与压强标量场的梯度之和$^{[2]}$： $$ w=u+\\nabla p \\tag {4} $$ &emsp;&emsp;其中的$u$满足$\\nabla \\cdot u=0$，即散度为零，如下图1所示。为此，我们引入一个正交投影操作算子$P$，通过算子$P$将速度场$w$投影到散度为零的部分$u$，即$u=Pw$。在线性代数中，投影算子$P$有这样的一种性质，即无论投影多少次，最终都等价于只投影一次，例如$P^2=P$。故同样有$Pu=u$，因为$Pu=P(Pw)=P^2w=Pw=u$。 图1 Helmholtz-Hodge分解 &emsp;&emsp;将算子$P$作用到公式$(4)$两边，有： Pw=Pu+P(\\nabla p)\\to u=u+P(\\nabla p) \\to P(\\nabla p)=0 \\tag {5}&emsp;&emsp;公式$(5)$得出了$P(\\nabla p)=0$，直观的意义就是经过算子$P$的作用，流体的压强梯度$\\nabla p$变为零，流体处于一个不可压缩的平衡状态。现在我们将算子$P$作用到流体力学公式$(3)$的两边，消去其中的$P(\\nabla p)$： \\frac{\\partial u}{\\partial t}=P(-(u\\cdot \\nabla u)+\\mu \\nabla^2u+f) \\tag {6}&emsp;&emsp;通过这个投影算子$P$，我们消去了原流体方程中的压强梯度项$-\\frac1\\rho \\nabla p$，其实是实质上转化成了投影算子$P$，从而解释了为什么求压强梯度项的算法被称为投影。公式$(6)$就是我们将要求解的核心方程，当然公式$(6)$其实是一个易于理解的形式，在最后做压强投影时实际上还是通过求解压强值，并将其作用到流体的速度场上来保证流体的不可压缩性。 &emsp;&emsp;在基于粒子的流体模拟算法中，我们将粒子的所有受力计算出来并进行叠加获取其合力，最后做一个时间积分。但是在基于欧拉网格的流体模拟中，我们是将公式$(6)$中的项拆分开来然后分步进行的，每一步的输出作为下一步的输入，依次进行，而且顺序不能乱。总的来说，需要计算的项分别是体积力项$f$，对流项$-(u\\cdot \\nabla u)$，粘度项$\\mu \\nabla^2 u$以及最后的压力投影项$-\\frac1\\rho \\nabla p$，算法的总体流程如下图2所示： 图2 算法流程 &emsp;&emsp;接下来就按照图2的顺序依次展开相应的计算算法。 二、MAC网格数据结构&emsp;&emsp;在展开各项的计算之前我们先要看看需要用到的一些数据结构。基于欧拉网格的流体模拟都采用了空间中固定划分的网格来承载流体的物理属性，包括一些标量场、矢量场，对于普通的标量场和矢量场，我们通常采用紧密的网格即可，每个网格点记载了当前这个点上的物理属性值，任意给定的物理属性值可以通过三线性插值得到，而且相应的梯度、散度和旋度以及拉普拉斯算子等的计算可以通过有限差分法计算得到。 &emsp;&emsp;但有个特殊的矢量需要做一些不同的处理，这个矢量就是流体的速度场。对于流体的速度场矢量，我们将采用交错的MAC网格：将流体的每个分量都分离出来，每个分量存储方式如下图3所示（以二维为例）： 图3 MAC网格 &emsp;&emsp;上图3中的二维流体分量分别为$u$和$v$，它们不是一起存储在网格的中心处，而是分别单独存储在网格的边上，$u$分量存储在每个网格格子的垂直于$x$轴的边的中心点，$v$分量存储在每个网格格子的垂直于$y$轴的边的中心点。而普通的一些标量场（例如上图中的压强$p$、温度）和矢量场被存储在网格的中心点处。这个就是基于欧拉网格的流体模拟算法的数据分布结构。MAC网格增加了我们模拟算法实现的复杂度，但也带来了一些便利，通过MAC网格我们可以放心地采用中心差分法来做一些微分操作而不用担心普通的致密网格出现的零域（null space）问题。 &emsp;&emsp;采用了MAC网格存储流体的速度场后，速度场的$u$分量、$v$分量和$w$分量分别单独储存，后续求解流体方程时我们需要分别更新流体的速度场的各个分量（三个分量的更新都是类似的）。 三、边界条件处理&emsp;&emsp;紧接着我们还要考虑处理流体的边界问题，这也是整个流体模拟过程中非常重要的一部分。首先是固体边界的表示，这一部分设计到如何处理流体与边界的碰撞。在欧拉网格的流体模拟方法中，最为自然的方法就是采用水平集，同样构建一个空间网格，每个网格点存储采样碰撞体的符号距离场值，下面只是整体的算法代码，关于如何计算给定物体的符号距离场函数之前已经深入了解过了，这里不再赘述。 123456789101112131415161718192021222324252627282930313233343536void GridFractionalBoundaryConditionSolver3::onColliderUpdated( const Size3&amp; gridSize, const Vector3D&amp; gridSpacing, const Vector3D&amp; gridOrigin)&#123; if (_colliderSdf == nullptr) _colliderSdf = std::make_shared&lt;CellCenteredScalarGrid3&gt;(); _colliderSdf-&gt;resize(gridSize, gridSpacing, gridOrigin); if (collider() != nullptr) &#123; Surface3Ptr surface = collider()-&gt;surface(); ImplicitSurface3Ptr implicitSurface = std::dynamic_pointer_cast&lt;ImplicitSurface3&gt;(surface); if (implicitSurface == nullptr) implicitSurface = std::make_shared&lt;SurfaceToImplicit3&gt;(surface); _colliderSdf-&gt;fill([&amp;](const Vector3D&amp; pt) &#123; return implicitSurface-&gt;signedDistance(pt); &#125;); _colliderVel = CustomVectorField3::builder() .withFunction([&amp;](const Vector3D&amp; x) &#123; return collider()-&gt;velocityAt(x); &#125;) .withDerivativeResolution(gridSpacing.x) .makeShared(); &#125; else &#123; _colliderSdf-&gt;fill(kMaxD); _colliderVel = CustomVectorField3::builder() .withFunction([](const Vector3D&amp;) &#123; return Vector3D(); &#125;) .withDerivativeResolution(gridSpacing.x) .makeShared(); &#125;&#125; &emsp;&emsp;有了水平集网格，我们就通过采样流体网格点的符号距离场来判断是否在流体的内部，若值为负，则在物体的内部发生了穿透，需要做一些矫正。当然在这里并不存在显式的流体粒子，因而矫正的不是流体粒子的位置，而是每个空间网格点上的流体属性值，似乎有点抽象，其实质上还是等价于纠正了流体微团的位置。 &emsp;&emsp;我们将要实现的就是无通量边界条件。无通量边界条件就是碰撞不穿透约束，即在碰撞体的表面处，流体的速度场投影到表面法线方向上的值为零，这种边界条件数学上的定义为： u\\cdot n=0 \\tag {7}&emsp;&emsp;其中，$u$是边界处流体的速度场矢量，$n$是边界出碰撞体的单位表面法线。当然上面说的仅仅是碰撞体静止时的情况，我们还需要考虑碰撞体的移动，因此实际上要求的是流体的速度场与碰撞体的速度场的差（即相对速度）在法线方向上的投影为零： (u-u_c)\\cdot n=u_{rel}\\cdot n=0 \\tag {8}&emsp;&emsp;上式中，$u_c$是表面处碰撞体的速度场矢量，$u_{rel}$记为流体与碰撞体之间的相对速度。为了使得相对速度在法线方向上的投影为零，我们可以将相对速度分解成法线方向上的向量与切面向上的向量之和，然后直接取切面上的向量，得到满足碰撞约束的相对速度$u^t_{rel}$，最后得到满足约束的流体速度场$u^n$： u^t_{rel}=u_{rel}-(u_{rel}\\cdot n)n \\\\u^n=u^n_{rel}+u_c \\tag {9}&emsp;&emsp;碰撞体边界统一用水平集构成的符号距离场表示，因而其表面法线$n$就是其符号距离场函数的梯度向量。我们实际上处理的不单单是碰撞体表面上的流体网格点，还需要处理在物体内部的点，因而对于符号距离值为负的流体网格点均需要做以上的边界处理，其余部分保持不变，下面是以流体速度场的$u$分量为例（后面均以$u$分量为例，其余的$v$分量和$w$分量类似）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 velocity-&gt;parallelForEachUIndex([&amp;](size_t i, size_t j, size_t k) &#123; Vector3D pt = uPos(i, j, k); if (isInsideSdf(_colliderSdf-&gt;sample(pt))) &#123; Vector3D colliderVel = collider()-&gt;velocityAt(pt); Vector3D vel = velocity-&gt;sample(pt); Vector3D g = _colliderSdf-&gt;gradient(pt); if (g.lengthSquared() &gt; 0.0) &#123; Vector3D n = g.normalized(); Vector3D velr = vel - colliderVel; Vector3D velt = projectAndApplyFriction(velr, n, collider()-&gt;frictionCoefficient()); Vector3D velp = velt + colliderVel; uTemp(i, j, k) = velp.x; &#125; else uTemp(i, j, k) = colliderVel.x; &#125; else uTemp(i, j, k) = u(i, j, k); &#125;); ...... u.parallelForEachIndex([&amp;](size_t i, size_t j, size_t k) &#123; u(i, j, k) = uTemp(i, j, k); &#125;); //------------------------------------------------------------------- template &lt;size_t N&gt; inline Vector&lt;double, N&gt; projectAndApplyFriction( const Vector&lt;double, N&gt;&amp; vel, const Vector&lt;double, N&gt;&amp; normal, double frictionCoefficient) &#123; Vector&lt;double, N&gt; velt = vel.projected(normal); if (velt.lengthSquared() &gt; 0) &#123; double veln = std::max(-vel.dot(normal), 0.0); velt *= std::max(1.0 - frictionCoefficient * veln / velt.length(), 0.0); &#125; return velt; &#125; &emsp;&emsp;上面的projectAndApplyFriction就是实现了公式$(9)$的函数代码，它负责计算投影到切面上的向量，最后还乘上了一个缩放系数，这个系数来源于我们如何处理无通量边界条件得到的切面上的速度场。通常分为两种情况：自由滑移、无滑移$^{[3]}$。自由滑移就是流体在边界处的速度场直接取值为表面切向上的速度场，无滑移就是流体在边界处的速度场直接取零而不是切向速度，这个不难理解。在这里我们考虑这两者的综合，我们考虑碰撞体的摩擦力： u_t=max(1-\\mu \\frac{max(-u\\cdot n, 0)}{|u_t|},0)u_t \\tag {10}&emsp;&emsp;公式中的$\\mu$就是碰撞体表面的摩擦系数，$u_t$是前面计算得到的切向上的速度场。然后在展开整个流体的边界处理之前，我们需要做一些预处理。通常在整个欧拉网格中，只有属于流体区域的网格点的速度场才有定义，但是在边边界处理时，我们要处理的是那些碰撞体内部的速度场，因而需要根据当前流体的速度场来外推（extrapolation）非流体区域的速度场，用于后续的边界处理，这些物体内部的流体速度场是一个虚拟的速度场。外推算法采用简单的广度优先，但是我们需要指定一个外推深度，因此采用迭代的方法，每一次的迭代表示向外推进一层。一开始我们需要创建一个标记数组marker，这个marker规模与存储速度场的大小一致，对于每一个marker，若当前的网格点在碰撞体内部，则标记为”Solid”，否则标记为”Fluid”，然后采用外推算法填充速度场数组中被标记为”Solid”的那些网格点，对于每一个”Solid“的网格点，其速度场值等于周围相邻的八个网格点中被标记为”Fluid”的速度场的平均值。以速度场的$u$分量为例，其他两个分量类似： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108 auto u = velocity-&gt;uAccessor(); auto uPos = velocity-&gt;uPosition(); Array3&lt;double&gt; uTemp(u.size()); Array3&lt;char&gt; uMarker(u.size(), 1); Vector3D h = velocity-&gt;gridSpacing(); // Assign collider's velocity first and initialize markers velocity-&gt;parallelForEachUIndex([&amp;](size_t i, size_t j, size_t k) &#123; Vector3D pt = uPos(i, j, k); double phi0 = _colliderSdf-&gt;sample(pt - Vector3D(0.5 * h.x, 0.0, 0.0)); double phi1 = _colliderSdf-&gt;sample(pt + Vector3D(0.5 * h.x, 0.0, 0.0)); double frac = fractionInsideSdf(phi0, phi1); frac = 1.0 - clamp(frac, 0.0, 1.0); if (frac &gt; 0.0) uMarker(i, j, k) = 1; else &#123; Vector3D colliderVel = collider()-&gt;velocityAt(pt); u(i, j, k) = colliderVel.x; uMarker(i, j, k) = 0; &#125; &#125;); ...... // Extrapolate fluid velocity into the collider extrapolateToRegion(velocity-&gt;uConstAccessor(), uMarker.constAccessor(), extrapolationDepth, u);//------------------------------------------------------------------------- template &lt;typename T&gt; void extrapolateToRegion( const ConstArrayAccessor3&lt;T&gt;&amp; input, const ConstArrayAccessor3&lt;char&gt;&amp; valid, unsigned int numberOfIterations, ArrayAccessor3&lt;T&gt; output) &#123; const Size3 size = input.size(); assert(size == valid.size()); assert(size == output.size()); Array3&lt;char&gt; valid0(size); Array3&lt;char&gt; valid1(size); valid0.parallelForEachIndex([&amp;](size_t i, size_t j, size_t k) &#123; valid0(i, j, k) = valid(i, j, k); output(i, j, k) = input(i, j, k); &#125;); for (unsigned int iter = 0; iter &lt; numberOfIterations; ++iter) &#123; valid0.forEachIndex([&amp;](size_t i, size_t j, size_t k) &#123; T sum = zero&lt;T&gt;(); unsigned int count = 0; if (!valid0(i, j, k)) &#123; if (i + 1 &lt; size.x &amp;&amp; valid0(i + 1, j, k)) &#123; sum += output(i + 1, j, k); ++count; &#125; if (i &gt; 0 &amp;&amp; valid0(i - 1, j, k)) &#123; sum += output(i - 1, j, k); ++count; &#125; if (j + 1 &lt; size.y &amp;&amp; valid0(i, j + 1, k)) &#123; sum += output(i, j + 1, k); ++count; &#125; if (j &gt; 0 &amp;&amp; valid0(i, j - 1, k)) &#123; sum += output(i, j - 1, k); ++count; &#125; if (k + 1 &lt; size.z &amp;&amp; valid0(i, j, k + 1)) &#123; sum += output(i, j, k + 1); ++count; &#125; if (k &gt; 0 &amp;&amp; valid0(i, j, k - 1)) &#123; sum += output(i, j, k - 1); ++count; &#125; if (count &gt; 0) &#123; output(i, j, k) = sum / static_cast&lt;typename ScalarType&lt;T&gt;::value&gt;(count); valid1(i, j, k) = 1; &#125; &#125; else valid1(i, j, k) = 1; &#125;); valid0.swap(valid1); &#125; &#125; &emsp;&emsp;通过上面的外推处理，我们可以放心地处理边界情况了。在处理完边界情况的最后，我们还需要处理欧拉网格的边界，因为我们模拟的是空间中的固定区域，因而这些固定区域的边界需要处理，默认不处理的情况下就是允许流体流出我们的模拟区域，这个时候流出去的流体不会再被考虑。当然我们也可以让流体不流出这些区域，或者指定一些开口允许流出，这个时候将边上的流体速度场设为零即可，以速度场的$u$分量为例： 123456789101112if (closedDomainBoundaryFlag() &amp; kDirectionLeft) &#123; for (size_t k = 0; k &lt; u.size().z; ++k) for (size_t j = 0; j &lt; u.size().y; ++j) u(0, j, k) = 0;&#125;if (closedDomainBoundaryFlag() &amp; kDirectionRight) &#123; for (size_t k = 0; k &lt; u.size().z; ++k) for (size_t j = 0; j &lt; u.size().y; ++j) u(u.size().x - 1, j, k) = 0;&#125; &emsp;&emsp;总的来说流体的边界处理流程就是：构建碰撞体的水平集$\\to$根据水平集构建marker并外推速度场$\\to$处理碰撞体内部的速度场$\\to$模拟区域的边界处理。上面针对流体的速度场展开了具体的边界处理，但从宏观角度来说，这里的边界条件分为两种，分别是诺伊曼（Neumann）边界条件、狄拉克（Dirichlet）边界条件。 &emsp;&emsp;给定一个场$f$（可以是标量场，也可以是矢量场），诺伊曼边界条件是给出了量场$f$在边界处法线方向$n$上的导数等于某个具体给定的值$c$，即： \\frac{\\partial f}{\\partial n}=c \\tag {11}&emsp;&emsp;例如$c=0$，那么表示量场$f$在边界处沿着法线方向上不变。因此我们前面提到的无通量边界条件（公式$(7)$）就是诺伊曼边界条件的子集。而狄拉克边界条件非常简单，它是直接给出边界处量场$f$的取值$c$： f=c \\tag {12}&emsp;&emsp;当公式$(12)$中的$c=0$时，就等价于前面我们提到的无滑移速度场。每一次更新速度场我们都要调用一次固体边界处理。 四、体积力项&emsp;&emsp;首先来看最简单的部分——体积力项，或者说外力项。这一项主要是考虑外部的作用力，这类力不是通过接触直接作用到流体上，而是类似于隔山打牛的凌空作用，在流体模拟中主要有重力、浮力。体积力项的计算非常简单，根据牛顿定律简单地做一个前向欧拉积分即可： w_1(x)=w_0(x)+\\Delta t f(x,t) \\tag {13}&emsp;&emsp;外部的体积力有几种，其中最常见的就是重力，下面以重力为例： 1234567891011121314151617181920212223242526272829303132333435363738394041void GridFluidSolver3::computeExternalForces(double timeIntervalInSeconds) &#123; computeGravity(timeIntervalInSeconds);&#125;void GridFluidSolver3::computeGravity(double timeIntervalInSeconds)&#123; if (_gravity.lengthSquared() &gt; kEpsilonD) &#123; auto vel = _grids-&gt;velocity(); auto u = vel-&gt;uAccessor(); auto v = vel-&gt;vAccessor(); auto w = vel-&gt;wAccessor(); if (std::abs(_gravity.x) &gt; kEpsilonD) &#123; vel-&gt;forEachUIndex([&amp;](size_t i, size_t j, size_t k) &#123; u(i, j, k) += timeIntervalInSeconds * _gravity.x; &#125;); &#125; if (std::abs(_gravity.y) &gt; kEpsilonD) &#123; vel-&gt;forEachVIndex([&amp;](size_t i, size_t j, size_t k) &#123; v(i, j, k) += timeIntervalInSeconds * _gravity.y; &#125;); &#125; if (std::abs(_gravity.z) &gt; kEpsilonD) &#123; vel-&gt;forEachWIndex([&amp;](size_t i, size_t j, size_t k) &#123; w(i, j, k) += timeIntervalInSeconds * _gravity.z; &#125;); &#125; applyBoundaryCondition(); &#125;&#125; &emsp;&emsp;计算重力对流体的速度场的作用之后，需要调用边界处理，修正固体边界处的流体速度值。事实上，在每一次更新了流体的速度场之后，都需要进行边界处理。后续的对流、粘性以及压力等等处理完后都需要处理边界情况。 五、半拉格朗日对流&emsp;&emsp;接下来就是求解流体的对流项，在基于粒子的流体模拟中，对流项被自动地执行，即不需要对粒子进行对流。对流本质上就是在流体的速度场作用下，不同网格点之间传递流体微团。从拉格朗日视角来看，就是流体微团在速度场的作用下，携带流体微团在空间中移动，流体微团的一些性质（如流体微团的密度）在移动的过程中保持不变。因而，求解关于任意一个物理量场$q$的对流项实际上求解的是下面的对流方程： \\frac{Dq}{Dt}=\\frac{\\partial q}{\\partial t}+u\\cdot \\nabla q = 0 \\tag {14}&emsp;&emsp;其中$u$是流体的速度场向量。在基于粒子的流体模拟中，每个流体粒子已经自动实现了$\\frac{Dq}{Dt}=0$，因而省去了这一步骤。在基于欧拉网格的流体模拟中，我们关注的是空间中的固定点，因而空间中的固定点的关于物理量场$q$随时间的变化率为$\\frac{\\partial q}{\\partial t}$，故需要求解这个变化率，并在这个变化率的作用下更新相应网格点上保存的物理量场值$q$，这一个过程就是对流（Advection）。将公式$(14)$挪项： \\frac{\\partial q}{\\partial t}=-u\\cdot \\nabla q \\tag {15}&emsp;&emsp;一种已经过时的方法就是采用有限差分法来估算$\\frac{\\partial q}{\\partial t}$，虽然这种方法效果很差且局限性太大，但是了解它能够加深我们对流体模拟中的”稳定性“的理解。我们现在来看看基于有限差分法的逆风对流法，以一维为例： \\frac{\\partial q}{\\partial t}=-u\\frac{\\partial q}{\\partial x} \\tag {16}&emsp;&emsp;采用有限差分法估算公式$(16)$： \\frac{\\partial q}{\\partial t}=\\begin{cases}-u\\frac{f_i-f_{i-1}}{\\Delta x},\\ \\ \\ \\ if\\ \\ u>0\\\\-u\\frac{f_{i+1}-f_i}{\\Delta x},\\ \\ \\ \\ otherwise\\end{cases} \\tag {17}&emsp;&emsp;当$u&gt;0$时我们在左边取差分，否则在右边取差分，这就是这种方法被称为”逆风”的原因。现在假设我们有一个速度$u&gt;0$，那么根据公式$(17)$估算了$\\frac{\\partial q}{\\partial t}$之后，再采用前向欧拉法来更新物理量场$q$，即有： q_{i}^{t+\\Delta t}=q_{i}^t-\\Delta t u\\frac{f_i^t-f_{i-1}^t}{\\Delta x} \\tag {18}&emsp;&emsp;下图4很好的描述了这个对流的过程，灰色的点的$f$值就是我们更新后的量场$q$的值。 图4 1D的逆风对流法 &emsp;&emsp;可以看到，当我们的$u\\Delta t$小于网格单元格长度$\\Delta x$时，对流算法没什么问题。但是当$u\\Delta t$大于$\\Delta x$时，将会导致追踪了错误的灰色点，如下图5所示，正确的点应该是灰色的点下面的那个点。所以基于有限差分法的对流方法会出现不稳定的问题，归根揭底还是因为差分方法仅仅考虑了一个$\\Delta x$范围内的网格点，而$u\\Delta t$有可能过大导致追踪的点超出$\\Delta x$的范围，基于有限差分法的对流方法仅在$u\\Delta t/\\Delta x&lt;1$时保持稳定，这里的$u\\Delta t/\\Delta x$就是流体模拟中常说的Courant数，这种限制条件被称为Courant–Friedrichs–Lewy条件（简称为CFL条件）。在这里CFL条件的限制为1，也就是不能超过一个$\\Delta x$的范围，因而使得模拟的时间步长$\\Delta t$不能太大，极大地降低模拟效率。 图5 逆风对流的不稳定 &emsp;&emsp;目前，基于有限差分法的对流方法已经几乎不用，更多的还是采用了Jos Stam$^{[1]}$提出的无条件稳定的半拉格朗日对流算法，这种对流算法已经被证明了无条件稳定的对流算法，因而可以使用大的时间步长去模拟。这种对流算法被称为半拉格朗日对流是有原因的，它从拉格朗日视角来展开对流过程。如下图6所示，当前的每一个网格点，都可以看作是上一个时间点模拟区域中的某个流体粒子经过移动刚好落在当前的网格点，因而当前网格点（图6中的黑色点）上的物理场$q$取值应该是这个流体粒子（图6中的灰色点）的物理量场值。这个就是它的核心思想，可以看到它采用了拉格朗日的视角去解决欧拉网格对流的问题，因而被称为半拉格朗日对流。 图6 半拉格朗日对流 &emsp;&emsp;给定一个网格点位置为$x$，其相应的速度场为$u$，现在要计算第$n+1$个时间步长时其物理量场的取值，可以后向追踪到灰色的粒子位置为$x-\\Delta tu$，然后这个粒子的物理量场的值通过周围网格点做线性插值得到，最后网格点$x$的物理量场值$q(x)^{n+1}$为： q(x)^{n+1}=\\overline q(x-\\Delta tu)^n \\tag {19}1234567891011121314151617181920212223void SemiLagrangian3::advect( const ScalarGrid3&amp; input, const VectorField3&amp; flow, double dt, ScalarGrid3* output, const ScalarField3&amp; boundarySdf) &#123; auto outputDataPos = output-&gt;dataPosition(); auto outputDataAcc = output-&gt;dataAccessor(); auto inputSamplerFunc = getScalarSamplerFunc(input); auto inputDataPos = input.dataPosition(); double h = min3(output-&gt;gridSpacing().x, output-&gt;gridSpacing().y, output-&gt;gridSpacing().z); output-&gt;parallelForEachDataPointIndex([&amp;](size_t i, size_t j, size_t k) &#123; if (boundarySdf.sample(inputDataPos(i, j, k)) &gt; 0.0) &#123; Vector3D pt = backTrace(flow, dt, h, outputDataPos(i, j, k), boundarySdf); outputDataAcc(i, j, k) = inputSamplerFunc(pt); &#125; &#125;);&#125; &emsp;&emsp;上面的代码以标量场的对流为例，其他矢量场以及MAC网格结构的类似。backTrace函数根据当前的网格点以及速度场进行后向追踪，返回一个追踪的点（其实就是$x-\\Delta tu$），然后根据这个点做一个线性插值得到新的物理量场的值。 &emsp;&emsp;可以看到半拉格朗日对流算法通过当前的速度场进行后向追踪，最终的物理量场值是通过其所在网格点做插值得到，因而不会超出原来量场中的最大值和最小值，而且也不会受限于$\\Delta tu$的大小，因而是无条件稳定的。半拉格朗日对流的实现非常简单，对于流体区域的每一个网格点的每一个物理量场，做一个公式$(19)$中的逆向追踪即可。但是存在一些准确率不高的问题，如下图7(a)所示，这是一个环形涡流，$p_0$后向追踪的点$p_1$实际上并不是上一个时间点流到$p_0$的点，因而计算的结果不准确。 图7 中点法 &emsp;&emsp;目前已经有许多方法来提高对流的准确性，其中一个遍是重点法。如图7(b)所示，我们后向步进半个时间步长而不是一个时间步长得到中点$p_{mid}=p_0-(\\Delta t/2)v_0$，然后在重点$p_{mid}$处采样其速度场$v_{mid}$，最后采用$v_{mid}$去后向追踪$p_1=p_0-(\\Delta t) v_{mid}$。通过这个简单的改动，对流算法的准确率得到了很大的提升。 &emsp;&emsp;然而半拉格朗日对流还存在另外一个比较严重的问题，就是数值耗散（numerical diffusion）。我们通过后向追踪得到的点$p_1$很少会刚刚好网格点上，大多数情况都是落在网格的格子内部，这个时候我们通过线性插值来计算$p_1$对应的物理量场$q$。问题就出现在线性插值这里，当我们采用线性插值的时候我们就默认了该量场$q$在这个网格内呈线性分布，这在欧拉网格的分辨率非常高时不会有什么大的问题，但当分辨率没那么高时，就会产生可观的估算误差。网格越粗糙，误差越大。这种误差带来的影响就是流体细节（例如涡旋细节）的消失，从而影响模拟的视觉效果。 &emsp;&emsp;因为这一步对流体的模拟效果非常关键，近年来，已经有不少学者针对这个数值耗散问题展开相关研究，提出了不少改进算法，有的是结合了粒子模拟方法的隐式粒子法（因为粒子法不需要对流），有的采用了一些技巧避免了插值，有的在插值方法上做一些改进，还有的采用特殊的方法还原消失在插值中的细节。这里我们先来看针对插值方法的改进——Catmull–Rom样条插值。 &emsp;&emsp;简单的线性插值只获取了周围最近点的函数值信息，然后根据给定点的位置去计算相应的值。Catmull–Rom样条插值借助了更多的邻域点信息进行插值，它的函数曲线不再是一条直线，而是样条曲线。以1D为例，Catmull–Rom样条插值函数为一个三阶多项式： f(t)=a_3t^3+a_2t^2+a_1t+a_0 \\tag {20} 图8 Catmull–Rom样条插值 &emsp;&emsp;构建Catmull–Rom样条插值函数除了要知道最近点的函数值（这里就是$f_0$和$f_1$），还需要再获取另外两个点的函数值$f_{-1}$和$f_2$。令$f_{-1}、f_0、f_1、f_2$（就是图8中从左到右的四个黑色点）分别对应$t=-1、0、1、2$时的样条函数值，现在需要求解插值函数（即公式$(20)$）中的系数$a_3、a_2、a_1、a_0$，通过一些差分法以及方程联立，可得下面的方程组（$f_{-1}、f_0、f_1、f_2$均已知）： \\begin{align}&a_0=f(0)=f_0\\\\&a_1=f'(0)=(f_1-f_{-1})/2 \\\\&f(1)=a_3+a_2+a_1+a_0\\\\&f'(1)=3a_3+2a_2+a_1=(f_2-f_1)/2\\end{align}\\tag {21}&emsp;&emsp;通过公式$(21)$的方程组我可以求得全部系数，我们将传入的介于$[0,1]$之间的权重值传入插值函数并返回插值的结果。 1234567891011121314template &lt;typename S, typename T&gt;inline S catmullRom(const S&amp; f0, const S&amp; f1, const S&amp; f2, const S&amp; f3, T f) &#123; S d1 = (f2 - f0) / 2; S d2 = (f3 - f1) / 2; S D1 = f2 - f1; S a3 = d1 + d2 - 2 * D1; S a2 = 3 * D1 - 2 * d1 - d2; S a1 = d1; S a0 = f1; return a3 * cubic(f) + a2 * square(f) + a1 * f + a0;&#125; &emsp;&emsp;上面我们讨论了一维的情况，二维和三维的情况只是一维的推广，这一方面与单线性插值与双线性插值、三线性插值之间的关系类似，不再赘述。但在半拉格朗日对流中直接用Catmull–Rom插值替换线性插值会带来一些问题。线性插值函数是单调的，通常是从$f_0$到$f_1$单调增或者单调减，这保证了通过插值得到的值不会超过原来的最大值也不会低于原来的最小值，但是Catmull–Rom样条插值函数不能保证它是单调的，有可能导致插值得到的值超出了原来的范围，存在不稳定的问题。为此，需要做一些修改，使之在$[0,1]$范围内变成单调函数。如下图9所示，虚线部分是原Catmull–Rom样条函数，而实线部分就是修改后的函数曲线。记$D1$为$f_1-f_0$，$d_0=(f_1-f_{-1})/2$，$d_1=(f_2-f_1)/2$，这里$d_1$、$d_2$是$f’(0)$、$f’(1)$的差分近似，使之： \\begin{cases}sign(d_0)=sign(d_1)=sign(D1),\\ \\ \\ \\ D1\\neq 0\\\\d_1=d_2=0,\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ D1=0\\end{cases} \\tag {22} 图9 单调的Catmull–Rom样条插值 1234567891011121314151617181920212223template &lt;typename T&gt;inline T monotonicCatmullRom(const T&amp; f0, const T&amp; f1, const T&amp; f2, const T&amp; f3, T f) &#123; T d1 = (f2 - f0) / 2; T d2 = (f3 - f1) / 2; T D1 = f2 - f1; if (std::fabs(D1) &lt; kEpsilonD) d1 = d2 = 0; if (sign(D1) != sign(d1)) d1 = 0; if (sign(D1) != sign(d2)) d2 = 0; T a3 = d1 + d2 - 2 * D1; T a2 = 3 * D1 - 2 * d1 - d2; T a1 = d1; T a0 = f1; return a3 * cubic(f) + a2 * square(f) + a1 * f + a0;&#125; &emsp;&emsp;最后，我们还需要考虑后向追踪时的边界处理，我们后向追踪得到的点可能位于碰撞体内部，这个时候就需要做一个截断，使后向追踪的点落到碰撞体的表面上而不是内部，这个过程如下图10所示。我们通过获取网格点的符号距离场与追踪点的符号距离场的符号是否相同来判断是否发生了碰撞体穿透，若发生了穿透也通过它们的符号距离场来获取表面上的点。 图10 截断处理 123456789101112131415161718192021222324252627282930313233343536373839Vector3D SemiLagrangian3::backTrace( const VectorField3&amp; flow, double dt, double h, const Vector3D&amp; startPt, const ScalarField3&amp; boundarySdf) &#123; double remainingT = dt; Vector3D pt0 = startPt; Vector3D pt1 = startPt; while (remainingT &gt; kEpsilonD) &#123; // Adaptive time-stepping Vector3D vel0 = flow.sample(pt0); double numSubSteps = std::max(std::ceil(vel0.length() * remainingT / h), 1.0); dt = remainingT / numSubSteps; // Mid-point rule Vector3D midPt = pt0 - 0.5 * dt * vel0; Vector3D midVel = flow.sample(midPt); pt1 = pt0 - dt * midVel; // Boundary handling double phi0 = boundarySdf.sample(pt0); double phi1 = boundarySdf.sample(pt1); if (phi0 * phi1 &lt; 0.0) &#123; double w = std::fabs(phi1) / (std::fabs(phi0) + std::fabs(phi1)); pt1 = w * pt0 + (1.0 - w) * pt1; break; &#125; remainingT -= dt; pt0 = pt1; &#125; return pt1;&#125; &emsp;&emsp;上面的代码实现实际上是一个循环，这也是为了提升追踪的精度，每一次后向追踪步长最好不要超过一个$\\Delta x$的大小，超过了$\\Delta x$我们就把这个追踪过程分成几步，每一步最多走$\\Delta x$。半拉格朗日对流的算法复杂度$O(n)$，可以很容易地实现并行版本。 六、流体粘度项&emsp;&emsp;在经过拉格朗日对流之后，我们就需要求解流体的粘度项，流体的粘度项也是流体模拟一个重要的内容。要实现好的流体粘性对于一些高粘度的流体来说至关重要，这是一个研究热点。根据流体力学方程，流体的粘度项为： a_v=\\mu \\nabla^2 u \\tag {23}&emsp;&emsp;其中$a_v$就是在流体的粘性力作用下的加速度，$\\mu$粘度系数，$u$流体速度场矢量。求解这一项最简单的方法就是采用一个显式的欧拉前向时间积分： u^{n+1}=u^n+\\Delta t\\mu \\nabla^2 u^n \\tag {24}&emsp;&emsp;其中，$\\Delta t$为时间步长，$u^n$是当前的速度场，$u^{n+1}$是加入了粘度作用力的速度场，简单的欧拉前向时间积分实现很简单。对于除了流体速度场中的其他物理量场，只需简单地替换公式$(24)$中的速度场矢量即可，下面的代码以标量场为例，我们仅对在流体区域内的网格点做粘度项计算。 1234567891011121314151617181920212223242526void GridForwardEulerDiffusionSolver3::solve( const ScalarGrid3&amp; source, double diffusionCoefficient, double timeIntervalInSeconds, ScalarGrid3* dest, const ScalarField3&amp; boundarySdf, const ScalarField3&amp; fluidSdf)&#123; auto src = source.constDataAccessor(); Vector3D h = source.gridSpacing(); auto pos = source.dataPosition(); buildMarkers(source.resolution(), pos, boundarySdf, fluidSdf); source.parallelForEachDataPointIndex( [&amp;](size_t i, size_t j, size_t k) &#123; if (_markers(i, j, k) == kFluid) &#123; (*dest)(i, j, k) = source(i, j, k) + diffusionCoefficient * timeIntervalInSeconds * laplacian(src, _markers, h, i, j, k); &#125; else (*dest)(i, j, k) = source(i, j, k); &#125;);&#125; &emsp;&emsp;似乎看起来直接采用前向欧拉法计算流体粘度项非常简单、快速，但与采用基于有限差分法的对流类似，当模拟的时间步长超过一定的阈值时，也会出现不稳定的问题。以1D的情况为例，假设我们要求解物理量场$q$的粘度项，根据中心差分法来估算拉普拉斯算子，有： q[i]=q[i]+\\mu \\Delta t\\frac{q[i+1]-2q[i]+q[i-1]}{(\\Delta x)^2} \\tag {25}&emsp;&emsp;在整个求解过程中，$\\mu$、$\\Delta t$和$\\Delta x$时保持不变的，我们把它提取出来并记为$c$，公式$(25)$转变为： \\begin{align}c&=\\frac{\\mu \\Delta t}{(\\Delta x)^2}\\\\q[i]&=c\\cdot q[i+1]+(1-2c)\\cdot q[i]+c\\cdot q[i-1]\\end{align} \\tag {26}&emsp;&emsp;可以看到，把前向欧拉法求解流体粘度项写成公式$(26)$的形式，它就看起来像是一个滤波过程（如下图11(a)所示），对周围的领域做一个加权和，这个权重由$c$决定，因而$c$就是滤波核宽度。$c$越大，则滤波范围越广，从图像处理来说就会导致图像越模糊，在流体模拟这里就会导致流体粘性越大。但是这里的$c$是有范围限制的，$c$的下界为$0$（如下图11(b)所示），这个时候表现为不做任何改变；$c$的上界为$\\frac14$（如下图11(c)所示）。超出上界的$c$将会引入中心差分法涉及到的邻域网格点之外的其他网格点，这一点跟前面讨论的逆风对流的缺陷类似。 图11 diffusion filters &emsp;&emsp;因为$c=\\frac{\\mu \\Delta t}{(\\Delta x)^2}$，所以$c$的范围限制作用到了流体的粘度系数$\\mu$上，假设保持$\\Delta t$和$\\Delta x$不变，根据$c\\leq\\frac14$，那么$\\mu\\leq\\frac{(\\Delta x)^2}{4\\Delta t}$，这意味着我们不能取超过这个范围的流体粘度系数，否则将导致出现不稳定的问题，因此不能实现高粘性的流体（如蜂蜜、油类液体）。事实上，这类不稳定的因素是前向欧拉法固有存在的问题，因此为了实现高粘性的流体，前向欧拉法不能采用。 &emsp;&emsp;所以我们将采用的是后向欧拉法，前向欧拉法根据当前的状态以时间推进的顺序计算下一个时间点的状态，而后向欧拉法相反，它假设下一时间点的状态已知，那么可以推算前一个时间点的状态，然后建立一个方程组。后向欧拉法的方程如下（以1D为例）： q_i^n=q_i^{n+1}-\\Delta t \\mu \\nabla^2 q_i^{n+1} \\tag {27}&emsp;&emsp;公式$(27)$中，我们假设已知下一个时间点的$q_i^{n+1}$，然后推算前一个时间点的$q_i^n$，这个过程是前向欧拉法的逆过程，因而被称为后向欧拉法。这种方法从前往后推算，是无条件稳定的。同样采用中心差分法计算拉普拉斯算子，有： q_i^{n+1}-\\Delta t\\mu \\frac{q_{i+1}^{n+1}-2q_i^{n+1}+q_{i-1}^{n+1}}{\\Delta x^2}=q_i^n \\tag {28}&emsp;&emsp;做一些调整可得方程： \\begin{align}&c=\\frac{\\Delta t\\mu}{(\\Delta x)^2}\\\\&-cq_{i+1}^{n+1}+(2c+1)q_i^{n+1}-cq_{i-1}^{n+1} = q_i^n\\end{align} \\tag {29}&emsp;&emsp;对于每一个$q_i^{n+1}$都可以构建一个上述方程，从而构建了一个大规模的线性方程组。其中$q_i^{n+1}$的系数中的$c$的系数等于其周围4-邻居的数量，例如1D时最左边和最右边其邻居数只有一个，因而其系数为$(c+1)$。二维网格大多数邻居为4，因而其系数为$(4c+1)$，三维依次类推。将线性方程组写成矩阵的形式如下： \\left[\\begin{matrix}c+1&-c&0&...&0&0\\\\-c&2c+1&-c&...&0&0\\\\...&...&...&...&...&...\\\\0&0&...&-c&c+1&...\\end{matrix}\\right]\\cdot q^{n+1} = q^{n} \\tag {30}&emsp;&emsp;公式$(30)$中的系数矩阵是一个大规模的对称稀疏矩阵，问题转成了求解线性方程组$Ax=b$。求解大规模正定稀疏矩阵我们将采用预处理的共轭梯度法，涉及到的数学内容较多，这就不再细细展开。在求解之前需要构建一个稀疏矩阵，这里我们可以不需要耗费大量的空间去存贮大多数的零，其实在这个特定问题下矩阵的每一行元素数量是固定的，1D有3个，2D有5个，3D有7个，又因矩阵是对称的，所以我们每一行只需存储原来的一半。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192void GridBackwardEulerDiffusionSolver3::buildMatrix(const Size3&amp; size, const Vector3D&amp; c) &#123; _system.A.resize(size); bool isDirichlet = (_boundaryType == Dirichlet); // Build linear system _system.A.parallelForEachIndex( [&amp;](size_t i, size_t j, size_t k) &#123; auto&amp; row = _system.A(i, j, k); // Initialize row.center = 1.0; row.right = row.up = row.front = 0.0; if (_markers(i, j, k) == kFluid) &#123; if (i + 1 &lt; size.x) &#123; if ((isDirichlet &amp;&amp; _markers(i + 1, j, k) != kAir) || _markers(i + 1, j, k) == kFluid) row.center += c.x; if (_markers(i + 1, j, k) == kFluid) row.right -= c.x; &#125; if (i &gt; 0 &amp;&amp; ((isDirichlet &amp;&amp; _markers(i - 1, j, k) != kAir) || _markers(i - 1, j, k) == kFluid)) row.center += c.x; if (j + 1 &lt; size.y) &#123; if ((isDirichlet &amp;&amp; _markers(i, j + 1, k) != kAir) || _markers(i, j + 1, k) == kFluid) row.center += c.y; if (_markers(i, j + 1, k) == kFluid) row.up -= c.y; &#125; if (j &gt; 0 &amp;&amp; ((isDirichlet &amp;&amp; _markers(i, j - 1, k) != kAir) || _markers(i, j - 1, k) == kFluid)) row.center += c.y; if (k + 1 &lt; size.z) &#123; if ((isDirichlet &amp;&amp; _markers(i, j, k + 1) != kAir) || _markers(i, j, k + 1) == kFluid) row.center += c.z; if (_markers(i, j, k + 1) == kFluid) row.front -= c.z; &#125; if (k &gt; 0 &amp;&amp; ((isDirichlet &amp;&amp; _markers(i, j, k - 1) != kAir) || _markers(i, j, k - 1) == kFluid)) row.center += c.z; &#125; &#125;);&#125;void GridBackwardEulerDiffusionSolver3::buildVectors(const ConstArrayAccessor3&lt;double&gt;&amp; f, const Vector3D&amp; c) &#123; Size3 size = f.size(); _system.x.resize(size, 0.0); _system.b.resize(size, 0.0); // Build linear system _system.x.parallelForEachIndex( [&amp;](size_t i, size_t j, size_t k) &#123; _system.b(i, j, k) = _system.x(i, j, k) = f(i, j, k); if (_boundaryType == Dirichlet &amp;&amp; _markers(i, j, k) == kFluid) &#123; if (i + 1 &lt; size.x &amp;&amp; _markers(i + 1, j, k) == kBoundary) _system.b(i, j, k) += c.x * f(i + 1, j, k); if (i &gt; 0 &amp;&amp; _markers(i - 1, j, k) == kBoundary) _system.b(i, j, k) += c.x * f(i - 1, j, k); if (j + 1 &lt; size.y &amp;&amp; _markers(i, j + 1, k) == kBoundary) _system.b(i, j, k) += c.y * f(i, j + 1, k); if (j &gt; 0 &amp;&amp; _markers(i, j - 1, k) == kBoundary) _system.b(i, j, k) += c.y * f(i, j - 1, k); if (k + 1 &lt; size.z &amp;&amp; _markers(i, j, k + 1) == kBoundary) _system.b(i, j, k) += c.z * f(i, j, k + 1); if (k &gt; 0 &amp;&amp; _markers(i, j, k - 1) == kBoundary) _system.b(i, j, k) += c.z * f(i, j, k - 1); &#125; &#125;);&#125; &emsp;&emsp;构建完了矩阵就可以调用共轭梯度算法求解方程组： 1234567891011121314151617181920212223242526272829void GridBackwardEulerDiffusionSolver3::solve( const ScalarGrid3&amp; source, double diffusionCoefficient, double timeIntervalInSeconds, ScalarGrid3* dest, const ScalarField3&amp; boundarySdf, const ScalarField3&amp; fluidSdf)&#123; auto pos = source.dataPosition(); Vector3D h = source.gridSpacing(); Vector3D c = timeIntervalInSeconds * diffusionCoefficient / (h * h); buildMarkers(source.dataSize(), pos, boundarySdf, fluidSdf); buildMatrix(source.dataSize(), c); buildVectors(source.constDataAccessor(), c); if (_systemSolver != nullptr) &#123; // Solve the system _systemSolver-&gt;solve(&amp;_system); // Assign the solution source.parallelForEachDataPointIndex( [&amp;](size_t i, size_t j, size_t k) &#123; (*dest)(i, j, k) = _system.x(i, j, k); &#125;); &#125;&#125; 七、流体压强梯度项&emsp;&emsp;流体的压强项同样是采用了后向欧拉法，这样才能够保证流体模拟的无条件稳定。根据Navier-Stokes方程，流体的压强梯度项为： a_p=-\\frac{\\nabla p}{\\rho} \\tag {31}&emsp;&emsp;其中的$a_p$是流体内部压强梯度力作用下的加速度。采用后向欧拉的思想，我们已知下一个时间点的速度场$u^{n+1}$，求解压强梯度项就是为了保证流体的不可压缩性，即散度为零： u^{n+1}=u^n-\\Delta t\\frac{\\nabla p}{\\rho}\\\\\\nabla \\cdot u^{n+1} = 0 \\tag {32}&emsp;&emsp;设流体的密度保持不变，从而可得： \\nabla \\cdot u^{n+1}=\\nabla \\cdot u^n-\\frac{\\Delta t}{\\rho}\\nabla^2p\\\\\\to \\frac{\\Delta t}{\\rho}\\nabla^2p=\\nabla \\cdot u^n \\tag {33}&emsp;&emsp;上面推导处的方程与流体的粘度方程类似，左边是关于压强的拉普拉斯项，为未知项目，右边是关于当前速度场的散度，为已知项。这个就是关于压强的泊松方程（Pressure Poisson equation，简称为PPE），同样采用中心差分法计算拉普拉斯算子和散度： \\frac{\\Delta t}{\\rho}\\frac{p_{i+1}-2p_i+p_{i-1}}{(\\Delta x)^2}=\\frac{u^n_{i+1/2}-u^n_{i-1/2}}{\\Delta x} \\tag {34}&emsp;&emsp;与求解粘度方程类似，将其写成大规模稀疏的对称正定矩阵： \\frac{\\Delta t}{\\rho \\Delta x^2}\\left[\\begin{matrix}1&-1&0&0&...\\\\-1&2&-1&0&...\\\\0&-1&2&-1&...\\\\...&...&...&...&...\\end{matrix}\\right]\\left[\\begin{matrix}p_1\\\\p_2\\\\p_3\\\\...\\\\\\end{matrix}\\right]=-\\frac{1}{\\Delta x}\\left[\\begin{matrix}u^n_{3/2}-u^n_{1/2}\\\\u^n_{5/2}-u^n_{3/2}\\\\u^n_{7/2}-u^n_{5/2}\\\\...\\end{matrix}\\right] \\tag {35}&emsp;&emsp;同样地采用共轭梯度法求解上述的大规模的稀疏对称正定线性方程组。 八、烟雾流体模拟&emsp;&emsp;我们要模拟的烟雾是燃烧或者爆炸产生的，通常在烟雾周围的温度高于其他区域，温度较高的空气密度较低，从而产生一个上升力。与此同时，烟雾聚集的区域其质量增大，因而也会受到重力的因素产生下降力。这些垂直方向上的力就是浮力。一种计算浮力的方式就是考虑流体密度的变化，原来的压强泊松方程中的密度场$\\rho$不再是常量，而是会发生变化的，故求散度时不能直接提出公式外面，前面的公式$(33)$就变成了$^{[3]}$： \\nabla \\cdot \\frac{\\nabla p}{\\rho}=c\\frac{\\nabla \\cdot u}{\\Delta t} \\tag {36}&emsp;&emsp;公式$(36)$求解的是精确的浮力，需要耗费大量的计算资源，在一些密度差异非常关键的场合中这是非常重要的。但是在计算机图形学的烟雾模拟中不需要这么高精度地去计算流体密度产生的浮力作用力，我们采用下面的近似计算公式$^{[5]}$： f_{buoy}=-\\alpha \\rho \\vec y+\\beta (T-T_{amb})\\vec y \\tag {37}&emsp;&emsp;公式$(37)$中的$\\alpha$和$\\beta$是缩放系数，用于控制流体密度和温度对浮力的影响，$f_{buoy}$就是流体浮力，$T$是流体的温度场，$T_{amb}$为周围环境的温度，$\\rho$密度场，$\\vec y$时垂直方向上的单位向量。$T_{amb}$可以通过计算温度场的平均温度得到。公式$(37)$综合考虑了流体密度和流体温度的影响。流体密度的影响是产生一个垂直向下的作用力，因而$\\alpha \\rho \\vec y$前面还有个负号；流体温度的影响取决于当前温度与环境温度的差，高于环境温度会产生一个向上的作用力。 &emsp;&emsp;烟雾模拟需要存储的流体属性有速度场、密度场以及温度场，后两个都是标量场。这些属性在流体模拟中，都需要求解整个流体方程，即均需求解体积力项、对流项、粘度项以及压力项。在这里体积力项仅仅有浮力项（即公式$(37)$），重力因素已经被考虑进公式$(37)$了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263void GridSmokeSolver3::computeBuoyancyForce(double timeIntervalInSeconds)&#123; auto grids = gridSystemData(); auto vel = grids-&gt;velocity(); Vector3D up(0, 1, 0); if (gravity().lengthSquared() &gt; kEpsilonD) up = -gravity().normalized(); if (std::abs(_buoyancySmokeDensityFactor) &gt; kEpsilonD || std::abs(_buoyancyTemperatureFactor) &gt; kEpsilonD) &#123; auto den = smokeDensity(); auto temp = temperature(); // Ambient temperature double tAmb = 0.0; temp-&gt;forEachCellIndex([&amp;](size_t i, size_t j, size_t k) &#123; tAmb += (*temp)(i, j, k); &#125;); tAmb /= static_cast&lt;double&gt;(temp-&gt;resolution().x * temp-&gt;resolution().y * temp-&gt;resolution().z); auto u = vel-&gt;uAccessor(); auto v = vel-&gt;vAccessor(); auto w = vel-&gt;wAccessor(); auto uPos = vel-&gt;uPosition(); auto vPos = vel-&gt;vPosition(); auto wPos = vel-&gt;wPosition(); if (std::abs(up.x) &gt; kEpsilonD) &#123; vel-&gt;parallelForEachUIndex([&amp;](size_t i, size_t j, size_t k) &#123; Vector3D pt = uPos(i, j, k); double fBuoy = _buoyancySmokeDensityFactor * den-&gt;sample(pt) + _buoyancyTemperatureFactor * (temp-&gt;sample(pt) - tAmb); u(i, j, k) += timeIntervalInSeconds * fBuoy * up.x; &#125;); &#125; if (std::abs(up.y) &gt; kEpsilonD) &#123; vel-&gt;parallelForEachVIndex([&amp;](size_t i, size_t j, size_t k) &#123; Vector3D pt = vPos(i, j, k); double fBuoy = _buoyancySmokeDensityFactor * den-&gt;sample(pt) + _buoyancyTemperatureFactor * (temp-&gt;sample(pt) - tAmb); v(i, j, k) += timeIntervalInSeconds * fBuoy * up.y; &#125;); &#125; if (std::abs(up.z) &gt; kEpsilonD) &#123; vel-&gt;parallelForEachWIndex([&amp;](size_t i, size_t j, size_t k) &#123; Vector3D pt = wPos(i, j, k); double fBuoy = _buoyancySmokeDensityFactor * den-&gt;sample(pt) + _buoyancyTemperatureFactor * (temp-&gt;sample(pt) - tAmb); w(i, j, k) += timeIntervalInSeconds * fBuoy * up.z; &#125;); &#125; applyBoundaryCondition(); &#125;&#125; &emsp;&emsp;然后就是流体密度和流体温度的对流以及粘性计算，对流没什么需要特殊处理，这里不再赘述。粘性扩散计算其实也没什么特殊的地方，我们直接调用前面的流体粘性解算器。在求解完流体密度和温度的粘性项之后，我们给流体的密度和温度做一个衰减： 12345678910111213141516171819202122232425262728293031323334353637void GridSmokeSolver3::computeDiffusion(double timeIntervalInSeconds) &#123; if (diffusionSolver() != nullptr) &#123; if (_smokeDiffusionCoefficient &gt; kEpsilonD) &#123; auto den = smokeDensity(); auto den0 = std::dynamic_pointer_cast&lt;CellCenteredScalarGrid3&gt;(den-&gt;clone()); diffusionSolver()-&gt;solve(*den0, _smokeDiffusionCoefficient, timeIntervalInSeconds, den.get(), *colliderSdf()); extrapolateIntoCollider(den.get()); &#125; if (_temperatureDiffusionCoefficient &gt; kEpsilonD) &#123; auto temp = smokeDensity(); auto temp0 = std::dynamic_pointer_cast&lt;CellCenteredScalarGrid3&gt;(temp-&gt;clone()); diffusionSolver()-&gt;solve(*temp0, _temperatureDiffusionCoefficient, timeIntervalInSeconds, temp.get(), *colliderSdf()); extrapolateIntoCollider(temp.get()); &#125; &#125; // Decay auto den = smokeDensity(); den-&gt;parallelForEachDataPointIndex([&amp;](size_t i, size_t j, size_t k) &#123; (*den)(i, j, k) *= 1.0 - _smokeDecayFactor; &#125;); auto temp = temperature(); temp-&gt;parallelForEachDataPointIndex([&amp;](size_t i, size_t j, size_t k) &#123; (*temp)(i, j, k) *= 1.0 - _temperatureDecayFactor; &#125;);&#125; &emsp;&emsp;整个工程的代码具体见这里。 参考资料：$[1]$ Stam J. Stable fluids[J]. Acm Transactions on Graphics, 1999, 1999:121—128. $[2]$ A. J. Chorin and J. E. Marsden. A Mathematical Introduction to Fluid Mechanics. Springer-Verlag. Texts in Applied Mathematics 4. Second Edition., New York, 1990. $[3]$ Kim, D. (2017). Fluid engine development. Boca Raton: Taylor &amp; Francis, a CRC Press, Taylor &amp; Francis Group. $[4]$ R. Bridson and M. Müller-Fischer. Fluid simulation: Siggraph 2007 course notes. In ACM SIGGRAPH 2007 Courses, pages 1–81, ACM, 2007. $[5]$ Fedkiw R, Stam J, Jensen H W. Visual simulation of smoke[C]// Conference on Computer Graphics &amp; Interactive Techniques. 2001.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/tags/Fluid-Simulation/"}]},{"title":"流体模拟Fluid Simulation：基于SPH的拉格朗日流体模拟","slug":"SPH","date":"2019-08-29T07:28:55.738Z","updated":"2019-09-12T08:43:16.748Z","comments":true,"path":"2019/08/29/SPH/","link":"","permalink":"https://yangwc.com/2019/08/29/SPH/","excerpt":"本文主要是关于拉格朗日粒子的流体模拟经典算法——SPH。","text":"本文主要是关于拉格朗日粒子的流体模拟经典算法——SPH。 光滑粒子动力学 SPH流体模拟算法 PCISPH流体模拟算法 参考资料 基于SPH的拉格朗日流体模拟 &emsp;&emsp;SPH，全称为Smoothed Particle Hydrodynamics，最初提出于天体物理学领域，然后被广泛的应用到计算流体力学领域，成为基于拉格朗日粒子模拟方法的典型代表。实际上，目前除了流体，还有刚体、软体等的物理模拟也有不少采用了SPH的方法。SPH是一种基于光滑粒子核的物理模型，它将模拟的对象离散成一个个粒子，然后以光滑核将粒子之间联系起来，显然这是一种基于拉格朗日视角的模拟方法，相对于欧拉视角的模拟方法，它比较简单、速度较快。 图1 基于粒子的流体模拟 一、光滑粒子动力学&emsp;&emsp;SPH的核心在于它的光滑核函数，首先我们来看看光滑核函数的推导。对于一个标量函数$f$，给定一个积分区域，有： f(x)=\\int f(y)\\delta (x-y)dy \\tag {1}&emsp;&emsp;且$\\delta$是一个狄拉克函数，故上式$(1)$成立： \\delta (x-y)= \\{ \\begin{matrix} \\infty\\ \\ ,x=y\\\\ 0\\ \\ \\ \\ ,x\\neq y \\end{matrix} \\tag {2}&emsp;&emsp;将公式$(1)$中的狄拉克函数替换成一个光滑的核函数$\\omega$： f(x)\\approx \\int f(y)\\omega (||x-y||/h)dy \\tag {3}&emsp;&emsp;其中$h$是光滑核的半径长度，光滑核函数$\\omega$具有以下的性质： 当$||w-y||/h &gt;1$时，$w=0$，即超出核半径之外的定义域，函数值均取零。 $lim_{h\\to 0}w(||x-y||/h)=\\delta (x-y)$，即当核半径$h$趋于零时，核函数收敛到公式$(2)$的狄拉克函数。 归一化条件，$\\int w(||x-y||/h)dy=1$。 函数$\\omega$是光滑连续的，因而是可微的。 &emsp;&emsp;通过公式$(3)$，我们可以根据周围邻域的函数值近似获取给定的点的函数值，这些函数值可以是密度、压强等流体的属性值。公式$(3)$给出的是一个积分形式，在二维空间中的积分变量是面积，在三维空间中的积分变量是体积，我们采用黎曼和来离散地计算公式$(3)$： \\begin{align} f(x)\\approx &\\int f(y)\\omega (||x-y||/h)dy\\\\ \\approx &\\Sigma_{i=1}^N f_i w(||x-x_i||/h_i)V_i\\\\ \\approx &\\Sigma_{i=1}^N \\frac{m_i}{\\rho_i}f_i w(||x-x_i||/h_i) \\end{align} \\tag {4}&emsp;&emsp;其中$V_i$是我们取的体积微元，又因$\\rho=\\frac mv$，所以最终可以写成如上所示的计算公式。$f_i$是第$i$个邻域粒子的函数取值。公式$(4)$并不需要我们遍历在光滑核半径$h$之内的所有空间微元，在没有粒子的空间微元中，其函数取值$f$为零，因此我们只需遍历在光滑核半径$h$之内的所有粒子，因而上述的黎曼和是是邻居粒子的叠加形式。 &emsp;&emsp;除此之外，我们还需要根据邻居粒子的函数值计算给定粒子的函数梯度向量以及拉普拉斯算子。首先推导出理论上的公式，然后再做进一步的离散化。计算公式$(3)$的梯度向量如下所示，微分变量为$x$，即对$x$求矢量微分： \\begin{align} \\nabla_xf(x)\\approx &\\nabla_x \\int f(y)\\omega (||x-y||/h)dy\\\\ \\approx & \\int [\\nabla_xf(y)] \\omega (||x-y||/h)dy +\\int f(y)[\\nabla_x \\omega(||x-y||/h)]dy\\\\ \\approx &\\int f(y)[\\nabla_x \\omega(||x-y||/h)]dy \\end{align} \\tag {5}&emsp;&emsp;上述过程的推导用到了求导的乘法法则以及$\\nabla_xf(y)=0$。然后将公式$(5)$离散化为： \\nabla f(x)\\approx \\Sigma_{i=1}^N\\frac{m_i}{\\rho_i}f_i \\nabla w(||x-x_i||/h_i) \\tag {6}&emsp;&emsp;根据公式$(6)$计算梯度存在两个问题：一个是对于常量函数，公式$(6)$计算出来的梯度向量不为零；另一个就是不对称，我们通常需要计算梯度向量来获取粒子之间相互作用的力，力是相互作用的，因而是对称的，但是上述公式计算得到的向量并不是对称的。我们首先来看第一个问题，考虑将求导的函数再乘上一个密度函数$\\rho$： \\nabla (\\rho f)=f\\nabla \\rho +\\rho \\nabla f \\Rightarrow \\nabla f=\\frac1\\rho \\nabla(\\rho f)-\\frac1\\rho f\\nabla \\rho \\tag {7}&emsp;&emsp;详细展开，有： \\begin{align} \\nabla f(x_i)\\approx & \\frac 1{\\rho_i} \\Sigma_{j=1}^N\\frac{m_j}{\\rho_j}\\rho_j f_j\\nabla w(||x_i-x_j||/h_j)-\\frac 1{\\rho_i}f_i\\Sigma_{j=1}^N\\frac{m_j}{\\rho_j}\\rho_j\\nabla w(||x_i-x_j||/h_j)\\\\ \\approx & \\frac{1}{\\rho_i}\\Sigma_{j=1}^N m_j(f_j-f_i)\\nabla w(||x_i-x_j||/h_j) \\end{align} \\tag {8}&emsp;&emsp;公式$(8)$得到的梯度计算公式解决了第一个问题，即对于常量函数，其导数为零。对于第二个对称性的问题，我们采用同样的技巧，将求导函数除以一个密度函数$\\rho$： \\nabla (\\frac f\\rho)=-\\frac{f}{\\rho^2}\\nabla \\rho+\\frac{1}{\\rho}\\nabla f \\Rightarrow \\nabla f=\\rho(\\nabla (\\frac f\\rho) + \\frac{f}{\\rho^2}\\nabla \\rho) \\tag {9}&emsp;&emsp;详细展开后，得： \\nabla f(x_i)\\approx \\rho_i\\Sigma_{j=1}^N m_j(\\frac{f_j}{\\rho_j^2}+\\frac{f_i}{\\rho_i^2})\\nabla w(||x_i-xj||/h_j) \\tag {10}&emsp;&emsp;公式$(10)$计算得到得梯度向量是对称的。然后，还有拉普拉斯算子的计算非常常见，其计算公式如下： \\nabla^2 f(x_i)\\approx m\\Sigma_{j=1}^N \\frac{m_j}{\\rho_j}(f_j-f_i)\\nabla^2 w(||x_i-x_j||/h) \\tag {11}&emsp;&emsp;接下来我们就来看看核函数$w$的选取。在流体模拟中，一个标准的核函数如下： W_{poly6}(r,h)=\\frac{315}{64\\pi h^3} \\begin{cases} (1-\\frac{r^2}{h^2})^3\\ \\ 0\\leq r\\leq h\\\\ 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ otherwise \\end{cases} \\tag {12}&emsp;&emsp;因而实现的代码如下： 12345678910inline double SphStdKernel3::operator()(double distance) const &#123; if (distance * distance &gt;= h2) return 0.0; else &#123; double x = 1.0 - distance * distance / h2; return 315.0 / (64.0 * kPiD * h3) * x * x * x; &#125;&#125; &emsp;&emsp;其梯度计算公式为： \\nabla W_{poly6}(r,h)=\\frac{-945r}{32\\pi h^5}(1-\\frac{r^2}{h^2})^2, 0\\leq r\\leq h \\tag {13}123456789101112131415161718192021222324inline double SphStdKernel3::firstDerivative(double distance) const &#123; if (distance &gt;= h) return 0.0; else &#123; double x = 1.0 - distance * distance / h2; return -945.0 / (32.0 * kPiD * h5) * distance * x * x; &#125;&#125;inline Vector3D SphStdKernel3::gradient(const Vector3D&amp; point) const &#123; double dist = point.length(); if (dist &gt; 0.0) return gradient(dist, point / dist); else return Vector3D(0, 0, 0);&#125;inline Vector3D SphStdKernel3::gradient(double distance, const Vector3D&amp; directionToCenter) const&#123; return -firstDerivative(distance) * directionToCenter;&#125; &emsp;&emsp;其二阶导数为： \\nabla ^2W_{poly6}(r,h)=\\frac{945}{32\\pi h^5}(1-\\frac{r^2}{h^2})(\\frac{5r^2}{h^2}-1) \\tag {14}12345678910inline double SphStdKernel3::secondDerivative(double distance) const &#123; if (distance * distance &gt;= h2) return 0.0; else &#123; double x = distance * distance / h2; return 945.0 / (32.0 * kPiD * h5) * (1 - x) * (5 * x - 1); &#125;&#125; &emsp;&emsp;但是，当我们需要求取核函数的梯度向量核拉普拉斯算子的时候，我们并不会采用poly6核函数，这是因为虽然其原函数很光滑，但其一阶导数和二阶导数的性质却不是很好，因而不会使用poly6的导数。对于梯度向量和拉普拉斯的求取，我们用spiky核函数取而代之： W_{poly6}(r,h)=\\frac{15}{\\pi h^3} \\begin{cases} (1-\\frac{r}{h})^3\\ \\ 0\\leq r\\leq h\\\\ 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ otherwise \\end{cases} \\tag {15}&emsp;&emsp;下面的图2是关于poly6核函数与spiky核函数的函数曲线、一阶导数曲线和二阶导数曲线，左图是poly6函数的，右图是spiky函数的，实心曲线为函数自身的曲线，点线是函数的二阶导数曲线，而虚线则是函数的一阶导数曲线。可以看到poly6的一阶导数和二阶导数呈现出一个波动的形态，在中心处一阶导数甚至降为零。我们需要计算流体的压力梯度来确保流体的不可压缩性，如果直接采用poly6的一阶导数来计算流体的压力梯度力，那么当两个粒子重合时，其压力梯度力为零，不存在一个力使它们分开，从而违背了流体的不可压缩性。二阶导数同理，其甚至在中心处取值为负，二阶导数将用于拉普拉斯算子的计算，而流体的粘性力计算将用到拉普拉斯算子。 图2 poly6和spiky的原函数、一阶导数和二阶导数比较 &emsp;&emsp;spiky函数的一阶导数为： \\nabla W_{spiky}(r,h)=-\\frac{45}{\\pi h^4}(1-\\frac{r}{h})^2 \\tag {16}123456789101112131415161718192021222324inline double SphSpikyKernel3::firstDerivative(double distance) const &#123; if (distance &gt;= h) return 0.0; else &#123; double x = 1.0 - distance / h; return -45.0 / (kPiD * h4) * x * x; &#125;&#125;inline Vector3D SphSpikyKernel3::gradient(const Vector3D&amp; point) const &#123; double dist = point.length(); if (dist &gt; 0.0) return gradient(dist, point / dist); else return Vector3D(0, 0, 0);&#125;inline Vector3D SphSpikyKernel3::gradient(double distance, const Vector3D&amp; directionToCenter) const &#123; return -firstDerivative(distance) * directionToCenter;&#125; &emsp;&emsp;spiky函数的二阶导数为： \\nabla^2 W_{spiky}(r,h)=\\frac{90}{\\pi h^5}(1-\\frac{r}{h}) \\tag {17}12345678910inline double SphSpikyKernel3::secondDerivative(double distance) const &#123; if (distance &gt;= h) return 0.0; else &#123; double x = 1.0 - distance / h; return 90.0 / (kPiD * h5) * x; &#125;&#125; 二、SPH流体模拟算法&emsp;&emsp;首先是简化的Naiver-Stokes方程，分为两部分，分别是动量方程$(18)$和连续方程$(19)$： \\frac{D \\vec v}{D t}=-\\frac{1}{\\rho}\\nabla p+\\mu \\nabla^2 \\vec v+\\vec g \\tag {18} \\frac{\\partial \\rho}{\\partial t}+\\nabla \\cdot (\\rho \\vec v) = 0 \\tag {19}&emsp;&emsp;连续方程描述了流体的密度随着时间的变化速率，我们的模拟的是不可压缩流体，因而密度守恒，故$\\frac{\\partial \\rho}{\\partial t}=0$，即$\\nabla \\cdot \\vec v=0$，流体的速度场满足无散度的条件。动量方程左边的是速度关于时间的物质导数，$\\frac{D\\vec v}{D t}=\\frac{\\partial \\vec v}{\\partial t}+\\vec v\\cdot \\nabla \\vec v$。在我们的基于粒子的流体模拟算法中，质量守恒自动满足，$\\vec v\\cdot \\nabla \\vec v=0$，粒子速度场关于时间的物质导数就是$\\frac{\\partial \\vec v}{\\partial t}$。上述公式中的$\\vec g$是重力加速度，$\\mu$是流体的黏度系数，$\\rho$是流体密度，$p$是流体压强。 &emsp;&emsp;每一个粒子的加速度为： a=-\\frac{1}{\\rho}\\nabla p+\\mu \\nabla^2\\vec v+\\vec g \\tag {20}&emsp;&emsp;在公式$(20)$的右边分别是流体的压力项$-\\frac{1}{\\rho}\\nabla p$、粘性力项$\\mu \\nabla^2\\vec v$以及体积力项$\\vec g$（在这里目前只有重力）。在模拟的过程中，我们需要分别计算这几项。一个完整的SPH流体求解器计算流程如下所示： 1234561.Measure the density with particles’ current locations2.Compute the gravity and other extra forces3.Compute the viscosity force4.Compute the pressure based on the density5.Compute the pressure gradient force6.Perform time integration &emsp;&emsp;接下来按照这个步骤一一展开。 1、计算粒子的密度&emsp;&emsp;首先我们要计算流体粒子的密度值，因为后面粘性力和压强梯度力的计算将会用到粒子的密度。粒子的密度同样是采用光滑核对周围粒子进行加权计算，将密度函数代入公式$(4)$, \\begin{align} f(x)\\approx & \\Sigma_{i=1}^N \\frac{m_i}{\\rho_i}\\rho_i w(||x-x_i||/h_i)\\\\ \\approx & \\Sigma_{i=1}^Nm_i w(||x-x_i||/h_i) \\end{align} \\tag {21}&emsp;&emsp;原公式中的密度项被消去了，所以我们直接根据粒子的位置和质量计算每个粒子的密度，在这里我们的每个粒子质量都相同： 1234567891011121314151617181920212223void SphSystemData3::updateDensities() &#123; auto p = positions(); auto d = densities(); const double m = mass(); parallelFor(kZeroSize, numberOfParticles(), [&amp;](size_t i) &#123; double sum = sumOfKernelNearby(p[i]); d[i] = m * sum; &#125;);&#125;double SphSystemData3::sumOfKernelNearby(const Vector3D&amp; origin) const &#123; double sum = 0.0; SphStdKernel3 kernel(_kernelRadius); neighborSearcher()-&gt;forEachNearbyPoint( origin, _kernelRadius, [&amp;](size_t, const Vector3D&amp; neighborPosition) &#123; double dist = origin.distanceTo(neighborPosition); sum += kernel(dist); &#125;); return sum;&#125; 2、计算外部作用力&emsp;&emsp;这里指的外部的作用力通常是体积力，即间接地作用在流体上而非通过直接接触产生的作用力。常见的体积力有重力和风力，这些体积力我们直接根据需要指定其加速度的值，然后采用牛顿定律叠加到粒子的力场上： 123456789101112131415161718void ParticleSystemSolver3::accumulateExternalForces() &#123; size_t n = _particleSystemData-&gt;numberOfParticles(); auto forces = _particleSystemData-&gt;forces(); auto velocities = _particleSystemData-&gt;velocities(); auto positions = _particleSystemData-&gt;positions(); const double mass = _particleSystemData-&gt;mass(); parallelFor(kZeroSize, n, [&amp;](size_t i) &#123; // Gravity Vector3D force = mass * _gravity; // Wind Vector3D relativeVel = velocities[i] - _wind-&gt;sample(positions[i]); force += -_dragCoefficient * relativeVel; forces[i] += force; &#125;);&#125; 3、计算流体粘性力&emsp;&emsp;流体的粘性力是流体内部的一种阻力，粘性力的计算需要计算流体速度场的拉普拉斯算子，这是因为拉普拉斯算子衡量了给定位置的物理量与周围邻域物理量的差距值： f_{v}=m\\mu \\nabla ^2\\vec v \\tag {22}&emsp;&emsp;在SPH的算法中，其同样是通过光滑核函数计算粘性力，注意这里为了避免常量函数的拉普拉斯算子为非零，采用了前面的公式$(8)$： f_v(x)=m^2\\mu\\Sigma_j(\\frac{\\vec v_j-\\vec v_i}{\\rho_j})\\nabla^2W(x-x_j) \\tag {23}123456789101112131415161718192021222324void SphSolver3::accumulateViscosityForce() &#123; auto particles = sphSystemData(); size_t numberOfParticles = particles-&gt;numberOfParticles(); auto x = particles-&gt;positions(); auto v = particles-&gt;velocities(); auto d = particles-&gt;densities(); auto f = particles-&gt;forces(); const double massSquared = square(particles-&gt;mass()); const SphSpikyKernel3 kernel(particles-&gt;kernelRadius()); parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; const auto&amp; neighbors = particles-&gt;neighborLists()[i]; for (size_t j : neighbors) &#123; double dist = x[i].distanceTo(x[j]); f[i] += viscosityCoefficient() * massSquared * (v[j] - v[i]) / d[j] * kernel.secondDerivative(dist); &#125; &#125;);&#125; 4、计算流体的压强&emsp;&emsp;流体的压强是流体内部的一种力，它是维持流体不可压缩的关键。目前我们只知道流体的密度值，流体的密度与压强存在着某种联系，密度大的趋于压强必然大，因此需要根据流体的密度计算其对应的压强值。当然我们可以直接求解关于流体压强的泊松方程$\\nabla ^2P=\\rho \\frac{\\nabla \\vec v}{\\Delta t}$，这样求解得到的压强是非常精确的，直接求解泊松方程在基于欧拉网格的流体模拟中比较常见，但是求解大规模稀疏矩阵的泊松方程非常耗时，因此在基于拉格朗日粒子的流体模拟中比较少（但也有），一种非常廉价且效果非常不错的方法就是采用泰特的状态方程： P=B((\\frac{\\rho}{\\rho_0})^\\gamma -1) \\tag {24}&emsp;&emsp;公式$(24)$中，$\\rho_0$流体静止时的密度，$\\rho$是流体粒子当前的密度，$\\gamma$是状态方程的指数，取值为$7$，而$B$则是一个缩放系数因子，与声波在流体中的传播速度有关，其计算公式如下： B=\\frac{\\rho_0 c_s^2}{\\gamma} \\tag {25}&emsp;&emsp;通过这个状态方程，我们可以计算出流体的压强值。但是存在一个问题，对于的粒子密度，我们是通过邻域粒子来计算的，这意味着流体表面的粒子因为邻域粒子较少使得其计算出来的密度值$\\rho$低于静止时的密度$\\rho_0$，导致通过状态方程计算出来的压强为负数，如下图3所示，从而导致流体在表面上的不正常聚集现象，这种现象类似于表面张力，但它并不是物理意义上准确的，所以我们需要消除这种现象。当计算得到的压强为负时，我们赋予为零。 图3 表面的低密度导致的负压强使粒子不正常地聚集 12345678910111213141516171819202122232425262728293031323334353637383940double SphSolver3::computePressureFromEos( double density, double targetDensity, double eosScale, double eosExponent, double negativePressureScale)&#123; double p = eosScale * (std::pow((density / targetDensity), eosExponent) - 1.0); // Negative pressure Scaling. if (p &lt; 0) p *= negativePressureScale; return p;&#125;void SphSolver3::computePressure()&#123; auto particles = sphSystemData(); size_t numberOfParticles = particles-&gt;numberOfParticles(); auto d = particles-&gt;densities(); auto p = particles-&gt;pressures(); // See Equation 9 from // http://cg.informatik.uni-freiburg.de/publications/2007_SCA_SPH.pdf const double targetDensity = particles-&gt;targetDensity(); const double eosScale = targetDensity * square(_speedOfSound) / _eosExponent; parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; p[i] = computePressureFromEos( d[i], targetDensity, eosScale, eosExponent(), negativePressureScale()); &#125;);&#125; 5、计算压强梯度力&emsp;&emsp;前面一步我们得到了流体粒子的压强，在压强的作用下，流体粒子从高压强区域流向低压强区域，是流体保持不可压缩的性质。因此，我们需要计算作用在流体粒子上的压强梯度力项，压力梯度力计算公式为： f_p=-m\\frac{\\nabla p}{\\rho} \\tag {26}&emsp;&emsp;利用前面的核函数拉普拉斯算子计算公式$(11)$，压强梯度力离散计算公式为： f_p=-m^2\\Sigma_j(\\frac{p_i}{\\rho_i^2}+\\frac{p_j}{\\rho_j^2})\\nabla W(x-x_j) \\tag {27}1234567891011121314151617181920212223242526272829void SphSolver3::accumulatePressureForce( const ConstArrayAccessor1&lt;Vector3D&gt;&amp; positions, const ConstArrayAccessor1&lt;double&gt;&amp; densities, const ConstArrayAccessor1&lt;double&gt;&amp; pressures, ArrayAccessor1&lt;Vector3D&gt; pressureForces)&#123; auto particles = sphSystemData(); size_t numberOfParticles = particles-&gt;numberOfParticles(); const double massSquared = square(particles-&gt;mass()); const SphSpikyKernel3 kernel(particles-&gt;kernelRadius()); parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; const auto&amp; neighbors = particles-&gt;neighborLists()[i]; for (size_t j : neighbors) &#123; double dist = positions[i].distanceTo(positions[j]); if (dist &gt; 0.0) &#123; Vector3D dir = (positions[j] - positions[i]) / dist; pressureForces[i] -= massSquared * (pressures[i] / (densities[i] * densities[i]) + pressures[j] / (densities[j] * densities[j])) * kernel.gradient(dist, dir); &#125; &#125; &#125;);&#125; 6、时间步进积分&emsp;&emsp;前面我们计算得到每个流体粒子的作用力合力，接着需要根据这个力计算粒子的加速度，然后在加速度的作用下更新粒子的速度值，最后在速度的作用下计算粒子的位置向量。这个步骤只需简单地利用牛顿定律即可。 123456789101112131415161718void ParticleSystemSolver3::timeIntegration(double timeStepInSeconds) &#123; size_t n = _particleSystemData-&gt;numberOfParticles(); auto forces = _particleSystemData-&gt;forces(); auto velocities = _particleSystemData-&gt;velocities(); auto positions = _particleSystemData-&gt;positions(); const double mass = _particleSystemData-&gt;mass(); parallelFor(kZeroSize, n, [&amp;](size_t i) &#123; // Integrate velocity first Vector3D&amp; newVelocity = _newVelocities[i]; newVelocity = velocities[i] + timeStepInSeconds * forces[i] / mass; // Integrate position. Vector3D&amp; newPosition = _newPositions[i]; newPosition = positions[i] + timeStepInSeconds * newVelocity; &#125;);&#125; 7、碰撞检测处理&emsp;&emsp;在获取了新得速度场和位置向量之后，我们需要做碰撞检测处理，使之不发生穿透。这里的碰撞检测采用了隐式表面的符号距离场，为每个碰撞体构建一个水平集，这方面的内容比较多，但不是这里的核心点，故不赘述。目前实现的是刚体碰撞。下面只是一部分代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778void Collider3::resolveCollision( double radius, double restitutionCoefficient, Vector3D* newPosition, Vector3D* newVelocity) &#123; assert(_surface); if (!_surface-&gt;isValidGeometry()) return; ColliderQueryResult colliderPoint; getClosestPoint(_surface, *newPosition, &amp;colliderPoint); // Check if the new position is penetrating the surface if (isPenetrating(colliderPoint, *newPosition, radius)) &#123; // Target point is the closest non-penetrating position from the // new position. Vector3D targetNormal = colliderPoint.normal; Vector3D targetPoint = colliderPoint.point + radius * targetNormal; Vector3D colliderVelAtTargetPoint = colliderPoint.velocity; // Get new candidate relative velocity from the target point. Vector3D relativeVel = *newVelocity - colliderVelAtTargetPoint; double normalDotRelativeVel = targetNormal.dot(relativeVel); Vector3D relativeVelN = normalDotRelativeVel * targetNormal; Vector3D relativeVelT = relativeVel - relativeVelN; // Check if the velocity is facing opposite direction of the surface // normal if (normalDotRelativeVel &lt; 0.0) &#123; // Apply restitution coefficient to the surface normal component of // the velocity Vector3D deltaRelativeVelN = (-restitutionCoefficient - 1.0) * relativeVelN; relativeVelN *= -restitutionCoefficient; // Apply friction to the tangential component of the velocity // From Bridson et al., Robust Treatment of Collisions, Contact and // Friction for Cloth Animation, 2002 // http://graphics.stanford.edu/papers/cloth-sig02/cloth.pdf if (relativeVelT.lengthSquared() &gt; 0.0) &#123; double frictionScale = std::max(1.0 - _frictionCoeffient * deltaRelativeVelN.length() / relativeVelT.length(), 0.0); relativeVelT *= frictionScale; &#125; // Reassemble the components *newVelocity = relativeVelN + relativeVelT + colliderVelAtTargetPoint; &#125; // Geometric fix *newPosition = targetPoint; &#125;&#125;void ParticleSystemSolver3::resolveCollision( ArrayAccessor1&lt;Vector3D&gt; newPositions, ArrayAccessor1&lt;Vector3D&gt; newVelocities) &#123; if (_collider != nullptr) &#123; size_t numberOfParticles = _particleSystemData-&gt;numberOfParticles(); const double radius = _particleSystemData-&gt;radius(); parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; _collider-&gt;resolveCollision( radius, _restitutionCoefficient, &amp;newPositions[i], &amp;newVelocities[i]); &#125;); &#125;&#125; 8、自适应时间步长&emsp;&emsp;最后还有个非常关键的点，就是模拟的时间步长选取。为了使得流体的密度守恒（即保持不可压缩性），我们采用了泰特的状态方程，该方程引入了两个参数，分别是指数$\\gamma$和声波在流体中的传播速度，这带来了一些时间步长的限制问题。如下图4所示，假设一个流体粒子从空中落到一滩流体中，这个过程会产生一些震荡波，白色的粒子是落下的粒子以及获取到了震荡波动信息的粒子。 图4 流体粒子的信息传播 &emsp;&emsp;受限于有限的光滑核半径，在每一个时间步长内，信息传播的范围最大为光滑核半径的长度。设信息的传播速度为$c$，那么我们能取得最大时间步长就为$h/c$。在我们得物理模拟中，这个信息传播速度实际上就是声波在流体中的传播速度。为此，研究者们提出自适应的时间步长，根据当前的流体状态计算最大的时间步长，如果取超过这个最大时间步长限制，那么将会导致流体崩溃，产生不稳定的问题。时间步长的上界计算公式为： \\begin{align} &\\Delta t_v=\\frac{\\lambda_v h}{c_s}\\\\ &\\Delta t_f=\\lambda_f \\sqrt{\\frac{mh}{F_{max}}}\\\\ &\\Delta t\\leq min(\\Delta t_v, \\Delta t_f) \\end{align} \\tag {28}&emsp;&emsp;其中，$h$是光滑核半径，$m$粒子质量，$c_s$是声波在流体中的传播速度，$F_{max}$是流体粒子当中受到的最大合力的长度值，$\\lambda_v$和$\\lambda_f$是一个缩放系数，分别取$0.4$和$0.25$。可以看到，时间步长的上界不仅仅取决于声波的传播速度，还取决于每一个不同的时刻流体所受的最大合力，因而每一次模拟都要重新计算下一个时间步长。 123456789101112131415161718192021unsigned int SphSolver3::numberOfSubTimeSteps(double timeIntervalInSeconds) const &#123; auto particles = sphSystemData(); size_t numberOfParticles = particles-&gt;numberOfParticles(); auto f = particles-&gt;forces(); const double kernelRadius = particles-&gt;kernelRadius(); const double mass = particles-&gt;mass(); double maxForceMagnitude = 0.0; for (size_t i = 0; i &lt; numberOfParticles; ++i) maxForceMagnitude = std::max(maxForceMagnitude, f[i].length()); double timeStepLimitBySpeed = kTimeStepLimitBySpeedFactor * kernelRadius / _speedOfSound; double timeStepLimitByForce = kTimeStepLimitByForceFactor * std::sqrt(kernelRadius * mass / maxForceMagnitude); double desiredTimeStep = _timeStepLimitScale * std::min(timeStepLimitBySpeed, timeStepLimitByForce); return static_cast&lt;unsigned int&gt;(std::ceil(timeIntervalInSeconds / desiredTimeStep));&#125; &emsp;&emsp;真实世界中的声波在流体中的传播速度很快，这导致通过公式$(28)$计算得到的时间步长是非常小的。举个例子，假设我们的光滑核半径为$0.1m$，质量为$0.01kg$，声波传播速度为$1482m/s$，在只有重力这个外力的作用下，根据公式$(28)$计算可得$\\Delta t_v=0.00002699055331s$，而$\\Delta t_f=0.0007985957062$，因此最大的时间步长为$0.00002699055331$，这意味着如果模拟$60$帧率的话，那么一秒需要计算$618$个子时间步长，耗费大量的时间和计算资源，即便是离线模拟，其代价也太大了。这个就是传统的SPH算法的缺点，被称为WCSPH（Weakly Compressible SPH），针对这个缺点，目前已经有不少研究者提出了不同的改进方法，其中比较优秀的算法就是PCISPH。 9、模拟效果&emsp;&emsp;整个模拟系统比较大，关于粒子数据的结构、邻域搜索算法、多线程并行、系统结构等方面不再赘述。我们模拟的结果是粒子群的位置向量，为了渲染出来需要采用一个算法根据粒子点云提取流体表面的三角网格，通常的流程就是根据粒子点云计算一个水平集，关于这方面目前已经不少学者研究并提出了几种算法，目前先采用最简单的一种提取表面算法，以后再回过头来查看这方面的算法。提取出来的表面之后采用了一个Mitsuba光追渲染器，模拟并渲染了两个场景，一个是Fluid Drop场景，一个是Dam Breaking场景。 &emsp;&emsp;由于传统的SPH模拟太过耗时，所以我就没有提取出流体的表面了，而是直接渲染流体的粒子。后面采用了改进的算法再提取流体表面，渲染极度真实的流体。下面的图5和图6分别展示了Fluid Drop场景和Dam Breaking场景的第0、10、20、30、40、50帧的流体粒子渲染结果。 图5 Fluid Drop的第0、10、20、30、40、50帧 图6 Dam Breaking的第0、10、20、30、40、50帧 三、PCISPH流体模拟算法&emsp;&emsp;传统的SPH算法采用了泰特的状态方程来计算流体压强，避免了求解大型稀疏矩阵的泊松方程，但是受限于状态方程中的声波速度，我们不能取太大的刚度系数，否则将导致时间步长非常小，极大地耗费计算资源。因此，后来又有学者提出了传统SPH算法的改进——PCISPH（全称为Predictive-Corrective Incompressible SPH，译为预测-矫正的不可压缩SPH，简称PCISPH）。与WCSPH算法不同，PCISPH不再采用泰特方程来计算流体的压强值，而是采用了一种非线性的迭代方法，尽可能地计算一个使得流体密度守恒的压强值。 &emsp;&emsp;PCISPH总体算法如下图7所示。可以看到，在求解流体的压强梯度力时，它是一个循环迭代的过程，迭代的终止条件为流体的当前密度与静止密度之差小于给定的阈值或者达到最大的迭代次数。 图7 PCISPH算法纵览 &emsp;&emsp;在每一重迭代的过程种，首先根据粒子的净合力计算加速度，并计算在当前加速度下的速度值、位置向量，这一过程称为预测过程。紧接着计算前面预测得到的粒子群的密度值以及当前的密度值与静止密度的差距，并根据这个密度差计算压强，使得在这个压强的作用下，我们的粒子能够移动到一个位置上从而缩小这个密度差距。最后根据压强计算压强梯度力，将其迭代到我们的合力当中，用以下一次的迭代。 &emsp;&emsp;迭代过程的关键一步就是如何根据密度差计算出所需的压强值，这涉及到一些数学内容。设光滑核半径长度为$h$，给定一点$i$在第$t+1$时间点的密度值通过下面的公式计算得到： \\begin{align} \\rho_i(t+1)&= m\\Sigma_j W(x_i(t+1)-x_j(t+1))\\\\ &= m\\Sigma_jW(x_i(t)+\\Delta x_i(t)-x_j(t)-\\Delta x_j(t))\\\\ &= m\\Sigma_jW(d_{ij}(t)+\\Delta d_{ij}(t)) \\end{align} \\tag {29}&emsp;&emsp;其中$d_{ij}(t)=x_i(t)-x_j(t)$，$\\Delta d_{ij}(t)=\\Delta x_i(t)-\\Delta x_j(t)$。假设$\\Delta d_{ij}$足够小，对$W(d_{ij}(t)+\\Delta d_{ij}(t))$做一阶泰勒展开： \\begin{align} \\rho_i(t+1) &= m\\Sigma_j[W(d_{ij}(t))+\\nabla W(d_{ij}(t))\\cdot \\Delta d_{ij}(t)]\\\\ &= m\\Sigma_j W(x_i(t)-x_j(t))+m\\Sigma_j \\nabla W(x_i(t)-x_j(t))\\cdot(\\Delta x_i(t)- \\Delta x_j(t))\\\\ &= \\rho_i(t)+\\Delta \\rho_i(t) \\end{align} \\tag {30}&emsp;&emsp;上式中，$\\Delta \\rho_i(t)$是未知项，我们对其做一些展开： \\begin{align} \\Delta \\rho_i(t) &=m\\Sigma_j\\nabla W_{ij}\\cdot (\\Delta x_i(t)-\\Delta x_j(t))\\\\ &=m(\\Sigma_j \\nabla W_{ij}\\Delta x_i(t)-\\Sigma_j\\nabla W_{ij}\\Delta x_j(t))\\\\ &=m(\\Delta x_i(t)\\Sigma_j \\nabla W_{ij}-\\Sigma_j \\nabla W_{ij}\\Delta x_j(t)) \\end{align} \\tag {31}&emsp;&emsp;令$\\Delta x_i$为在仅仅考虑流体压强梯度力的情况下的位移向量，则有： \\Delta x_i=\\Delta t^2\\frac{F_i^p}{m} \\tag {32}&emsp;&emsp;在理想的情况下，我们的流体应该处于一种这样的平衡状态：流体粒子的压强值均等于$\\overline p_i$，流体粒子的密度值均为静止时的密度$\\rho_0$，即有： F_i^p=-m^2\\Sigma_j(\\frac{\\overline p_i}{\\rho_0^2}+\\frac{\\overline p_i}{\\rho_0^2})\\nabla W_{ij}=-m^2\\frac{2\\overline p_i}{\\rho_0^2}\\Sigma_j \\nabla W_{ij} \\tag {33}&emsp;&emsp;将公式$(33)$代入公式$(32)$，可得： \\Delta x_i=-\\Delta t^2 m\\frac{2\\overline p_i}{\\rho_0^2}\\Sigma_j\\nabla W_{ij} \\tag {34}&emsp;&emsp;记在粒子$i$的压强作用下，邻域粒子$j$的位移为$\\Delta x_{j|i}$，因力是相互作用、相互对称的，则粒子$j$受到来自$i$的压强梯度力为： F_{j|i}^p=m^2(\\frac{\\overline p_i}{\\rho_0^2}+\\frac{\\overline p_i}{\\rho_0^2}) \\nabla W_{ij}=m^2\\frac{2\\overline p_i}{\\rho_0^2}\\nabla W_{ij} \\tag {35}&emsp;&emsp;在上述的压强梯度力作用下： \\Delta x_{j|i}=\\Delta t^2m\\frac{2\\overline p_i}{\\rho_0^2}\\nabla W_{ij} \\tag {36}&emsp;&emsp;公式$(34)$和公式$(36)$给出了$\\Delta x_i$和$\\Delta x_{j|i}$，将其代入公式$(31)$中，得到了$\\Delta \\rho_i(t)$项的表达式： \\begin{align} \\Delta \\rho_i(t) &=m(-\\Delta t^2m\\frac{2\\overline p_i}{\\rho_0^2}\\Sigma_j \\nabla W_{ij}\\cdot \\Sigma_j \\nabla W_{ij}-\\Sigma_j(\\nabla W_{ij}\\cdot \\Delta t^2 m\\frac{2\\overline p_i}{\\rho_0^2}\\nabla W_{ij}))\\\\ &=\\Delta t^2m^2\\frac{2\\overline p_i}{\\rho_0^2}(-\\Sigma_j \\nabla W_{ij}\\cdot \\Sigma_j \\nabla W_{ij}-\\Sigma_j(\\nabla W_{ij}\\cdot \\nabla W_{ij})) \\end{align} \\tag {37}&emsp;&emsp;对公式$(37)$做一些变化，可得关于$\\overline p_i$的一个表达式： \\overline p_i=\\frac{\\Delta \\rho_i(t)} {\\beta (-\\Sigma_j\\nabla W_{ij}\\cdot \\Sigma_j\\nabla W_{ij}-\\Sigma_j(\\nabla W_{ij}\\cdot \\nabla W_{ij}))}\\\\ \\beta=\\Delta t^2m^2\\frac{2}{\\rho_0^2} \\tag {38}&emsp;&emsp;而$\\Delta \\rho_i(t)=-\\rho_{err_i}^t=-(\\rho_i^t-\\rho_0)$，即当前粒子的密度值与静止密度的差。公式$(38)$中的系数可以提出来提前计算，即下面的公式$(39)$，一方面是为了性能考虑，另一方面是因为表面粒子的邻域粒子数量少，其计算的系数是错的。 \\delta =\\frac{-1}{\\beta(-\\Sigma_j\\nabla W_{ij}\\cdot \\Sigma_j\\nabla W_{ij}-\\Sigma_j(\\nabla W_{ij}\\cdot \\nabla W_{ij}))} \\tag {39}12345678910111213141516171819202122232425262728293031323334353637383940414243444546double PciSphSolver3::computeBeta(double timeStepInSeconds) &#123; auto particles = sphSystemData(); return 2.0 * square(particles-&gt;mass() * timeStepInSeconds / particles-&gt;targetDensity());&#125;double PciSphSolver3::computeDelta(double timeStepInSeconds)&#123; auto particles = sphSystemData(); const double kernelRadius = particles-&gt;kernelRadius(); Array1&lt;Vector3D&gt; points; BccLatticePointGenerator pointsGenerator; Vector3D origin; BoundingBox3D sampleBound(origin, origin); sampleBound.expand(1.5 * kernelRadius); pointsGenerator.generate(sampleBound, particles-&gt;targetSpacing(), &amp;points); SphSpikyKernel3 kernel(kernelRadius); double denom = 0; Vector3D denom1; double denom2 = 0; for (size_t i = 0; i &lt; points.size(); ++i) &#123; const Vector3D&amp; point = points[i]; double distanceSquared = point.lengthSquared(); if (distanceSquared &lt; kernelRadius * kernelRadius) &#123; double distance = std::sqrt(distanceSquared); Vector3D direction = (distance &gt; 0.0) ? point / distance : Vector3D(); // grad(Wij) Vector3D gradWij = kernel.gradient(distance, direction); denom1 += gradWij; denom2 += gradWij.dot(gradWij); &#125; &#125; denom += -denom1.dot(denom1) - denom2; return (std::fabs(denom) &gt; 0.0) ? -1 / (computeBeta(timeStepInSeconds) * denom) : 0;&#125; &emsp;&emsp;然后$\\overline p_i=\\delta \\rho_{err_i}^t$，在每一次的迭代中更新粒子的压强，$p_i+=\\overline p_i$。PCISPH算法与WCSPH算法的不同就在计算压强梯度力上，除了更新粒子的压强值，还要更新粒子的密度值，并根据粒子的预测位置计算压强梯度力，整个迭代的流程不难理解。迭代结束之后，需要将迭代得到的压强梯度力加到粒子的合力上。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104void PciSphSolver3::accumulatePressureForce(double timeIntervalInSeconds) &#123; auto particles = sphSystemData(); const size_t numberOfParticles = particles-&gt;numberOfParticles(); const double delta = computeDelta(timeIntervalInSeconds); const double targetDensity = particles-&gt;targetDensity(); const double mass = particles-&gt;mass(); auto p = particles-&gt;pressures(); auto d = particles-&gt;densities(); auto x = particles-&gt;positions(); auto v = particles-&gt;velocities(); auto f = particles-&gt;forces(); // Predicted density ds Array1&lt;double&gt; ds(numberOfParticles, 0.0); SphStdKernel3 kernel(particles-&gt;kernelRadius()); // Initialize buffers parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; p[i] = 0.0; _pressureForces[i] = Vector3D(); _densityErrors[i] = 0.0; ds[i] = d[i]; &#125;); unsigned int maxNumIter = 0; double maxDensityError; double densityErrorRatio = 0.0; for (unsigned int k = 0; k &lt; _maxNumberOfIterations; ++k) &#123; // Predict velocity and position parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; _tempVelocities[i] = v[i] + timeIntervalInSeconds / mass * (f[i] + _pressureForces[i]); _tempPositions[i] = x[i] + timeIntervalInSeconds * _tempVelocities[i]; &#125;); // Resolve collisions resolveCollision(_tempPositions.accessor(), _tempVelocities.accessor()); // Compute pressure from density error parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; double weightSum = 0.0; const auto&amp; neighbors = particles-&gt;neighborLists()[i]; for (size_t j : neighbors) &#123; double dist = _tempPositions[j].distanceTo(_tempPositions[i]); weightSum += kernel(dist); &#125; weightSum += kernel(0); double density = mass * weightSum; double densityError = (density - targetDensity); double pressure = delta * densityError; if (pressure &lt; 0.0) &#123; pressure *= negativePressureScale(); densityError *= negativePressureScale(); &#125; p[i] += pressure; ds[i] = density; _densityErrors[i] = densityError; &#125;); // Compute pressure gradient force _pressureForces.set(Vector3D()); SphSolver3::accumulatePressureForce(x, ds.constAccessor(), p, _pressureForces.accessor()); // Compute max density error maxDensityError = 0.0; for (size_t i = 0; i &lt; numberOfParticles; ++i) maxDensityError = absmax(maxDensityError, _densityErrors[i]); densityErrorRatio = maxDensityError / targetDensity; maxNumIter = k + 1; if (std::fabs(densityErrorRatio) &lt; _maxDensityErrorRatio) break; &#125; std::cout &lt;&lt; \"Number of PCI iterations: \" &lt;&lt; maxNumIter &lt;&lt; std::endl; std::cout &lt;&lt; \"Max density error after PCI iteration: \" &lt;&lt; maxDensityError &lt;&lt; std::endl; if (std::fabs(densityErrorRatio) &gt; _maxDensityErrorRatio) &#123; std::cout &lt;&lt; \"Max density error ratio is greater than the threshold!\\n\"; std::cout &lt;&lt; \"Ratio: \" &lt;&lt; densityErrorRatio &lt;&lt; \" Threshold: \" &lt;&lt; _maxDensityErrorRatio &lt;&lt; std::endl; &#125; // Accumulate pressure force parallelFor(kZeroSize, numberOfParticles, [this, &amp;f](size_t i) &#123; f[i] += _pressureForces[i]; &#125;);&#125; &emsp;&emsp;与传统的WCSPH相比，PCISPH看起来步骤更多，因为它需要迭代多次，但是其不再依赖泰特方程，从而支持更大的时间步长，因此速度比WCSPH快很多。下图8和图9展示了Fluid Drop场景和Dam Breaking的场景模拟效果，采用了Marching Cubes重建流体表面进行渲染，效果与WCSPH几乎无差别，但是加速比起码有10倍以上，而且不可压缩性更好。这就是PCISPH算法的优势。 图8 Fluid Drop的第0、10、20、30、40、50、60、70、80帧 图9 Dam Breaking的第0、10、20、30、40、50、60、70、80帧 &emsp;&emsp;最后，用PCISPH的流体求解算法模拟了一个稍微比较复杂的流体场景，模拟了600帧，不敢用WCSPH模拟600帧，因为WCSPH算法实在是太慢了。下图10展示了第0、20、40、60、80、100、120、140、160帧的效果。 图10 一个复杂场景的第0、20、40、60、80、100、120、140、160帧 参考资料：$[1]$ Müller M, Charypar D, Gross M. Particle-based fluid simulation for interactive applications[C]//Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation. Eurographics Association, 2003: 154-159. $[2]$ Becker M, Teschner M. Weakly compressible SPH for free surface flows[C]//Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer animation. Eurographics Association, 2007: 209-217. $[3]$ Schechter H, Bridson R. Ghost SPH for animating water[J]. ACM Transactions on Graphics (TOG), 2012, 31(4): 61. $[4]$ Kim, D. (2017). Fluid engine development. Boca Raton: Taylor &amp; Francis, a CRC Press, Taylor &amp; Francis Group. $[5]$ Adams and Wicke, Meshless approximation methods and applications in physics based modeling and animation, Eurographics tutorials 2009. $[6]$ Dan Koschier, Jan Bender. Smoothed Particle Hydrodynamics Techniques for the Physics Based Simulation of Fluids and Solids, Eurographics Tutorial 2019. $[7]$ Solenthaler B, Pajarola R. Predictive-corrective incompressible SPH[C]// Acm Siggraph. 2009.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/tags/Fluid-Simulation/"}]},{"title":"流体模拟Fluid Simulation：求解流体不可压缩的泊松方程","slug":"MakingFluidImcompressible","date":"2019-08-03T03:06:23.934Z","updated":"2019-09-12T07:45:21.455Z","comments":true,"path":"2019/08/03/MakingFluidImcompressible/","link":"","permalink":"https://yangwc.com/2019/08/03/MakingFluidImcompressible/","excerpt":"本文主要是流体不可压缩投影，在进行半拉格朗日对流之后，还需要对流体速度进行修正，使流体的速度场散度为零。主要是关于流体压强的泊松方程求解，通常采用共轭梯度法求解大规模稀疏矩阵的线性方程组，辅以不完全的Cholesky因子化。涉及到的数学内容比较多。","text":"本文主要是流体不可压缩投影，在进行半拉格朗日对流之后，还需要对流体速度进行修正，使流体的速度场散度为零。主要是关于流体压强的泊松方程求解，通常采用共轭梯度法求解大规模稀疏矩阵的线性方程组，辅以不完全的Cholesky因子化。涉及到的数学内容比较多。 离散的压力梯度 离散的速度场散度 求解关于压强的泊松方程 不可压缩投影 参考资料 流体的不可压缩投影 &emsp;&emsp;在流体模拟中，确保流体的不可压缩性是非常重要、关键的一步，也是最耗时的一步，这是整个流体模拟过程中的核心，涉及到模拟效果的真实性、模拟过程的效率快慢两方面的内容，因而相关的数学内容比较多。在流体模拟中，确保流体的不可压缩性的算法我们通常用$project(\\Delta t, \\vec u)$来表示，它输入时间步长$\\Delta t$和流体的速度场$\\vec u$，然后在流体内部的压力的作用下更新速度场： \\vec u^{n+1}=\\vec u-\\Delta t\\frac1\\rho \\nabla p \\tag {1}&emsp;&emsp;上述公式中，$\\vec u^{n+1}$就是新的速度场，$\\rho$是流体密度，$p$为压力。通过公式$(1)$，我们就可以得到满足流体不可压缩性的流体，此时得到的新速度场$\\vec u^{n+1}$的散度为0： \\nabla \\cdot \\vec u^{n+1}=0 \\tag {2}&emsp;&emsp;且在固体边界处，其速度场在法线方向上的投影等于固定墙的速度场在法线方向上的投影： \\vec u^{n+1}\\cdot \\vec n=\\vec u_{solid}\\cdot \\vec n \\tag {3}&emsp;&emsp;在前一步展开如何求解上述的几个公式之前，我们先把流体模拟区域划分成一个一个格子，有流体的格子标记为F，固体边界区域的格子标记为$S$，空白的区域则标记为$E$。下图1展示了一个二维的例子。 图1 模拟区域标记 &emsp;&emsp;模拟的网格依旧是MAC网格，即交错的网格结构，如下图2所示。格子中心存储流体压力值，边缘部分存储流体的速度场向量，依次交错存储。 图2 MAC网格 一、离散的压力梯度&emsp;&emsp;MAC网格解决了普通的网格的零域（null space）问题，在使用中心差分法计算梯度时鲁棒性更强。目前暂时假设流体的压力值已经知道且存储在MAC网格上，那么二维和三维的压力梯度计算采用中心差分法如下所示： \\nabla p=(\\frac{p_{i+1,j}-p_{i,j}}{\\Delta x},\\frac{p_{i,j+1}-p_{i,j}}{\\Delta x})\\\\ \\nabla p=(\\frac{p_{i+1,j,k}-p_{i,j,k}}{\\Delta x}, \\frac{p_{i,j+1,k}-p_{i,j,k}}{\\Delta x}, \\frac{p_{i,j,k+1}-p_{i,j,k}}{\\Delta x}) \\tag {4}&emsp;&emsp;然后将公式$(4)$中的离散压力梯度公式代入公式$(1)$，可以得到实际上的速度场更新公式： \\begin{cases} u_{i+1/2,j}^{n+1}=u_{i+1/2,j}-\\Delta t\\frac1\\rho\\frac{p_{i+1,j}-p_{i,j}}{\\Delta x}\\\\ v_{i,j+1/2}^{n+1}=v_{i,j+1/2}-\\Delta t\\frac1\\rho\\frac{p_{i,j+1}-p_{i,j}}{\\Delta x} \\end{cases}\\\\ \\begin{cases} u_{i+1/2,j,k}^{n+1}=u_{i+1/2,j,k}-\\Delta t\\frac1\\rho\\frac{p_{i+1,j,k}-p_{i,j,k}}{\\Delta x}\\\\ v_{i,j+1/2,k}^{n+1}=v_{i,j+1/2,k}-\\Delta t\\frac1\\rho\\frac{p_{i,j+1/2,k}-p_{i,j,k}}{\\Delta x}\\\\ w_{i,j,k+1/2}^{n+1}=w_{i,j,k+1/2}-\\Delta t\\frac1\\rho\\frac{p_{i,j,k+1/2}-p_{i,j,k}}{\\Delta x} \\end{cases} \\tag {5}&emsp;&emsp;公式$(5)$给出了二维和三维的流体不可压缩速度场更新公式。注意，固体墙边界并没有压力一说，而自由面边界压力我们设置为零，这样上述的公式仅适用于那些不邻近固体边界的速度场分量，一般情况下至少需要邻近一个流体区域。 &emsp;&emsp;现在我们把目标放到边界条件的处理上。首先是自由面，这部分通常就是空气，我们直接设置其压力值为零，这类边界处理我们称之为第一类边界条件——狄利克雷边界条件（Dirichlet boundary condition）$^{[1]}$。所谓狄利克雷边界条件，就是我们直接设定边界处的值，这是在数值方法中最简单的一种边界处理方法。 &emsp;&emsp;然后就是固体墙边界的处理。在固体墙边界部分，我们需要将流体的速度场在法线方向上的投影等于固体边界在法相方向上的速度场向量值，一种方法就是直接设置，例如在$(i+1/2,j,k)$处的速度场分量$u$，假设其左边$(i,j,k)$为固体边界区域，右边$(i+1,j,k)$为流体区域，那么直接赋值$u$： u_{i+1/2,j,k}^{n+1}=u_{i+1/2,j,k}^{solid} \\tag {6}&emsp;&emsp;直接赋值需要一个额外的分支进行。另外一种方案就是给固定墙区域设定一个ghost压强值，只要通过公式$(5)$更新后固体边界处的速度场为公式$(6)$即可，此时不需要额外的分支。通过一些变换，可得固体墙区域的ghost压强计算公式： p_{i,j,k}^{ghost}=p_{i+1,j,k}-\\frac{\\rho\\Delta x}{\\Delta t}(u_{i+1/2,j,k}-u_{i+1/2,j,k}^{solid}) \\tag {7}&emsp;&emsp;这种方案在进行不可压缩投影时不需要考虑邻域是否是固体边界，统一用公式$(5)$计算，在固体边界处用ghost压强值计算，依旧以上面为例： u_{i+1/2,j,k}^{n+1}=u_{i+1/2,j,k}-\\Delta t\\frac{1}{\\rho}\\frac{p_{i+1,j,k}-p_{i,j,k}^{ghost}}{\\Delta x} \\tag {8}&emsp;&emsp;实际上，将公式$(7)$代入公式$(8)$中，就可以得到公式$(6)$，通过这种方式不再将固体边界的处理作为一个单独考虑的情况。将公式$(8$做一些移项，重写为： \\frac{\\Delta t}{\\rho}\\frac{p_{i+1,j,k}-p_{i,j,k}^{ghost}}{\\Delta x}= u_{i+1/2,j,k}-u_{i+1/2,j,k}^{solid} \\tag {9}&emsp;&emsp;上述公式中的左边压强部分可以看成是关于压强梯度的有限差分，即关于下面公式的一个近似： \\frac{\\Delta t}{\\rho}\\frac{\\partial p}{\\partial x}=u-u^{solid} \\tag {10}&emsp;&emsp;公式$Math Processing Error$仅仅是关于一个方向，同样地，我们可以取其他方向的固体边界条件，得到更一般的公式： \\frac{\\Delta t}{\\rho}\\nabla p\\cdot \\vec n=(\\vec u-\\vec u^{solid})\\cdot \\vec n \\tag {11}&emsp;&emsp;公式$(11)$给出了固体边界处的压力梯度值，与狄利克雷条件处理方式不同，这里给出的是梯度的取值，这种边界处理方式被称为第二类边界条件——冯诺依曼边界条件（Neumannn boundary condition）$^{[1]}$。最后，假设流体的压力已知（实际上未知），根据压力梯度更新流体的速度场，算法伪代码如下： 1234567891011121314151617181920212223242526scale = dt/(density*dx);loop over i,j,k: #update u component. if label(i-1,j,k)==FLUID or label(i,j,k)==FLUID: if label(i-1,j,k)==SOLID or label(i,j,k)==SOLID: u(i,j,k) = usolid(i,j,k); else u(i,j,k) -= scale * (p(i,j,k) - p(i-1,j,k)); else mark u(i,j,k) as unknown; #update v component. if label(i,j-1,k)==FLUID or label(i,j,k)==FLUID: if label(i,j-1,k)==SOLID or label(i,j,k)==SOLID: v(i,j,k) = vsolid(i,j,k); else v(i,j,k) -= scale * (p(i,j,k) - p(i,j-1,k)); else mark v(i,j,k) as unknown; #update w component. if label(i,j,k-1)==FLUID or label(i,j,k)==FLUID: if label(i,j,k-1)==SOLID or label(i,j,k)==SOLID: w(i,j,k) = wsolid(i,j,k); else w(i,j,k) -= scale * (p(i,j,k) - p(i,j,k-1)); else mark w(i,j,k) as unknown; 二、离散的速度场散度&emsp;&emsp;流体不可压缩的最终表现就是速度场的散度为零，即$\\nabla \\cdot \\vec u=0$。经过不可压缩投影之后新的速度场$\\vec u^{n+1}$应该满足散度为零的条件，我们通过有限差分法估算新速度场$\\vec u^{n+1}$的散度。首先是散度的数学定义，二维和三维的散度定义如下所示： \\nabla \\cdot \\vec u=\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}\\\\ \\nabla \\cdot \\vec u=\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}+ \\frac{\\partial w}{\\partial z} \\tag {12}&emsp;&emsp;给定一个二维欧拉网格$(i,j)$，采用中心差分法计算离散的散度公式如下： (\\nabla\\cdot \\vec u)_{i,j}\\approx \\frac{u_{i+1/2,j}-u_{i-1/2,j}}{\\Delta x}+\\frac{v_{i,j+1/2}-v_{i,j-1/2}}{\\Delta x} \\tag {13}&emsp;&emsp;同理，三维$(i,j,k)$的离散散度计算公式为： (\\nabla\\cdot \\vec u)_{i,j,k}\\approx \\frac{u_{i+1/2,j,k}-u_{i-1/2,j,k}}{\\Delta x} +\\frac{v_{i,j+1/2,k}-v_{i,j-1/2,k}}{\\Delta x} +\\frac{w_{i,j,k+1/2}-w_{i,j,k-1/2}}{\\Delta x} \\tag {14}&emsp;&emsp;事实上，我们更关心散度的相反数，且仅对于流体区域计算其散度值，其余的固体边界、自由面部分不需要计算散度值。下面是一个计算流体散度的伪代码。 12345scale = 1/dx;loop over i,j,k where label (i,j,k)==FLUID: rhs(i,j,k)=-scale * (u(i+1,j,k)-u(i,j,k) +v(i,j+1,k)-v(i,j,k) +w(i,j,k+1)-w(i,j,k)); &emsp;&emsp;给定一个流体区域及其表面$\\partial cell$，流体散度的意义就是流体流进流出的总比例： \\int\\int_{\\partial cell}\\vec u \\cdot \\vec n \\tag {15}&emsp;&emsp;若流体区域为一个欧拉网格的立方体格子，那么上述的积分就是对立方体六个面上的速度场进行积分。这种方法与我们前面讨论的有限差分法不同，这是有限体积法（Finite volume method）。这里只是顺带一提。 三、求解关于压强的泊松方程&emsp;&emsp;在前面我们讨论了如何在已知流体内部压强的情况下进行不可压缩投影，更新速度场，并估算速度场的散度值。实际上，流体内部压强并不知道，这需要我们计算得到。接下来的内容就是关于如何求解流体的压力数值，这部分涉及的数学内容比较多，是比较难的一部分。 1、泊松方程&emsp;&emsp;我们知道新的速度场$\\vec u^{n+1}$需要满足散度为零，从而保证流体的不可压缩条件。又已知新的速度场$\\vec u^{n+1}$计算公式为上面的公式$(5)$，离散的散度计算公式为公式$(13)$和公式$(14)$，我们将公式$(5)$得到的速度场代入公式$(13)$、$(14)$中，就可以得到以流体压力为未知量的线性方程。 &emsp;&emsp;我们先来看二维的情况，关于$\\vec u^{n+1}$的散度为零表示成如下： \\frac{u_{i+1/2,j}^{n+1}-u_{i-1/2,j}^{n+1}}{\\Delta x} + \\frac{v_{i,j+1/2}^{n+1}-v_{i,j-1/2}^{n+1}}{\\Delta x} = 0 \\tag {16}&emsp;&emsp;将公式$(5)$中二维的速度更新公式代入上面的公式$(16)$中，可得： \\frac{1}{\\Delta x}[(u_{i+1/2,j}-\\Delta t\\frac1\\rho\\frac{p_{i+1,j}-p_{i,j}}{\\Delta x}) -(u_{i-1/2,j}-\\Delta t\\frac1\\rho\\frac{p_{i,j}-p_{i-1,j}}{\\Delta x})\\\\ +(v_{i,j+1/2}-\\Delta t\\frac1\\rho\\frac{p_{i,j+1}-p_{i,j}}{\\Delta x}) -(v_{i,j-1/2}-\\Delta t\\frac1\\rho\\frac{p_{i,j}-p_{i,j-1}}{\\Delta x})]=0\\\\ \\to \\\\ \\frac{\\Delta t}{\\rho}(\\frac{4p_{i,j}-p_{i+1,j}-p_{i,j+1}-p_{i-1,j}-p_{i,j-1}}{\\Delta x^2}) =-(\\frac{u_{i+1/2,j}-u_{i-1/2,j}}{\\Delta x}+\\frac{v_{i,j+1/2}-v_{i,j-1/2}}{\\Delta x}) \\tag {17}&emsp;&emsp;三维的情况同理，只不过多了一维需要处理： \\frac{u_{i+1/2,j,k}^{n+1}-u_{i-1/2,j,k}^{n+1}}{\\Delta x} + \\frac{v_{i,j+1/2,k}^{n+1}-v_{i,j-1/2,k}^{n+1}}{\\Delta x} + \\frac{w_{i,j,k+1/2}^{n+1}-w_{i,j,k-1/2}^{n+1}}{\\Delta x} = 0 \\tag {18} \\frac{1}{\\Delta x}[(u_{i+1/2,j,k} -\\Delta t\\frac1\\rho\\frac{p_{i+1,j,k}-p_{i,j,k}}{\\Delta x}) -(u_{i-1/2,j,k}-\\Delta t\\frac1\\rho\\frac{p_{i,j,k}-p_{i-1,j,k}}{\\Delta x})\\\\ +(v_{i,j+1/2,k}-\\Delta t\\frac1\\rho\\frac{p_{i,j+1,k}-p_{i,j,k}}{\\Delta x}) -(v_{i,j-1/2,k}-\\Delta t\\frac1\\rho\\frac{p_{i,j,k}-p_{i,j-1,k}}{\\Delta x})]\\\\ +(w_{i,j,k+1/2}-\\Delta t\\frac1\\rho\\frac{p_{i,j,k+1}-p_{i,j,k}}{\\Delta x}) -(w_{i,j,k-1/2}-\\Delta t\\frac1\\rho\\frac{p_{i,j,k}-p_{i,j,k-1}}{\\Delta x})]=0\\\\ \\to \\\\ \\frac{\\Delta t}{\\rho}(\\frac{6p_{i,j,k}-p_{i+1,j,k}-p_{i,j+1,k}-p_{i,j,k+1}-p_{i-1,j,k}-p_{i,j-1,k}-p_{i,j,k-1}}{\\Delta x^2}) \\\\ = -( \\frac{u_{i+1/2,j,k}-u_{i-1/2,j,k}}{\\Delta x} + \\frac{v_{i,j+1/2,k}-v_{i,j-1/2,k}}{\\Delta x} + \\frac{w_{i,j,k+1/2}-w_{i,j,k-1/2}}{\\Delta x} ) \\tag {19}&emsp;&emsp;仔细观察上面的公式$(17)$和公式$(19)$，可以发现他们都是泊松问题$-\\Delta t/\\rho \\nabla \\cdot \\nabla p=-\\Delta \\cdot \\vec u$的一个离散计算公式，左边是关于压力的拉普拉斯算子的数值近似，右边是关于速度场散度的数值近似，我们最终要求解的就是这么一个关于压力的方程。方程右边是散度的相反数，这就是我们前面计算并保存的是散度的相反数的原因。当邻居格子是自由面边界时，我们直接将其相应的压力值取零；当邻居格子是固体墙边界时，我们根据冯诺依曼边界条件计算得到的压强替换相应的压力项。 2、写成矩阵形式&emsp;&emsp;上面讨论的仅仅是一个格子的压力项求解方程，事实上我们要求解的是整个流体区域的压力项值。为此，为了便于阐述和求解，我们将上面的线性方程写成矩阵乘向量的形式。我们将方程中所有压力项的系数提出来，写成一个系数矩阵$A$，所有流体区域的压力项写成一个未知变量的向量$p$，相应的所有流体区域的负散度作为线性方程组右边的向量$b$，那么我们要求解的就是下面的一个大规模非齐次线性方程组： Ap=b \\tag {20}&emsp;&emsp;矩阵$A$每一行存储的是一个流体格子求解压力项的系数。仔细观察公式$(17)$和$(19)$，实际上每一行的系数中仅仅只有几个不为0，不为0的系数是其周围邻居和自身的。在三维情况下，给定$(i,j,k)$这个流体格子，那么其对应的矩阵$A$中的那一行中，仅仅是$p_{i,j,k}$、$p_{i\\pm 1,j,k}$、$p_{i,j\\pm 1,k}$和$p_{i,j,k\\pm 1}$对应的系数不为0，即又7个不为的系数，剩下的系数全部为0。二维就是一行有5个系数不为0。可以看到矩阵$A$是一个大规模的稀疏矩阵，大部分的矩阵元素都为0，所以我们没有必要直接存储矩阵$A$，因为这样将会极大地耗费内存，甚至出现内存不足的情况，因为矩阵A实在是太大了。 &emsp;&emsp;仔细观察前面公式$(17)$和$(19)$的线性方程，对于给定的流体格子$(i,j,k)$，其邻域压力项的系数为$-\\Delta t/(\\rho \\Delta x^2)$，自身压力项的系数为邻居格子中流体格子或者空气（自由面）格子的数量$n_{i,j,k}$在乘上$\\Delta t/(\\rho\\Delta x^2)$，即$n_{i,j,k}\\Delta t/(\\rho \\Delta x^2)$，可以看到在三维时$n_{i,j,k}$至多为6、在二维时至多为$4$。 &emsp;&emsp;同时，系数矩阵$A$也是一个对阵矩阵。举个例子，$A_{(i,j,k)(i+1,j,k)}$是方程组中关于$p_{i+1,j,k}$的系数，那么它必然等于$A_{(i+1,j,k)(i,j,k)}$，这是因为$(i+1,j,k)$是$(i,j,k)$的邻居，那么$(i,j,k)$必然也是$(i+1,j,k)$的邻居。系数矩阵$A$既然是对阵矩阵，那么我们存储系数矩阵$A$时又可以减少一半的存储量了，只需存储上三角形、或者下三角矩阵。 &emsp;&emsp;经过上述的讨论，对于大规模的稀疏矩阵$A$，以二维为例，我们采用的存储方案为：每一个流体格子存储关于自身压力项的系数$A_{(i,j)(i,j)}$，这个就是矩阵$A$中的对角线元素，而关于邻居流体格子压力项的系数我们仅存储正向方向上的（因为对称）$A_{(i,j),(i+1,j)}$和$A_{(i,j)(i,j+1)}$。我们记矩阵$A$的对角线元素为Adiag(i,j)，$x$轴方向的矩阵元素Ax(i,j)，$y$轴方向的矩阵元素Ay(i,j)。三维同理，分别存储到Adiag(i,j,k)、Ax(i,j,k)、Ay(i,j,k)、Az(i,j,k)，一个三维的系数矩阵$A$的计算并存储的伪代码如下所示： 123456789101112131415161718192021222324252627282930scale = dt / (density*dx*dx);loop over i,j,k: if label(i,j,k)==FLUID: # handle negative x neighbor if label(i-1,j,k)==FLUID: Adiag(i,j,k) += scale; # handle positive x neighbor if label(i+1,j,k)==FLUID: Adiag(i,j,k) += scale; Ax(i,j,k) = -scale; else if label(i+1,j,k)==EMPTY: Adiag(i,j,k) += scale; # handle negative y neighbor if label(i,j-1,k)==FLUID: Adiag(i,j,k) += scale; # handle positive y neighbor if label(i,j+1,k)==FLUID: Adiag(i,j,k) += scale; Ay(i,j,k) = -scale; else if label(i,j+1,k)==EMPTY: Adiag(i,j,k) += scale; # handle negative z neighbor if label(i,j,k-1)==FLUID: Adiag(i,j,k) += scale; # handle positive z neighbor if label(i,j,k+1)==FLUID: Adiag(i,j,k) += scale; Az(i,j,k) = -scale; else if label(i,j,k+1)==EMPTY: Adiag(i,j,k) += scale; 3、共轭梯度算法&emsp;&emsp; 现在我们把目标放到求解大规模的非齐次线性方程组上，即上面的公式$(20)$。系数矩阵$A$除了是一个大规模的稀疏对称矩阵之外，还是一个正定（Positive definite）矩阵，即对于任意的非零向量$q$，$q^TAq&gt;0$均成立，且系数矩阵$A$的所有特征值均大于0。当然在实际情况下，系数矩阵$A$可能不是一个严格的正定矩阵，而是一个半正定矩阵，即对于任意非零向量$q$，有$q^TAq\\geq0$。这种情况出现在流体模拟区域中，存在一些流体区域完全被固体墙边界包围，此时的系数矩阵$A$不是一个严格的正定矩阵，甚至此时的矩阵$A$是一个奇异矩阵，它不存在逆矩阵，从而导致方程$(20)$不一定有解。因而在一些特殊情况下，我们求解的是兼容条件下的解，即只要使得流体-固体边界处流体的流进和流出保持平衡（流进量等于流出量）即可。 &emsp; &emsp; 求解一个线性方程组最常用、最暴力的方法就是高斯消元法，逐行进行消元，最后使得矩阵变成一个上三角矩阵，从最后一行往回代即可求解出方程的解（只要解存在）。但是高斯消元法实在是太慢了，小规模的矩阵来说还好，大规模的矩阵方程求解一般不会这么直接暴力解。这里我们采用的求解方程$(20)$的方法是共轭梯度算法（Conjugate gradient method，简称为CG）$^{[2]}$。相比于普通的高斯消元法，共轭梯度算法采用了一个迭代的流程，迭代到给定的收敛程度即可，整个算法流程只涉及到矩阵-向量乘法、向量加减、向量-标量乘法以及非常少的点乘操作，这些都可以快速并行地计算。 &emsp; &emsp; 共轭梯度算法就是针对对称正定矩阵的线性方程组的求解，给定一个线性方程组如下$(21)$所示，其中系数矩阵$A$是一个元素均已知的实对称正定矩阵，右边的向量$\\vec b$也已知，我们要求解的就是未知向量$\\vec x$。这个就是共轭梯度所要解决的线性方程组。 A x= b \\tag {21}&emsp;&emsp;给定两个非零向量$u$和$v$，我们说它们是共轭的（相对于$A$），若： u^TAv = 0 \\tag {22}&emsp;&emsp;而矩阵$A$是一个对称的正定矩阵（故有$A^T=A$），那么上式的左边可以写成多种向量内积的形式，它们完全是等价的。这样两个非零向量是共轭的，当且仅当这两个向量相对于下面的内积是正交的。共轭是一个对称的相互关系，$u$共轭于$v$，那么$v$也共轭于$u$。 u^TAv=_A=== \\tag {23}&emsp;&emsp;令$P=\\{p_1,…,p_n\\}$是一个这样的向量集合，集合中的所有向量都相对于$A$互相共轭，那么$P$构成了$R^n$空间的一个基，方程$(21)$的解$x_*$可以用这组基的线性组合来表示： x_*=\\Sigma_{i=1}^n\\alpha_i p_i \\tag {24}&emsp;&emsp;将上述的$x_*$表达式代入方程$(21)$中，有： Ax_*=\\Sigma_{i=1}^n\\alpha_iAp_i \\tag {25}&emsp;&emsp;再左乘上$P$中的一个向量$p_k^T$： p_k^TAx_*=\\Sigma_{i=1}^n\\alpha_ip_k^TAp_i \\tag {26}&emsp;&emsp;然后将$Ax_*=b$、$u^TAv=(u,v)_A$代入公式$(26)$中： p_k^Tb=\\Sigma_{i=1}^n\\alpha_i _A \\tag {27}&emsp;&emsp;最后根据$u^Tv=(u,v)$以及$\\forall i\\neq k: (p_k,p_i)_A=0$，可得： =\\alpha_k _A \\\\ \\to \\\\ \\alpha_k=\\frac{}{_A} \\tag {28}&emsp;&emsp;由此，我们可以得到一个求解方程$Ax=b$的方法，首先找到$n$个互相共轭的向量，然后计算其组合系数$\\alpha_k$，最后未知向量就由这$n$个互相共轭的向量线性组合而成（即公式$(24)$）。但显然直接寻找$n$个互相共轭的向量在$n$比较大的时候不现实，因为这将耗费大量的时间。 &emsp;&emsp;共轭梯度采用的是迭代法，我们给出了一个初始预测解$x_0$，致力于寻找最终解$x_d$，通过迭代的方法使得初始的预测解$x_0$逐步向$x_d$逼近，每一次的迭代中我们挑选出一个共轭向量$p$。迭代过程中的目标就是最小化预测向量与最终解$x_d$之间的差距，此时我们的问题就转换为如下的二次方程的最优化问题： f(x)=x^T(\\frac12Ax-b)=\\frac12x^TAx-x^Tb, x\\in R^n \\tag {29}&emsp;&emsp;公式$(29)$是一个关于$n$维向量的二次方程，是一个$n$元函数，其一阶导数和二阶导数分别如下所示： \\nabla f(x)=Ax-b \\\\ \\nabla^2 f(x)=A \\tag {30}&emsp;&emsp;可以看到当二次方程的一阶导数$Ax-b=0$时，此时的$x$就是我们要求的线性方程组的解，此时二次方程取得最小值。共轭梯度法在初次迭代时直接取第一个共轭向量$p_0=b-Ax_0$，即负梯度方向，这一步与梯度下降法类似。但是后续迭代取的向量就不再是负梯度向量了，而是取一个与梯度向量共轭的向量，这就是名称共轭梯度法的由来。实际上，每一次迭代时我们取的负梯度向量就是当前预测解与最终解之间的残差（Residual），第$k$次迭代时的残差为： r_k=b-Ax_k \\tag {31}&emsp;&emsp;为了确保在每一次迭代中取得的向量共轭于梯度以及前面迭代得到的向量，我们结合残差向量以及前面迭代的向量计算当前迭代挑选的向量： p_k=r_k-\\Sigma_{i","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/tags/Fluid-Simulation/"}]},{"title":"流体模拟Fluid Simulation：Level Set & Marching Cube","slug":"LevelSet","date":"2019-07-30T03:38:07.238Z","updated":"2019-10-20T09:05:38.773Z","comments":true,"path":"2019/07/30/LevelSet/","link":"","permalink":"https://yangwc.com/2019/07/30/LevelSet/","excerpt":"本文主要是基于欧拉网格流体模拟中的算法常客——Level Set和Marching Cube，Level Set算法负责构建复杂的边界几何体，而Marching Cube负责从欧拉网格中重建流体表面。","text":"本文主要是基于欧拉网格流体模拟中的算法常客——Level Set和Marching Cube，Level Set算法负责构建复杂的边界几何体，而Marching Cube负责从欧拉网格中重建流体表面。 符号距离场 三角形网格的SDF计算 Marching Cube重建算法 参考资料 水平集（Level Set） &emsp;&emsp;在流体模拟中，我们需要处理流体与固体边界的交互作用。这对于流体模拟来说至关重要，因为流体通常是作用到固体以及其他边界上才会体现流体的特性，让人看起来是流体。这就涉及到两个问题： &emsp;&emsp;1、给定一个点（例如用半拉格朗日后向追踪得到的点），如何判断它是否在固体边界内？ &emsp;&emsp;2、如何计算给定一点到几何体表面上的最近的点？ &emsp;&emsp;上面的问题在流体模拟中非常常见，在基于欧拉网格的流体模拟中，我们常用的是水平集的方法。我们将几何体表示成一个隐式的表面，给定关于这个几何体的连续标量函数$\\phi(x,y,z)$，我们定义该几何体构成的空间如下： &emsp;&emsp;1、当$\\phi(x,y,z)&gt;0$时，点$x$在几何体外部，此时满足该条件的$x$的点集构成几何体的外部空间； &emsp;&emsp;2、当$\\phi(x,y,z) &lt;0$时，点$x$在几何体内部，此时满足该条件的$x$的点集构成几何体的内部空间； &emsp;&emsp;3、当$\\phi(x,y,z)=0$时，点$x$在几何体的表面上，此时的$x$的点集构成了几何体的表面区域。 &emsp;&emsp;可以看到，我们采用了一个连续的标量函数$\\phi(x,y,z)$来隐式地表示一个几何体，几何体的表面就是函数$\\phi(x,y,z)$取值为$0$的所有点集，在三维的情况下这个点集构成了关于$\\phi(x,y,z)$的一个等高面（或者说等值面），在这个等高面上$\\phi(x,y,z)$取值均为$0$，这就是“水平集”一词的由来，几何体的表面就是$\\phi(x,y,z)$的$0$水平集。事实上，我们要寻找的这个连续标量函数$\\phi(x,y,z)$就是符号距离场（Signed Distance Field，简称SDF）。 一、符号距离场&emsp;&emsp;任意给定一个要隐式表达的点的闭集$S$，关于这个点集$S$的距离场函数为： distance_S(\\vec x)=min_{\\vec p\\in S}||\\vec x-\\vec p|| \\tag {1}&emsp;&emsp;该距离场函数给出了$\\vec x$到点集$S$的最近距离。若点集$S$将三维空间划分成了良定义的外部和内部（例如一个封闭的三角形网格模拟），那么其符号距离场函数就定义为： \\phi(\\vec x)= \\{ \\begin{matrix} +distance_S(\\vec x), x\\ \\ is\\ \\ outside.\\\\ -distance_S(\\vec x), x\\ \\ is\\ \\ inside. \\end{matrix} \\tag {2}&emsp;&emsp;符号距离场具有需要非常有用的性质。首先是符号距离场的梯度，根据矢量微积分，梯度指向函数值变换最快的方向，在这里显然沿着法线方向距离场函数值的变换最快，因而表面上的法线向量就等于表面上的符号距离场函数的梯度$\\nabla \\phi(\\vec x)$。在几何体的外部，$-\\nabla \\phi(\\vec x)$指向$\\vec x$到几何体表面上的最近点$\\vec p$；在几何体内部，$\\nabla \\phi(\\vec x)$指向$\\vec x$到几何体表面上的最近点$\\vec p$。 &emsp;&emsp;以外部的点$\\vec x$为例，记$\\vec p$为点$\\vec x$到几何体表面上的最近的点，$\\vec c$为点$\\vec x$到最近点$\\vec p$的单位方向向量： \\vec c = \\frac{\\vec p-\\vec x}{||\\vec p-\\vec x||} \\tag {3}&emsp;&emsp;如果我们将点$\\vec x$沿着$\\vec c$方向移动非常小的距离$\\epsilon$，即$\\vec x’=\\vec x+ \\epsilon \\vec c$，那么点$\\vec x’$到几何体表面上的最近点依然是$\\vec p$。即若沿着$\\vec c$方向移动$\\vec x$点，其到物体表面上的最近点$\\vec p$保持不变。挪动得到的新的点$\\vec x’$的符号距离场函数为$\\phi(\\vec x’)=\\phi(x)-\\epsilon$，从而我们可以得出符号距离场关于$\\vec c$方向向量上的方向导数为： \\frac{\\partial \\phi(\\vec x)}{\\partial \\vec c} =lim_{\\epsilon\\to 0}\\frac{\\phi(\\vec x+\\epsilon \\vec c)-\\phi(\\vec x)}{\\epsilon}\\\\ =lim_{\\epsilon\\to 0}\\frac{\\phi(\\vec x)-\\epsilon-\\phi(\\vec x)}{\\epsilon}\\\\ =lim_{\\epsilon\\to 0}\\frac{-\\epsilon}{\\epsilon}=-1 \\tag {4}&emsp;&emsp;根据上面的推导，我们知道$\\phi(\\vec x)$沿着梯度方向的变化速率为$1$，从而可知梯度向量长度为$1$，这意味着SDF的梯度实际上是一个单位向量，因而表面上的单位法线就是等于SDF函数的梯度向量： ||\\nabla\\phi|| = 1 \\tag {5}&emsp;&emsp;除此之外，上面的公式$(5)$是一个非线性偏微分方程——程函方程（Eikonal equation）。根据SDF函数及其梯度，我们可以求得外部的点$\\vec x$到几何体表面上的距离最近的点$\\vec p$： \\vec p=\\vec x-\\phi(\\vec x)\\nabla \\phi(\\vec x) \\tag {6}&emsp;&emsp;一般情况下，符号距离场函数都是光滑的，即梯度$\\nabla \\phi$和更高阶的导数都存在，但是也存在一些例外的点。一些中轴上的点到几何体表面的最近距离的点可能不唯一，例如球体的圆心到球体表面最近的距离就是半径$r$，但是其最近距离上的点有无穷多个，凹几何体同样存在这种情况。在这些特定的区域，符号距离场函数依然连续，只是存在一些扭曲，但函数不可微，因而梯度以及更高阶的导数不存在。在这些不可微的区域，我们的数值近似方法依旧会给出一个梯度向量，但这个梯度向量远远小于单位向量，甚至接近于$0$。 二、三角形网格的SDF计算&emsp;&emsp;了解了符号距离场之后，我们现在就把目标转到几何体的符号距离场计算上面。对于一些简单的几何体我们可以轻松地写出其SDF的解析表达式，例如一个球心为$\\vec c$、半径为$r$的球体其SDF函数为$\\phi_{sphere}(\\vec x)=||\\vec x-\\vec c||-r$，一个以$\\vec n$为法线、过点$\\vec p$的无限平面其SDF函数为$\\phi_{plane}(\\vec x)=(\\vec x-\\vec p)\\cdot \\vec n$，还有一些简单的锥体、柱体、立方体等等都可以直接写出其SDF表达式。但是通常情况下我们接触的几何体更多的是采用三角形的网格模型来表示，三角形构成的网格模型并没有一个显式的几何函数，因而不可能像简单的几何体那样直接写出SDF的解析函数。 &emsp;&emsp;与直接获取几何体的解析SDF函数不一样，对于三角网格模型，我们将其所开的空间划分成一个欧拉网格，在每一个网格点上采样并存储网格模型的SDF值。然后在后面获取给定点$\\vec x$的SDF函数值时，我们根据$\\vec x$获取其周围网格点的SDF值，接着做一个插值操作，从而快速地获取$\\vec x$对应的SDF函数值。这就是所谓的水平集方法：对符号距离场函数进行离散采样，并存储到均匀划分的网格中。显然这是求解了SDF函数的数值表达式，相对于解析表达式，它没有那么精确，但是对于流体模拟来说这些误差还是能够接受的。 &emsp;&emsp;我们首先要计算每个网格点上的符号距离场的值，目前主要有两种算法：第一种从几何角度展开，寻找给定的点到三角网格模型上的最近点并计算最近的距离场；第二种围绕偏微分方程展开，求解SDF的程函方程$||\\nabla\\phi||=1 $。两种算法都有各自的用途，第一种方法更好理解，结果也更为精确，而第二中方法则适用于非显式表示的几何体。这里我们首先考虑第一种基于几何的方法。 &emsp;&emsp;我们首先来看一个简化了的问题：给定一个有限的点集，计算任一点$\\vec x$到这个点集的距离场。由于点集并没有内部和外部的划分概念，因此我们的距离场也无需带上符号。计算的伪代码如下图1所示。 图1 点集的水平集计算伪代码 &emsp;&emsp;算法主要分为两步，首先初始化一个三维的$\\phi_{i,k,k}$数组，我们计算得到的SDF将存储到这个数组中。第一步，输入的点计算其对应的欧拉网格点，并更新该欧拉网格的SDF函数。第二步就是将第一步计算的SDF值扩散到其他未填充的欧拉网格点。上面的伪代码中第二步循环的顺序非常重要，因为这影响到其他欧拉网格点的SDF值的正确性。关于第二步的循环顺序，有两种比较推荐的方法，分别是快速移动算法和快速扫描算法，算法的复杂度分别为$O(nlog n)$和$O(n)$。我们目前仅看后一种，即快速扫描算法。 &emsp;&emsp;经过图1中的第一步之后，点集周围的欧拉网格点已经获取了距离场信息。第二步就是剩下的欧拉网格点需要从这些已知的欧拉网格点获取距离场信息。快速扫描算法基于这样的一个事实：最终每个未知的欧拉网格点的距离场信息必然是从其周围的某一个方向传播过来。为了确保每个欧拉网格点获取正确的距离场信息，我们遍历所有的传播方向。在三维空间中，传播方向有八个，即$i$递增、递减，$j$递增、递减，以及$k$递增、递减。因此算法将循环遍历八次，每次循环按照指定的方向进行遍历。为了提升算法的准确率，我们还可以多重复几次执行快速扫描算法。 &emsp;&emsp;上面我们讨论点集的水平集计算方法，接下来我们就继续深入到关于三角网格的水平集计算方法，这里的三角形网格是一个封闭的三角形网格。三角网格的水平集计算方法也是在上面点集的水平集计算方法的基础上展开，与之不同的是，这里我们追踪的不是点而是三角形。除此之外，我们还需要一些额外的步骤用于判断每个欧拉网格点是属于外部还是内部，这将影响到SDF函数值的符号。 图2 三角网格的水平集计算伪代码 &emsp;&emsp;三角网格的水平集计算方法比较复杂，图2的伪代码其实有些地方没有详细地说明，而这些地方恰恰非常关键。除了申请了一个三维数组用于存储$\\phi_{i,j,k}$和一个三维数组存储每个最近点的索引$t_{i,j,k}$，我们还申请了一个整数数组$c_{i,j,k}$记录相交次数。为了判断一个欧拉网格点是在三角网格内部还是外部，我们采用了投射射线法：如果一个点按照某个方向发射射线，该射线与三角网格模型的交点个数为奇数个，则该点处于网格模型内部；若该射线与三角网格模型的交点个数为偶数个，则该点处于网格模型的外部。 &emsp;&emsp;我们向$x$轴的负方向投射射线，暴力的方法就是每一个欧拉网格点都投射一条射线，但这其实是没必要的。我们创建了一个相交次数的记录数组$c_{i,j,k}$，每一个欧拉网格点$i,j,k$检测其到$i+1,j,k$点的边是否与三角形相交，然后在算法的最后，我们将$i$从$0$开始递增，理解$c_{i,j,k}$数组，并判断当前的累加值是奇数还是偶数，从而确定距离场的符号值。检测每一条$i,j,k$到$i+1,j,k$的边与三角形的相交情况时，因为投射的射线是平行于$x$轴的，我们可以将三角形投影到$yz$平面上，然后检测$j,k$是否在投影到$yz$平面上的三角形内。若在投影的三角形内，则必然相交，反之不相交。因此，我们需要一个方法快速判断给定的点是否在二维的三角形内。 &emsp;&emsp;这里我们采用SoS（Simulation Of Simplicity$^{[2]}$）的方法。给定两个二维向量$\\vec a$和$\\vec b$，在不考虑三维的情况下，二维的向量乘积结果为标量，其几何意义为由$\\vec a$和$\\vec b$构成的平行四边形的有向面积，$|\\vec a\\times \\vec b|=|\\vec a||\\vec b|sin\\theta$。当$\\vec a\\times \\vec b &lt; 0$时，两者夹角大于180度；当$\\vec a\\times \\vec b =0$时，两者共线（可能同向，可能反向）；当$\\vec a\\times \\vec b&gt;0$时，两者夹角小于180度。观察下面的图3，三角形由$x_1$、$x_2$和$x_3$构成，设要判断的点为$p$，我们可得向量$\\vec l_1 = {x_1-p}$、$\\vec l_2 = x_2-p$以及$\\vec l_3 = x_3-p$。注意到这样的一个事实，若点$p$在三角形内部，则向量$\\vec l_1$、$\\vec l_2$和$\\vec l_3$两两之间的夹角必定小于180度。若点$p$在三角形外部，则必定（以逆时针方向为例）存在两个向量之间的夹角大于0，此时的叉乘结果小于0，正如下图中的$p_2$，逆时针方向上，$x_1-p_2$和$x_2-p_2$夹角大于0了。 图3 判断点是否在三角形内部 &emsp;&emsp;上面的讨论我们已经知道如何判断给定的点是否在三角形内部了，接下来我们计算给定的点在三角形内部的重心坐标，后面我们在求交运算时需要根据重心坐标进行插值获取第三维的信息。还是一样，注意到叉乘的几何意义，我们通过叉乘得到了三个四边形的有向面积，然后除以2就是点$p$与三个顶点$x_1$、$x_2$和$x_3$构成的三角形的有向面积$S_1$、$S_2$、$S_3$，重心坐标就可以根据这些有向面积再除三角形总的有向面积和： \\alpha=\\frac{S_1}{S_1+S_2+S_3}\\\\ \\beta=\\frac{S_2}{S_1+S_2+S_3}\\\\ \\gamma=\\frac{S_3}{S_1+S_2+S_3} \\tag {7}&emsp;&emsp;最后编程代码如下所示，我们首先判断三个有向面积符号是否相同，若有一个不同则直接返回不在三角形内部的情况。最后根据有向面积计算相应的重心坐标。这里的二维判断仅用于相交计算。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849int LevelSet::orientation( float x1, float y1, float x2, float y2, float &amp; signedArea)&#123; signedArea = y1 * x2 - x1 * y2; if (signedArea &gt; 0) return 1; else if (signedArea &lt; 0) return -1; else if (y2 &gt; y1) return 1; else if (y2 &lt; y1) return -1; else if (x1 &gt; x2) return 1; else if (x1 &lt; x2) return -1; else return 0;&#125;bool LevelSet::pointInTriangle2D( float x0, float y0, float x1, float y1, float x2, float y2, float x3, float y3, float &amp; a, float &amp; b, float &amp; c)&#123; x1 -= x0; x2 -= x0; x3 -= x0; y1 -= y0; y2 -= y0; y3 -= y0; int signA = orientation(x2, y2, x3, y3, a); if (signA == 0) return false; int signB = orientation(x3, y3, x1, y1, b); if (signB != signA) return false; int signC = orientation(x1, y1, x2, y2, c); if (signC != signA) return false; float sum = a + b + c; a /= sum; b /= sum; c /= sum; return true;&#125; &emsp;&emsp;除了前面的二维相交计算，在三维空间中，我们还要求欧拉网格点到三角形的垂直距离，从而得到每个点的距离场。这里采用重心法求点到三角形距离。求的三角形重心坐标$\\alpha$、$\\beta$、$\\gamma$，设三角形的三个顶点为$\\vec x_1$、$\\vec x_2$、$\\vec x_3$，求重心坐标就是求下面的$2\\times2$的矩阵方程组： \\left[ \\begin{matrix} \\vec x_{21}\\cdot \\vec x_{21} & \\vec x_{21}\\cdot \\vec x_{31}\\\\ \\vec x_{21}\\cdot \\vec x_{31} & \\vec x_{31}\\cdot \\vec x_{31} \\end{matrix} \\right] \\left[ \\begin{matrix} \\beta \\\\ \\gamma \\end{matrix} \\right] = \\left[ \\begin{matrix} \\vec x_{21}\\cdot \\vec x_{01}\\\\ \\vec x_{31}\\cdot \\vec x_{01} \\end{matrix} \\right]\\\\ \\alpha = 1 - \\beta - \\gamma \\tag {8}&emsp;&emsp;上面公式中的$\\vec x_{21}$是$\\vec x_2-\\vec x_1$的简写，其他的类似。当且仅当求得的重心坐标$\\alpha$、$\\beta$和$\\gamma$均大于0时，交点才在三角形的内部，此时的最近点就为$\\alpha \\vec x_1+\\beta \\vec x_2+\\gamma \\vec x_3$。否则SDF最近的点应该在三角形的边上，这里有一个小小的优化，就是若有一个重心坐标值大于0，则该重心坐标对应的点的对边不需要考虑进来。例如$\\alpha&gt;0$，则无需考虑$\\vec x_2-\\vec x_1$这条边，但通常我们需要考虑剩下的两条边，然后取两者的距离最小值。 1234567891011121314151617181920212223242526272829303132float LevelSet::pointToTriangle( const glm::vec3 &amp; x0, const glm::vec3 &amp; x1, const glm::vec3 &amp; x2, const glm::vec3 &amp; x3)&#123; glm::vec3 x13 = x1 - x3; glm::vec3 x23 = x2 - x3; glm::vec3 x03 = x0 - x3; float m13 = glm::dot(x13, x13); float m23 = glm::dot(x23, x23); float d = glm::dot(x13, x23); float invDet = 1.0f / (glm::max(m13 * m23 - d * d, 1e-30f)); float a = glm::dot(x13, x03), b = glm::dot(x23, x03); // the barycentric coordinates float w23 = invDet * (m23 * a - d * b); float w31 = invDet * (m13 * b - d * a); float w12 = 1.0f - w23 - w31; if (w23 &gt;= 0 &amp;&amp; w31 &gt;= 0 &amp;&amp; w12 &gt;= 0) &#123; return glm::distance(x0, w23 * x1 + w31 * x2 + w12 * x3); &#125; else &#123; if (w23 &gt; 0) return glm::min(pointToLineSegment(x0, x1, x2), pointToLineSegment(x0, x1, x3)); else if(w31 &gt; 0) return glm::min(pointToLineSegment(x0, x1, x2), pointToLineSegment(x0, x2, x3)); else return glm::min(pointToLineSegment(x0, x1, x3), pointToLineSegment(x0, x2, x3)); &#125;&#125; &emsp;&emsp;求一点到给定线段的距离其实跟三角形的重心法类似，我们将给定的点投影到边上，然后取比值$\\theta$（即线段的重心坐标）。设求点$\\vec x_0$到线段$\\vec x_1-\\vec x_2$的最近的距离，则最近的点为$(1-\\theta)\\vec x_1+\\vec x_2$。 \\theta=\\frac{(\\vec x_2 - \\vec x_1)\\cdot(\\vec x_0-\\vec x_1)}{||\\vec x_2-\\vec x_1||^2} \\tag {9}123456789101112float LevelSet::pointToLineSegment( const glm::vec3 &amp; x0, const glm::vec3 &amp; x1, const glm::vec3 &amp; x2)&#123; glm::vec3 dx = x2 - x1; float lengthSquared = glm::dot(dx, dx); float ret = glm::dot(x2 - x0, dx); ret /= lengthSquared; ret = glm::clamp(ret, 0.0f, 1.0f); return glm::distance(x0, (x1 * ret + x2 * (1.0f - ret)));&#125; &emsp;&emsp;解决了以上问题，接下来我们就能完成Level Set算法中的第一步了：遍历模型的所有三角形，对于每一个三角形，我们计算其包围盒，在包围盒内的所有欧拉网格点，计算其到该三角形的垂直距离并与当前自己记录的最小距离进行比较、替换，与此同时记录最近的三角形的索引；然后将三角形投影（这里的投影很简单，不需要任何的计算，直接舍弃第三维$x$的值即可）到$yz$平面，计算其二维的包围盒，在这个二维包围盒内的所有点，判断其是否在三角形内，若在三角形内，则对应的相交记录数加1。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394void LevelSet::makeLevelSet3D( const std::vector&lt;Renderer::Vertex&gt;&amp; vertices, const std::vector&lt;unsigned int&gt;&amp; indices, const glm::vec3 &amp; origin, float dx, glm::ivec3 dim, std::vector&lt;float&gt;&amp; phi)&#123; // initialization. // signed distance value. phi.resize(dim.x * dim.y * dim.z, FLT_MAX); // closest triangle's index. std::vector&lt;int&gt; closestTriangle(dim.x * dim.y * dim.z, -1); // intersection counting. std::vector&lt;int&gt; intersectionCount(dim.x * dim.y * dim.z, 0); // setp 1. for (int x = 0; x &lt; indices.size(); x += 3) &#123; glm::vec3 p[3]; p[0] = vertices[indices[x + 0]].position; p[1] = vertices[indices[x + 1]].position; p[2] = vertices[indices[x + 2]].position; glm::ivec3 index[3]; index[0] = glm::ivec3((p[0].x - origin.x) / dx, (p[0].y - origin.y) / dx, (p[0].z - origin.z) / dx); index[1] = glm::ivec3((p[1].x - origin.x) / dx, (p[1].y - origin.y) / dx, (p[1].z - origin.z) / dx); index[2] = glm::ivec3((p[2].x - origin.x) / dx, (p[2].y - origin.y) / dx, (p[2].z - origin.z) / dx); // bounding box. glm::ivec3 minIndex, maxIndex; minIndex.x = (glm::clamp((int)glm::min(glm::min(index[0].x, index[1].x), index[2].x) - 1, 0, dim.x - 1)); minIndex.y = (glm::clamp((int)glm::min(glm::min(index[0].y, index[1].y), index[2].y) - 1, 0, dim.y - 1)); minIndex.z = (glm::clamp((int)glm::min(glm::min(index[0].z, index[1].z), index[2].z) - 1, 0, dim.z - 1)); maxIndex.x = (glm::clamp((int)glm::max(glm::max(index[0].x, index[1].x), index[2].x) + 2, 0, dim.x - 1)); maxIndex.y = (glm::clamp((int)glm::max(glm::max(index[0].y, index[1].y), index[2].y) + 2, 0, dim.y - 1)); maxIndex.z = (glm::clamp((int)glm::max(glm::max(index[0].z, index[1].z), index[2].z) + 2, 0, dim.z - 1)); // update level set. for (int i = minIndex.x; i &lt;= maxIndex.x; ++i) &#123; for (int j = minIndex.y; j &lt; maxIndex.y; ++j) &#123; for (int k = minIndex.z; k &lt; maxIndex.z; ++k) &#123; int ind = i * dim.y * dim.z + j * dim.z + k; glm::vec3 pos(origin.x + i * dx, origin.y + j * dx, origin.z + k * dx); float dist = pointToTriangle(pos, p[0], p[1], p[2]); if (dist &lt; phi[ind]) &#123; phi[ind] = dist; closestTriangle[ind] = x; &#125; &#125; &#125; &#125; // x-z plane bounding box. minIndex.y = (glm::clamp((int)glm::ceil( glm::min(glm::min(index[0].y, index[1].y), index[2].y)), 0, dim.y - 1)); minIndex.z = (glm::clamp((int)glm::ceil( glm::min(glm::min(index[0].z, index[1].z), index[2].z)), 0, dim.z - 1)); maxIndex.y = (glm::clamp((int)glm::floor( glm::max(glm::max(index[0].y, index[1].y), index[2].y)), 0, dim.y - 1)); maxIndex.z = (glm::clamp((int)glm::floor( glm::max(glm::max(index[0].z, index[1].z), index[2].z)), 0, dim.z - 1)); // intersection count. for (int j = minIndex.y; j &lt;= maxIndex.y; ++j) &#123; for (int k = minIndex.z; k &lt;= maxIndex.z; ++k) &#123; float a, b, c; // project to y-z plane if (pointInTriangle2D(j, k, index[0].y, index[0].z, index[1].y, index[1].z, index[2].y, index[2].z, a, b, c)) &#123; float fi = a * index[0].x + b * index[1].x + c * index[2].x; int iInterval = int(glm::ceil(fi)); if (iInterval &lt; 0) intersectionCount[j * dim.z + k] += 1; else if (iInterval &lt; dim.x) intersectionCount[iInterval * dim.y * dim.z + j * dim.z + k] += 1; &#125; &#125; &#125; &#125; std::cout &lt;&lt; \"Step1 finished.\\n\"; // ......&#125; &emsp;&emsp;紧接着第二步就是扩散步骤，在第一步中有些欧拉网格点已经获取了距离场信息，现在要将其传递到其它未知的欧拉网格中，从而得到完整的水平集结构。这里我们采用fast sweep算法，在三维空间中，传播方向有八个，即$i$递增、递减，$j$递增、递减，以及$k$递增、递减。因此算法将循环遍历八次，每次循环按照指定的方向进行遍历。同时，为了提高算法的准确率，我们设置迭代两次，每次传递八个方向。传播时，每个欧拉网格点根据给定的方向获取其邻居的距离场信息，若根据邻居的距离场信息得到的距离场数值更小，则进行替换。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105void LevelSet::neighbour( const std::vector&lt;Renderer::Vertex&gt;&amp; vertices, const std::vector&lt;unsigned int&gt;&amp; indices, std::vector&lt;float&gt;&amp; phi, std::vector&lt;int&gt; closestTriangle, const glm::vec3 &amp; pos, glm::ivec3 dim, glm::ivec3 targetInd, glm::ivec3 neighInd)&#123; int index = neighInd.x * dim.y * dim.z + neighInd.y * dim.z + neighInd.z; if (closestTriangle[index] &gt;= 0) &#123; int closestTriInd = closestTriangle[index]; glm::vec3 p[3]; p[0] = vertices[indices[closestTriInd + 0]].position; p[1] = vertices[indices[closestTriInd + 1]].position; p[2] = vertices[indices[closestTriInd + 2]].position; float dist = pointToTriangle(pos, p[0], p[1], p[2]); if (dist &lt; phi[targetInd.x * dim.y * dim.z + targetInd.y * dim.z + targetInd.z]) &#123; phi[targetInd.x * dim.y * dim.z + targetInd.y * dim.z + targetInd.z] = dist; closestTriangle[targetInd.x * dim.y * dim.z + targetInd.y * dim.z + targetInd.z] = closestTriInd; &#125; &#125;&#125;void LevelSet::fastSweep( const std::vector&lt;Renderer::Vertex&gt;&amp; vertices, const std::vector&lt;unsigned int&gt;&amp; indices, std::vector&lt;float&gt;&amp; phi, std::vector&lt;int&gt; closestTriangle, const glm::vec3 &amp; origin, float dx, glm::ivec3 dim, glm::ivec3 dir)&#123; int i0, i1; if (dir.x &gt; 0) i0 = 1, i1 = dim.x; else i0 = dim.x - 2, i1 = -1; int j0, j1; if (dir.y &gt; 0) j0 = 1, j1 = dim.y; else j0 = dim.y - 2, j1 = -1; int k0, k1; if (dir.z &gt; 0) k0 = 1, k1 = dim.z; else k0 = dim.z - 2, k1 = -1; for (int i = i0; i != i1; i += dir.x) &#123; for (int j = j0; j != j1; j += dir.y) &#123; for (int k = k0; k != k1; k += dir.z) &#123; glm::vec3 pos(origin.x + i * dx, origin.y + j * dx, origin.z + k * dx); neighbour(vertices, indices, phi, closestTriangle, pos, dim, glm::ivec3(i,j,k), glm::ivec3(i - dir.x, j, k)); neighbour(vertices, indices, phi, closestTriangle, pos, dim, glm::ivec3(i, j, k), glm::ivec3(i, j - dir.y, k)); neighbour(vertices, indices, phi, closestTriangle, pos, dim, glm::ivec3(i, j, k), glm::ivec3(i, j, k - dir.z)); neighbour(vertices, indices, phi, closestTriangle, pos, dim, glm::ivec3(i, j, k), glm::ivec3(i - dir.x, j - dir.y, k)); neighbour(vertices, indices, phi, closestTriangle, pos, dim, glm::ivec3(i, j, k), glm::ivec3(i - dir.x, j, k - dir.z)); neighbour(vertices, indices, phi, closestTriangle, pos, dim, glm::ivec3(i, j, k), glm::ivec3(i, j - dir.y, k - dir.z)); neighbour(vertices, indices, phi, closestTriangle, pos, dim, glm::ivec3(i, j, k), glm::ivec3(i - dir.x, j - dir.y, k - dir.z)); &#125; &#125; &#125;&#125;void LevelSet::makeLevelSet3D( const std::vector&lt;Renderer::Vertex&gt;&amp; vertices, const std::vector&lt;unsigned int&gt;&amp; indices, const glm::vec3 &amp; origin, float dx, glm::ivec3 dim, std::vector&lt;float&gt;&amp; phi)&#123; // initialization. ...... // setp 1. ...... // step 2. for (unsigned int pass = 0; pass &lt; 2; ++pass) &#123; fastSweep(vertices, indices, phi, closestTriangle, origin, dx, dim, glm::ivec3(+1, +1, +1)); fastSweep(vertices, indices, phi, closestTriangle, origin, dx, dim, glm::ivec3(-1, -1, -1)); fastSweep(vertices, indices, phi, closestTriangle, origin, dx, dim, glm::ivec3(+1, +1, -1)); fastSweep(vertices, indices, phi, closestTriangle, origin, dx, dim, glm::ivec3(+1, -1, +1)); fastSweep(vertices, indices, phi, closestTriangle, origin, dx, dim, glm::ivec3(-1, +1, +1)); fastSweep(vertices, indices, phi, closestTriangle, origin, dx, dim, glm::ivec3(-1, -1, +1)); fastSweep(vertices, indices, phi, closestTriangle, origin, dx, dim, glm::ivec3(-1, +1, -1)); fastSweep(vertices, indices, phi, closestTriangle, origin, dx, dim, glm::ivec3(+1, -1, -1)); &#125; std::cout &lt;&lt; \"Step2 finished.\\n\";&#125; &emsp;&emsp;最后的第三步就是修正距离场的符号，前面我们计算得到的距离场都是没有符号的。有了前面计算得到的相交记录数组，判断符号就容易了。注意前面我们计算得到的相交记录的值仅仅是一条边的相交记录，最后我们从$x$轴负方向朝着$x$轴正方面推进，叠加相交记录次数，得到以当前欧拉网格点为起点、朝向$x$轴负方向的射线与网格的相交情况，若为奇数，则需要将距离场的值置为负数。 1234567891011121314151617181920212223242526272829303132void LevelSet::makeLevelSet3D( const std::vector&lt;Renderer::Vertex&gt;&amp; vertices, const std::vector&lt;unsigned int&gt;&amp; indices, const glm::vec3 &amp; origin, float dx, glm::ivec3 dim, std::vector&lt;float&gt;&amp; phi)&#123; // initialization. ...... // setp 1. ...... // step 2. ... // step 3. for (int j = 0; j &lt; dim.y; ++j) &#123; for (int k = 0; k &lt; dim.z; ++k) &#123; int totalCount = 0; for (int i = 0; i &lt; dim.x; ++i) &#123; int index = i * dim.y * dim.z + j * dim.z + k; totalCount += intersectionCount[index]; if (totalCount % 2 == 1) phi[index] = -phi[index]; &#125; &#125; &#125; std::cout &lt;&lt; \"Step3 finished.\\n\";&#125; 三、Marching Cube重建算法&emsp;&emsp;在前面我们实现了Level Set的构造方法，通过离散空间将三角网格的SDF值保存下来。在欧拉方法的流体模拟中，除了将三角网格模型离散化为水平集之外，还涉及到根据给定的水平集以及等高值，从中重建出网格模型，最经典的就是流体表面的网格重建过程。根据给定的欧拉网格结构重建网格模型最经典、常用的算法就是Marching Cube算法。Marching Cube算法的基本思想就是在欧拉网格的每一个立方体内，寻找在等值面上的网格点。一个立方体有八个顶点，每个顶点对应的函数值各有不同。以0等值面为例，我们要寻找函数值为0的网格点，从而生成三角网格模型。假设有一条边及其两个端点$(i,j,k)$和$(i+1,j,k)$，倘若其一端$(i,j,k)$的函数值大于0，另一端$(i+1,j,k)$的函数值小于0，那么必然在这条线段上存在一个0等值点$p$，可以通过线性插值得到： \\theta=\\frac{\\phi_{i,j,k}}{\\phi_{i,j,k}-\\phi_{i+1,j,k}} \\\\ p=((i+\\theta)\\Delta x,j\\Delta x,k\\Delta x) \\tag {10}123456789float MarchingCube::fraction(float value, float lower, float upper)&#123; return (value - lower) / (upper - lower);&#125;glm::vec3 MarchingCube::interpolate(const glm::vec3 &amp; v0, const glm::vec3 &amp; v1, float value)&#123; return (1.0f - value) * v0 + value * v1;&#125; &emsp;&emsp;立方体有八个顶点，给定一个值，相对于这个值，每个顶点的函数值分为大于给定值和小于给定值两种情况。因此在一个立方体上，传统的Marching Cube算法需要考虑$2^8=256$中情况，这通常通过预先生成一个查找表来进行。即便如此，Robert Brdison指出Lorensen等人的Marching Cube算法因为拓扑结构的歧义性偶尔会产生一些孔洞，从而得到非封闭的网格模型。这里我们采用Marching Cube中的一个鲁棒性更强的算法——Marching Tetrahedra算法，与Marching Cube的直接对整个立方体进行操作不同，Marching Tetrahedra算法将立方体进一步划分成多个四面体（Tetrahedra），然后对四面体进行操作，即计算四面体四个顶点的函数值，并判断是否存在等值点，存在的话就根据等值点构建三角形。 &emsp;&emsp;将立方体划分成多个四面体的方法如下图4所示，立方体有三对对立面，即上和下、左和右、前和后。对于每一个对立面，取同一轨迹的对角线，以这两条对角线所在的平面将立方体，如此切割三次就能得到构成立方体的六个四面体（下图4中六个不同颜色的四面体）。 图4 划分得到的Tetrahedra &emsp;&emsp;与传统的Marching Cube算法相比，Marching Tetrahedra算法的优势更多。在Marching Cube算法中，操作单位为一个立方体，一个等值面截过立方体构成的截面可以是很复杂的曲面；而在Marching Tetrahedra算法中，操作单位变成了一个四面体，一个等值面截过四面体构成的截面通常只能是单个三角形、或者一个四边形（这个四边形可以分成两个三角形）。另外，因为四面体只有4个顶点，因而单个四面体只需考虑$2^4=16$中情况，相对于原来的$256$种情况大大降低了代码的复杂度。Marching Tetrahedra唯一的缺点就是生成的三角形偏多。 &emsp;&emsp;Marching Tetrahedra算法不太复杂，算法总体上就是一个循环，遍历所有的cube，然后将cube划分成六个四面体，根据四面体四个顶点的函数值判断是否存在三角形。上面已经提到过了，在四面体中最多就存在两个三角形。Marching Tetrahedra算法总体上分为三种情况：不存在三角形、存在一个三角形、存在两个三角形。 &emsp;&emsp;首先来看第一种情况，即不存在三角形。这种情况很好理解，当四面体所有顶点的函数值均为正数或者负数时（这里的正数或负数是相对于给定的函数值点来说，大于给定值则为正，小于则为负），不存在一个0等值点（或者其他等值点），从而不存在三角形。 12345678910111213141516171819202122std::vector&lt;glm::vec3&gt; MarchingCube::marchingTetrahedra( const glm::vec3 &amp; v0, const glm::vec3 &amp; v1, const glm::vec3 &amp; v2, const glm::vec3 &amp; v3, const float &amp; value0, const float &amp; value1, const float &amp; value2, const float &amp; value3, float isovalue)&#123; std::vector&lt;glm::vec3&gt; triangles; // signs. int sign0 = (value0 &gt; isovalue) ? 1 : 0; int sign1 = (value1 &gt; isovalue) ? 1 : 0; int sign2 = (value2 &gt; isovalue) ? 1 : 0; int sign3 = (value3 &gt; isovalue) ? 1 : 0; // all negative or all positive, no triangles. if ((sign0 == 0 &amp;&amp; sign1 == 0 &amp;&amp; sign2 == 0 &amp;&amp; sign3 == 0) || (sign0 == 1 &amp;&amp; sign1 == 1 &amp;&amp; sign2 == 1 &amp;&amp; sign3 == 1)) return triangles; ....&#125; &emsp;&emsp;然后是第二种情况，即存在一个三角形。注意到一个这样的事实：当四面体的一个顶点函数值为正数，其余三个顶点为负数时，在正值顶点与负值顶点相连的边上可以得到一个等值点，正值顶点分别与三个负值顶点相连的边上可以得到三个等值点，从而构成一个三角形。反过来也一样，当只有一个顶点为负数，其余顶点为正数时同理。如下图5所示，四面体的四个顶点分别为$v_1$、$v_2$、$v_3$和$v_4$，其中$v_1$的函数值为负值，$v_2$、$v_3$和$v_4$的函数值为正，从而通过线性插值得到图中的三个等值点$p_1$、$p_2$和$p_3$，将这三个顶点相连即可得到一个三角形。 图5 一个三角形的情况 &emsp;&emsp;一个三角形的情形共有8种，一正三负4种，一负三正4种，在代码种逐一枚举即可。这里需要注意，当三角形退化成一个点时我们不将将其纳入我们的网格顶点中。同时为了使得生成的三角形环绕顺序遵循向外的逆时针方向，在实现时需特别注意顶点的顺序。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879std::vector&lt;glm::vec3&gt; MarchingCube::getOneTriangle( const glm::vec3 &amp; v0, const glm::vec3 &amp; v1, const glm::vec3 &amp; v2, const glm::vec3 &amp; v3, const float &amp;value0, const float &amp;value1, const float &amp;value2, const float &amp;value3, float isovalue)&#123; float t01 = fraction(isovalue, value0, value1); float t02 = fraction(isovalue, value0, value2); float t03 = fraction(isovalue, value0, value3); glm::vec3 ver0 = interpolate(v0, v1, t01); glm::vec3 ver1 = interpolate(v0, v2, t02); glm::vec3 ver2 = interpolate(v0, v3, t03); if (isTriangle(ver0, ver1, ver2)) return &#123; ver0, ver1, ver2 &#125;; return &#123;&#125;;&#125;std::vector&lt;glm::vec3&gt; MarchingCube::marchingTetrahedra( const glm::vec3 &amp; v0, const glm::vec3 &amp; v1, const glm::vec3 &amp; v2, const glm::vec3 &amp; v3, const float &amp; value0, const float &amp; value1, const float &amp; value2, const float &amp; value3, float isovalue)&#123; ...... // One positive vertex, all others negative =&gt; One triangle if (sign0 == 1 &amp;&amp; sign1 == 0 &amp;&amp; sign2 == 0 &amp;&amp; sign3 == 0) &#123; // Vertices are in edges 0-1, 0-2 and 0-3 triangles = getOneTriangle(v0, v1, v2, v3, value0, value1, value2, value3, isovalue); &#125; else if (sign0 == 0 &amp;&amp; sign1 == 1 &amp;&amp; sign2 == 0 &amp;&amp; sign3 == 0) &#123; // Vertices are in edges 1-2, 1-3 and 1-0 triangles = getOneTriangle(v1, v2, v0, v3, value1, value2, value0, value3, isovalue); &#125; else if (sign0 == 0 &amp;&amp; sign1 == 0 &amp;&amp; sign2 == 1 &amp;&amp; sign3 == 0) &#123; // Vertices are in edges 2-0, 2-1 and 2-3 triangles = getOneTriangle(v2, v0, v1, v3, value2, value0, value1, value3, isovalue); &#125; else if (sign0 == 0 &amp;&amp; sign1 == 0 &amp;&amp; sign2 == 0 &amp;&amp; sign3 == 1) &#123; // Vertices are in edges 3-2, 3-0 and 3-1 triangles = getOneTriangle(v3, v2, v1, v0, value3, value2, value1, value0, isovalue); &#125; // One negative vertex, all others positive =&gt; One triangle else if (sign0 == 0 &amp;&amp; sign1 == 1 &amp;&amp; sign2 == 1 &amp;&amp; sign3 == 1) &#123; // Vertices are in edges 0-1, 0-3 and 0-2 triangles = getOneTriangle(v0, v3, v2, v1, value0, value3, value2, value1, isovalue); &#125; else if (sign0 == 1 &amp;&amp; sign1 == 0 &amp;&amp; sign2 == 1 &amp;&amp; sign3 == 1) &#123; // Vertices are in edges 1-2, 1-3 and 1-0 triangles = getOneTriangle(v1, v3, v0, v2, value1, value3, value0, value2, isovalue); &#125; else if (sign0 == 1 &amp;&amp; sign1 == 1 &amp;&amp; sign2 == 0 &amp;&amp; sign3 == 1) &#123; // Vertices are in edges 2-0, 2-3 and 2-1 triangles = getOneTriangle(v2, v3, v1, v0, value2, value3, value1, value0, isovalue); &#125; else if (sign0 == 1 &amp;&amp; sign1 == 1 &amp;&amp; sign2 == 1 &amp;&amp; sign3 == 0) &#123; // Vertices are in edges 3-2, 3-0 and 3-1 triangles = getOneTriangle(v3, v0, v1, v2, value3, value0, value1, value2, isovalue); &#125; ......&#125; &emsp;&emsp;最后就是第三种情况，即存在两个三角形。当四个顶点中有两个顶点函数值为正，两个顶点函数值为负时，从正值顶点分别于负值顶点相连的边上可以获取一个等值点，一共可以得到四个等值点，构成一个四边形，或者说两个三角形。如下图6所示，负值顶点为$v_1$、$v_2$，正值顶点为$v_3$、$v_4$，$v_1$分别与$v_3$、$v_4$相连的边上可以得到等值点$p_1$、$p_2$，$v_2$分别与$v_3$、$v_4$相连的边上可以得到等值点$p_3$、$p_4$。最后根据这四个点分别作两个三角形即可。两个三角形的情况共有$C_4^2=6$种，逐一枚举即可。 图6 两个三角形的情况 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182std::vector&lt;glm::vec3&gt; MarchingCube::getTwoTriangle( const glm::vec3 &amp; v0, const glm::vec3 &amp; v1, const glm::vec3 &amp; v2, const glm::vec3 &amp; v3, const float &amp;value0, const float &amp;value1, const float &amp;value2, const float &amp;value3, float isovalue)&#123; float t02 = fraction(isovalue, value0, value2); float t03 = fraction(isovalue, value0, value3); float t12 = fraction(isovalue, value1, value2); float t13 = fraction(isovalue, value1, value3); glm::vec3 ver0 = interpolate(v0, v2, t02); glm::vec3 ver1 = interpolate(v0, v3, t03); glm::vec3 ver2 = interpolate(v1, v2, t12); glm::vec3 ver3 = interpolate(v1, v3, t13); std::vector&lt;glm::vec3&gt; ret; if (isTriangle(ver0, ver1, ver2)) &#123; ret.push_back(ver0); ret.push_back(ver1); ret.push_back(ver2); &#125; if (isTriangle(ver2, ver1, ver3)) &#123; ret.push_back(ver2); ret.push_back(ver1); ret.push_back(ver3); &#125; return ret;&#125;std::vector&lt;glm::vec3&gt; MarchingCube::marchingTetrahedra( const glm::vec3 &amp; v0, const glm::vec3 &amp; v1, const glm::vec3 &amp; v2, const glm::vec3 &amp; v3, const float &amp; value0, const float &amp; value1, const float &amp; value2, const float &amp; value3, float isovalue)&#123; ...... // Two positive vertice, two negative vertice =&gt; Two triangle else if (sign0 == 1 &amp;&amp; sign1 == 1 &amp;&amp; sign2 == 0 &amp;&amp; sign3 == 0) &#123; // Vertices are in edges 0-2, 0-3, 1-2 and 1-3. triangles = getTwoTriangle(v0, v1, v2, v3, value0, value1, value2, value3, isovalue); &#125; else if (sign0 == 0 &amp;&amp; sign1 == 1 &amp;&amp; sign2 == 1 &amp;&amp; sign3 == 0) &#123; // Vertices are in edges 1-0, 1-3, 2-0 and 2-3. triangles = getTwoTriangle(v1, v2, v0, v3, value1, value2, value0, value3, isovalue); &#125; else if (sign0 == 0 &amp;&amp; sign1 == 0 &amp;&amp; sign2 == 1 &amp;&amp; sign3 == 1) &#123; // Vertices are in edges 2-0, 2-1, 3-0 and 3-1. triangles = getTwoTriangle(v2, v3, v0, v1, value2, value3, value0, value1, isovalue); &#125; else if (sign0 == 1 &amp;&amp; sign1 == 0 &amp;&amp; sign2 == 0 &amp;&amp; sign3 == 1) &#123; // Vertices are in edges 0-1, 0-2, 3-1 and 3-2. triangles = getTwoTriangle(v0, v3, v1, v2, value0, value3, value1, value2, isovalue); &#125; else if (sign0 == 1 &amp;&amp; sign1 == 0 &amp;&amp; sign2 == 1 &amp;&amp; sign3 == 0) &#123; // Vertices are in edges 0-1, 0-3, 2-1 and 2-3. triangles = getTwoTriangle(v0, v2, v3, v1, value0, value2, value3, value1, isovalue); &#125; else if (sign0 == 0 &amp;&amp; sign1 == 1 &amp;&amp; sign2 == 0 &amp;&amp; sign3 == 1) &#123; // Vertices are in edges 1-0, 1-2, 3-0 and 3-2. triangles = getTwoTriangle(v1, v3, v2, v0, value1, value3, value2, value0, isovalue); &#125; return triangles;&#125; &emsp;&emsp;上面讨论的都是单个四面体的情形，然后我们需要将一个立方体划分成六个四面体，每个四面体执行上面的Marching Tetrahedra算法，最后将每个四面体得到的三角形合起来。 1234567891011121314151617181920212223242526272829303132void MarchingCube::marchingCube( const glm::vec3 &amp; v0, const glm::vec3 &amp; v1, const glm::vec3 &amp; v2, const glm::vec3 &amp; v3, const glm::vec3 &amp; v4, const glm::vec3 &amp; v5, const glm::vec3 &amp; v6, const glm::vec3 &amp; v7, const float &amp; value0, const float &amp; value1, const float &amp; value2, const float &amp; value3, const float &amp; value4, const float &amp; value5, const float &amp; value6, const float &amp; value7, float isovalue, std::vector&lt;glm::vec3&gt;&amp; mesh)&#123; std::vector&lt;glm::vec3&gt; tmp; tmp = marchingTetrahedra(v5, v0, v3, v1, value5, value0, value3, value1, isovalue); mesh.insert(mesh.begin(), tmp.begin(), tmp.end()); tmp = marchingTetrahedra(v5, v1, v3, v2, value5, value1, value3, value2, isovalue); mesh.insert(mesh.begin(), tmp.begin(), tmp.end()); tmp = marchingTetrahedra(v5, v4, v3, v0, value5, value4, value3, value0, isovalue); mesh.insert(mesh.begin(), tmp.begin(), tmp.end()); tmp = marchingTetrahedra(v5, v3, v6, v2, value5, value3, value6, value2, isovalue); mesh.insert(mesh.begin(), tmp.begin(), tmp.end()); tmp = marchingTetrahedra(v5, v4, v7, v3, value5, value4, value7, value3, isovalue); mesh.insert(mesh.begin(), tmp.begin(), tmp.end()); tmp = marchingTetrahedra(v5, v3, v7, v6, value5, value3, value7, value6, isovalue); mesh.insert(mesh.begin(), tmp.begin(), tmp.end());&#125; &emsp;&emsp;这里为了验证前面水平集算法实现的正确性，将构建得到的水平集传入Marching Cube中，构建一个网格模型，即构建水平集的逆过程。重构过程根据前面水平集划分的欧拉网格大小，对每一个立方体执行Marching Cube算法，三重循环即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445std::vector&lt;glm::vec3&gt; MarchingCube::reconstructMeshFromLevelSet( const std::vector&lt;float&gt;&amp; phi, float isovalue, glm::ivec3 dim, float dx, glm::vec3 origin)&#123; // reconstruct a mesh from given level set. std::vector&lt;glm::vec3&gt; mesh; for (int i = 0; i &lt; dim.x - 1; ++i) &#123; for (int j = 0; j &lt; dim.y - 1; ++j) &#123; for (int k = 0; k &lt; dim.z - 1; ++k) &#123; // 8 vertice of a cube. glm::vec3 p[8]; p[0] = glm::vec3(origin.x + i * dx, origin.y + j * dx, origin.z + k * dx); p[1] = glm::vec3(p[0].x + dx, p[0].y, p[0].z); p[2] = glm::vec3(p[0].x + dx, p[0].y, p[0].z + dx); p[3] = glm::vec3(p[0].x, p[0].y, p[0].z + dx); p[4] = glm::vec3(p[0].x, p[0].y + dx, p[0].z); p[5] = glm::vec3(p[0].x + dx, p[0].y + dx, p[0].z); p[6] = glm::vec3(p[0].x + dx, p[0].y + dx, p[0].z + dx); p[7] = glm::vec3(p[0].x, p[0].y + dx, p[0].z + dx); // the corresponding sdf value. float value[8]; value[0] = phi[i * dim.y * dim.z + j * dim.z + k]; value[1] = phi[(i + 1) * dim.y * dim.z + j * dim.z + k]; value[2] = phi[(i + 1) * dim.y * dim.z + j * dim.z + k + 1]; value[3] = phi[i * dim.y * dim.z + j * dim.z + k + 1]; value[4] = phi[i * dim.y * dim.z + (j + 1) * dim.z + k]; value[5] = phi[(i + 1) * dim.y * dim.z + (j + 1) * dim.z + k]; value[6] = phi[(i + 1) * dim.y * dim.z + (j + 1) * dim.z + k + 1]; value[7] = phi[i * dim.y * dim.z + (j + 1) * dim.z + k + 1]; // marching a cube. marchingCube(p[0], p[1], p[2], p[3], p[4], p[5], p[6], p[7], value[0], value[1], value[2], value[3], value[4], value[5], value[6], value[7], isovalue, mesh); &#125; &#125; &#125; return mesh;&#125; &emsp;&emsp;下面几张图展示了从水平集重建的网格。左边是原三角网格模型，精度均比较高，右边的是通过构建了水平集之后再从水平集重建的网格模型。受限于水平集的欧拉网格精度，重建得到的网格模型显然精度没有那么高，是一个粗糙的low poly模型。通过Marching Tetrahedra算法得到的网格依然是封闭的，这也验证了我们前面的水平集算法实现的正确性。 参考资料：$[1]$ Marching tetrahedra. From Wikipedia, the free encyclopedia $[2]$ Edelsbrunner H, Mucke E P. Simulation of Simplicity: A Technique to Cope with Degenerate Cases in Geometric Algorithms[J]. Acm Transactions on Graphics, 1990, 9(1):66-104. $[3]$ 《Fluid Simulation For Computer Graphics》, Robert Bridson.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/tags/Fluid-Simulation/"}]},{"title":"二维渲染2D Rendering：2D Lighting","slug":"2DLighting","date":"2019-07-23T10:36:10.868Z","updated":"2021-02-25T10:59:22.151Z","comments":true,"path":"2019/07/23/2DLighting/","link":"","permalink":"https://yangwc.com/2019/07/23/2DLighting/","excerpt":"偶然在知乎上看到图形学大佬的几篇关于二维渲染绘制的文章，感觉非常有趣。与普通的光线求交不同，这里的二维渲染采用了光线步进法和符号距离场。下图就是渲染出来的效果，二维虽然少了一维，但渲染开销依旧大，渲染下面的这张图开启了多线程也花费了1个半小时多的时间。","text":"偶然在知乎上看到图形学大佬的几篇关于二维渲染绘制的文章，感觉非常有趣。与普通的光线求交不同，这里的二维渲染采用了光线步进法和符号距离场。下图就是渲染出来的效果，二维虽然少了一维，但渲染开销依旧大，渲染下面的这张图开启了多线程也花费了1个半小时多的时间。 二维渲染 SDF构造实体几何 比尔-郎伯定律 实现效果 参考资料 2D Lighting &emsp;&emsp;偶然在知乎上看到图形学大佬的几篇关于二维渲染绘制的文章，感觉非常有趣，二维的相对而言比较简单但是涉及到的知识也不少，遂记录下来一些自己的学习总结。一些内容跟前面光线追踪的重合了，所以就不细细展开了。本文主要是关于SDF方面的内容。 ## 一、二维渲染 &emsp;&emsp;我们要绘制的世界是一个二维的场景，场景中有发光的二维体。对于一个二维屏幕上的像素，我们要求在360度方向上接收的光照值，故给定二维空间的坐标$(x,y)$，渲染方程为对$[0,2\\pi]$方向的单重积分： $$ L_o(x,y)=\\int_0^{2\\pi}L_i(x,y,\\theta)d\\theta \\tag {1} $$ &emsp;&emsp;在这里我们就对二维图像的每个像素求解方程$(1)$的积分公式，用tbb多线程库加速渲染，然后将像素矩阵用stb_image保存成png文件。原点在图像的左上角。 123456789101112131415161718192021unsigned char * Renderer::render()&#123; parallel_for(blocked_range&lt;size_t&gt;(0, m_height * m_width, 5000), [&amp;](blocked_range&lt;size_t&gt;&amp; r) &#123; for (size_t i = r.begin(); i != r.end(); ++i) &#123; size_t col = i % m_width; size_t row = i / m_width; glm::vec3 color(0.0f); // sampling and lighting. color.x = color.y = color.z = sample(static_cast&lt;float&gt;(col) / m_width, static_cast&lt;float&gt;(row) / m_height); // save to pixel. drawPixel(row, col, color); &#125; &#125;); return m_image;&#125; &emsp;&emsp;由于被积函数$L(x,y,\\theta)$并没有一个显示的数学表达式，我们无法求解公式$(1)$单重积分的解析解，因此采用蒙特卡洛数值积分法。我们首先考虑随机均匀采样$N$个方向，每个方向用$\\theta_1,\\theta_2,...,\\theta_N$表示，因为是均匀采样，所以概率密度函数$pdf=\\frac {1}{2\\pi}$，蒙特卡洛数值积分公式如下： $$ L_o(x,y)=\\frac 1N\\Sigma_{i=1}^N\\frac{L_i(x,y,\\theta)}{\\frac{1}{2\\pi}} =\\frac {2\\pi}N\\Sigma_{i=1}^N{L_i(x,y,\\theta)} \\tag {2} $$ &emsp;&emsp;因此一个随机均匀采样的蒙特卡洛积分如下所示，此外在这里我们不考虑实际单位，所以实现时把系数$2\\pi$去掉了。 1234567891011float Renderer::sample(float x, float y)&#123; float sum = 0.0f; for (int i = 0; i &lt; m_samples; ++i) &#123; // randome sampling. float angle = M_2PI * rand() / RAND_MAX; sum += trace(x, y, cosf(angle), sinf(angle)); &#125; return sum / m_samples;&#125; &emsp;&emsp;紧接着我们将实现trace函数，这个函数返回给定方向上的光照值。我们采用光线步进法（Ray Marching），场景中的物体以符号距离场（Signed Distance Field，简称SDF）表示，符号距离场是一个这样的映射：$\\phi:R^2\\to R$，具有如下的性质： &emsp;&emsp;(1).当$\\phi(x)>0$时，坐标$x$位于场景的物体外面，$x$到最近物体表面的距离为$\\phi(x)$； &emsp;&emsp;(2).当$\\phi(x)","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"2D Rendering","slug":"2D-Rendering","permalink":"https://yangwc.com/categories/2D-Rendering/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"2D Rendering","slug":"2D-Rendering","permalink":"https://yangwc.com/tags/2D-Rendering/"}]},{"title":"实时渲染Real-time Rendering：Image Based Lighting","slug":"ImageBasedLighting","date":"2019-07-21T05:05:21.069Z","updated":"2019-10-20T09:05:23.359Z","comments":true,"path":"2019/07/21/ImageBasedLighting/","link":"","permalink":"https://yangwc.com/2019/07/21/ImageBasedLighting/","excerpt":"本文在前一篇PBR渲染器的基础上，进一步深入理解并实现了Image Based Lighting的全局光照。Image Based Lighting预先烘培辐照度贴图、构建BRDF积分查找表，然后在实时渲染中直接查询查找表快速计算渲染方程，实现的效果高效且真实。这种技术已经集成在虚幻4引擎中。","text":"本文在前一篇PBR渲染器的基础上，进一步深入理解并实现了Image Based Lighting的全局光照。Image Based Lighting预先烘培辐照度贴图、构建BRDF积分查找表，然后在实时渲染中直接查询查找表快速计算渲染方程，实现的效果高效且真实。这种技术已经集成在虚幻4引擎中。 渲染方程的求解 hdr文件转成cubemap 预计算漫反射积分 预计算镜面反射积分 计算渲染方程 实现效果 参考资料 Image Based Lighting &emsp;&emsp;在前面一篇基于物理的渲染中我们仅仅考虑了直接光照部分，仅仅考虑直接光照时求解渲染方程非常简单，只需需根据给定的光源直接计算并叠加，而不需要在法线轴向的半球方向做积分。Image Based Lighting（以下简称IBL）翻译过来的意思就是基于图像的光照，这种技术专门计算来自于周围环境的间接光照，周围环境的光影信息存储在一张环境贴图中，这个环境贴图可以是预先生成好的，也可以是运行时动态生成好的。IBL技术使得物体的光照效果与周围的环境更加融合，大大增强沉浸感。 一、渲染方程的求解&emsp;&emsp;IBL的渲染方程与PBR一样，即特化的渲染方程——反射方程，如下所示： L_o(p,\\omega_o)=\\int_{\\Omega}(k_d\\frac{c}{\\pi}+\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\tag {1}&emsp;&emsp;我们实现基于物理的光照效果的本质就是要计算上面的渲染方程，它是对半球方向$\\Omega$的所有$\\omega_i$积分。在前面一偏PBR中我们求解这个积分方程非常容易，因为仅考虑直接光照且光源为非面积光源时，上述的被积函数部分是一个狄拉克函数，即除了在光源方向上函数值不为$0$，在半球内的其余定义域该被积函数均为$0$，因此没有必要求半球积分，或者说半球积分的值就等于在光源方向上的被积函数值。但是在IBL中，就没有这么简单了。我们把环境贴图中的每一个像素都当作一个光源，这使得我们必须要求解半球方向的积分，即在整个半球方向采样光照信息。如果我们直接暴力求解渲染方程$(1)$的话，在每一个片元着色器上我们都要计算渲染方程的半球积分，耗费极大的性能，这对于实时性应用来说几乎不可能。 &emsp;&emsp;因此，为了高效地求解渲染积分方程，IBL方法将渲染方程中的积分项预先计算出来并存储到指定的纹理图片上，在后面进行光照计算的时候根据相关的信息索引预先计算好的积分值，从而快速高效地求解积分。这其实就是以空间换时间的思想。为了方便预先计算积分值，我们把公式$(1)$写成如下的形式： L_o(p,\\omega_o) =\\int_{\\Omega}(k_d\\frac{c}{\\pi}+\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i\\\\ =\\int_{\\Omega}k_d\\frac{c}{\\pi}L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i +\\int_{\\Omega}(\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\tag {2}&emsp;&emsp;上面公式$(2)$的两个项分别对应光照的漫反射部分和镜面反射部分，我们把这两部分分开来，然后分别单独计算漫反射的积分值、镜面反射的积分值，这个其实相当于对立方体环境贴图做一个特殊的卷积操作，计算得到的结果分别存储到各自指定查找表中供后续的渲染使用。这个就是IBL算法的核心思想，接下来我们就围绕这个核心思想展开相应的实现细节。 二、hdr文件转成cubemap&emsp;&emsp;在实现积分预计算之前我们还有一件事要完成，就是获取周围环境贴图的辐射率值，因为我们把环境贴图的每一个像素都当作一个光源。在进行积分计算时，我们要根据给定的入射方向$\\omega_i$去获取该方向上辐射过来的辐射率值。如果环境贴图是一个立方体贴图的话，这很容易实现，因为立方体贴图就是根据三维向量进行索引的。但是在IBL中我们的环境贴图并不是一个普通的环境贴图，普通的环境贴图的像素值在低动态范围（Low Dynamic Range），即每个像素的分量不超过1.0。IBL同样是PBR渲染方法，为了能够捕捉真实的物理光影和细节，我们的环境贴图应该是高动态范围的（High Dynamic Range）。 &emsp;&emsp;目前有一种以.hdr为后缀的图像格式保存了高动态范围的像素值，这种格式的文件采用一种巧妙的方法将超出$0.0$到$1.0$的正确的颜色值保存下来。高动态范围的环境贴图就采用了这种格式保存，stb_image支持.hdr文件加载。这里收集了一些hdr格式的环境贴图，stb_image加载.hdr文件也非常简单，如下所示： 1float *data = stbi_loadf(path.c_str(), &amp;m_width, &amp;m_height, &amp;m_channel, 0); &emsp;&emsp;加载后得到的图片如下图1所示，这是一个二维的图片，类似于广角镜头拍摄得到的画面。.hdr格式保存的不是六张独立的图片，它将6张图片从球面投影到一个二维的平面上，从而构成下面这张略带扭曲的矩形纹理图。为了后续方便获取指定像素的纹理值，我们需要把这张矩形的hdr图转换成立方体贴图。 图1 .hdr贴图 &emsp;&emsp;从hdr转换到cubemap的过程其实就是利用球面投影到二维平面，可以理解为球面的纹理映射为二维的纹理（或者说球面上的点映射到二维平面上的点）。这个映射过程就是利用了球坐标和笛卡尔坐标的关系，对于一个球心在原点的单位球体上的点$(x,y,z)$，我们也可以用采用天顶角$\\theta$（与$xz$平面的夹角）和方位角$\\phi$（在$xz$平面上与$x$轴的夹角）来唯一地表示，他们的关系如下： x=cos(\\theta)cos(\\phi)\\\\ y=sin(\\theta) \\\\ z=cos(\\theta)sin(\\phi) \\tag {3}&emsp;&emsp;其中$\\theta$的取值范围为$[-\\pi/2,+\\pi/2]$，$\\phi$的取值范围为$[-\\pi,+\\pi]$，我们很容易地可以分别将其映射到二维纹理坐标uv的$[0,1]$，这样我们就将一个三维立方体贴图纹理坐标映射到了二维的纹理坐标，这个其实就是构建.hdr时的映射过程。现在我们要再现这个过程，获取三维纹理坐标对应的二维纹理坐标，然后索引上面的hdr贴图，从而得到三维纹理坐标对应的像素值，并将其保存到cubemap当中。我们采用的方法就是绘制6次立方体，每次保存1个立方体面的像素值： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051void IBLAuxiliary::convertToCubemap(int width, int height, unsigned int hdrTexIndex, unsigned int cuebmapTexIndex)&#123; // manager. TextureMgr::ptr texMgr = TextureMgr::getSingleton(); ShaderMgr::ptr shaderMgr = ShaderMgr::getSingleton(); // load shader. unsigned int shaderIndex = shaderMgr-&gt;loadShader(\"convertToCubemap\", \"./glsl/convertToCubemap.vert\", \"./glsl/convertToCubemap.frag\"); // load cube mesh. Mesh::ptr cubeMesh = std::shared_ptr&lt;Mesh&gt;(new Cube(1.0f, 1.0f, 1.0f)); // load framebuffer. FrameBuffer::ptr framebuffer = std::shared_ptr&lt;FrameBuffer&gt;(new FrameBuffer(width, height, \"convertDepth\", &#123;&#125;, true)); // projection matrix and view matrix. glm::mat4 captureProjectMatrix = glm::perspective(glm::radians(90.0f), 1.0f, 0.1f, 10.0f); glm::mat4 captureViewMatrix[] = &#123; glm::lookAt(glm::vec3(0.0f), glm::vec3(+1.0f, 0.0f, 0.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(-1.0f, 0.0f, 0.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f,+1.0f, 0.0f), glm::vec3(0.0f, 0.0f,+1.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f,-1.0f, 0.0f), glm::vec3(0.0f, 0.0f,-1.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f, 0.0f,+1.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f, 0.0f,-1.0f), glm::vec3(0.0f, -1.0f, 0.0f)), &#125;; // convert. framebuffer-&gt;bind(); glDisable(GL_BLEND); glDisable(GL_CULL_FACE); glEnable(GL_DEPTH_TEST); glDepthFunc(GL_LEQUAL); glClearColor(0.0f, 1.0f, 0.0f, 1.0f); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); GLuint cubemapId = texMgr-&gt;getTexture(cuebmapTexIndex)-&gt;getTextureId(); Shader::ptr shader = shaderMgr-&gt;getShader(shaderIndex); shader-&gt;bind(); shader-&gt;setInt(\"hdrMap\", 0); shader-&gt;setMat4(\"projectMatrix\", captureProjectMatrix); texMgr-&gt;bindTexture(hdrTexIndex, 0); for (unsigned int i = 0; i &lt; 6; ++i) &#123; shader-&gt;setMat4(\"viewMatrix\", captureViewMatrix[i]); glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_CUBE_MAP_POSITIVE_X + i, cubemapId, 0); cubeMesh-&gt;draw(false, 0); &#125; shader-&gt;unBind(); texMgr-&gt;unBindTexture(hdrTexIndex); framebuffer-&gt;unBind();&#125; &emsp;&emsp;相应的，在着色器中我们将三维纹理坐标投影到二维纹理坐标，索引hdr值，输出为片元着色器的值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344=======================Vertex Shader=======================#version 330 corelayout (location = 0) in vec3 position;layout (location = 1) in vec3 normal;layout (location = 2) in vec2 texcoord;layout (location = 3) in vec3 color;out vec3 worldPos;uniform mat4 viewMatrix;uniform mat4 projectMatrix;void main()&#123; worldPos = position; gl_Position = projectMatrix * viewMatrix * vec4(position,1.0f);&#125;=======================Fragment Shader=====================#version 330 corein vec3 worldPos;out vec4 fragColor;uniform sampler2D hdrMap;// (1/(pi/2), 1/(pi))const vec2 invAtan = vec2(0.1591, 0.3183);vec2 sampleSphericalMap(vec3 v)&#123; vec2 uv = vec2(atan(v.z, v.x), asin(v.y)); // to [0,1]. uv *= invAtan; uv += vec2(0.5f); return uv;&#125;void main()&#123; // map 3d texcoord to 2d texcoord. vec2 uv = sampleSphericalMap(normalize(worldPos)); // sample hdr map. vec3 sampler = texture(hdrMap, uv).rgb; // save to one face of cubemap. fragColor = vec4(sampler, 1.0f);&#125; &emsp;&emsp;通过上面的步骤，我们就将一个二维的hdr贴图转换成cubemap，如下图2所示，方便我们后续的使用。有了这个高动态范围的环境贴图，接下来我们就展开漫反射积分和镜面反射积分的预计算。 图2 hdr立方体贴图 三、预计算漫反射积分&emsp;&emsp;首先来看渲染方程中的漫反射积分部分，把它单独拎出来： \\int_{\\Omega}k_d\\frac{c}{\\pi}L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i =k_d\\frac{c}{\\pi}\\int_{\\Omega}L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\tag {4}&emsp;&emsp;上面的积分公式中，积分变量为$\\omega_i$，和$\\frac{c}{\\pi}$项都与$\\omega_i$无关，故可以提出积分外。我们实际上要求积分的被积函数是$L_i(p,\\omega_i)n\\cdot \\omega_i$。假设点$p$在原点，则对于每一个$n$确定的半球方向，积分值仅仅取决于$\\omega_i$。因此，我们遍历所有的$n$，然后预先计算其积分值$\\int_{\\Omega}L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i$到一张cubemap当中，最后渲染时直接用$n$去采样这个cubemap得到预先计算好的积分值。这个就是预计算漫反射积分的思路。 &emsp;&emsp;$n$就是给定法线向量，它的取值范围就是所有方向，相当于一个球心在原点的单位球体上的所有点。为了遍历所有的$n$，我们送入一个立方体进行预计算的渲染，不需要球体，因为立方体的顶点归一化normalize之后也就是对应的球面上的点。然后在片元着色器根据这个$n$进行半球方向的积分，最后存储到cubemap当中。同样的，我们需要绘制6次，每次绘制保存到cubemap的六个面中的一面。 &emsp;&emsp;然后就是关于公式$(4)$当中的积分数值计算。首先需要注意的是公式$(4)$当中的积分变量$\\omega_i$是立体角，为了方便计算，我们需要把他转成以天顶角$\\theta$和方位角$\\phi$为变量的表示形式。前面在光线追踪器Ray Tracer：进阶篇已经提到过微分立体角$d\\omega_i$如何转成用$d\\theta$和$d\\phi$表示，即$d\\omega_i=sin\\theta d\\theta d\\phi$。因此，公式$(4)$中的积分项转成如下的二重积分： k_d\\frac{c}{\\pi}\\int_{\\Omega}L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i = k_d\\frac{c}{\\pi}\\int_{\\phi=0}^{2\\pi}\\int_{\\theta=0}^{\\frac12\\pi}L_i(p,\\phi_i,\\theta_i)cos\\theta sin \\theta d\\theta d\\phi \\tag {5}&emsp;&emsp;这里要提一下，公式$(4)$中的$n\\cdot \\omega_i$就是$cos\\theta$，半球方向内$\\theta$取值$[0,\\frac12\\pi]$，$\\phi$取值$[0,2\\pi]$。公式$(5)$等号两边完全是等价的。然后我们需要采用数值方法近似计算公式$(5)$的积分值，在半球方向做均匀的离散采样$(\\phi,\\theta)$，计算采样的方向对应的被积函数值，最后叠加起来，叠加的结果就是黎曼积分和。公式$(5)$的黎曼积分近似公式为： k_d\\frac{c}{\\pi}\\int_{\\phi=0}^{2\\pi}\\int_{\\theta=0}^{\\frac12\\pi}L_i(p,\\phi_i,\\theta_i)cos\\theta sin \\theta d\\theta d\\phi \\approx k_d\\frac{c}{\\pi}\\Sigma_0^{2\\pi}\\Sigma_0^{\\frac12\\pi}L_i(p, \\phi_i, \\theta_i)cos\\theta sin\\theta d\\theta d\\phi \\tag{6}&emsp;&emsp;对于公式$(6)$中的黎曼积分，我们需要确定$d\\theta$和$d\\phi$，即数值积分步长。这个步长越小，则计算结果越接近于理论值，但耗费的时间也越多。此外还需要提的是，我们是在切线空间的半球方向进行采样的，在切线空间获取采样的方向向量之后，我们需要把它转换到世界空间，这里我们没有构建变换矩阵，直接计算切线空间的三个基向量。计算近似的黎曼积分的着色器代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152============================Vertex Shader=======================#version 330 corelayout (location = 0) in vec3 position;layout (location = 1) in vec3 normal;layout (location = 2) in vec2 texcoord;layout (location = 3) in vec3 color;out vec3 worldPos;uniform mat4 viewMatrix;uniform mat4 projectMatrix;void main()&#123; worldPos = position; gl_Position = projectMatrix * viewMatrix * vec4(position,1.0f);&#125; ============================Fragment Shader=======================#version 330 corein vec3 worldPos;out vec4 fragColor;uniform samplerCube environmentMap;const float PI = 3.14159265359;void main()&#123; vec3 normal = normalize(worldPos); vec3 irradiance = vec3(0.0f); // tangent space. vec3 up = vec3(0.0f, 1.0f, 0.0f); vec3 right = cross(up, normal); up = cross(normal, right); float sampleDelta = 0.025; float sampleDeltaSquared = sampleDelta * sampleDelta; for(float phi = 0.0f; phi &lt; 2.0 * PI;phi += sampleDelta) &#123; for(float theta = 0.0f; theta &lt; 0.5 * PI;theta += sampleDelta) &#123; vec3 sampleDir = vec3(sin(theta) * cos(phi), sin(theta) * sin(phi), cos(theta)); // tangent space to world space. sampleDir = sampleDir.x * right + sampleDir.y * up + sampleDir.z * normal; irradiance += texture(environmentMap, sampleDir).rgb * cos(theta) * sin(theta) * sampleDeltaSquared; ++ nSamples; &#125; &#125; irradiance = (1.0f / PI) * irradiance ; fragColor = vec4(irradiance, 1.0f);&#125; &emsp;&emsp;在片元着色器中，我们取采样步长$d\\phi$和$d\\theta$均为$0.025$，然后是两重循环，获取采样方向向量的$(\\phi,\\theta)$后我们需要把它转换成笛卡尔坐标下的方向向量，用这个方向相应去获取该方向对应的hdr立方体纹理。最后结果除以一个$\\pi$，对应公式$(6)$中的$\\pi$，而剩下的漫反射因子$k_d$和反照率$c$由渲染时确定。上面的过程相当于对hdr立方体贴图做一个卷积操作，存储结果的纹理我们称之为辐照度纹理（Irradiance Map）。由于辐照度纹理没有高频的细节，因此我们不需要设置太大分辨率，在cpu端创建一个$64\\times 64$大小的cubemap，然后调用着色器绘制6次写入这个cubemap当中，具体如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950void IBLAuxiliary::convoluteDiffuseIntegral(int width, int height, unsigned int cubemapTexIndex, unsigned int irradianceTexIndex)&#123; // manager. TextureMgr::ptr texMgr = TextureMgr::getSingleton(); ShaderMgr::ptr shaderMgr = ShaderMgr::getSingleton(); // load shader. unsigned int shaderIndex = shaderMgr-&gt;loadShader(\"diffuseIntegral\", \"./glsl/diffuseIntegral.vert\", \"./glsl/diffuseIntegral.frag\"); // load cube mesh. Mesh::ptr cubeMesh = std::shared_ptr&lt;Mesh&gt;(new Cube(1.0f, 1.0f, 1.0f)); // load framebuffer. FrameBuffer::ptr framebuffer = std::shared_ptr&lt;FrameBuffer&gt;( new FrameBuffer(width, height, \"irradianceDepth\", &#123;&#125;, true)); // projection matrix and view matrix. glm::mat4 captureProjectMatrix = glm::perspective(glm::radians(90.0f), 1.0f, 0.1f, 10.0f); glm::mat4 captureViewMatrix[] = &#123; glm::lookAt(glm::vec3(0.0f), glm::vec3(+1.0f, 0.0f, 0.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(-1.0f, 0.0f, 0.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f,+1.0f, 0.0f), glm::vec3(0.0f, 0.0f,+1.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f,-1.0f, 0.0f), glm::vec3(0.0f, 0.0f,-1.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f, 0.0f,+1.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f, 0.0f,-1.0f), glm::vec3(0.0f, -1.0f, 0.0f)), &#125;; // begin to convolute. framebuffer-&gt;bind(); glDisable(GL_BLEND); glDisable(GL_CULL_FACE); glEnable(GL_DEPTH_TEST); glDepthFunc(GL_LEQUAL); glClearColor(0.0f, 1.0f, 0.0f, 1.0f); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); GLuint irradianceTexId = texMgr-&gt;getTexture(irradianceTexIndex)-&gt;getTextureId(); Shader::ptr shader = shaderMgr-&gt;getShader(shaderIndex); shader-&gt;bind(); shader-&gt;setInt(\"environmentMap\", 0); shader-&gt;setMat4(\"projectMatrix\", captureProjectMatrix); texMgr-&gt;bindTexture(cubemapTexIndex, 0); for (unsigned int i = 0; i &lt; 6; ++i) &#123; shader-&gt;setMat4(\"viewMatrix\", captureViewMatrix[i]); glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_CUBE_MAP_POSITIVE_X + i, irradianceTexId, 0); cubeMesh-&gt;draw(false, 0); &#125; shader-&gt;unBind(); texMgr-&gt;unBindTexture(cubemapTexIndex); framebuffer-&gt;unBind();&#125; &emsp;&emsp;以上的预计算或者说预烘培步骤只需执行一次，然后我们就得到了一张辐照度立方体贴图，如下图3所示，可以看到类似对原来的立方体贴图做了一个模糊处理，但这并不是模糊处理。 图3 预烘培得到的辐照度纹理 四、预计算镜面反射积分&emsp;&emsp;接下来我们就把目标转到镜面反射的积分预计算上面。我们要预计算的就是渲染方程$(2)$中的镜面反射部分，如下所示： \\int_{\\Omega}(\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i =\\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\tag {7}&emsp;&emsp;被积函数中的$f_r(p,\\omega_i,\\omega_o)$是之前提到的brdf函数，它与积分变量$\\omega_i$有关，因此它不是一个常量，我们不能像前面的漫反射积分那样把brdf函数当作常数项并提出积分外。除此之外，我们还注意到最终的积分值还取决于出射方向或者说观察方向$\\omega_o$，因此公式$(7)$的积分值取决于$n$和$\\omega_o$这两个三维的向量。前面的漫反射积分仅取决于$n$，所以我们可以很容易地遍历所有的$n$，然后将积分值存储到立方体贴图中。但是在镜面反射积分这里，积分值取决于$n$和$\\omega_o$，这使得问题变得复杂起来，这是因为我们不能同时用$n$和$\\omega_o$去索引立方体贴图。Epic Games公司提出了提出了一种近似的方案，这种方法就是将公式$(7)$中的积分分成两部分： \\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\approx \\int_{\\Omega}L_i(p,\\omega_i)d\\omega_i * \\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)n\\cdot \\omega_i d\\omega_i \\tag {8}&emsp;&emsp;其中$\\int_{\\Omega}L_i(p,\\omega_i)d\\omega_i$是对半球方向的辐射率进行积分，$\\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)n\\cdot\\omega_id\\omega_i$则对半球方向的brdf函数进行积分，我们将原来的积分分成了这两部分，预计算然后保存到两张纹理当中。可以看到前面一部分的积分仅仅取决于法线向量$n$，后面一部分表面上取决于$n$和$\\omega_o$，后面深入了解之后就会发现不用$n$和$\\omega_o$去索引brdf函数的积分值。 1、Pre-Filtered Environment Map&emsp;&emsp;首先我们来看前面一部分的积分值，即$\\int_{\\Omega}L_i(p,\\omega_i)d\\omega_i$，这部分预计算得到的纹理为预过滤的环境贴图，它返回的是反射的像素值。我们知道，给定一个物体表面，若其表面的粗糙程度越高，则反射的内容越模糊。因此，为了把物体的粗糙度考虑进去，我们将考虑物体的粗糙度划分成几个等级，每个等级预计算一遍积分，并存储到cubemap的一个mipmap当中。我们为生成的预过滤环境贴图构建mipmap，越粗糙则存储到mipmap的等级越高，正如如下图4所示： 图4 不同粗糙度对应的mipmap &emsp;&emsp;表面越粗糙则反射得越模糊得现象其实涉及到一个反射波瓣的问题，所以的反射波瓣就是反射光线的分布范围。如下图5所示，对于一个光滑的完美镜面，其反射波瓣就是反射向量，越粗糙则反射波瓣越大，且基本上都是以反射向量为中心。 图5 不同粗糙度下的反射波瓣 &emsp;&emsp;因此，当我们进行积分采样时，我们没有必要再在半球方向做一个均匀的采样，因为超出反射波瓣的采样是无效的。我们应该偏向反射波瓣采样，这里就涉及到了一个重要性采样的问题，重要性采样的内容在前面的光线追踪器Ray Tracer：进阶篇已经详细地展开过，这里就不再赘述。采用了重要性采样方法，我们求解积分方程的数值方法不再是黎曼积分法，而是蒙特卡洛积分法，蒙特卡洛积分同样在光线追踪渲染器中提及过。 &emsp;&emsp;为了加速蒙特卡洛积分方法的收敛速度，Epic Games公司提出使用超均匀分布序列（Low-discrepancy Sequence）——Hammersley序列。相对于普通的伪随机数，Hammersley序列的随机数分布更加均匀，如下图6所示，将其应用到蒙特卡洛采样能够提升收敛速度。 图6 伪随机数vs超均匀分布随机数 &emsp;&emsp;我们将使用的是二维的Hammersley序列。一个二维的Hammersley序列$H_N=\\{x_1,…,x_N\\}, N\\geq 1$是散落分布在单位正方形内的点集，其数学定义为： H_N=\\{x_i= (\\begin{matrix} i/N\\\\ \\Phi_2(i) \\end{matrix} ),\\ \\ for\\ \\ i=0,...,N-1 \\} \\tag {9}&emsp;&emsp;其中$\\Phi_2(i)$是Van der Corput序列，它输入$i$，然后将$i$的二进制编码以小数点为对称做一个镜像操作，返回$[0,1)$的浮点数，其数学定义为： \\Phi_2(i)=\\frac{a_0}{2}+\\frac{a_1}{2^2}+...+\\frac{a_r}{2^{r+1}} \\tag {10}&emsp;&emsp;其中的$a_0a_1…a_n$是$i$的二进制编码每一位二进制位，即$i=a_0+a_1\\cdot 2+a_2\\cdot 2^2+a_3\\cdot 2^3+…+a_r\\cdot 2^r$。然后我们需要将这个二维的序列转换成我们对半球方向的三维采样，同样我们利用球面坐标和笛卡尔坐标之间的关系，首先将Hammersley序列$x_i=(u,v)^T\\in H_N$映射到$(\\phi,\\theta)$，然后转换成笛卡尔坐标下的向量形式。一个均匀映射和一个余弦映射公式如下： Uniform\\ \\ mapping= \\{\\begin{matrix} \\theta=cos^{-1}(1-u)\\\\ \\phi=2\\pi v \\end{matrix} \\\\ Cosinus\\ \\ mapping= \\{\\begin{matrix} \\theta=cos^{-1}(\\sqrt{(1-u)})\\\\ \\phi=2\\pi v \\end{matrix} \\tag {11}&emsp;&emsp;均匀映射就是将序列映射到一个均匀的分布，余弦映射则将序列映射到一个更偏向于半球中心轴上的分布。两者的区别如下图7所示。Hammersley序列可以通过位移操作快速地实现，具体代码见下面。 图7 均匀映射vs余弦映射 1234567891011121314float radicalInverseVdc(uint bits) &#123; bits = (bits &lt;&lt; 16u) | (bits &gt;&gt; 16u); bits = ((bits &amp; 0x55555555u) &lt;&lt; 1u) | ((bits &amp; 0xAAAAAAAAu) &gt;&gt; 1u); bits = ((bits &amp; 0x33333333u) &lt;&lt; 2u) | ((bits &amp; 0xCCCCCCCCu) &gt;&gt; 2u); bits = ((bits &amp; 0x0F0F0F0Fu) &lt;&lt; 4u) | ((bits &amp; 0xF0F0F0F0u) &gt;&gt; 4u); bits = ((bits &amp; 0x00FF00FFu) &lt;&lt; 8u) | ((bits &amp; 0xFF00FF00u) &gt;&gt; 8u); return float(bits) * 2.3283064365386963e-10; // / 0x100000000&#125;vec2 hammersley(uint i, uint N)&#123; return vec2(float(i) / float(N), radicalInverseVdc(i));&#125; &emsp;&emsp;紧接着我们要用Hammersley序列实现我们的重要性采样。前面已经提到过，我们将考虑物体的粗糙度，因为不同粗糙度下的反射波瓣大小不同。我们将粗糙度换分成5个等级，每个等级根据当前的粗糙度进行重要性采样。因此做重要性采样时我们需要根据粗糙度确定当前的反射波瓣大小，反射波瓣越大则采样范围越大。我们将结合之前PBR提到的法线分布函数，法线分布函数给定一个法线向量，它返回微平面法线与给定法线朝向一致的分布概率。Trowbridge-Reitz GGX法线分布函数的数学定义为： NDF_{GGXTR}(n,h,\\alpha)=\\frac{\\alpha^2}{\\pi((n\\cdot h)^2(\\alpha^2-1)+1)^2} \\tag {12}&emsp;&emsp;我们将法线分布函数与余弦映射结合起来做重要性采样： Important\\ \\ sampling= \\{\\begin{matrix} \\theta=cos^{-1}(\\sqrt{\\frac{1-u}{u(\\alpha^2-1)+1}})\\\\ \\phi=2\\pi v \\end{matrix} \\tag {13}&emsp;&emsp;同样这也是Epic Games公司提出的映射方法，注意到与公式$(11)$中的余弦映射相比，公式$(13)$多了一个分母$u(\\alpha^2-1)+1$取自公式$(12)$中的法线分布函数。当粗糙度$\\alpha$增大时，余弦值减小，$\\theta$取值范围越大，反射波瓣也就越大，这个就是公式$(13)$的大体思路。对于给定的Hammersley序列、法线向量N、粗糙度roughness，一个重要性采样代码如下所示： 12345678910111213141516171819vec3 importanceSampleGGX(vec2 Xi, vec3 N, float roughness)&#123; float a = roughness * roughness; float phi = 2.0 * PI * Xi.x; float cosTheta = sqrt((1.0 - Xi.y) / (1.0 + (a * a - 1.0) * Xi.y)); float sinTheta = sqrt(1.0 - cosTheta * cosTheta); vec3 H; H.x = cos(phi) * sinTheta; H.y = sin(phi) * sinTheta; H.z = cosTheta; // from tangent space to world space. vec3 up = abs(N.z) &lt; 0.999 ? vec3(0.0, 0.0, 1.0) : vec3(1.0, 0.0, 0.0); vec3 tangent = normalize(cross(up, N)); vec3 bitangent = cross(N, tangent); vec3 sampleVec = H.x * tangent + H.y * bitangent + H.z * N; return normalize(sampleVec);&#125; &emsp;&emsp;最后我们就利用重要性采样进行数值积分的计算，如下所示： \\int_{\\Omega}L_i(p,\\omega_i)d\\omega_i \\approx \\frac1N\\Sigma_{k=1}^NL_i(l_k) \\tag {14}&emsp;&emsp;然后在实现时，Ephic Games公司发现再乘上一个权重$cos\\theta_{l_k}$效果更加，因而实现的积分计算代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940#version 330 corein vec3 worldPos;out vec4 fragColor;uniform float roughness;uniform samplerCube environmentMap;const float PI = 3.14159265359;float radicalInverseVdc(uint bits);vec2 hammersley(uint i, uint N);vec3 importanceSampleGGX(vec2 Xi, vec3 N, float roughness);void main()&#123; vec3 N = normalize(worldPos); vec3 V = N; const uint sampleCount = 1024u; float totalWeight = 0.0f; vec3 prefilteredColor = vec3(0.0f); for(uint i = 0u;i &lt; sampleCount;++ i) &#123; vec2 Xi = hammersley(i, sampleCount); // sample halfway vector. vec3 H = importanceSampleGGX(Xi, N, roughness); // reflect vector. vec3 L = normalize(2.0 * dot(V, H) * H - V); float NdotL = max(dot(N, L), 0.0); if(NdotL &gt; 0.0f); &#123; prefilteredColor += texture(environmentMap, L).rgb * NdotL; totalWeight += NdotL; &#125; &#125; prefilteredColor = prefilteredColor / totalWeight; fragColor = vec4(prefilteredColor, 1.0f);&#125; &emsp;&emsp;在cpu端生成多个mipmap层次的cubemap，对于每一个粗糙度等级，我们将预计算的结果存储到对应mipmap等级的cubemap纹理当中，最后我们就得到如图4所示的多个mipmap等级的Pre-Filtered Environment Map。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162void IBLAuxiliary::convoluteSpecularIntegral(int width, int height, unsigned int cubemapTexIndex, unsigned int prefilteredTexIndex)&#123; // manager. TextureMgr::ptr texMgr = TextureMgr::getSingleton(); ShaderMgr::ptr shaderMgr = ShaderMgr::getSingleton(); // load shader. unsigned int shaderIndex = shaderMgr-&gt;loadShader(\"prefilterEnvMap\", \"./glsl/prefilterEnvMap.vert\", \"./glsl/prefilterEnvMap.frag\"); // load cube mesh. Mesh::ptr cubeMesh = std::shared_ptr&lt;Mesh&gt;(new Cube(1.0f, 1.0f, 1.0f)); // projection matrix and view matrix. glm::mat4 captureProjectMatrix = glm::perspective(glm::radians(90.0f), 1.0f, 0.1f, 10.0f); glm::mat4 captureViewMatrix[] = &#123; glm::lookAt(glm::vec3(0.0f), glm::vec3(+1.0f, 0.0f, 0.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(-1.0f, 0.0f, 0.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f,+1.0f, 0.0f), glm::vec3(0.0f, 0.0f,+1.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f,-1.0f, 0.0f), glm::vec3(0.0f, 0.0f,-1.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f, 0.0f,+1.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f, 0.0f,-1.0f), glm::vec3(0.0f, -1.0f, 0.0f)), &#125;; // begin to filter. GLuint prefilteredTexId = texMgr-&gt;getTexture(prefilteredTexIndex)-&gt;getTextureId(); Shader::ptr shader = shaderMgr-&gt;getShader(shaderIndex); shader-&gt;bind(); shader-&gt;setInt(\"environmentMap\", 0); shader-&gt;setMat4(\"projectMatrix\", captureProjectMatrix); texMgr-&gt;bindTexture(cubemapTexIndex, 0); unsigned int maxMipLevels = 5; for (unsigned int mip = 0; mip &lt; maxMipLevels; ++mip) &#123; unsigned int mipWidth = width * std::pow(0.5, mip); unsigned int mipHeight = height * std::pow(0.5, mip); std::stringstream ss; ss &lt;&lt; mip; FrameBuffer::ptr framebuffer = std::shared_ptr&lt;FrameBuffer&gt;( new FrameBuffer(mipWidth, mipHeight, \"prefilteredDepth\" + ss.str(), &#123;&#125;, true)); framebuffer-&gt;bind(); glDisable(GL_BLEND); glDisable(GL_CULL_FACE); glEnable(GL_DEPTH_TEST); glEnable(GL_TEXTURE_CUBE_MAP_SEAMLESS); glDepthFunc(GL_LEQUAL); glClearColor(0.0f, 1.0f, 0.0f, 1.0f); float roughness = (float)mip / (float)(maxMipLevels - 1); shader-&gt;setFloat(\"roughness\", roughness); for (unsigned int i = 0; i &lt; 6; ++i) &#123; shader-&gt;setMat4(\"viewMatrix\", captureViewMatrix[i]); glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_CUBE_MAP_POSITIVE_X + i, prefilteredTexId, mip); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); cubeMesh-&gt;draw(false, 0); &#125; framebuffer-&gt;unBind(); &#125; shader-&gt;unBind(); texMgr-&gt;unBindTexture(cubemapTexIndex);&#125; 2、Pre-computing the BRDF&emsp;&emsp;接下来我们把目标方法第二部分的积分计算，即brdf函数积分： \\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)n\\cdot \\omega_id\\omega_i \\tag {15}&emsp;&emsp;公式$(15)$的积分值取决于三个变量，即$n\\cdot \\omega_o$、表面粗糙度以及$F_0$，$F_0$是菲涅尔方程的一个输入值。三个变量太多了，为了简化且方便预计算，我们设法将一些变量提出积分符号外： \\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)n\\cdot \\omega_id\\omega_i =\\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)\\frac{F(\\omega_o,h)}{F(\\omega_o,h)}n\\cdot \\omega_id\\omega_i\\\\ =\\int_{\\Omega}\\frac{f_r(p,\\omega_i,\\omega_o)}{F(\\omega_o,h)}F(\\omega_o,h)n\\cdot \\omega_id\\omega_i \\tag{16}&emsp;&emsp;将菲涅尔项$F(\\omega_o,h)=(F_0+(1-F_0)(1-\\omega_o\\cdot h)^5)$代入上式，得： \\int_{\\Omega}\\frac{f_r(p,\\omega_i,\\omega_o)}{F(\\omega_o,h)}(F_0+(1-F_0)(1-\\omega_o\\cdot h)^5)n\\cdot \\omega_id\\omega_i \\\\ = \\int_{\\Omega}\\frac{f_r(p,\\omega_i,\\omega_o)}{F(\\omega_o,h)}(F_0(1-(1-\\omega_o\\cdot h)^5)+(1-\\omega_o\\cdot h)^5)n\\cdot \\omega_id\\omega_i \\tag {17}​ &emsp;&emsp;然后将公式$(17)$得到的结果分成两部分： \\int_{\\Omega}\\frac{f_r(p,\\omega_i,\\omega_o)}{F(\\omega_o,h)}(F_0(1-(1-\\omega_o\\cdot h)^5)+(1-\\omega_o\\cdot h)^5)n\\cdot \\omega_id\\omega_i\\\\ =F_o\\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)(1-(1-\\omega_o\\cdot h)^5)n\\cdot \\omega_id\\omega_i +\\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)(1-\\omega_o\\cdot h)^5 n\\cdot \\omega_i d\\omega_i \\tag {18}&emsp;&emsp;注意，公式$(18)$中的是去掉了菲涅尔项的brdf函数，因为菲涅尔项被消去了。现在我们把$F_0$提出积分外了，公式$(18)$中的两项积分取决于$n\\cdot \\omega_o$和粗糙度roughness。我们构建这样的一个二维查找表，它的横轴坐标取值为$n\\cdot \\omega_o$，纵轴坐标取值为粗糙度roughness，我们渲染屏幕空间大小的四边形，遍历$n\\cdot \\omega_o$和粗糙度的所有取值，计算其对应的公式$(18)$中的两项积分的结果，存储为纹理的像素值。最后渲染时使用纹理坐标$(n\\cdot \\omega_o, roughness)$去索引像素值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859float geometrySchlickGGX(float NdotV, float roughness)&#123; float a = roughness; float k = (a * a) / 2.0f; float nom = NdotV; float denom = NdotV * (1.0 - k) + k; return nom / denom;&#125;float geometrySmith(vec3 N, vec3 V, vec3 L, float roughness)&#123; float NdotV = max(dot(N, V), 0.0); float NdotL = max(dot(N, L), 0.0); float ggx2 = geometrySchlickGGX(NdotV, roughness); float ggx1 = geometrySchlickGGX(NdotL, roughness); return ggx1 * ggx2;&#125;vec2 integrateBRDF(float NdotV, float roughness)&#123; vec3 V; V.x = sqrt(1.0 - NdotV * NdotV); V.y = 0.0f; V.z = NdotV; float A = 0.0; float B = 0.0; vec3 N = vec3(0.0, 0.0, 1.0); const uint sampleCount = 1024u; for(uint i = 0u;i &lt; sampleCount;++i) &#123; vec2 Xi = hammersley(i, sampleCount); vec3 H = importanceSampleGGX(Xi, N, roughness); vec3 L = normalize(2.0 * dot(V, H) * H - V); float NdotL = max(L.z, 0.0); float NdotH = max(H.z, 0.0); float VdotH = max(dot(V, H), 0.0); if(NdotL &gt; 0.0) &#123; float G = geometrySmith(N, V, L, roughness); float G_Vis = (G * VdotH) / (NdotH * NdotV); float Fc = pow(1.0 - VdotH, 5.0); A += (1.0 - Fc) * G_Vis; B += Fc * G_Vis; &#125; &#125; A /= float(sampleCount); B /= float(sampleCount); return vec2(A, B);&#125;void main()&#123; vec2 value = integrateBRDF(Texcoord.x, Texcoord.y); fragColor = vec4(value, 0.0f, 1.0f);&#125; &emsp;&emsp;在cpu端创建一个二维纹理，并送入一个屏幕大小的四边形进行预计算的渲染。 1234567891011121314151617181920212223242526272829void IBLAuxiliary::convoluteSpecularBRDFIntegral(int width, int height, unsigned int brdfLutTexIndex)&#123; // manager. TextureMgr::ptr texMgr = TextureMgr::getSingleton(); ShaderMgr::ptr shaderMgr = ShaderMgr::getSingleton(); // load shader. unsigned int shaderIndex = shaderMgr-&gt;loadShader(\"genBrdfLUT\", \"./glsl/genBrdfLUT.vert\", \"./glsl/genBrdfLUT.frag\"); // load quad mesh. Mesh::ptr quadMesh = std::shared_ptr&lt;Mesh&gt;(new ScreenQuad()); FrameBuffer::ptr framebuffer = std::shared_ptr&lt;FrameBuffer&gt;( new FrameBuffer(width, height, \"brdfDepth\", &#123;&#125;, true)); Shader::ptr shader = shaderMgr-&gt;getShader(shaderIndex); framebuffer-&gt;bind(); glDisable(GL_BLEND); glDisable(GL_CULL_FACE); glDisable(GL_DEPTH_TEST); glDepthFunc(GL_LEQUAL); glClearColor(0.0f, 1.0f, 0.0f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); GLuint brdfLutTexId = texMgr-&gt;getTexture(brdfLutTexIndex)-&gt;getTextureId(); glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, brdfLutTexId, 0); shader-&gt;bind(); quadMesh-&gt;draw(false, 0); shader-&gt;unBind(); framebuffer-&gt;unBind();&#125; &emsp;&emsp;然后我们就可以得到下面这张纹理。 图8 brdf积分查找表 五、计算渲染方程&emsp;&emsp;在前面的步骤中我们预计算获取了Irradiance Map、Pre-Filtered Environment Map以及BRDF Lookup Texture，最后我们就直接查找这些纹理，用以我们的光照计算。光照计算部分直接就是一开始提到的渲染方程，渲染方程中的积分项从纹理中直接获取，不再需要实时计算。 12345678910111213141516// ambient lighting.vec3 ambientS = fresnelSchlickRoughness(max(dot(normal, viewDir), 0.0f), F0, roughness);vec3 ambientD = vec3(1.0f) - ambientS;ambientD *= (1.0 - metallic);vec3 irradiance = texture(irradianceMap, normal).rgb;vec3 R = normalize(reflect(-viewDir, normal));const float MAX_REFLECTION_LOD = 4.0;vec3 prefilteredColor = texture(prefilteredMap, R, roughness * MAX_REFLECTION_LOD).rgb;vec2 envBrdf = texture(brdfLutMap, vec2(max(dot(normal, viewDir), 0.0f), roughness)).rg;vec3 envSpecular = prefilteredColor * (ambientS * envBrdf.x + envBrdf.y);vec3 ambient = (albedo * irradiance * ambientD + envSpecular) * ao;fragColor.xyz = ambient + fragColor.xyz * shadow + pointLightRadiance; 六、实现效果 参考资料：$[1]$ Hammersley Points on the Hemisphere $[2]$ Real Shading in Unreal Engine 4, by Brian Karis, Epic Games $[3]$ https://learnopengl.com/PBR/IBL/Diffuse-irradiance $[4]$ https://learnopengl.com/PBR/IBL/Specular-IBL","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Real-time Rendering","slug":"Real-time-Rendering","permalink":"https://yangwc.com/categories/Real-time-Rendering/"},{"name":"Physically Based Rendering","slug":"Physically-Based-Rendering","permalink":"https://yangwc.com/categories/Physically-Based-Rendering/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Real-time Rendering","slug":"Real-time-Rendering","permalink":"https://yangwc.com/tags/Real-time-Rendering/"},{"name":"Physically Based Rendering","slug":"Physically-Based-Rendering","permalink":"https://yangwc.com/tags/Physically-Based-Rendering/"}]},{"title":"实时渲染Real-time Rendering：基于物理的光照模型","slug":"PhysicallyBasedRendering","date":"2019-07-14T09:25:05.766Z","updated":"2019-09-27T13:04:01.396Z","comments":true,"path":"2019/07/14/PhysicallyBasedRendering/","link":"","permalink":"https://yangwc.com/2019/07/14/PhysicallyBasedRendering/","excerpt":"本文采用OpenGL搭建了一个基于物理着色的渲染器，目前大多数的实时应用都是采用了PBR，相对于传统的Phong等基于经验的光照模型，基于物理着色的渲染方法更为真实。","text":"本文采用OpenGL搭建了一个基于物理着色的渲染器，目前大多数的实时应用都是采用了PBR，相对于传统的Phong等基于经验的光照模型，基于物理着色的渲染方法更为真实。 能量守恒 微平面模型 基于物理的BRDF 屏幕空间环境光遮蔽 实现效果 参考资料 基于物理的渲染 &emsp;&emsp;基于物理的渲染（Physically Based Rendering，简称PBR）技术致力于渲染出更贴近于真实物理世界的光影效果，它倾向于探索光影背后的物理规律，然后在此基础上构建一个基于物理规律的光照模型，最后应用到光照计算中。基于物理的渲染除了更为真实，它也给光照计算的赋予了更多的物理意义，从而使得设计师们摆脱基于经验的参数调整，只要设置的物理量正确，则最终光照效果也将会是正确的。即便如此，基于物理的渲染技术依然只是现实物理世界的一个逼近。对于一个基于物理渲染的光照模型，它通常需要满足以下的三个条件： &emsp;&emsp;1、能量守恒 &emsp;&emsp;2、基于微平面的表面模型 &emsp;&emsp;3、使用基于物理的BRDF &emsp;&emsp;接下来我们就按照上面的顺序一一展开。 ## 一、能量守恒（Energy Conservation） &emsp;&emsp;基于物理的光照模型必须遵守这样的一个能量守恒原则：对于一个非自发光的物体，出射光线的能量永远不能超过入射光线的能量。当一束光线照射到物体表面，它就被分割成两个部分，分别是折射部分和反射部分。反射部分的光线则是直接撞击到表面然后反弹开来的那部分光线，这部分构成中我们日常生活中常见的镜面高光。折射部分的光线则进入物体内部，光线在内部与物体的粒子发生碰撞，此时光线的一部分能量就转变成热能。一般情况下，并非所有光能都被转化成热能，还有一些光线在内部经过多次散射最终又离开物体表面，这部分的光线构成了物体的漫反射光。这里还要特别区分一下金属材质，与非金属材质和电介质不同，金属材质会直接吸收折射光而不会散开，只表现出镜面反射光。即金属表面不会显示出漫反射的颜色。 &emsp;&emsp;根据能量守恒的原则，反射光与折射光是相互排斥的，因此我们只要知道其中一部分占总入射光的百分比，就能立马得到另外一部分的能量占比。通过这样的一个方案，我们就能保证出射光的总能量小于等于入射光的总能量。下面的图1展示Blin-Phong的光照渲染结果，Blin-Phong光照模型并不是一个基于物理的光照模型，它并不满足能量守恒的原则，可以看到，图1场景看起来太亮了，而我仅仅将光照的辐射率设置为vec3(0.6)。 图1 Blin-Phong光照模型的渲染结果 二、微平面模型（Microfacet Model）&emsp;&emsp;PBR技术采用了微平面理论：在微观尺度下，任意的一个平面都可以用一组微小的光滑镜面来描述，这个微小的光滑镜面就是微平面（Microfacet）。根据平面粗糙程度的不同，这些微平面的排列取向也各不相同。一个平面越是粗糙，则其平面上的微平面排列就越混乱。微平面的排列越混乱，则入射光线照射到该平面上时更趋向于朝向完全不同的方向散射开来，从而产生更大范围的镜面光。同理，若平面越光滑，则其微平面排列取向越规整，入射光线大体上越趋向于向同一个方向反射，产生更小范围、更加锐利的镜面高光。正如如下图2所示。 图2 粗糙和光滑的微平面 &emsp;&emsp;微平面的取向排列混乱程度我们采用一个粗糙度（Roughness）的参数来衡量。直接在微观尺度下操作显然不可行，因为我们将在宏观尺度下采用统计学的方法来估算微平面的粗糙度。这个粗糙度我们定义为某个向量的方向与微平面平均取向方向一致的概率，这个向量便是光线向量$l$和视线向量$v$之间的中间向量$h$： h=\\frac{l+v}{||l+v||} \\tag {1}&emsp;&emsp;在一个表面的微平面中，越多微平面的法线方向与中间向量的方向一致，则镜面的反射效果就越强烈、越锐利。 图3 不同粗糙度的镜面高光 三、基于物理的BRDF&emsp;&emsp;在前面的能量守恒原则和微平面理论的基础上，我们将展开基于物理的光照计算。首先我们要了解的是渲染方程，PBR采用的渲染方程是一个特化版本，也被称为反射方程，如下所示： L_o(p,\\omega_o)=\\int _{\\Omega}f_r(p,\\omega_i,\\omega_o)L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\tag {2}&emsp;&emsp;上式中，$L_o$是反射辐射率，$L_i$是入射辐射率，$p$为物体表面上的一点，$\\omega_o$为出射方向向量，$\\omega_i$为入射方向向量，$n$是表面法线向量，$f_r(p,\\omega_i,\\omega_o)$是后面我们将要提到的BRDF函数。积分区域$\\Omega$是以表面法线$n$为轴的半球领域。PBR渲染方程主要是关于光能辐射度量学的（Radiometry）的内容，这里简单介绍一些辐射度量学的物理量。 &emsp;&emsp;辐射通量（Radiant Flux）：辐射通量以瓦特为单位，符号为$\\Phi$，它衡量一个光源所辐射的能量。光是由多种不同波长的能量所集合而成的，一个光源所放射出来的能量可以被视作这个光源包含的所有各种波长的一个函数。但是在计算机图形学中，我们通常采用三原色编码即RGB来简化辐射通量的表示，这套编码带来的损失基本可以忽略。 &emsp;&emsp;立体角（Solid Angle）：这个物理量在前面的文章都有提及过了，立体角符号为$\\omega$，它描述了一个几何体投影到单位球面上的大小，立体角可以看成是带有体积的方向向量。 &emsp;&emsp;辐射强度（Radiant Intensity）：辐射强度衡量了在单位球面上，一个光源每单位立体角所辐射的辐射通量。其定义公式为$I=\\frac{d\\Phi}{d\\omega}$，即微分辐射通量除以微分立体角。对于一个全向且向所有方向均匀辐射的光源，辐射强度表示了光源在一个单位球面上单位立体角的辐射能量。 &emsp;&emsp;辐射率（Radiance）：辐射率就是具有辐射强度$\\Phi$的光源在单位面积$A$、单位立体角$\\omega$下的辐射总能量，其定义见下面的公式$(3)$。$d\\omega cos\\theta$将单位立体角（也就是单位球体上的面积）投影到法线方向。 L=\\frac{d^2\\Phi}{dAd\\omega cos\\theta} \\tag {3} 图4 辐射率示意图 &emsp;&emsp;辐射率是辐射度量学上表示一个区域平面上光线总量的物理量，它受到入射光线与平面法线间的夹角$\\theta$的余弦值$cos\\theta$的影响：当直接辐射到平面上的程度越低时，光线就越弱，而当光线完全垂直于平面时强度最高。当立体角$\\omega$和面积$A$趋向于无穷小时，我们能用辐射率来表示单束光线穿过空间中的一个点的通量。这就使我们可以计算得出作用于单个片段或点上的单束光线的辐射率，即把立体角$\\omega$转变为方向向量然后把面$A$转换为点$p$，这样我们就能直接在我们的着色器中使用辐射率来计算单束光线对每个片段的作用。 &emsp;&emsp;上面讨论的仅仅是一束光线投射到点$p$上，但是通常我们需要计算的是所有投射到点$p$上的光线总和，这个和就是辐照度（Irradiance）。注意到反射方程$(2)$对半球领域$\\Omega$进行积分，这是因为我们要计算的不只是单一一个方向上的入射光，而是一个以点$p$为球心、以法向为中轴的半球领域$\\Omega$内所有方向上的入射光。 图5 半球领域 &emsp;&emsp;由于渲染方程都没有解析解，求解公式$(2)$即反射方程时我们将采用离散的方法来积分的数值解。目前常用的就是梯形法，在半球领域$\\Omega$按一定的步长将反射率方程分散求解，然后再按照步长大小将所得的结果平均化，这个就是黎曼和（Riemann Sum）。 &emsp;&emsp;然后剩下的就是BRDF函数，也就是公式$(2)$中的$f_r(p,\\omega_i,\\omega_o)$部分。BRDF的全称为Bidirectional Reflective Distribution Function，即双向反射分布函数。对于一个给定材质属性，BRDF函数给出了入射光和反射光的关系，一束给定入射方向的入射光照射到物体表面时，会被反射到表面半球范围内的各个方向，不同反射角度的反射光线在入射光线中的占比各不相同，BRDF函数就用来表示这种比例关系，其定义如下： f(l,v)=\\frac{dL_o(v)}{dE(l)} \\tag {4}&emsp;&emsp;Cook-Torrance模型是目前应用最为广泛的基于物理的BRDF模型，它被用于很多实时渲染管线的材质和光照环境下。Cook-Torrance的BRDF包含漫反射和镜面反射两个部分，其中的镜面反射部分比较复杂： f_r=k_df_{lambert}+k_sf_{cook-torrance} \\tag {5}&emsp;&emsp;其中，$k_d$就是前面提到的入射光线中被折射的光线部分的能量占比，而$k_s$则是被反射的光线部分所占的比例。$f_{lambert}$是BRDF的漫反射部分，这个是Lambertian漫反射模型，其计算公式如下所示： f_{lambert}=\\frac{c}{\\pi} \\tag {6}&emsp;&emsp;其中$c$是物体的反照率（Albedo），大部分的实时渲染应用都采用了Lambertian漫反射模型。然后就是镜面反射部分，镜面反射部分就是Cook-Torrance的各向同性光照模型： f_{cook-torrance} = \\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)} \\tag {7}&emsp;&emsp;其中$\\omega_i$是入射方向，$\\omega_o$是观察方向。Cook-Torrance的模型包含三个函数，D、F、G分别是法线分布函数、菲尼尔方程、几何函数。接下来我们将讨论的是Trowbridge-Reitz GGX法线分布函数，Fresnel-Schlick菲涅尔方程以及Smith’s Schlick-GGX几何函数。 &emsp;&emsp;首先是法线分布函数（Normal Distribution Function），给定表面的粗糙度，法线分布函数估算平面法线取向与中间向量一致的微平面数量。从统计学上讲，法线分布函数近似地描述了与中间向量$h$取向相同的微平面占全部微平面的比例。例如，给定中间向量$h$，若我们要估算的微平面中有$35\\%$与向量$h$取向相同，那么法线分布函数将返回$0.35$。Trowbridge-Reitz GGX法线分布函数的数学定义如下所示： NDF_{GGXTR}(n,h,\\alpha)=\\frac{\\alpha ^2}{\\pi ((n\\cdot h)^2(\\alpha^2-1)+1)^2} \\tag {8}&emsp;&emsp;其中，$n$为宏观法线，$h$是中间向量，而$\\alpha$则表示平面的粗糙度。当粗糙度$\\alpha$值很低时，即表面比较光滑时，与中间向量$h$取向相同的微平面会高度地集中在一个小半径范围内。此时镜面反射会形成一个非常明亮的光斑。相反，当表面的粗糙度值较高时，与$h$向量取向一致的微平面分布在一个比较大的半径范围内，这使得最终的镜面反射效果显得较为灰暗。 &emsp;&emsp;然后就是菲涅尔方程（Fresnel Equation），描述了指定角度下表面反射的光线所占的比例。当一束光线照射到表面时，菲涅尔方程会依据观察角度给出反射光线所占的百分比。然后根据这个反射光所占的百分比和能量守恒定律就可以得出光线折射部分所占的比率。当我们垂直观察物体的时候，任何表面都有一个基础的反射率。例如，用垂直的视角看向木制桌面或者金属桌面，此时只有最基本的反射，但若近乎与平面平行的角度去观察的话就会看到非常明显的反光效果。Fresnel-Schlick近似菲涅尔方程如下所示： F_{schlick}(h,v,F_0)=F_0+(1-F_0)(1-(h\\cdot v))^5 \\tag {9}&emsp;&emsp;其中，$F_0$就是表面的基础反射率，它是通过折射系数计算得到的，$h$即前面提到的中间向量，$v$为观察方向向量。 &emsp;&emsp;最后就是几何函数（Geometry Function），几何函数描述了微平面自我遮挡的属性。当一个平面比较粗糙的时候，表面上的微平面可能会挡住其他的微平面从而减弱表面反射光的强度。与法线分布函数类似，几何函数也是从统计学的角度近似求出微平面之间相互遮蔽的比率。 图6 微平面的相互遮蔽现象 &emsp;&emsp;几何函数也采用一个材质的粗糙度作为输入的参数，越粗糙的表面其微平面之间相互遮挡的概率也就越高。Schlick-GGX几何函数的数学定义如下： G_{schlickGGX}(n,v,k)=\\frac{n\\cdot v}{(n\\cdot v)(1-k)+k} \\tag {10}&emsp;&emsp;公式$(10)$中的$k$是关于粗糙度$\\alpha$的重映射，取决于几何函数是针对直接光照还是针对IBL（Image Based Lighting）光照： k_{direct}=\\frac{(\\alpha+1)^2}{8} \\\\ k_{IBL}=\\frac{\\alpha^2}{2} \\tag {11}&emsp;&emsp;微平面的相互遮蔽主要有两个方面，分别是几何遮蔽（Geometry Obstruction）和几何阴影（Geometry Shadowing），几何遮蔽与视线向量有关，而几何阴影则于入射方向向量相关。我们采用史密斯法将两者纳入其中： G(n,v,l,k)=G_{schlickGGX}(n,v,k)G_{schlickGGX}(n,l,k) \\tag {12}&emsp;&emsp;最终，我们得到反射方程$(2)$中的BRDF计算公式： f_r=k_d\\frac{c}{\\pi}+k_s\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)} \\tag {13}&emsp;&emsp;实际上，BRDF的计算公式$(13)$有个错误，公式中的$F$即菲涅尔项就是$k_s$，因为菲涅尔项表示的就是反射光线的占比，因此应该把$k_s$去掉，然后$k_d=1-F$。 f_r=k_d\\frac{c}{\\pi}+\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)} \\tag {14}&emsp;&emsp;将公式$(14)$代入到公式$(2)$中，我们最终得到一个具体的渲染方程： L_o(p,\\omega_o)=\\int _{\\Omega}(k_d\\frac{c}{\\pi}+\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\tag {15}三、PBR渲染器的实现&emsp;&emsp;有了前面的理论基础，接下来我们就展开相关的PBR实现。首先我们要讨论的是PBR材质，仔细观察渲染方程公式$(15)$，求解这个渲染方程我们需要获取物体的反照率向量、法线向量、粗糙度。除此之外，我还需要物体的金属度参数，这是因为Fresnel-Schlick近似仅仅对电介质或者说非金属表面有定义，对于导体(Conductor)表面（金属），使用它们的折射指数计算基础折射率并不能得出正确的结果。金属度用来描述一个材质表面是金属还是非金属的，基于金属表面特性，我们要么使用电介质的基础反射率要么就使用物体的表面颜色。因为金属表面会吸收所有折射光线而没有漫反射，所以我们可以直接使用表面颜色纹理来作为它们的基础反射率。 &emsp;&emsp;因此，对于一个PBR渲染器，我们需要获取物体的PBR材质，PBR材质包含了反照率（Albedo）纹理、法线（Normal）纹理、粗糙度（Roughness）纹理以及金属度（Metallic）纹理，正如如下图7所示。在一些PBR渲染器中，还有一个环境遮蔽光贴图（Ambient Occulsion），这里我们不考虑AO贴图，而是考虑在后面采用SSAO实现环境遮蔽光的效果。 图7 PBR材质 &emsp;&emsp;然后还需要提一点的是，我们目前仅考虑直接光照部分，不考虑反弹多次的间接光照。仅考虑直接光照时，对于渲染方程$(15)$，我们不需要对整个半球领域进行积分，因为此时的被积函数是一个狄拉克函数。也就是被积函数仅在某一个特定的方向上才不为0，剩余部分函数值全为0，因此没有必要进行积分。在光照计算中，我们是可以直接知道空间中的光源位置，因此可以直接计算。当空间中有多个光源时，渲染方程的值就是直接计算点与这些光源之间的被击函数值最后累加起来。 L_o(p,\\omega_o)=\\Sigma_{i}^m(k_d\\frac{c}{\\pi}+\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot \\omega_i \\tag {16}&emsp;&emsp;公式$(16)$就是场景中有$m$个光源时的实际渲染方程。为了支持大量的光源，我采用了延迟渲染，将物体空间位置和PBR材质信息存储到多张纹理中，然后在屏幕空间计算光照。首先将物体的信息渲染到纹理中，因为采用了法线贴图，所以要在顶点着色器中构建TBN矩阵提取出法线向量： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768---------------Vertex Shader-------------------#version 330 corelayout (location = 0) in vec3 position;layout (location = 1) in vec3 normal;layout (location = 2) in vec2 texcoord;layout (location = 3) in vec3 color;layout (location = 4) in vec3 tangent;layout (location = 5) in vec3 bitangent;layout (location = 6) in mat4 instanceMatrix;out vec3 FragPos;out vec2 Texcoord;out mat3 TBNMatrix;uniform bool instance;uniform mat4 modelMatrix;uniform mat4 viewMatrix;uniform mat4 projectMatrix;uniform mat4 normalMatrix;uniform mat4 lightSpaceMatrix;void main()&#123; vec3 T = normalize(vec3(modelMatrix * vec4(tangent, 0.0f))); vec3 B = normalize(vec3(modelMatrix * vec4(bitangent, 0.0f))); vec3 N = normalize(vec3(modelMatrix * vec4(normal, 0.0f))); TBNMatrix = mat3(T, B, N); Texcoord = texcoord; if(!instance) FragPos = vec3(modelMatrix * vec4(position,1.0f)); else FragPos = vec3(modelMatrix * instanceMatrix * vec4(position,1.0f)); gl_Position = projectMatrix * viewMatrix * vec4(FragPos,1.0f);&#125;---------------Fragment Shader-------------------#version 330 corein vec3 FragPos;in vec2 Texcoord;in mat3 TBNMatrix;uniform float nearPlane;uniform float farPlane;uniform sampler2D albedoMap;uniform sampler2D normalMap;uniform sampler2D roughMap;uniform sampler2D metallicMap;uniform sampler2D depthMap;layout(location = 0) out vec3 dposition;layout(location = 1) out vec3 dnormal;layout(location = 2) out vec3 dalbedo;layout(location = 3) out vec3 droughness;void main()&#123; // sample albedo. vec3 albedo = texture(albedoMap, Texcoord).rgb; // sample normal. vec3 normal = normalize(2.0f * texture(normalMap, Texcoord).rgb - vec3(1.0f)); normal = TBNMatrix * normal; // sample roughness. float roughness = texture(roughMap, Texcoord).r; // sample metallic. float metallic = texture(metallicMap, Texcoord).r; dposition = FragPos; dnormal = normal; dalbedo = albedo; droughness = vec3(roughness, metallic, gl_FragCoord.z);&#125; &emsp;&emsp;然后在屏幕空间中实现我们的PBR算法。首先是BRDF的三个函数。根据公式$(8)$，法线分布函数如下所示： 1234567891011float NormalDistributionGGX(vec3 N, vec3 H, float roughness)&#123; float a = roughness * roughness; float aSquared = a * a; float NdotH = max(dot(N, H), 0.0f); float NdotHSquared = NdotH * NdotH; float nom = aSquared; float denom = (NdotHSquared * (aSquared - 1.0f) + 1.0f); denom = PI * denom * denom; return nom / denom;&#125; &emsp;&emsp;根据公式$(9)$，菲涅尔方程的计算代码： 1234vec3 fresnelSchlick(float cosTheta, vec3 F0)&#123; return F0 + (1.0f - F0) * pow(1.0 - cosTheta, 5.0f);&#125; &emsp;&emsp;根据公式$(10)$、$(11)$、$(12)$，几何函数的计算代码： 123456789101112131415161718float GeometrySchlickGGX(float NdotV, float roughness)&#123; float r = (roughness + 1.0f); float k = (r * r) / 8.0f; float nom = NdotV; float denom = NdotV * (1.0f - k) + k; return nom / denom;&#125;float GeometrySmith(vec3 N, vec3 V, vec3 L, float roughness)&#123; float NdotV = max(dot(N, V), 0.0f); float NdotL = max(dot(N, L), 0.0f); float ggx2 = GeometrySchlickGGX(NdotV, roughness); float ggx1 = GeometrySchlickGGX(NdotL, roughness); return ggx2 * ggx1;&#125; &emsp;&emsp;然后就是光照部分，我实现的渲染器支持一个平行光、多个点光源。首先来看平行光部分，平行光部分因为不用考虑衰减，因而更为简单： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465struct DirLight &#123; vec3 direction; vec3 radiance;&#125;;uniform DirLight dirLight;void main()&#123; // sample position. vec3 FragPos = texture(dposition, Texcoord).rgb; // sample albedo. vec3 albedo = texture(dalbedo, Texcoord).rgb; // sample normal. vec3 normal = texture(dnormal, Texcoord).rgb; // sample roughness. float roughness = texture(droughness, Texcoord).r; // sample metallic. float metallic = texture(droughness, Texcoord).g; // sample depth. float depth = texture(droughness, Texcoord).b; // sample ambient occlusion. float ao = texture(ddepth, Texcoord).r; // emssive if(normal.x == 0.0f &amp;&amp; normal.y == 0.0f &amp;&amp; normal.z == 0.0f) &#123; fragColor.rgb = albedo; // glow map. float brightness = dot(fragColor.rgb, vec3(0.2126, 0.7152, 0.0722)); brightColor = vec4(fragColor.rgb, 1.0f); // write depth to buffer for forwarding shading. gl_FragDepth = depth; return; &#125; // some directions. vec3 viewDir = normalize(cameraPos - FragPos); // index of refracted. vec3 F0 = vec3(0.04); F0 = mix(F0, albedo, metallic); // ************************directional light************************************ vec3 lightDir = dirLight.direction; vec3 halfwayDir = normalize(lightDir + viewDir); // fresnel factor. vec3 fresnel = fresnelSchlick(max(dot(halfwayDir, viewDir), 0.0f), F0); // normal distribution factor. float distribution = NormalDistributionGGX(normal, halfwayDir, roughness); // geometry facror. float geometryFactor = GeometrySmith(normal, viewDir, lightDir, roughness); // brdf function. vec3 brdf = distribution * fresnel * geometryFactor / (4.0f * max(dot(viewDir, normal), 0.0f) * max(dot(lightDir, normal), 0.0f) + 0.0001f); vec3 kSpecular = fresnel; vec3 kDiffuse = vec3(1.0f) - kSpecular; kDiffuse *= (1.0f - metallic); // rendering equation. fragColor.rgb = (kDiffuse * albedo / PI + brdf) * dirLight.radiance * max(dot(normal, lightDir), 0.0f); // ***************************************************************************** // other. ....&#125; &emsp;&emsp;需要特别说明的就是基础反射率部分，在上面的代码第41、42行，对于电介质我们令其基础反射率为0.04，然后根据材质的金属度在0.04和反照率直接做一个混合。然后就是点光源部分，点光源通常要有一个衰减的过程，这里我采用的衰减因子计算公式如下： attenuation = \\frac{1.0}{c\\cdot d^2}\\\\ d = length(lightPosition - fragPosition) \\tag {17}&emsp;&emsp;即点光的光照强度以距离的平方的倒数衰减，其中$c$是衰减系数，可由用户根据想要的效果指定。确定了衰减方程之后，我们还需要计算点光源的光体积，这是因为当光源与当前点的距离超过一定的值时，计算得到的光照值将小到可以忽略不计。因此，我们可以做这样的一个优化，当距离超过一定值时直接不计算光照，这对于拥有大量光源的场景来说是非常有意义的，它能够减少大量的计算。 &emsp;&emsp;那么如何知道这个距离的阈值呢？这个距离的阈值必须要刚刚好，太小则会产生明显的光照硬边，太大则优化又没有那么明显。事实上，这个距离阈值与上面的衰减因子计算（即公式$(17)$）息息相关。理想情况下，当$attenuation$变为0时，光照的贡献值也变为0。但是事实上$attenuation$不能为0，只能无限地趋于0，我们可以根据一个自己设置的阈值来求解$d$，我设置的阈值为$\\frac{1}{256}$，当光照贡献值小于这个值时，可以忽略不计了： \\frac{1}{256}=I_{max}\\cdot attenuation = I_{max}\\frac{1.0}{c\\cdot d^2}\\\\ \\to d=\\sqrt{\\frac{256I_{max}}{c}} \\tag {18}&emsp;&emsp;上式中的$I_{max}$是光照颜色中的最大分量，根据公式$(18)$我们就得到了点光源的光体积，这是一个以该$d$为半径的球体。当片段位置到光源位置的距离大于这个半径时，我们直接跳过该光源的光照计算。这个光体积直接在CPU上计算一次即可： 123456789101112131415void PointLight::setAttenuationCoff(float coff)&#123; m_atteunationCoff = coff; // calculate point light's volume. GLfloat lightMax = std::fmaxf(std::fmaxf(m_radiance.r, m_radiance.g), m_radiance.b); m_radius = sqrt(256.0f * lightMax / (1.0f * m_atteunationCoff));&#125;void PointLight::setLightColor(glm::vec3 radiance)&#123; Light::setLightColor(radiance); // calculate point light's volume. GLfloat lightMax = std::fmaxf(std::fmaxf(m_radiance.r, m_radiance.g), m_radiance.b); m_radius = sqrt(256.0f * lightMax / (1.0f * m_atteunationCoff));&#125; &emsp;&emsp;最后完整的着色器代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204#version 330 corein vec2 Texcoord;struct DirLight &#123; vec3 direction; vec3 radiance;&#125;;struct PointLight&#123; float radius; vec3 position; vec3 radiance;&#125;;uniform vec3 cameraPos;// lighting.#define MAX_POINT_LIGHT 128uniform int pointLightNum;uniform DirLight dirLight;uniform PointLight pointLight[MAX_POINT_LIGHT];uniform float lightAttenuationCoff;uniform mat4 lightSpaceMatrix;// pbr material texture.uniform sampler2D dposition;uniform sampler2D dnormal;uniform sampler2D dalbedo;uniform sampler2D droughness;uniform sampler2D ddepth;uniform sampler2D shadowDepth;layout(location = 0) out vec4 fragColor;layout(location = 1) out vec4 brightColor;// brdf auxiliary functions.float NormalDistributionGGX(vec3 N, vec3 H, float roughness);float GeometrySchlickGGX(float NdotV, float roughness);float GeometrySmith(vec3 N, vec3 V, vec3 L, float roughness);vec3 fresnelSchlick(float cosTheta, vec3 F0);float shadowCalculation(vec4 fragPosLightSpace, float bias);const float PI = 3.14159265359;void main()&#123; // sample position. vec3 FragPos = texture(dposition, Texcoord).rgb; // sample albedo. vec3 albedo = texture(dalbedo, Texcoord).rgb; // sample normal. vec3 normal = texture(dnormal, Texcoord).rgb; // sample roughness. float roughness = texture(droughness, Texcoord).r; // sample metallic. float metallic = texture(droughness, Texcoord).g; // sample depth. float depth = texture(droughness, Texcoord).b; // sample ambient occlusion. float ao = texture(ddepth, Texcoord).r; // emssive if(normal.x == 0.0f &amp;&amp; normal.y == 0.0f &amp;&amp; normal.z == 0.0f) &#123; fragColor.rgb = albedo; // glow map. float brightness = dot(fragColor.rgb, vec3(0.2126, 0.7152, 0.0722)); brightColor = vec4(fragColor.rgb, 1.0f); // write depth to buffer for forwarding shading. gl_FragDepth = depth; return; &#125; // some directions. vec3 viewDir = normalize(cameraPos - FragPos); // index of refracted. vec3 F0 = vec3(0.04); F0 = mix(F0, albedo, metallic); // *******************************directional light************************************ vec3 lightDir = dirLight.direction; vec3 halfwayDir = normalize(lightDir + viewDir); // fresnel factor. vec3 fresnel = fresnelSchlick(max(dot(halfwayDir, viewDir), 0.0f), F0); // normal distribution factor. float distribution = NormalDistributionGGX(normal, halfwayDir, roughness); // geometry facror. float geometryFactor = GeometrySmith(normal, viewDir, lightDir, roughness); // brdf function. vec3 brdf = distribution * fresnel * geometryFactor / (4.0f * max(dot(viewDir, normal), 0.0f) * max(dot(lightDir, normal), 0.0f) + 0.0001f); vec3 kSpecular = fresnel; vec3 kDiffuse = vec3(1.0f) - kSpecular; kDiffuse *= (1.0f - metallic); // rendering equation. fragColor.rgb = (kDiffuse * albedo / PI + brdf) * dirLight.radiance * max(dot(normal, lightDir), 0.0f); // ************************************************************************************ // *************************************point lights*********************************** vec3 pointLightRadiance = vec3(0.0f); for(int i = 0;i &lt; pointLightNum;++ i) &#123; vec3 lightDir = normalize(pointLight[i].position - FragPos); vec3 halfwayDir = normalize(viewDir + lightDir); float distance = length(pointLight[i].position - FragPos); if(distance &gt; pointLight[i].radius) continue; float attenuation = 1.0f / (lightAttenuationCoff * distance * distance + 0.00001); vec3 radiance = pointLight[i].radiance * attenuation; vec3 fresnel = fresnelSchlick(max(dot(halfwayDir, viewDir), 0.0f), F0); // normal distribution factor. float distribution = NormalDistributionGGX(normal, halfwayDir, roughness); // geometry facror. float geometryFactor = GeometrySmith(normal, viewDir, lightDir, roughness); // brdf function. vec3 brdf = distribution * fresnel * geometryFactor / (4.0f * max(dot(viewDir, normal), 0.0f) * max(dot(lightDir, normal), 0.0f) + 0.0001f); vec3 kSpecular = fresnel; vec3 kDiffuse = vec3(1.0f) - kSpecular; kDiffuse *= (1.0f - metallic); // rendering equation. pointLightRadiance += (kDiffuse * albedo / PI + brdf) * radiance * max(dot(normal, lightDir), 0.0f); &#125; // ************************************************************************************ // shadow float shadow = 1.0f; vec4 FragPosLightSpace = lightSpaceMatrix * vec4(FragPos, 1.0f); shadow = 1.0f - shadowCalculation(FragPosLightSpace, 0.0f); fragColor.xyz = ao * albedo * 0.02f + fragColor.xyz * shadow + pointLightRadiance; // glow map. float brightness = dot(fragColor.rgb / (fragColor.rgb + vec3(1.0f)), vec3(0.2126, 0.7152, 0.0722)); if(brightness &gt; 0.55f) brightColor = vec4(fragColor.rgb / (fragColor.rgb + vec3(1.0f)), 1.0f); // write depth to buffer for forwarding shading. gl_FragDepth = depth;&#125;float NormalDistributionGGX(vec3 N, vec3 H, float roughness)&#123; float a = roughness * roughness; float aSquared = a * a; float NdotH = max(dot(N, H), 0.0f); float NdotHSquared = NdotH * NdotH; float nom = aSquared; float denom = (NdotHSquared * (aSquared - 1.0f) + 1.0f); denom = PI * denom * denom; return nom / denom;&#125;float GeometrySchlickGGX(float NdotV, float roughness)&#123; float r = (roughness + 1.0f); float k = (r * r) / 8.0f; float nom = NdotV; float denom = NdotV * (1.0f - k) + k; return nom / denom;&#125;float GeometrySmith(vec3 N, vec3 V, vec3 L, float roughness)&#123; float NdotV = max(dot(N, V), 0.0f); float NdotL = max(dot(N, L), 0.0f); float ggx2 = GeometrySchlickGGX(NdotV, roughness); float ggx1 = GeometrySchlickGGX(NdotL, roughness); return ggx2 * ggx1;&#125;vec3 fresnelSchlick(float cosTheta, vec3 F0)&#123; return F0 + (1.0f - F0) * pow(1.0 - cosTheta, 5.0f);&#125;float shadowCalculation(vec4 fragPosLightSpace, float bias)&#123; // perspective division. vec3 projCoords = fragPosLightSpace.xyz / fragPosLightSpace.w; projCoords = projCoords * 0.5 + 0.5; if(projCoords.z &gt; 1.0) return 0.0f; // pcf. float shadowFactor = 0.0f; float currentDepth = projCoords.z; vec2 texelSize = 1.0 / textureSize(shadowDepth, 0); for(int x = -1; x &lt;= 1; ++x) &#123; for(int y = -1; y &lt;= 1; ++y) &#123; float pcfDepth = texture(shadowDepth, projCoords.xy + vec2(x, y) * texelSize).r; shadowFactor += ((currentDepth - bias) &gt; pcfDepth) ? 1.0 : 0.0; &#125; &#125; shadowFactor /= 9.0; return shadowFactor;&#125; 四、屏幕空间环境光遮蔽&emsp;&emsp;本文前面主要介绍了PBR的直接光照，这意味着在没有被光源直接照亮的区域，依然没有产生符合物理规律的光影效果，这是因为我们还没有考虑间接光照。在实时应用中，为了实现物体的相互遮蔽效果，通常采用SSAO（即Screen Space Ambient Occlusion），实际上这是一个比较tricky的做法，但是产生的效果非常不错。 &emsp;&emsp;SSAO采用的原理非常简单，：对于每一个片段，我们都会根据周边深度值计算一个遮蔽因子(Occlusion Factor)。这个遮蔽因子之后会被用来减少或者抵消片段的环境光照分量。遮蔽因子是通过采集片段周围球型核心(Kernel)的多个深度样本，并和当前片段深度值对比而得到的。高于片段深度值样本的个数就是我们想要的遮蔽因子。正如下图8所示。到这里文章篇幅有点太长了，SSAO也比较简单，因此我就不再赘述了。SSAO因子计算的核心代码： 图8 Occlusion Factor 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#version 430 corein vec2 Texcoord;uniform vec3 samples[64];uniform mat4 viewMatrix;uniform mat4 projectMatrix;uniform sampler2D dposition;uniform sampler2D dnormal;uniform sampler2D ddepth;uniform sampler2D randomNoise;uniform float farPlane;uniform float nearPlane;const float radius = 10.0f;const int sampleNum = 64;void main()&#123; // sample position. vec3 FragPos = texture(dposition, Texcoord).rgb; // sample normal. vec3 normal = texture(dnormal, Texcoord).rgb; // sample depth. float depth = texture(ddepth, Texcoord).r; // sample random vectors. vec2 depthTextureSize = textureSize(ddepth, 0); vec2 noiseTextureSize = textureSize(randomNoise, 0); vec2 noiseTexScale = vec2(depthTextureSize.x / noiseTextureSize.x, depthTextureSize.y / noiseTextureSize.y); vec3 randomVec = texture(randomNoise, Texcoord * noiseTexScale).rgb; vec3 tangent = normalize(randomVec - normal * dot(randomVec, normal)); vec3 bitangent = cross(normal, tangent); mat3 TBNMatrix = mat3(tangent, bitangent, normal); // calculate ambient occlusion. float occlusion = 0.0f; for(int i = 0;i &lt; sampleNum;++i) &#123; // change to world space. vec3 samplePoint = TBNMatrix * samples[i]; samplePoint = FragPos + samplePoint * radius; // change to view space. samplePoint = vec3(viewMatrix * vec4(samplePoint, 1.0f)); // change to ndc space &amp; screen space. vec4 tmp = vec4(samplePoint, 1.0f); tmp = projectMatrix * tmp; tmp.xyz /= tmp.w; tmp.xyz = tmp.xyz * 0.5f + 0.5f; // get sample point's depth. float sampleDepth = texture(ddepth, tmp.xy).r; samplePoint.z /= -farPlane; // range check and accumulate. float rangeCheck = smoothstep(0.0, 1.0, radius / (abs(depth - sampleDepth) * farPlane)); occlusion += (sampleDepth &gt; samplePoint.z ? 0.0 : 1.0) * rangeCheck; &#125; occlusion /= sampleNum; occlusion = 1.0f - occlusion; gl_FragDepth = occlusion;&#125; &emsp;&emsp;SSAO对于场景的真实感觉有着非常重要的作用，可能我们平时不会太过注意，但是却又是一个非常关键的点。下面左边就是计算得到的AO因子，最后将AO因子的乘上物体的反照率以及环境光缩放系数即可。 图9 ao因子计算结果 五、实现效果&emsp;&emsp;除了PBR、SSAO，其他如延迟渲染、HDR、Glow Effect、因子等不再赘述。 参考资料：$[1]$ https://learnopengl.com/PBR/Theory $[2]$ https://learnopengl.com/PBR/Lighting $[3]$ https://learnopengl.com/Advanced-Lighting/Deferred-Shading $[4]$ https://learnopengl.com/Advanced-Lighting/SSAO","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Real-time Rendering","slug":"Real-time-Rendering","permalink":"https://yangwc.com/categories/Real-time-Rendering/"},{"name":"Physically Based Rendering","slug":"Physically-Based-Rendering","permalink":"https://yangwc.com/categories/Physically-Based-Rendering/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Real-time Rendering","slug":"Real-time-Rendering","permalink":"https://yangwc.com/tags/Real-time-Rendering/"},{"name":"Physically Based Rendering","slug":"Physically-Based-Rendering","permalink":"https://yangwc.com/tags/Physically-Based-Rendering/"}]},{"title":"流体渲染Fluid Rendering：基于屏幕空间的液体渲染","slug":"FluidRendering","date":"2019-07-04T13:13:24.745Z","updated":"2019-07-14T09:18:21.952Z","comments":true,"path":"2019/07/04/FluidRendering/","link":"","permalink":"https://yangwc.com/2019/07/04/FluidRendering/","excerpt":"本文主要是关于屏幕空间的液体渲染算法，分别介绍了高斯滤波、双边滤波和曲率流方法平滑流体深度的方法。","text":"本文主要是关于屏幕空间的液体渲染算法，分别介绍了高斯滤波、双边滤波和曲率流方法平滑流体深度的方法。 基于屏幕空间的流体渲染 水体的光照计算 流体深度贴图的平滑处理 实现效果 参考资料 基于屏幕空间的流体渲染 &emsp;&emsp;在计算机图形学中，我们最后都要把模拟的物体渲染出来，这是图形学的最终目的。而目前对于流体渲染，无论是基于拉格朗日视角的还是基于欧拉视角的流体模拟都要经过流体表面重建这一步，然后再做进一步的光照着色计算。流体表面重建有个非常经典的算法——Marching Cubes$^{[2]}$，该方法采用水平集（Level Set），首先将空间划分称均匀的立方体网格，然后计算每个网格上8个顶点的密度。对于每一个立方体，如果立方体上的一条边两个端点的密度值大于给定的一个阈值$\\rho_{boundary}$，则这条边上存在着一个流体表面上的顶点。最后将每个立方体构造的多边形拼接，即可得到流体的表面网格。该方法基于这样的一个事实：流体表面处的密度应该等于某个固定的值，流体表面是一个三维的密度等高面，密度值为$\\rho_{boundary}$，这就是水平集的思想。 图1 Marching Cube的15种模式 &emsp;&emsp;Marching Cube是流体表面重建的传统做法，实现效果非常不错，但是算法的时间复杂度大，重建一次需要花费不少的时间。对于流动的流体来说，需要每帧构建流体表面，因而很难保证实时性。除了Marching Cube这类传统的流体表面重建方法，还有一些技巧性比较强的方法适合实时性应用，基于屏幕空间的流体渲染方法就是这一类。基于屏幕空间的流体渲染以一种新的思路角度展开流体的渲染，这种方法对并行友好，不涉及到直接对液体表面网格的重建，实现也相对简单。 一、基于屏幕空间的流体渲染&emsp;&emsp;首先介绍一些基于屏幕空间的流体渲染算法纵览。与在世界空间构建流体表面网格的思路不同，基于屏幕空间的流体渲染算法直接在屏幕空间做流体表面的复原工作，操作维度从三维降到了二维。这是一种与图形处理紧密结合的渲染算法。算法分为两个步骤，重点在于第一个步骤。第一个步骤是屏幕空间流体处理步骤，首先将流体粒子按照当前的投影矩阵和视图矩阵渲染到屏幕空间，获取流体粒子的深度轮廓信息、流体厚度信息，这些信息存储到纹理中，紧接着我们对存储深度信息的纹理做一些后处理操作，模糊掉球形粒子的坑坑洼洼，使得深度信息更为平滑，最后我们根据深度纹理和厚度纹理做流体的光照计算。 图2 Screen Space Fluid Rendering算法总览 &emsp;&emsp;算法的总流程如图2所示，其中的噪声图像生成不是必须的，实际上我觉得加上这个噪声处理反而是个败笔。对于水体来说，它的表面通常是比较光滑的而不是坑坑洼洼。背景图用于处理流体的折射计算，因此流体通常应该放到最后渲染。该算法可以看成是针对流体的延迟渲染，流体的表面法线信息并不能直接获得，需要从深度贴图中重建法线，而在这之前深度贴图也要经过一些特殊的处理。基于屏幕空间的流体渲染步骤中，如何对深度贴图做处理使之能真实反映流体的特性是算法的核心，剩下的光照部分直接采用Blin-Phong光照模型，并综合考虑水体的折射和反射，上面提到的厚度贴图用于水体的折射计算，因为越厚的流体其透光率越低。 二、水体的光照计算&emsp;&emsp;首先我们先实现一个不对深度贴图和厚度贴图做处理的Naive版的流体渲染算法，这有利于我们弄清整个算法的思路，而且使得实现的过程更加清晰、有条理、便于调试。去掉对深度贴图和厚度贴图的处理部分（噪声部分我们不考虑），剩下的算法流程就分为：渲染不包含流体的场景纹理、渲染粒子深度信息、渲染粒子厚度信息、光照着色计算。这里我们采用OpenGL的render to target，将场景、深度贴图、厚度贴图都渲染到纹理当中，供最后的光照着色计算使用。 &emsp;&emsp;一开始我们为了获取流体的深度信息和厚度信息，需要流体以球形粒子的形态进行绘制，这里我们采用OpenGL的点精灵Point Sprite做粒子绘制。在OpenGL中，点精灵Point Sprite就是一个内建的始终朝向摄像机视角的正方形，我们可以指定它的大小，但是它的大小是定义在屏幕空间的。因此，在利用点精灵绘制流体粒子时，一方面我们要根据世界空间的流体粒子大小设置点精灵的大小，另一方面要以将四边形变成圆形。 &emsp;&emsp;首先根据世界空间的流体粒子大小设置点精灵的大小，上面说了，点精灵的大小是定义在屏幕空间的，而通常我们设置的流体粒子大小是世界空间的。为了正确设置点精灵的大小，使之符合透视原理，需要计算世界空间的长度投影到屏幕空间的长度，这个比较基础，根据三角形相似的原理即可。 1234567891011121314151617181920212223242526// calculate particle size scale factor.float aspect = camera-&gt;getAspect();float fovy = camera-&gt;getFovy();float pointScale = 1.0f * m_screenWidth / aspect * (1.0f / tanf(glm::radians(fovy) * 0.5f));......shader-&gt;setFloat(\"pointScale\", pointScale);shader-&gt;setFloat(\"pointSize\", m_particleRadius);......#version 430 corelayout (location = 0) in vec4 position;uniform mat4 modelMatrix;uniform mat4 viewMatrix;uniform mat4 projectMatrix;uniform float pointScale;uniform float pointSize;out vec3 eyeSpacePos;void main()&#123; eyeSpacePos = (viewMatrix * modelMatrix * vec4(position.xyz, 1.0f)).xyz; gl_PointSize = -pointScale * pointSize / eyeSpacePos.z; gl_Position = projectMatrix * viewMatrix * modelMatrix * vec4(position.xyz, 1.0f);&#125; &emsp;&emsp;然后就是让正方形的点精灵变成一个圆形。在片元着色器中，点精灵提供了一个gl_PointCoord变量，这是一个内建的专用于点精灵的纹理坐标，已经经过光栅化插值之后的点精灵纹理坐标。我们根据这个坐标来获取每一个点精灵的像素所在的相对位置，gl_PointCoord纹理坐标以左上角为原点，我们需要将其变换到点精灵中心。接着获取每个像素的法线信息，并计算半径长度，半径超过1.0的像素我们给它discard掉。 12345678910111213141516#version 430 coreuniform float pointSize;uniform mat4 projectMatrix;in vec3 eyeSpacePos;void main()&#123; vec3 normal; normal.xy = gl_PointCoord.xy * vec2(2.0, -2.0) + vec2(-1.0,1.0); float mag = dot(normal.xy, normal.xy); if(mag &gt; 1.0) discard; normal.z = sqrt(1.0 - mag); ......&#125; &emsp;&emsp;点精灵给我们提供一个非常方便的球心粒子绘制方法，这种方法绘制的球体是基于解析解的，精度非常高，球体非常光滑，而如果采用球体网格则需要绘制很多个网格，球体精度受限于网格的精度，速度也慢。现在知道如何根据点精灵绘制，接下来我们需要从流体粒子中获取深度贴图和厚度贴图。 1、流体深度贴图&emsp;&emsp;绘制流体深度贴图需要开启深度测试，并创建一个帧缓冲，深度的信息绘制到给定帧缓冲的深度缓冲当中。 12345678910111213141516171819202122232425262728293031323334353637383940void LiquidDrawable::drawLiquidDepth(Camera3D::ptr camera, Light::ptr sunLight, Camera3D::ptr lightCamera, Shader::ptr shader)&#123; m_framebuffer-&gt;bind(); // calculate particle size scale factor. float aspect = camera-&gt;getAspect(); float fovy = camera-&gt;getFovy(); float pointScale = 1.0f * m_screenWidth / aspect * (1.0f / tanf(glm::radians(fovy) * 0.5f)); // render state. glEnable(GL_DEPTH_TEST); glDisable(GL_BLEND); glClear(GL_DEPTH_BUFFER_BIT); glEnable(GL_PROGRAM_POINT_SIZE); glEnable(GL_VERTEX_PROGRAM_POINT_SIZE); // shader. shader = m_shaderMgr-&gt;getShader(\"liquidDepth\"); shader-&gt;bind(); shader-&gt;setFloat(\"farPlane\", camera-&gt;getFar()); shader-&gt;setFloat(\"nearPlane\", camera-&gt;getNear()); shader-&gt;setFloat(\"pointScale\", pointScale); shader-&gt;setFloat(\"pointSize\", m_particleRadius); shader-&gt;setFloat(\"densityLowerBound\", m_densityLowerBound); shader-&gt;setMat4(\"modelMatrix\", m_transformation.getWorldMatrix()); shader-&gt;setMat4(\"viewMatrix\", camera-&gt;getViewMatrix()); shader-&gt;setMat4(\"projectMatrix\", camera-&gt;getProjectMatrix()); // draw glBindVertexArray(m_particleVAO); glDrawArrays(GL_POINTS, 0, m_numParticles); glBindVertexArray(0); // restore. m_shaderMgr-&gt;unBindShader(); glDisable(GL_PROGRAM_POINT_SIZE); m_framebuffer-&gt;unBind();&#125; &emsp;&emsp;在片元着色器，我们需要计算像素的深度信息，为了还原出粒子的深度信息，我们首先将粒子在摄像机空间的位置传到片元着色器中，这个其实就是点精灵在摄像机空间的中心点。然后计算每个像素的法线，每个像素在摄像机空间的点就等于$eyeSpacePos + normal * pointSize$，其中$pointSize$就是流体粒子的半径大小。得到每个像素在摄像机空间的点，我们再将其乘上投影矩阵，并做透视除法，得到标准化设备空间$[-1,+1]$的深度值。 123456789101112131415161718192021222324252627282930313233343536373839404142============vertex shader================#version 430 corelayout (location = 0) in vec4 position;uniform mat4 modelMatrix;uniform mat4 viewMatrix;uniform mat4 projectMatrix;uniform float pointScale;uniform float pointSize;uniform float densityLowerBound;out vec3 eyeSpacePos;void main()&#123; eyeSpacePos = (viewMatrix * modelMatrix * vec4(position.xyz, 1.0f)).xyz; gl_PointSize = -pointScale * pointSize / eyeSpacePos.z; // to prevent single or a few particles. if(position.w &lt; densityLowerBound) gl_PointSize = 0.0f; gl_Position = projectMatrix * viewMatrix * modelMatrix * vec4(position.xyz, 1.0f);&#125; ============fragment shader================#version 430 coreuniform float pointSize;uniform mat4 projectMatrix;in vec3 eyeSpacePos;void main()&#123; vec3 normal; normal.xy = gl_PointCoord.xy * vec2(2.0, -2.0) + vec2(-1.0,1.0); float mag = dot(normal.xy, normal.xy); if(mag &gt; 1.0) discard; normal.z = sqrt(1.0 - mag); vec4 pixelEyePos = vec4(eyeSpacePos + normal * pointSize, 1.0f); vec4 pixelClipPos = projectMatrix * pixelEyePos; float ndcZ = pixelClipPos.z / pixelClipPos.w; gl_FragDepth = ndcZ;&#125; &emsp;&emsp;经过上面的一个pass，我们得到了流体的深度信息。下面是一张近看的流体粒子深度贴图，为什么要近看？因为投影矩阵对深度信息做了一个非线性变换，这个非线性变换使得深度值密度在靠近1.0这一端，稍微远一点，就全白看不见了。（所以如果渲染出来一张全白的深度贴图，不要着急，拉近看看） 图3 流体深度贴图 2、流体厚度贴图&emsp;&emsp;然后我们需要获取流体的厚度贴图，获取厚度贴图不是必须的，这是因为厚度贴图是为了流体折射计算服务。有些流体并不透光（如牛奶），所以也不存在折射。与渲染流体深度贴图一样，我们以将流体粒子以球形的点精灵形态进行绘制。给定一个方向，流体厚度衡量在这个方向上有多少流体粒子，越多越厚。因此，为了计算流体的厚度，我们采用了OpenGL的blending技巧，也就是透明融合，每个流体粒子计算各自贡献的厚度值，然后将其输出到颜色缓冲中，借助OpenGL的透明融合，将厚度值累积起来，这样就能得到从摄像机方向看去的流体厚度信息。此外，由于我们是采用球形点精灵绘制一个流体粒子，一个流体粒子，其厚度贡献值从中心到边缘应该逐渐减少，为此我们采用计算得到的normal的z分量作为厚度值，并乘上一个缩放系数。具体如下所示： 123456789101112131415#version 430 coreuniform float pointSize;uniform mat4 projectMatrix;layout(location = 0) out vec4 fragColor;void main()&#123; vec3 normal; normal.xy = gl_PointCoord.xy * vec2(2.0, -2.0) + vec2(-1.0,1.0); float mag = dot(normal.xy, normal.xy); if(mag &gt; 1.0) discard; normal.z = sqrt(1.0 - mag); fragColor = vec4(normal.z*0.005, 0.0, 0.0, 1.0);&#125; &emsp;&emsp;在CPU端，我们关闭深度测试，开启透明融合，并设置融合函数为additive blending，即加法融合。所谓加法融合，就是原像素值和目标像素值直接相加，不乘上任何的缩放系数。在OpenGL的加法融合就是设置glBlendFunc的参数均为GL_ONE、GL_ONE。这样不同流体粒子之间的厚度值直接叠加，达到我们所需的效果。同时为了凸显厚度，我们设当地设大一点粒子的大小。 123456789101112131415161718192021222324252627282930313233343536373839404142void LiquidDrawable::drawLiquidThick(Camera3D::ptr camera, Light::ptr sunLight, Camera3D::ptr lightCamera, Shader::ptr shader)&#123; m_framebuffer-&gt;bind(); // calculate particle size scale factor. float aspect = camera-&gt;getAspect(); float fovy = camera-&gt;getFovy(); float pointScale = m_screenWidth / aspect * (1.0f / tanf(glm::radians(fovy) * 0.5f)); // render state. glEnable(GL_BLEND); glBlendFunc(GL_ONE, GL_ONE); glDepthMask(GL_FALSE); glEnable(GL_DEPTH_TEST); glDisable(GL_DEPTH_TEST); glEnable(GL_PROGRAM_POINT_SIZE); glEnable(GL_VERTEX_PROGRAM_POINT_SIZE); glClearColor(0.0, 0.0, 0.0, 1.0); glClear(GL_COLOR_BUFFER_BIT); // shader. shader = m_shaderMgr-&gt;getShader(\"liquidThick\"); shader-&gt;bind(); shader-&gt;setFloat(\"pointScale\", pointScale); shader-&gt;setFloat(\"pointSize\", 4.0f * m_particleRadius); shader-&gt;setMat4(\"modelMatrix\", m_transformation.getWorldMatrix()); shader-&gt;setMat4(\"viewMatrix\", camera-&gt;getViewMatrix()); shader-&gt;setMat4(\"projectMatrix\", camera-&gt;getProjectMatrix()); // draw glBindVertexArray(m_particleVAO); glDrawArrays(GL_POINTS, 0, m_numParticles); glBindVertexArray(0); // restore. glDepthMask(GL_TRUE); glDisable(GL_BLEND); glDisable(GL_PROGRAM_POINT_SIZE); m_shaderMgr-&gt;unBindShader(); m_framebuffer-&gt;unBind();&#125; 图4 流体厚度贴图 &emsp;&emsp;图4就是渲染出来的流体厚度贴图，越红的地方越厚。 3、水体折射、Blin-Phong光照&emsp;&emsp;在前面的步骤我们获取了两张纹理贴图：流体深度纹理、流体厚度纹理。然后我们需要根据这两张纹理计算水体的光照效果。前面已经提到过，这是一种类似于延迟渲染的技术，光照计算都是在屏幕空间的四边形上进行。我们首先要根据流体的深度纹理信息重建流体表面的法线向量。在屏幕空间中，我们已知当前像素的纹理坐标，和对应的流体深度信息，那么如何根据这些信息重建法线向量呢？这就涉及到了如何从设备标准空间ndc顶点转换到摄像机空间$^{[1]}$。 &emsp;&emsp;设摄像机空间的顶点坐标为$(vx,vy,vz,vw)$，其中$vw=1.0$，ndc空间的顶点坐标为$(nx,ny,nz)$，其中$x、y、z\\in[-1,+1]$。我们知道从摄像机空间变换到ndc空间，需要经过投影变换、透视除法这两个步骤： clip=projectMatrix \\cdot (vx,vy,vz,1.0)\\\\ (nx,ny,nz) = clip.xyz/clip.w \\tag {1}&emsp;&emsp;其中的clip就是裁剪空间的顶点坐标。所以我们的问题就是，已知ndc空间的$(nx,ny,nz)$、摄像机空间的$vw=1.0$以及投影变换矩阵，如何计算得到摄像机空间的$(vx,vy,vz)$顶点坐标。根据公式$(1)$，我们有： (vx,vy,vz,1.0)=(projectMatrix)^{-1}\\cdot clip \\tag {2}&emsp;&emsp;而根据ndc顶点坐标$(nx,ny,nz)$与裁剪空间顶点坐标$clip$的关系，我们又有： clip=(clip.xyz,clip.w)=((nx,ny,nz)\\cdot clip.w, clip.w)=clip.w \\cdot (nx,ny,nz,1.0) \\tag {3}&emsp;&emsp;联立公式$(2)$与公式$(3)$，我们有： (vx,vy,vz,1.0)=clip.w \\cdot (projectMatrix)^{-1}\\cdot (nx,ny,nz,1.0) \\tag {4}&emsp;&emsp;公式$(4)$直接表明了摄像机空间顶点坐标与ndc空间坐标之间的关系，其中只有$clip.w$是未知，这是一个关键点。我们注意到$vw=1.0$，也就是在摄像机空间，顶点的分量为1.0。这一点可以利用起来： clip.w\\cdot[(projectMatrix)^{-1}\\cdot (nx,ny,nz,1.0)].w = 1.0 \\\\ \\to\\\\ clip.w=\\frac{1.0}{[(projectMatrix)^{-1}\\cdot (nx,ny,nz,1.0)].w} \\tag {5}&emsp;&emsp;公式$(5)$意思就是$clip.w$等于ndc空间的坐标$(nx,ny,nz,1.0)$右乘逆投影矩阵所得向量的$w$分量的倒数。将其代入公式$(4)$中，我们可得： (vx,vy,vz,1.0)=\\frac{1.0}{[(projectMatrix)^{-1}\\cdot (nx,ny,nz,1.0)].w} \\cdot (projectMatrix)^{-1}\\cdot (nx,ny,nz,1.0)&emsp;&emsp;因此，从ndc空间的坐标到摄像机空间的坐标，我们首先计算$(projectMatrix)^{-1}\\cdot (nx,ny,nz,1.0)$，然后再将其$xyz$分量除以$w$分量即可。 1234567vec3 uvToEye(vec2 coord, float z)&#123; vec2 pos = coord * 2.0f - 1.0f; vec4 clipPos = vec4(pos, z, 1.0f); vec4 viewPos = invProjectMatrix * clipPos; return viewPos.xyz / viewPos.w;&#125; &emsp;&emsp;知道了摄像机空间的顶点坐标，紧接着我们需要根据顶点坐标重建顶点法线。法线向量可以根据沿着x方向和沿着y方向的偏导数做叉乘得到，这里的x方向和y方向均为屏幕空间中的纹理坐标方向。求偏导数可以根据有限差分法： \\frac{\\partial f}{\\partial s}\\approx\\frac{f(s+\\Delta s)-f(s)}{\\Delta s} \\tag {6}&emsp;&emsp;这里我们采用了单向差分法（前向差分法、后向差分法），没有采用中心差分法。为了避免计算出错误的法线向量，我们综合考虑前向差分法和后向差分法，取较小的那个差分值（差分值较大的很可能是在边界处与背景或距离比较远的流体深度值做了一个差分，这个时候得到的法线是错误的，因为深度值不连续）。我们对深度纹理图的周围四个像素分别做x方向和y方向的差分，$\\Delta s=1$。最后根据得到的偏导向量做叉乘得到法线。 123456789101112131415161718192021// -----------------reconstruct normal----------------------------vec2 depthTexelSize = 1.0 / textureSize(depthTex, 0);// calculate eye space position.vec3 eyeSpacePos = uvToEye(Texcoord, depth);// finite difference.vec3 ddxLeft = eyeSpacePos - uvToEye(Texcoord - vec2(depthTexelSize.x,0.0f), texture(depthTex, Texcoord - vec2(depthTexelSize.x,0.0f)).r);vec3 ddxRight = uvToEye(Texcoord + vec2(depthTexelSize.x,0.0f), texture(depthTex, Texcoord + vec2(depthTexelSize.x,0.0f)).r) - eyeSpacePos;vec3 ddyTop = uvToEye(Texcoord + vec2(0.0f,depthTexelSize.y), texture(depthTex, Texcoord + vec2(0.0f,depthTexelSize.y)).r) - eyeSpacePos;vec3 ddyBottom = eyeSpacePos - uvToEye(Texcoord - vec2(0.0f,depthTexelSize.y), texture(depthTex, Texcoord - vec2(0.0f,depthTexelSize.y)).r);vec3 dx = ddxLeft;vec3 dy = ddyTop;if(abs(ddxRight.z) &lt; abs(ddxLeft.z)) dx = ddxRight;if(abs(ddyBottom.z) &lt; abs(ddyTop.z)) dy = ddyBottom;vec3 normal = normalize(cross(dx, dy));vec3 worldPos = (invViewMatrix * vec4(eyeSpacePos, 1.0f)).xyz; &emsp;&emsp;下图5显示了重建得到的法线情况，需要特别注意的是，这里我们重建得到的法线是在摄像机空间而非世界空间，这是因为我们还原的顶点也是摄像机空间的，因此计算光照时需要将光线向量变换到摄像机空间。 图5 重建的法线 &emsp;&emsp;有了法线向量，接下来我们就计算水体折射、Blin-Phong光照。我这里只考虑水体折射，反射不考虑，因为一般清澈的水很少有反射现象。Beer-Lambert定律揭示了液体的光吸收现象。根据Beer-Lambert定律，液体的透光率随着液体的厚度增加呈指数衰减： I(d)=I_0\\cdot e^{-kd} \\tag {7}&emsp;&emsp;公式$(7)$中，$d$是液体的厚度值，$I_0$是光照强度rgb向量，$k$是液体的光能衰减因子向量，通常等于$1.0-liquidColor$。液体的透光率计算如下所示，厚度值从之前渲染得到的贴图中直接采样得到。 12float thickness = max(texture(thicknessTex, Texcoord).r, 0.3f);vec3 transmission = exp(-(vec3(1.0f) - liquidColor.xyz) * thickness); &emsp;&emsp;然后，我们还需要从背景纹理图中采样，因为我们要实现折射的效果。为了实现折射的效果，我们需要对采样的纹理坐标做一个偏移，使之产生折射的效果。 1234float refractScale = 1.33 * 0.025; // refracted index.refractScale *= smoothstep(0.1, 0.4, worldPos.y);vec2 refractCoord = Texcoord + normal.xy * refractScale;vec3 refractedColor = texture(backgroundTex, refractCoord).xyz * transmission; &emsp;&emsp;最后就是关于Blin-Phong光照部分，比较简单，不再赘述。唯一需要注意的是我们获取的法线是在摄像机空间的，需要将光的方向也变换到摄像机空间。 12345678910// -----------------Phong lighting----------------------------vec3 viewDir = -normalize(eyeSpacePos);vec3 lightDir = normalize((viewMatrix * vec4(dirLight.direction, 0.0f)).xyz);vec3 halfVec = normalize(viewDir + lightDir);vec3 specular = vec3(dirLight.specular * pow(max(dot(halfVec, normal), 0.0f), 400.0f));vec3 diffuse = liquidColor.xyz * max(dot(lightDir, normal), 0.0f) * dirLight.diffuse * liquidColor.w;// -----------------Merge all effect----------------------------fragColor.rgb = diffuse + specular + refractedColor;fragColor.a = 1.0f; &emsp;&emsp;实现出来的效果如下图6所示。可以看到，渲染的效果很差，液体看起来像是由很多粒果冻组成，流体的粒子感非常明显，这并不是我们想要的。产生果冻壮流体的根本原因是因为我们的法线向量并不平滑，仔细观察图5，我们重建后的法线还是原来的球面上的法线，这造成光照效果的失真。 图6 实现的果冻壮流体 &emsp;&emsp;因此，为了进一步提升渲染效果，我们要想办法使得流体表面得法线平滑。而法线来源于深度贴图，因为我们追溯到源头就是使得深度贴图尽可能地平滑。接下来我们将讨论在屏幕空间对深度贴图做的一些平滑后处理方案。 三、流体深度贴图的平滑处理&emsp;&emsp;我们现在需要对深度贴图做平滑处理，这其实属于图像处理的范畴。 1、采用高斯滤波平滑深度贴图&emsp;&emsp;首先是高斯模糊的平滑方案，我们把深度纹理当场一张图片，采用高斯权重做一个滤波操作。为了性能，同样我们将二维的高斯模糊分成水平方向和垂直方向两个pass。在之前实现辉光效果时已经介绍过，不再赘述。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 unsigned int DepthGaussianBlurFilter::blurTexture(unsigned int targetTexIndex, const glm::mat4 &amp;projectMat) &#123; m_framebuffer-&gt;bind(); glDepthMask(GL_TRUE); glEnable(GL_DEPTH_TEST); glClear(GL_DEPTH_BUFFER_BIT); Shader::ptr blurShader = ShaderMgr::getSingleton()-&gt;getShader(m_blurShaderIndex); blurShader-&gt;bind(); blurShader-&gt;setInt(\"image\", 0); TextureMgr::getSingleton()-&gt;bindTexture(targetTexIndex, 0); for (unsigned int index = 0; index &lt; 5; ++index) &#123; // horizontal blur. blurShader-&gt;setInt(\"horizontal\", 1); MeshMgr::getSingleton()-&gt;drawMesh(m_screenQuadIndex, false, 0); // copy to target texture. glCopyTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, 0, 0, m_width, m_height); // vertical blur. blurShader-&gt;setInt(\"horizontal\", 0); MeshMgr::getSingleton()-&gt;drawMesh(m_screenQuadIndex, false, 0); // copy to target texture. glCopyTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, 0, 0, m_width, m_height); &#125; TextureMgr::getSingleton()-&gt;unBindTexture(targetTexIndex); blurShader-&gt;unBind(); m_framebuffer-&gt;unBind(); return targetTexIndex; &#125;==============Fragment Shader==================#version 330 corein vec2 Texcoord;uniform int horizontal;uniform sampler2D image;const float weight[8] = float[] (0.197448, 0.174697, 0.120999, 0.065602, 0.02784, 0.009246, 0.002403, 0.000489);void main()&#123; // gets size of single texel. vec2 tex_offset = 1.0 / textureSize(image, 0); float result = texture(image, Texcoord).r * weight[0]; result += texture(image, Texcoord + vec2(tex_offset.x * 1 * horizontal, tex_offset.y * 1 * (1-horizontal))).r * weight[1]; result += texture(image, Texcoord - vec2(tex_offset.x * 1 * horizontal, tex_offset.y * 1 * (1-horizontal))).r * weight[1]; result += texture(image, Texcoord + vec2(tex_offset.x * 2 * horizontal, tex_offset.y * 2 * (1-horizontal))).r * weight[2]; result += texture(image, Texcoord - vec2(tex_offset.x * 2 * horizontal, tex_offset.y * 2 * (1-horizontal))).r * weight[2]; result += texture(image, Texcoord + vec2(tex_offset.x * 3 * horizontal, tex_offset.y * 3 * (1-horizontal))).r * weight[3]; result += texture(image, Texcoord - vec2(tex_offset.x * 3 * horizontal, tex_offset.y * 3 * (1-horizontal))).r * weight[3]; result += texture(image, Texcoord + vec2(tex_offset.x * 4 * horizontal, tex_offset.y * 4 * (1-horizontal))).r * weight[4]; result += texture(image, Texcoord - vec2(tex_offset.x * 4 * horizontal, tex_offset.y * 4 * (1-horizontal))).r * weight[4]; result += texture(image, Texcoord + vec2(tex_offset.x * 5 * horizontal, tex_offset.y * 5 * (1-horizontal))).r * weight[5]; result += texture(image, Texcoord - vec2(tex_offset.x * 5 * horizontal, tex_offset.y * 5 * (1-horizontal))).r * weight[5]; result += texture(image, Texcoord + vec2(tex_offset.x * 6 * horizontal, tex_offset.y * 6 * (1-horizontal))).r * weight[6]; result += texture(image, Texcoord - vec2(tex_offset.x * 6 * horizontal, tex_offset.y * 6 * (1-horizontal))).r * weight[6]; result += texture(image, Texcoord + vec2(tex_offset.x * 7 * horizontal, tex_offset.y * 7 * (1-horizontal))).r * weight[7]; result += texture(image, Texcoord - vec2(tex_offset.x * 7 * horizontal, tex_offset.y * 7 * (1-horizontal))).r * weight[7]; gl_FragDepth = result;&#125; &emsp;&emsp;经过高斯模糊之后的，重建得到的法线向量平滑了一下如下所示。 图7 高斯模糊平滑得到的法线 &emsp;&emsp;然后用高斯模糊平滑得到法线向量做光照着色，得到下面的效果。渲染得到的效果看起来明显比图6的果冻壮好很多了。 图8 高斯模糊-光照着色 &emsp;&emsp;然后高斯模糊的平滑方法存在一个问题，高斯模糊没有考虑深度值，它仅仅考虑当前像素到中心像素的距离。这就导致了高斯模糊会明显地模糊掉边界，使得边界与背景或较远处的流体看起来融合在一起了。如下图9所示，上面的这张图红框部分，流体的边界于后面的流体融合在了一起，看不出有一个边界在那里。下面这张图可以看到，流体是分开的，但是从上面的这个视角看起又是连在一起的。 图9 高斯滤波模糊掉了边界 3、采用双边滤波平滑深度贴图&emsp;&emsp;既然高斯模糊没有良好地保留边界，我们就选取一个能够保边去噪的滤波器，高斯双边滤波算法就是这样的一个滤波器。高斯模糊仅仅考虑了像素的空间分布，权重从中间向周边降低。而双边滤波则进一步考虑了图像的像素值，从而保证边缘部分不会被过滤掉。根据维基百科$^{[3]}$，一个双边滤波器定义为： I^{filtered}(x)=\\frac{1}{W_p}\\Sigma_{x_i\\in \\Omega}I(x_i)f_r(||I(x_i)-I(x)||)g_s(||x_i-x||) \\tag {8}&emsp;&emsp;其中，$I_{filtered}(x)$是过滤后的图像，$x$是被过滤的像素坐标，$\\Omega$是像素$x$的滤波核领域，$I(x)$是初始未被过滤的图像。然后$f_r$是域值权重函数，$g_r$是空间权重函数。而$W_p$是归一化因子，其计算方式如下： W_p=\\Sigma_{x_i\\in \\Omega}f_r(||I(x_i)-I(x)||)g_s(||x_i-x||) \\tag {9}&emsp;&emsp;可以看到$f_r$函数输入的是两个像素之间的差，而$g_s$输入的是两个像素坐标之间的差。考虑一个像素，其坐标为$(i,j)$，而其邻域像素的坐标为$(k,l)$，则计算与邻域像素$(k,l)$的滤波核函数为： w(i,j,k,l)=exp(-\\frac{(i-k)^2+(j-l)^2}{2\\sigma_d^2}-\\frac{||I(i,j)-I(k,l)||^2}{2\\sigma_r^2}) \\tag {10}&emsp;&emsp;公式$(10)$给出的核函数包含了$f_r$和$g_s$，其中左边部分就是$g_s$，而右边部分则为$f_r$。在片元着色器中，我实现的双边滤波函数如下所示。这是一个暴力的两重循环，因为图像的邻域是二维的。 1234567891011121314151617181920212223242526272829303132333435#version 330 corein vec2 Texcoord;uniform sampler2D image;uniform float filterRadius;const float blurScale = 0.05f;const float blurDepthFalloff = 500.0f;void main()&#123; // gets size of single texel. vec2 tex_offset = 1.0 / textureSize(image, 0); float sum = 0.0f; float wsum = 0.0f; float value = texture(image, Texcoord).r; for(float y = -filterRadius;y &lt;= filterRadius;y += 1.0f) &#123; for(float x = -filterRadius;x &lt;= filterRadius;x += 1.0f) &#123; float sample = texture(image, Texcoord + vec2(x, y) * tex_offset).r; // spatial domain. float r = length(vec2(x, y)) * blurScale; float w = exp(-r * r); // range domain. float r2 = (sample - value) * blurDepthFalloff; float g = exp(-r2 * r2); sum += sample * w * g; wsum += w * g; &#125; &#125; if(wsum &gt;= 0.0f) sum /= wsum; gl_FragDepth = sum;&#125; &emsp;&emsp;然后根据双边滤波平滑后的深度图重建的法线如下图10所示。 图10 双边滤波平滑深度重建的法线 &emsp;&emsp;由双边滤波平滑得到的法线做流体渲染如下图11，可以看到流体边界部分的保持得非常良好。 图11 双边滤波-保边良好 &emsp;&emsp;但是这里又出现了一个性能方面的问题。双边滤波并不能像高斯模糊一样，拆分成水平方向依次、垂直方向一次，这是因为双边滤波考虑了图像的像素内容。一个核直径为20的双边滤波，每个像素要处理$20\\times20=400$个邻域像素，这对于我的rtx2070显卡来说还好，但是在其他一些不那么好的显卡上会严重地拖慢渲染速度。Nvidia$^{[5]}$指出，可以强行将双边滤波分成两个pass，将$O(n^2)$的时间复杂度降到$O(n)$，一些失真可以勉强接受。然后我就尝试了将双边滤波分割成两个pass，水平方向和垂直方向各一次。效果如下图12所示，并不是非常好，在流体的一些边缘部分出现了拉伸的失真。 图12 双边分离滤波-边缘失真 4、采用曲率流平滑深度贴图&emsp;&emsp;由于双边滤波的不可分割性，Wladimir等人转向了另外一个不同的思路$^{[4]}$。我们的目标是寻转一个算法，能够平滑掉不同流体粒子之间的曲率突变，构建一个平滑、连续的流体表面。因此，一种思路就是最小化流体的曲率，这也跟流体的自然物理属性-流体表面张力相对应。我们称这个过程为曲率流（curvature flow）。 &emsp;&emsp;曲率流沿着表面法线方向扩展，其速度取决于表面平均曲率。在我们的这个流体渲染中，我们仅仅处理的是流体表面的深度。在一帧当中，视角固定了，我们同样可以通过沿着曲率修改深度值达到平滑的效果： \\frac{\\partial z}{\\partial t} = H \\tag {11}&emsp;&emsp;其中$t$是平滑时间步长，$z$就是我们的深度值，$H$就是流体表面的平均曲率，公式$(11)$意思是在每一次的平滑迭代中，深度值的变化梯度为流体表面的平均曲率。因此，我们首先要求流体表面的平均曲率，平均曲率的定义为表面单位法线的散度： 2H=\\nabla\\cdot \\overline n \\tag {12}&emsp;&emsp;给定屏幕空间的坐标以及深度值，视口宽高$V_x$、$V_y$，以及投影的x和y方向的焦距长（就是投影矩阵的mat[0][0]、mat[1][1]），我们得到摄像机空间顶点坐标与屏幕空间的x和y的关系： P(x,y)= \\left( \\begin{matrix} \\frac{\\frac{2x}{V_x}-1.0}{F_x}\\\\ \\frac{\\frac{2y}{V_y}-1.0}{F_y}\\\\ 1.0 \\end{matrix} \\right)z(x,y) = \\left( \\begin{matrix} W_x\\\\ W_y\\\\ 1.0 \\end{matrix} \\right)z(x,y) \\tag {13}&emsp;&emsp;然后法线向量由两个偏导叉乘得到： n(x,y)=\\frac{\\partial P}{\\partial x}\\times \\frac{\\partial P}{\\partial y} \\\\= \\left( \\begin{matrix} C_xz+W_x\\frac{\\partial z}{\\partial x}\\\\ W_y\\frac{\\partial z}{\\partial x}\\\\ \\frac{\\partial z}{\\partial x} \\end{matrix} \\right) \\times \\left( \\begin{matrix} W_x\\frac{\\partial z}{\\partial y}\\\\ C_yz+W_y\\frac{\\partial z}{\\partial y}\\\\ \\frac{\\partial z}{\\partial y} \\end{matrix} \\right)\\\\ \\approx \\left( \\begin{matrix} C_xz\\\\ 0\\\\ \\frac{\\partial z}{\\partial x} \\end{matrix} \\right) \\times \\left( \\begin{matrix} 0\\\\ C_yz\\\\ \\frac{\\partial z}{\\partial y} \\end{matrix} \\right) = \\left( \\begin{matrix} -C_y\\frac{\\partial z}{\\partial x}\\\\ -C_x\\frac{\\partial z}{\\partial y}\\\\ C_xC_yz \\end{matrix} \\right)z \\tag {14}&emsp;&emsp;其中$C_x=\\frac{2}{V_xF_x}$，$C_y=\\frac{2}{V_yF_y}$，公式$(14)$中我们忽略$W_x$和$W_y$是因为能够大大简化计算，且其贡献非常小。然后单位法线向量则为： \\overline n=\\frac{n(x,y)}{|n(x,y)|}=\\frac{(-C_y\\frac{\\partial z}{\\partial x},-C_x\\frac{\\partial z}{\\partial y},C_xC_yz)^T}{\\sqrt{D}} \\tag {15} D=C_y^2(\\frac{\\partial z}{\\partial x})^2+C_x^2(\\frac{\\partial z}{\\partial y})^2+C_x^2C_y^2z^2 \\tag {16}&emsp;&emsp;单位法线公式有了，现在回过头来看平均曲率的计算公式$(12)$，我们要求单位法线的散度。由于深度值z是关于屏幕空间坐标x和y的函数，因此$\\frac{\\partial \\overline n}{\\partial z}=0$，因为求偏导时x和y定为常数。故： 2H=\\frac{\\partial \\overline n_x}{\\partial x}+\\frac{\\partial \\overline n_y}{\\partial y} =\\frac{C_yE_x+C_xE_y}{D^{\\frac32}} \\tag {17} E_x=\\frac12\\frac{\\partial z}{\\partial x}\\frac{\\partial D}{\\partial x}-\\frac{\\partial^2z}{\\partial x^2}D \\tag {18} E_y=\\frac12\\frac{\\partial z}{\\partial y}\\frac{\\partial D}{\\partial y}-\\frac{\\partial^2z}{\\partial y^2}D \\tag {19}&emsp;&emsp;求得了平均曲率之后，我们就根据公式$(11)$采用简单的欧拉差分法修改深度值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#version 330 corein vec2 Texcoord;uniform float step;uniform sampler2D image;uniform mat4 projectMatrix;void main()&#123; float depth = texture(image, Texcoord).r; if(depth &gt;= 1.0f || depth &lt;= -1.0f) &#123; gl_FragDepth = depth; return; &#125; vec2 imageDim = textureSize(image, 0); vec2 texelSize = 1.0 / imageDim; // central differences. float depthRight = texture(image, Texcoord + vec2(texelSize.x, 0)).r; float depthLeft = texture(image, Texcoord - vec2(texelSize.x, 0)).r; float zdx = 0.5f * (depthRight - depthLeft); if(depthRight == 0.0f || depthLeft == 0.0f) zdx = 0.0f; float depthUp = texture(image, Texcoord + vec2(0, texelSize.y)).r; float depthDown = texture(image, Texcoord - vec2(0, texelSize.y)).r; float zdy = 0.5f * (depthUp - depthDown); if(depthUp == 0.0f || depthDown == 0.0f) zdy = 0.0f; float zdxx = depthRight + depthLeft - 2.0f * depth; float zdyy = depthUp + depthDown - 2.0f * depth; float depthFalloff = 0.00005f; if(abs(depth - depthRight) &gt; depthFalloff || abs(depth - depthLeft) &gt; depthFalloff) zdx = zdxx = 0.0f; if(abs(depth - depthDown) &gt; depthFalloff || abs(depth - depthUp) &gt; depthFalloff) zdy = zdyy = 0.0f; float Fx = projectMatrix[0][0]; float Fy = projectMatrix[1][1]; float Cx = -2.0f/(imageDim.x * Fx); float Cy = -2.0f/(imageDim.y * Fy); float D = Cy * Cy * zdx * zdx + Cx * Cx * zdy * zdy + Cx * Cx * Cy * Cy * depth; float Ex = 0.5f * zdx * dFdx(D) - zdxx * D; float Ey = 0.5f * zdy * dFdy(D) - zdyy * D; // curvature flow. float curvature = 0.5f * (Cy * Ex + Cx * Ey)/ pow(D, 1.5); if(curvature &gt; 1.0f) curvature = 1.0f; gl_FragDepth = depth + curvature * step;&#125; &emsp;&emsp;然后在CPU端设置迭代多次，注意平滑的时间步长不能设置得太长。 1234567891011121314151617181920212223242526272829unsigned int DepthCurvatureFlowBlurFilter::blurTexture(unsigned int targetTexIndex, const glm::mat4 &amp;projectMat)&#123; m_framebuffer-&gt;bind(); glDepthMask(GL_TRUE); glEnable(GL_DEPTH_TEST); glClear(GL_DEPTH_BUFFER_BIT); // blur. Shader::ptr blurShader = ShaderMgr::getSingleton()-&gt;getShader(m_blurShaderIndex); blurShader-&gt;bind(); blurShader-&gt;setInt(\"image\", 0); blurShader-&gt;setFloat(\"step\", 0.00070f); blurShader-&gt;setMat4(\"projectMatrix\", projectMat); TextureMgr::getSingleton()-&gt;bindTexture(targetTexIndex, 0); for (unsigned int iter = 0; iter &lt; m_iterations; ++iter) &#123; // blur. MeshMgr::getSingleton()-&gt;drawMesh(m_screenQuadIndex, false, 0); // copy to target texture. glCopyTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, 0, 0, m_width, m_height); &#125; TextureMgr::getSingleton()-&gt;unBindTexture(targetTexIndex); blurShader-&gt;unBind(); m_framebuffer-&gt;unBind(); return targetTexIndex;&#125; &emsp;&emsp;然而采用曲率流得方法我实现的效果并不是想象中的那么好，总体上不如采用双边滤波的方法，颗粒感较强。而且不知道为什么，流体的表面有一些抖动，看起来很奇怪，大概这就是买家秀跟买家秀的区别吧。 图13 采用曲率流的平滑效果 四、实现效果 参考资料：$[1]$ How to go from device coordinates back to worldspace in OpenGL $[2]$ W. E. Lorensen and H. E. Cline, “Marching cubes: A high resolution 3D surface construction algorithm,” in Proceedings of the 14th annual conference on Computer graphics and interactive techniques - SIGGRAPH ’87, 1987, pp. 163–169. $[3]$ Bilateral filter. From Wikipedia, the free encyclopedia $[4]$ W. J. van der Laan, S. Green, and M. Sainz, “Screen space fluid rendering with curvature flow” in Proceedings of the 2009 symposium on Interactive 3D graphics and games - I3D ’09, 2009, p. 91. $[5]$ Screen Space Fluid Rendering for Games - Nvidia","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/categories/Fluid-Simulation/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"https://yangwc.com/categories/Position-Based-Dynamics/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/tags/Fluid-Simulation/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"https://yangwc.com/tags/Position-Based-Dynamics/"}]},{"title":"流体模拟Fluid Simulation：基于CUDA的PBF实现","slug":"PBF_CUDA","date":"2019-06-26T02:54:32.694Z","updated":"2019-10-20T09:05:48.614Z","comments":true,"path":"2019/06/26/PBF_CUDA/","link":"","permalink":"https://yangwc.com/2019/06/26/PBF_CUDA/","excerpt":"继之前实现了CPU端的PBF算法之后，我去学了CUDA编程模型，本文就是关于用CUDA实现PBF算法在GPU上高速地模拟。","text":"继之前实现了CPU端的PBF算法之后，我去学了CUDA编程模型，本文就是关于用CUDA实现PBF算法在GPU上高速地模拟。 基于GPU的空间哈希 基于CUDA的PBF流体模拟 程序结果 参考资料 基于CUDA的PBF流体模拟 一、基于GPU的空间哈希&emsp;&emsp;在之前我们讨论了N-body的GPU并行实现，实际上这是一种比较极端的例子。在N-body系统中，每个粒子的都要与剩下的所有粒子计算两两之间的相互作用，这种相互作用通常有穿透力（如万有引力，称为体积力），无论远近、大小。但是在一些其他的物理模拟中，粒子只与周围邻近的粒子产生相互作用，如刚体、流体等，这时距离当前粒子比较远的其他粒子不会对当前的粒子产生任何的影响，因为刚体、流体之间的相互作用通常需要接触之后才产生（极端的例子不考虑，如万有引力，因为通常考虑的质量太小，万有引力几乎为0），所以我们没有必要采用N-body的方法逐个计算剩下的所有例子与当前粒子的物理作用，因为这会大大增加冗余计算。 &emsp;&emsp;对于流体、刚体、软体等的物理交互模拟，我们通常都是考虑的局部相互作用，这种局部相互作用要么就是接触才产生，要么就是随着距离的增大而迅速减小至消失。因此在这类的物理模拟中，我们需要采用一种快速的算法，该算法获取周围邻近的粒子，用以后续的物理模拟计算。这种算法就是空间哈希，在前面用CPU实现PBF时我们已经讨论过，它将整个空间做一个分割，每个粒子映射到一个空间数据结构，寻找邻域粒子时直接搜索周围空间的存储列表，算法复杂度只有$O(N)$。但是在GPU上实现该类算法稍微麻烦了点，因为GPU上的数据结构通常只有线性表，同时还要慎重考虑内存访问的开销等。 &emsp;&emsp;在GPU上实现该类算法有两种思路，先说说第一种方案。第一种方案是直接存储均匀分割的所有空间，为每个空间单元预先申请一个固定的大小的存储空间，这样需要提前申请固定大小的显卡内存空间，而且通常非常大，申请之后每个粒子计算哈希值索引，根据索引将其存储到对应的空间单元，由于流体和刚体通常是聚集的，因为这样将导致大部分的显卡内存空间都是空置状态，浪费了大量的显存空间，存储方式比较稀疏。 图1 方案一 &emsp;&emsp;如上图1的二维示例所示，方案一申请两个线性表，大小均为空间分割的分辨率，图中分割空间为$4\\times 4=16$个。一个线性表为Count，记录当前的空间单元存在的粒子数目（为了避免写冲突，必须调用CUDA的原子操作指令atomicAdd），另外一个线性表记录每个空间单元中的粒子索引。当然我们也可以采用两个pass将粒子存储到一个连续的空间中，充分利用存储空间，但这需要两个pass，第一个pass计算每个空间单元的粒子数，然后第二个pass采用前缀和的方法计算每个空间单元存储的粒子的起始地址，最后将粒子连续地存储到线性表中。这个方法和接下来提到的采用排序的方法非常类似。 &emsp;&emsp;第二种方法采用排序将所有的粒子存储到紧凑的连续空间，节省大量的显存开销。如下图2所示，首先我们根据粒子的位置计算粒子的对应的空间哈希值，在这里我们目前只是简单地将粒子对应的线性编号作为它的哈希值cell id，然后将cell id和particle id这对数据存储线性数组中，其中我们要存储particle id是因为我们计算当前的cell id的时是根据粒子的位置来确定的，在后面我们需要用到这个对应关系，相当于一个索引。根据上面的步骤我们就得到了一个cell id乱序的线性表，接下来我们就根据这个cell id对整个数组做一个并行排序，使得数组的顺序是以cell id的顺序来排列的。这个过程相当于一个基数排序，因为一开始数组是以particle id为序的。得到这个以cell id为序的数组之后，我们需要记录每个空间单元cell记录的粒子起始地址和结束地址。判断起始地址很简单，只需将当前的cell id与数组的前一个cell id做比较，若不相同，则说明当前的粒子地址是当前粒子所在空间单元cell的起始地址，而且是其前一个粒子所在空间单元cell的终止地址。这样就能准确地记录每个空间单元的粒子。 图2 方案二 &emsp;&emsp;这里我们采用方案二的做法。首先需要计算每个粒子的空间hash值，前面已经说过，我们直接采用粒子的所在空间单元的线性编号，注意防止越界访问内存。下面的kernel代码calcGridPosKernel计算粒子所在空间单元的三维编号，接着calcGridHashKernel根据这个三维编号计算一维编号，最后将粒子索引id和哈希值存入两个对应的数组。 123456789101112131415161718192021222324252627282930313233343536373839__device__int3 calcGridPosKernel(float3 p)&#123; int3 gridPos; gridPos.x = floor((p.x - params.m_worldOrigin.x) / params.m_cellSize.x); gridPos.y = floor((p.y - params.m_worldOrigin.y) / params.m_cellSize.y); gridPos.z = floor((p.z - params.m_worldOrigin.z) / params.m_cellSize.z); return gridPos;&#125;__device__unsigned int calcGridHashKernel(int3 gridPos)&#123; gridPos.x = gridPos.x &amp; (params.m_gridSize.x - 1); gridPos.y = gridPos.y &amp; (params.m_gridSize.y - 1); gridPos.z = gridPos.z &amp; (params.m_gridSize.z - 1); return gridPos.z * params.m_gridSize.x * params.m_gridSize.y + gridPos.y * params.m_gridSize.x + gridPos.x;&#125;__global__void calcParticlesHashKernel( unsigned int *gridParticleHash, unsigned int *gridParticleIndex, float4 *pos, unsigned int numParticles)&#123; unsigned int index = blockIdx.x * blockDim.x + threadIdx.x; if (index &gt;= numParticles) return; volatile float4 curPos = pos[index]; int3 gridPos = calcGridPosKernel(make_float3(curPos.x, curPos.y, curPos.z)); unsigned int hashValue = calcGridHashKernel(gridPos); gridParticleHash[index] = hashValue; gridParticleIndex[index] = index;&#125; &emsp;&emsp;紧接着，我们需要对前面计算得到的两个数组做一个key-value排序，就是根据key的顺序来排列。同样为了效率，需要使用并行排序。CUDA提供了一个thrust库，直接调用库中的sort_by_key方法帮我们省去了这一个比较繁琐的工作。 1234567891011void sortParticles( unsigned int *deviceGridParticleHash, unsigned int *deviceGridParticleIndex, unsigned int numParticles)&#123; thrust::sort_by_key( thrust::device_ptr&lt;unsigned int&gt;(deviceGridParticleHash), thrust::device_ptr&lt;unsigned int&gt;(deviceGridParticleHash + numParticles), thrust::device_ptr&lt;unsigned int&gt;(deviceGridParticleIndex));&#125; &emsp;&emsp;前面对cell id和particle id排序之后，我们需要对存储粒子位置和速度属性的数组做一个相应的调整，使得其顺序与前面拍好的顺序一一对应。与此同时，还需要计算每个空间单元cell的起始地址和终止地址。这里我们充分利用同一个线程块的共享内存，设线程数有$n$个，那么我们申请$n+1$个单位大小的共享内存，每个线程首先将自己对应的那个粒子哈希值存储到共享内存中，第一个线程还要将其前一个粒子对应的哈希值存储到该共享内存中，这样可以避免每个线程访问全局内存2次（共2n次共享内存的访问），访问全局内存数变为了n+1次。然后每个粒子线程将当前对应的哈希值与其前一个粒子哈希值做比较，若不相同，则表明当前线程的index是空间单元cell的起始地址，终止地址同理。具体过程看如下的代码。我们设置cellStart初始为0xffffffff来表示它是一个空的cell单元，即没有任何的粒子落到该空间单元cell中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980__global__void reoderDataAndFindCellRangeKernel( unsigned int *cellStart, // output: cell start index unsigned int *cellEnd, // output: cell end index float4 *sortedPos, // output: sorted positions float4 *sortedVel, // output: sorted velocities unsigned int *gridParticleHash, // input: sorted grid hashes unsigned int *gridParticleIndex, // input: sorted particle indices float4 *oldPos, // input: not sorted position array float4 *oldVel, // input: not sorted velocity array unsigned int numParticles)&#123; thread_block cta = this_thread_block(); extern __shared__ unsigned int sharedHash[]; unsigned int index = blockIdx.x * blockDim.x + threadIdx.x; unsigned int hashValue; if (index &lt; numParticles) &#123; hashValue = gridParticleHash[index]; sharedHash[threadIdx.x + 1] = hashValue; // first thread in block must load neighbor particle hash if (index &gt; 0 &amp;&amp; threadIdx.x == 0) sharedHash[0] = gridParticleHash[index - 1]; &#125; sync(cta); if (index &lt; numParticles) &#123; if (index == 0 || hashValue != sharedHash[threadIdx.x]) &#123; cellStart[hashValue] = index; if (index &gt; 0) cellEnd[sharedHash[threadIdx.x]] = index; &#125; if (index == numParticles - 1) cellEnd[hashValue] = index + 1; unsigned int sortedIndex = gridParticleIndex[index]; float4 pos = oldPos[sortedIndex]; float4 vel = oldVel[sortedIndex]; sortedPos[index] = pos; sortedVel[index] = vel; &#125;&#125;void reorderDataAndFindCellRange( unsigned int *cellStart, unsigned int *cellEnd, float *sortedPos, float *sortedVel, unsigned int *gridParticleHash, unsigned int *gridParticleIndex, float *oldPos, float *oldVel, unsigned int numParticles, unsigned int numCell)&#123; unsigned int numThreads, numBlocks; numThreads = 256; numBlocks = (numParticles % numThreads != 0) ? (numParticles / numThreads + 1) : (numParticles / numThreads); // set all cell to empty. cudaMemset(cellStart, 0xffffffff, numCell * sizeof(unsigned int)); unsigned int memSize = sizeof(unsigned int) * (numThreads + 1); reoderDataAndFindCellRangeKernel &lt;&lt; &lt;numBlocks, numThreads, memSize &gt;&gt; &gt; ( cellStart, cellEnd, (float4*)sortedPos, (float4*)sortedVel, gridParticleHash, gridParticleIndex, (float4*)oldPos, (float4*)oldVel, numParticles);&#125; &emsp;&emsp;以上就是基于GPU的空间哈希过程，经过以上的步骤，粒子被紧凑地存储到线性空间，后面做物理计算时能快速地得到邻域空间的粒子。下面代码是整个基于GPU的空间哈希调用代码。 123456789101112131415161718192021222324252627// calculate grid Hash.computeHash( m_deviceGridParticleHash, m_deviceGridParticleIndex, m_devicePos, m_params.m_numParticles);// sort particles based on hash value.sortParticles( m_deviceGridParticleHash, m_deviceGridParticleIndex, m_params.m_numParticles);// reorder particle arrays into sorted order// and find start index and end index of each cell.reorderDataAndFindCellRange( m_deviceCellStart, m_deviceCellEnd, m_deviceSortedPos, m_deviceSortedVel, m_deviceGridParticleHash, m_deviceGridParticleIndex, m_devicePos, m_deviceVel, m_params.m_numParticles, m_params.m_numGridCells); 二、基于CUDA的PBF流体模拟&emsp;&emsp;在之前我们实现了CPU的PBF流体模拟，受限于CPU的低并行度，我们只能实时模拟数量很少的流体粒子。为了能够快速模拟大量的粒子，我特意去学了CUDA，接下来就用CUDA实现之前讨论的PBF算法。暂时不用刚体粒子来实现流体碰撞边界。首先我们回顾一下之前提到的PBF（Position Based Fluid）算法，算法的伪代码如下所示。基于PBD的流体模拟算法大致可以分成几个部分：流体粒子对流、领域粒子搜索、不可压缩的压力约束投影、更新速度、涡轮修复、粘度计算以及最后的粒子位置更新。可以看到，第5行到第7行邻域粒子搜索的实现在本文的前面部分已经讨论过了，所以就不再赘述了。 \\begin{align} &1.\\ forall\\ \\ particles\\ \\ i\\ \\ do\\\\ &2.\\ \\ \\ \\ \\ apply\\ \\ force\\ \\ v_i\\leftarrow v_i+\\Delta tf_{ext}(x_i)\\\\ &3.\\ \\ \\ \\ \\ predict\\ \\ position\\ \\ x_i^*\\leftarrow x_i+\\Delta t v_i\\\\ &4.\\ endfor\\\\ &5.\\ forall\\ \\ particles\\ \\ i\\ \\ do\\\\ &6.\\ \\ \\ \\ \\ find\\ \\ neighboring\\ \\ particles\\ \\ N_i(x_i^*)\\\\ &7.\\ endfor\\\\ &8.\\ while\\ \\ iter\\ \\","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/categories/Fluid-Simulation/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"https://yangwc.com/categories/Position-Based-Dynamics/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/tags/Fluid-Simulation/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"https://yangwc.com/tags/Position-Based-Dynamics/"}]},{"title":"多体问题N-body：基于CUDA的快速N-body模拟","slug":"NbodySimulation","date":"2019-06-20T05:17:42.095Z","updated":"2019-06-26T02:51:59.924Z","comments":true,"path":"2019/06/20/NbodySimulation/","link":"","permalink":"https://yangwc.com/2019/06/20/NbodySimulation/","excerpt":"本篇文章主要是关于用cuda实现天体的N-body系统，以及粒子渲染的辉光效果。相关的完整代码请看这个链接。","text":"本篇文章主要是关于用cuda实现天体的N-body系统，以及粒子渲染的辉光效果。相关的完整代码请看这个链接。 天体的N-body系统 基于CUDA的N-body实现 粒子渲染-辉光特效 参考资料 基于CUDA的快速N-body模拟 &emsp;&emsp;N-body问题（或者说N体问题），是一个常见的物理模拟问题。在N-body系统中，每个粒子体都会与剩下的其他粒子产生交互作用（交互作用因具体问题而异），从而产生相应的物理现象。天体模拟就是一个非常经典的N-body系统，根据牛顿的万有引力定律，宇宙中的不同天体之间会产生相互作用的吸引力，吸引力根据两个天体之间的质量和距离的不同而各不相同，一个天体的运动轨迹最终取决于剩下的所有的天体对该天体的引力的合力。除了天体系统之外，N-body模拟在其他计算学科中也是常客。例如模拟蛋白质折叠现象就是通过计算N-body之间的静电和范德华力，**计算机图形学中的湍流流体的流动模拟和全局光照明计算都涉及到N-body问题的求解**。 &emsp;&emsp;这周主要学习CUDA（全称为Computer Unified Device Architecture），它是一个NVIDIA的GPU编程模型，搞图形学不免常要与GPU打交道，所以非常有必要学习这个统一的GPU编程框架。N-body问题是计算机图形学中物理模拟的常见问题，在此我采用CUDA实现了一个天体星系的N-body模拟系统，充分利用GPU的并行能力去加速N-body的巨额计算过程。下面这张图就是我实现的天体星系模拟效果。 图1 天体星系的N-body模拟 1、天体的N-body系统&emsp;&emsp;一种最简单的求解N-body的方法就是暴力法，被称为all-pairs法，它直接计算一个粒子体与剩下的所有粒子体的相互作用，对每一个粒子都做类似的处理，这样可以确保每个粒子体都与剩下的所有粒子体都产生交互作用，这种方法简单、暴力，但是算法的复杂度非常高，达到了$O(N^2)$量级，当模拟的N-body系统有$N=10000$个粒子时，算法就需要处理1亿次的相互作用计算。显然对于非常庞大的N-body系统，直接使用暴力法将非常耗时，因而通常不是简单地采用该算法。all-pairs法通常与一种基于长距离力的远场近似法结合，目前此类形式的算法包括Barnes-Hut法$^{[1]}$、快速多极法$^{[2]}$和粒子网格法$^{[3]}$等等。 &emsp;&emsp;上面提到的几种加速方法最耗时的部分依旧是all-pairs部分，因此这是一个非常关键的部分，如果能够加速这一部分，那么模拟的速度将大大提升。因此，目前我们只关注all-pairs算法部分，而且不是在CPU上实现该算法的串行，而是接用CUDA编程模型实行一个GPU并行的快速版本。接下来先介绍一下天体星系的N-body系统。 &emsp;&emsp;天体星系模拟主要考虑的是万有引力。给定$N$个天体，我们记每个天体$0\\leq i&lt;N$的位置向量为$x_i$、速度向量为$v_i$、质量为$m_i$，根据牛顿的万有引力定律，任意两个不同天体$i$和$j$之间的万有引力计算公式如下所示： f_{ij}=G\\frac{m_im_j}{||r_{ij}||^2}\\cdot \\frac{r_{ij}}{||r_{ij}||} \\tag {1}&emsp;&emsp;公式$(1)$中的$r_{ij}=x_j-x_i$为从天体$i$到$j$的一个方向向量，故其模长为两者之间的距离。$G$是万有引力常数。上面这个公式可能看起来跟我们高中时学的万有引力公式略有不同，这是因为高中时还没有将力是一个矢量这个概念显示地表达出来，公式$(1)$中的$\\frac{r_{ij}}{||r_{ij}||}$是一个从$i$到$j$的单位方向向量。上述公式描述的是天体$j$对天体$i$的引力，那么除$i$之外的所有天体对天体$i$的引力合力的计算公式为： F_i=\\Sigma_{0\\leq j","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"N-body","slug":"N-body","permalink":"https://yangwc.com/categories/N-body/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"N-body","slug":"N-body","permalink":"https://yangwc.com/tags/N-body/"},{"name":"Glow effect","slug":"Glow-effect","permalink":"https://yangwc.com/tags/Glow-effect/"}]},{"title":"体素化Voxelization：基于GPU的三维体素化","slug":"Voxelization","date":"2019-06-11T09:28:14.040Z","updated":"2020-06-17T06:05:02.781Z","comments":true,"path":"2019/06/11/Voxelization/","link":"","permalink":"https://yangwc.com/2019/06/11/Voxelization/","excerpt":"本篇文章主要是关于三维网格模型的基于GPU并行的体素化算法，这个算法我是偶然从NVIDIA官网上看到的。基于GPU的体素化算法巧妙地借助了渲染流程的光栅化处理，将整个体素化的过程并行化，速度极快，缺点就是占用的内存较高。相关的完整代码请看这个链接中的Renderer目录下的Voxelization.h文件和Voxelization.cpp文件。","text":"本篇文章主要是关于三维网格模型的基于GPU并行的体素化算法，这个算法我是偶然从NVIDIA官网上看到的。基于GPU的体素化算法巧妙地借助了渲染流程的光栅化处理，将整个体素化的过程并行化，速度极快，缺点就是占用的内存较高。相关的完整代码请看这个链接中的Renderer目录下的Voxelization.h文件和Voxelization.cpp文件。 体素化 修补裂缝 修补孔洞 参考资料 &emsp;&emsp;在基于位置动力学的物理模拟中，所有要模拟的物体都由一组粒子来表示，每个粒子都是一个给定半径大小的球体，对于固体这类的物体，粒子通常是紧密相连的。为此，为了实现基于位置动力学的物理模拟，我们需要采用一种算法将网格物体的三角网格模型用一个个粒子表示，这个并不是简单地取网格模型的所有顶点就行，因为我们需要紧密连接的粒子，面片网格模型的顶点通常是稀疏的。这个过程其实就是体素化，三维体素是二维像素的三维扩展，体素的基本单元不再是二维的正方形，而是三维的立方体，立方体的边长决定了体素化的分辨率，通常边长越长，则分辨率越低。将网格体素化后我们得到了一组体素的中心顶点位置，可将其用于后续的基于位置动力学的物理模拟当中。 &emsp;&emsp;目前常用的体素化方法大都是基于CPU的，这类方法通常是将射线与物体求交，根据是奇数个交点还是偶数个交点来判断当前的体素是否在物体的内部。在没有采用特殊的数据结构时，每次求交都要遍历一次网格模型的所有三角形，效率非常低。在采用了八叉树加速之后，速度有所提升，但随着模型的三角形面片数增加，串行的体素化算法耗费的时间越来越长。我没有采用CPU串行的体素化方法，而是采用了基于GPU并行的体素化算法，这个算法我是偶然从NVIDIA官网上看到的。基于GPU的体素化算法巧妙地借助了渲染流程的光栅化处理，将整个体素化的过程并行化，速度极快，缺点就是占用的内存较高。 图1 三维体素模型 一、体素化&emsp;&emsp;基于GPU的三维体素化大致思想就是：首先计算出需要体素化模型的AABB包围盒，然后将模型投影到AABB包围盒的某个平面上，经过渲染管线的光栅化插值操作，我们可以在片元着色器得到每个像素点对应的世界空间的顶点坐标，根据这个顶点坐标标记三维空间数组（这个三维空间数组就是根据体素划分的空间序列）的相应位置，最后在CPU端读出这个三维空间数组，若当前的数组位置有标记，则将该数组位置对应的立方体作为一个体素。可以看到，整个流程思路非常清晰，但是还需要借助一些手段修正算法存在的缺陷，这个在后面会提到。 &emsp;&emsp;首先就是计算网格模型的AABB包围盒，在导入模型时获取$x$、$y$、$z$轴分量的最大值和最小值，从而得到包围盒的最大顶点和最小顶点。这个比较简单，不再赘述： 12345678910111213// bounding box.if (mesh-&gt;mVertices[x].x &lt; m_min.x) m_min.x = mesh-&gt;mVertices[x].x;if (mesh-&gt;mVertices[x].y &lt; m_min.y) m_min.y = mesh-&gt;mVertices[x].y;if (mesh-&gt;mVertices[x].z &lt; m_min.z) m_min.z = mesh-&gt;mVertices[x].z;if (mesh-&gt;mVertices[x].x &gt; m_max.x) m_max.x = mesh-&gt;mVertices[x].x;if (mesh-&gt;mVertices[x].y &gt; m_max.y) m_max.y = mesh-&gt;mVertices[x].y;if (mesh-&gt;mVertices[x].z &gt; m_max.z) m_max.z = mesh-&gt;mVertices[x].z; 图2 模型包围盒 &emsp;&emsp;获取了模型的包围盒之后，我们就需要根据这个包围盒设置我们的观察角度和投影平面，这关系到后面的体素化结果。同时为了保证正确地体素化模型，我们采用的投影方式是正交投影。首先我们要选择一个观察方向和投影平面，AABB包围盒有六个面，其中前和后、上和下、左和右的投影结果是一样的，因此实际的选择只有三个平面，分别是前、上、右（或者后、下、左）。显然一个物体投影到这个三个平面上的结果都不一样，目前我们暂时先选择投影到前面这个平面上，摄像机的视线朝向z轴的负方向。注意正确地设置摄像机的位置，否则什么看不到。既然我们选择投影到前面这个平面上，我们就设置摄像机的位置在包围盒前面这个平面的中心再往前一点。同时了为了确保模型全部投影到屏幕上，我们设置的正交投影平面比选定的包围盒平面稍微大一点点。具体代码如下所示： 图3 三个面上的投影结果 12345678910111213141516171819// bounding box and resolution.glm::vec3 min, max;glm::ivec3 resolution;target-&gt;getAABB(min, max);glm::vec3 range(max.x - min.x, max.y - min.y, max.z - min.z);resolution.x = static_cast&lt;int&gt;(range.x / step);resolution.y = static_cast&lt;int&gt;(range.y / step);resolution.z = static_cast&lt;int&gt;(range.z / step);int length = static_cast&lt;int&gt;(resolution.x * resolution.y * resolution.z);// cameraglm::vec3 cameraPos;cameraPos.x = (min.x + max.x) * 0.5f;cameraPos.y = (min.y + max.y) * 0.5f;cameraPos.z = max.z + 0.2f;FPSCamera::ptr camera(new FPSCamera(cameraPos));camera-&gt;lookAt(glm::vec3(0.0f, 0.0f, -1.0f));camera-&gt;setOrthographicProject(-range.x * 0.51, +range.x * 0.51, -range.y * 0.51, +range.y * 0.51, 0.1, range.z * 1.2f + 0.2f); 图4 包围盒投影平面 &emsp;&emsp;设置好投影矩阵和视图矩阵之后，我们需要申请一个着色器可写的缓冲，这个缓冲的大小等于AABB包围盒的分辨率，在片元着色器阶段我们需要根据当前片元的世界空间位置对这个缓冲做标记，表示该缓冲位置上有一个体素。我们采用OpenGL的GL_SHADER_STORAGE_BUFFER，这是一个着色器可读写的缓冲类型。申请缓冲之后，将缓冲全部初始化为0。然后将需要体素化的网格模型送入渲染管线进行渲染。在片元着色器中，将每个片元的世界空间位置对应的缓冲位置加1。最后在CPU端读出缓冲内容，缓冲值大于0时，则表示该位置有一个体素。CPU端的整个流程代码如下所示。这里需要特别注意的是，我们应该关闭深度测试和背面剔除，保证模型的全面三角形都进入片元着色器，确保所有的三角形不被剔除，从而使得全部的三角形都被处理，最后得到正确的体素化结果。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788bool Voxelization::voxelize(Drawable* target, const float &amp; step, std::vector&lt;glm::vec3&gt;&amp; ret)&#123; // shader ShaderMgr::ptr shaderMgr = ShaderMgr::getSingleton(); unsigned int voxelizeCount = shaderMgr-&gt;loadShader(\"voxelizeCount\", \"./glsl/voxelizeCount.vert\", \"./glsl/voxelizeCount.frag\"); Shader::ptr shader = shaderMgr-&gt;getShader(voxelizeCount); // bounding box and resolution. glm::vec3 min, max; glm::ivec3 resolution; target-&gt;getAABB(min, max); glm::vec3 range(max.x - min.x, max.y - min.y, max.z - min.z); resolution.x = static_cast&lt;int&gt;(range.x / step) + 1; resolution.y = static_cast&lt;int&gt;(range.y / step) + 1; resolution.z = static_cast&lt;int&gt;(range.z / step) + 1; int length = static_cast&lt;int&gt;(resolution.x * resolution.y * resolution.z); // camera glm::vec3 cameraPos; cameraPos.x = (min.x + max.x) * 0.5f; cameraPos.y = (min.y + max.y) * 0.5f; cameraPos.z = max.z + 0.2f; FPSCamera::ptr camera(new FPSCamera(cameraPos)); camera-&gt;lookAt(glm::vec3(0.0f, 0.0f, -1.0f)); camera-&gt;setOrthographicProject(-range.x * 0.51, +range.x * 0.51, -range.y * 0.51, +range.y * 0.51, 0.1, range.z * 1.2f + 0.2f); // polygon mode. glPolygonMode(GL_FRONT_AND_BACK, GL_FILL); glDisable(GL_CULL_FACE); glDisable(GL_DEPTH_TEST); glClearColor(1.0f, 0.0f, 0.0f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); // generate ssbo. glGenBuffers(1, &amp;m_cntBuffer); glBindBuffer(GL_SHADER_STORAGE_BUFFER, m_cntBuffer); glBufferData(GL_SHADER_STORAGE_BUFFER, length * sizeof(int), nullptr, GL_STATIC_DRAW); glBindBufferBase(GL_SHADER_STORAGE_BUFFER, 0, m_cntBuffer); // bind shader and ssbo. shader-&gt;bind(); shader-&gt;setVec3(\"boxMin\", min); shader-&gt;setFloat(\"step\", step); shader-&gt;setVec3(\"resolution\", resolution); int *writePtr = reinterpret_cast&lt;int*&gt;(glMapBuffer(GL_SHADER_STORAGE_BUFFER, GL_WRITE_ONLY)); for (int x = 0; x &lt; length; ++x) &#123; writePtr[x] = 0; &#125; if (!glUnmapBuffer(GL_SHADER_STORAGE_BUFFER)) std::cout &lt;&lt; \"unMap error\\n\" &lt;&lt; std::endl; // draw and count. target-&gt;render(camera, nullptr, nullptr, shader); glMemoryBarrier(GL_SHADER_STORAGE_BARRIER_BIT); // get count buffer. glBindBuffer(GL_SHADER_STORAGE_BUFFER, m_cntBuffer); int *readPtr = reinterpret_cast&lt;int*&gt;(glMapBuffer(GL_SHADER_STORAGE_BUFFER, GL_READ_ONLY)); if (readPtr != nullptr) &#123; for (int x = 0; x &lt; length; ++x) &#123; if (*(readPtr + x) != 0) &#123; int iy = x / (resolution.x * resolution.z); int iz = (x - iy * resolution.x * resolution.z) / (resolution.x); int ix = x - iy * resolution.x * resolution.z - iz * resolution.x; ret.push_back(min + glm::vec3(ix * step, iy * step, iz * step)); &#125; &#125; &#125; else &#123; std::cout &lt;&lt; \"nullptr error!\\n\"; &#125; glUnmapBuffer(m_cntBuffer); glBindBuffer(GL_SHADER_STORAGE_BUFFER, 0); glDeleteBuffers(1, &amp;m_cntBuffer); return false;&#125; &emsp;&emsp;接下来就需要在着色器中做一些操作。首先是顶点着色器，在顶点着色器中并没有什么复杂的操作，我们需要将当前的顶点位置传到片元着色器，借助渲染管线的光栅化功能，从而在片元着色器中得到每个片元对应的世界空间位置。下面顶点着色器的代码，其余部分乘上视图矩阵和投影矩阵就不说了。 123456789101112#version 430 corelayout (location = 0) in vec3 position;out vec3 FragPos;uniform mat4 viewMatrix;uniform mat4 projectMatrix;void main()&#123; FragPos = position; gl_Position = projectMatrix * viewMatrix * vec4(position,1.0f);&#125; &emsp;&emsp;中间经过光栅化处理，我们在片元着色器得到每个片元的世界空间坐标。根据这个世界空间的坐标去索引计数缓冲，注意这里采用了GLSL的原子操作函数atmoicAdd，避免GPU线程之间的写冲突。缓冲下标索引的计算基本就是根据体素的大小和包围盒来确定。 12345678910111213141516171819202122#version 430 corein vec3 FragPos;layout (std430, binding = 0) buffer CountBuffer&#123; int cnts[];&#125;;uniform float step;uniform vec3 boxMin;uniform vec3 resolution;out vec4 color;void main()&#123; int x = int((FragPos.x - boxMin.x)/step); int y = int((FragPos.y - boxMin.y)/step); int z = int((FragPos.z - boxMin.z)/step); int index = int(y * (resolution.z * resolution.x) + z * resolution.x + x); atomicAdd(cnts[index], 1); color = vec4(0.0,1.0,0.0,1.0);&#125; &emsp;&emsp;然后下面就是我实现的体素化效果，每个体素用一个立方体绘制，当然也可以用球体绘制。看起来颇有游戏《我的世界》的风格。 二、修补裂缝&emsp;&emsp;上面的实现效果看起来貌似非常不错，但是却存在一个非常严重的问题。前面我们在选择投影平面的时候固定投影在了z轴方向的包围盒平面，这是问题产生的根源。因为模型的每个三角形面片在每个包围盒投影面上的投影结果都不同，若当前的三角形与选取的投影面垂直，那么三角形投影到平面上的将是一条直线，这丢失了很多信息，从而导致裂缝的产生。 图5 不同投影平面的体素化结果 &emsp;&emsp;图5中，左图选取的投影面是摄像机在右边，朝向坐标，这时光栅化得到的结果很好，因而体素化的结果也很好。但是右边的这张图选取的投影面是摄像机在上面，朝向下边，这时光栅化得到的几何面片较少，很多相邻的位置都被投影到了一个片元像素，一些地方没有被体素化，从而导致了裂缝的产生！下面是我实现的程序产生的裂缝，选取的投影方向是z轴方向，下图中的红框部分的几何面片几乎平行于xz平面，从而导致投影光栅化产生的是一个被“压缩“的结果。由于裂缝非常明显且几乎必然会产生（因为通常模型都很复杂，三角形面片朝向很随机），因此有必要采取一些措施来修补这些裂缝。 图6 根据前面步骤产生的裂缝 &emsp;&emsp;如前面的图3所示，每个三角形面片在不同包围盒投影面上的投影结果不同，根据三角形的朝向不同，投影到平面上的三角形大小也各不相同。裂缝产生的原因就是因为投影到平面上的三角形面积被”压缩“了，因此我们需要选取一个投影方向，在该投影方向上三角形的投影面积最大，这样就能够确保所有的三角形面片被充分地体素化，从而使得裂缝小时。 图7 分别投影到包围盒的右、上、前平面上 &emsp;&emsp;因此，我们首先创建三个投影摄像机，将物体分别投影到沿着$x$、$y$、$z$轴的平面上，如图7所示，用以后面着色器中根据三角形的投影面积做选择。代码如下： 1234567891011121314151617181920212223242526272829303132// Camerasfloat offset = 0.2f;glm::vec3 cameraPosZ, cameraPosX, cameraPosY;// looking along z axis.cameraPosZ.x = (min.x + max.x) * 0.5f;cameraPosZ.y = (min.y + max.y) * 0.5f;cameraPosZ.z = max.z + offset;FPSCamera::ptr cameraZ(new FPSCamera(cameraPosZ));cameraZ-&gt;lookAt(glm::vec3(0.0f, 0.0f, -1.0f), Camera3D::LocalUp);cameraZ-&gt;setOrthographicProject(-range.x * 0.51, +range.x * 0.51, -range.y * 0.51, +range.y * 0.51, 0.1, range.z * 1.2f + offset);// looking along x axis.cameraPosX.x = max.x + offset;cameraPosX.y = (min.y + max.y) * 0.5f;cameraPosX.z = (min.z + max.z) * 0.5f;FPSCamera::ptr cameraX(new FPSCamera(cameraPosX));cameraX-&gt;lookAt(glm::vec3(-1.0f, 0.0f, 0.0f), Camera3D::LocalUp);cameraX-&gt;setOrthographicProject(-range.z * 0.51, +range.z * 0.51, -range.y * 0.51, +range.y * 0.51, 0.1, range.x * 1.2f + offset);// looking along y axis.cameraPosY.x = (min.x + max.x) * 0.5f;cameraPosY.y = max.y + offset;cameraPosY.z = (min.z + max.z) * 0.5f;FPSCamera::ptr cameraY(new FPSCamera(cameraPosY));cameraY-&gt;lookAt(glm::vec3(0.0f, -1.0f, 0.0f), glm::vec3(0, 1.0, 0.001));cameraY-&gt;setOrthographicProject(-range.x * 0.51, +range.x * 0.51, -range.z * 0.51, +range.z * 0.51, 0.1, range.y * 1.2f + offset);......shader-&gt;setMat4(\"viewProject[0]\", cameraX-&gt;getProjectMatrix() * cameraX-&gt;getViewMatrix());shader-&gt;setMat4(\"viewProject[1]\", cameraY-&gt;getProjectMatrix() * cameraY-&gt;getViewMatrix());shader-&gt;setMat4(\"viewProject[2]\", cameraZ-&gt;getProjectMatrix() * cameraZ-&gt;getViewMatrix()); &emsp;&emsp;接下来我们将用到几何着色器，几何着色器阶段在顶点着色器之后、光栅化之前，它根据给定的输入图元和输出图元进行相关的几何图元操作，正好我们可以接用它来根据三角形的投影面积选择采用哪一个投影相机。这里有一个技巧，直观上我们说是根据三角形的投影面积来渲染采用哪个投影相机，实际上没有必要真正地去计算三角形的投影面积，我们可以直接根据当前三角形的世界空间法线朝向来决定投影方向。举个例子，当法线向量的x分量比其余两个分量大时，则当前的三角形肯定投影到x轴方向的投影平面上的面积更大。更深入的理解：设法线向量为$n=(nx,ny,nz)$，我们将法线向量$n$与$(1,0,0)$、$(0,1,0)$、$(0,0,1)$分别做点乘，结果为$nx$、$ny$、$nz$，而法线向量分别与该三个基向量点乘的意义为法线向量在$x$、$y$、$z$轴上的投影值，该值越大则三角形投影到该平面上的面积也越大。所以，我们直接根据最大的法线分量来选择采用哪个投影相机。如下所示： 12345678910111213141516171819202122uint selectViewProject()&#123; vec3 p1 = gl_in[1].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 p2 = gl_in[2].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 faceNormal = cross(p1, p2); float nDX = abs(faceNormal.x); float nDY = abs(faceNormal.y); float nDZ = abs(faceNormal.z); if( nDX &gt; nDY &amp;&amp; nDX &gt; nDZ ) &#123; return 0; &#125; else if( nDY &gt; nDX &amp;&amp; nDY &gt; nDZ ) &#123; return 1; &#125; else &#123; return 2; &#125;&#125; &emsp;&emsp;然后我们将上述的代码应用到我们的几何着色器中，因为视图投影过程挪到了几何着色器阶段，所以顶点着色器直接输入顶点的位置，不做任何变换。几何着色器设置输入图元为三角形，输出图元为最大顶点数为3的三角形带，设置一个viewProject的uniform数组。通过几何着色器，我们对模型的每个三角形面片都做了一个投影选择的处理。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// vertex shader#version 430 corelayout (location = 0) in vec3 position;void main()&#123; gl_Position = vec4(position, 1.0f);&#125;// geometry shader#version 430 corelayout (triangles) in;layout (triangle_strip, max_vertices = 3) out;out vec3 FragPos;uniform mat4 viewProject[3];uint selectViewProject()&#123; vec3 p1 = gl_in[1].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 p2 = gl_in[2].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 faceNormal = cross(p1, p2); float nDX = abs(faceNormal.x); float nDY = abs(faceNormal.y); float nDZ = abs(faceNormal.z); if( nDX &gt; nDY &amp;&amp; nDX &gt; nDZ ) &#123; return 0; &#125; else if( nDY &gt; nDX &amp;&amp; nDY &gt; nDZ ) &#123; return 1; &#125; else &#123; return 2; &#125;&#125; void main() &#123; uint projectIndex = selectViewProject(); FragPos = gl_in[0].gl_Position.xyz; gl_Position = viewProject[projectIndex] * gl_in[0].gl_Position; EmitVertex(); FragPos = gl_in[1].gl_Position.xyz; gl_Position = viewProject[projectIndex] * gl_in[1].gl_Position; EmitVertex(); FragPos = gl_in[2].gl_Position.xyz; gl_Position = viewProject[projectIndex] * gl_in[2].gl_Position; EmitVertex(); EndPrimitive();&#125; &emsp;&emsp;最终，我们成功的修补了体素的裂缝，如下图所示，先前的裂缝已经填上了体素。 图8 成功修补裂缝 三、修补孔洞&emsp;&emsp;然而，通过前面2部分的处理，另外一个问题出现了。由于模型的每个三角形都是各自根据在每个平面上的投影面积来选择投影相机，这意味着两个相邻的三角形片面可能选取了不同投影相机，使得三角形面片之间因为体素化投影平面的不同而产生过渡问题，从而出现孔洞，即有些部分没有被体素化到。如下图9所示。 图9 体素孔洞 &emsp;&emsp;孔洞的产生根源于光栅化处理，一个像素是否作为当前图元的光栅片元，是通过判断当前图元是否覆盖了该像素中心来完成的。对于那些没有覆盖像素中心的片元，不作为该图元的光栅片元送入片元着色器做进一步的处理，因而模型的一些部分可能会被丢失，从而造成孔洞。为了解决这个问题，我们将在几何着色器中实现一种被称为保守光栅化（Conservative Rasterization）的算法，依旧在几何着色器中实现。 &emsp;&emsp;通常的硬件光栅化，都是默认只取那些中心被图元覆盖的像素单元。而保守光栅化则将所有被图元覆盖（无论是否覆盖到像素单元的中心点）的像素单元都作为光栅化的片元，从而确保图元覆盖的所有区域都被光栅化，故名思意，这就是“保守”一词的由来。如下图10所示，通常情况下硬件默认的光栅片元是绿色部分，边缘红色部分的片元没有被光栅化，导致我们的体素化结果出现孔洞。为了修补体素化的孔洞，我们必须使得被图元哪怕一点点覆盖到的像素（就是下图中的红色部分）都作为当前图元的光栅化结果，这个过程就是保守光栅化算法。 图10 保守光栅化 &emsp;&emsp;那么怎么实现保守光栅化算法，使得上面的红色部分也被光栅化到呢？一个简单直观的思路就是手动扩充三角形图元面片。如上图10所示，里面的三角形是最初的我们要光栅化的三角形，为了使得边缘红色的像素也包含进来，我们扩张最初的三角形得到外面的那个三角形，这个三角形比原来的三角形稍微大一点，此时若将该扩大的三角形送入硬件默认的光栅化单元进行处理，则红色像素也被当作光栅片元，从而达到了我们的目的。注意，这里三角形的扩大程度非常关键，上面的扩大的三角形将我们不需要的像素单元也包含了进来，即黄色像素部分，我们将通过计算三角形的包围盒来剔除那些黄色像素单元，剔除像素部分我们将在片元着色器中实现。下图11是我实现的保守光栅化（图右）效果，图左是默认光栅化的效果。 图11 默认的光栅化和保守光栅化对比 &emsp;&emsp;扩大三角形和剔除像素整个过程都是在裁剪空间中进行的，也就是经过摄像机空间变换和投影变换之后。故而三角形的包围盒只需二维即可，然后需要适当地扩大一点，以免剔除红色的像素片元。一个裁剪空间的三角形包围盒计算如下所示，我们采用GLSL的vec4存储包围盒的最小顶点和最大顶点。 12345678910vec4 calcAABB(vec4 pos[3], vec2 pixelDiagonal)&#123; vec4 aabb; aabb.xy = min(pos[2].xy, min(pos[1].xy, pos[0].xy)); aabb.zw = max(pos[2].xy, max(pos[1].xy, pos[0].xy)); // enlarge by half-pixel aabb.xy -= pixelDiagonal; aabb.zw += pixelDiagonal; return aabb;&#125; &emsp;&emsp;接下来对于给定的三角形的三个顶点，我们要适当地扩大三角形。总体的思路就是：首先计算三角形的三条边与原点构成的齐次空间的平面，然后适当挪动这三个平面，接着就计算偏移后的这三个齐次平面的交线，最后计算三条交线与三角形平面的交点，从而得到扩大后的三角形的三个顶点。整个计算过程都是在裁剪空间中进行的，所以我们忽略顶点的$z$分量，但是上面又提到了齐次平面一词，我们采用一个齐次平面来描述三角形边的线段。所谓齐次平面，就是我们把顶点的齐次分量$w$和$x$、$y$分量合并一起来表示一条线段，直观来看，这就是一个齐次空间的平面，但实际上就是一段二维空间的直线。如下所示： Ax_c+By_c+Cw_c=0 \\tag {1}&emsp;&emsp;公式$(1)$就是一个齐次空间的过原点的平面方程，但是它实际上就是一个二维空间的直线方程。这是因为我们采用的都是正交投影，正交投影并没有透视除法之类的处理，因为正交投影都是线性变换，故而$w_c=1$，所以公式$(1)$表示的过原点的齐次空间的平面方程就是如下所示的二维直线方程： Ax_c+By_c+C=0 \\tag {2}&emsp;&emsp;之所以采用齐次空间的平面方程，是为了方便我们的计算。首先我们根据三角形的三条边计算三个齐次空间的平面，我们已知该齐次空间的平面过原点，平面方程的$(A,B,C)$就是该平面的法线向量，我们直接做叉乘计算可得平面的法线，如下所示： 1234vec3 edgePlanes[3];edgePlanes[0] = cross(pos[0].xyw - pos[2].xyw, pos[2].xyw);edgePlanes[1] = cross(pos[1].xyw - pos[0].xyw, pos[0].xyw);edgePlanes[2] = cross(pos[2].xyw - pos[1].xyw, pos[1].xyw); &emsp;&emsp;然后对这三个平面分别进行偏移。直观上来说，我们分别令三角形的三条边在其法线的方向上挪一段距离，这个距离由像素单元格的大小（即下面的halfPixel）在法线方向的投影决定，如下所示： 123edgePlanes[0].z -= dot(halfPixel[projectIndex], abs(edgePlanes[0].xy));edgePlanes[1].z -= dot(halfPixel[projectIndex], abs(edgePlanes[1].xy)); edgePlanes[2].z -= dot(halfPixel[projectIndex], abs(edgePlanes[2].xy)); &emsp;&emsp;接着计算三个齐次平面的交线向量，这个不难理解，两个平面的交线必然垂直于这两个平面的法线向量，因而交线向量可由这两个平面的法线向量做叉乘得到： 1234567vec3 intersection[3];intersection[0] = cross(edgePlanes[0], edgePlanes[1]);intersection[1] = cross(edgePlanes[1], edgePlanes[2]);intersection[2] = cross(edgePlanes[2], edgePlanes[0]);intersection[0] /= intersection[0].z;intersection[1] /= intersection[1].z;intersection[2] /= intersection[2].z; &emsp;&emsp;最后我们根据上面的三条射线向量与初试三角形所在的平面求交点，从而得到最终扩大后的三角形的三个顶点。由于我们是正交投影，所以上面求到的三条射线向量的$x$分量和$y$分量就是扩大三角形顶点的$x$分量和$y$分量，即交点的$x$、$y$已知，需要求$z$值。一个三维平面方程如下所示，从直观的几何意义上来说，$(A,B,C)$就是平面的法线向量，$D$就是原点到平面的直线距离。 Ax+By+Cz+D=0 \\tag {3}&emsp;&emsp;已知初始三角形的三个点，我们可以求出它的法线向量，然后原点到平面的直线距离就等于平面上的点在法线向量方向上的投影长度，这里要特别注意符号，具体看下面的代码： 123vec4 trianglePlane;trianglePlane.xyz = normalize(cross(pos[1].xyz - pos[0].xyz, pos[2].xyz-pos[0].xyz));trianglePlane.w = -dot(pos[0].xyz, trianglePlane.xyz); &emsp;&emsp;然后还需要提一点的是，我们要确保输入的三角形的顶点环绕顺序都是逆时针方向，这个逆时针方向是针对当前的相机投影方向。对于背向的面片，我们要做一个纠正的过程。判断是否是背向面片很简单，只需通过计算三角形法线向量与$(0,0,1)$做点乘，判断其符号即可。 1234567// change winding, otherwise there are artifacts for the back faces.if (dot(trianglePlane.xyz, vec3(0.0, 0.0, 1.0)) &lt; 0.0)&#123; vec4 vertexTemp = pos[2]; pos[2] = pos[1]; pos[1] = vertexTemp;&#125; &emsp;&emsp;已知交点的$x$和$y$，我们代入平面方程$(3)$求得$z$值。 z=-\\frac{Ax+By+D}{C} \\tag {4}12345678// calculate dilated triangle verticesfloat z[3];z[0] = -(intersection[0].x * trianglePlane.x + intersection[0].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z;z[1] = -(intersection[1].x * trianglePlane.x + intersection[1].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z;z[2] = -(intersection[2].x * trianglePlane.x + intersection[2].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z;pos[0].xyz = vec3(intersection[0].xy, z[0]);pos[1].xyz = vec3(intersection[1].xy, z[1]);pos[2].xyz = vec3(intersection[2].xy, z[2]); &emsp;&emsp;最终，我们求得到扩大后的三角形的三个顶点，我们还需要对三个顶点做逆视图投影变换，将裁剪空间的顶点变换到世界空间，得到扩大后的三角形的世界坐标，因为我们最终目的是根据世界空间坐标做体素化的处理。与此同时，我们还将在裁剪空间的扩大三角形的顶点传到片元着色器，因为我们要剔除不必要的片元。以下是保守光栅化算法的几何着色器代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117#version 430 corelayout (triangles) in;layout (triangle_strip, max_vertices = 3) out;out vec3 FragPos;out vec3 ProjectPos;out vec4 BoundingBox;uniform vec2 halfPixel[3];uniform mat4 viewProject[3];uniform mat4 viewProjectInverse[3];uint selectViewProject()&#123; vec3 p1 = gl_in[1].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 p2 = gl_in[2].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 faceNormal = cross(p1, p2); float nDX = abs(faceNormal.x); float nDY = abs(faceNormal.y); float nDZ = abs(faceNormal.z); if( nDX &gt; nDY &amp;&amp; nDX &gt; nDZ ) &#123; return 0; &#125; else if( nDY &gt; nDX &amp;&amp; nDY &gt; nDZ ) &#123; return 1; &#125; else &#123; return 2; &#125;&#125; vec4 calcAABB(vec4 pos[3], vec2 pixelDiagonal)&#123; vec4 aabb; aabb.xy = min(pos[2].xy, min(pos[1].xy, pos[0].xy)); aabb.zw = max(pos[2].xy, max(pos[1].xy, pos[0].xy)); // enlarge by half-pixel aabb.xy -= pixelDiagonal; aabb.zw += pixelDiagonal; return aabb;&#125;void main() &#123; uint projectIndex = selectViewProject(); vec4 pos[3] = vec4[3] ( viewProject[projectIndex] * gl_in[0].gl_Position, viewProject[projectIndex] * gl_in[1].gl_Position, viewProject[projectIndex] * gl_in[2].gl_Position ); vec4 trianglePlane; trianglePlane.xyz = normalize(cross(pos[1].xyz - pos[0].xyz, pos[2].xyz - pos[0].xyz)); trianglePlane.w = -dot(pos[0].xyz, trianglePlane.xyz); // change winding, otherwise there are artifacts for the back faces. if (dot(trianglePlane.xyz, vec3(0.0, 0.0, 1.0)) &lt; 0.0) &#123; vec4 vertexTemp = pos[2]; pos[2] = pos[1]; pos[1] = vertexTemp; &#125; if(trianglePlane.z == 0.0f) return; BoundingBox = calcAABB(pos, halfPixel[projectIndex]); vec3 edgePlanes[3]; edgePlanes[0] = cross(pos[0].xyw - pos[2].xyw, pos[2].xyw); edgePlanes[1] = cross(pos[1].xyw - pos[0].xyw, pos[0].xyw); edgePlanes[2] = cross(pos[2].xyw - pos[1].xyw, pos[1].xyw); edgePlanes[0].z -= dot(halfPixel[projectIndex], abs(edgePlanes[0].xy)); edgePlanes[1].z -= dot(halfPixel[projectIndex], abs(edgePlanes[1].xy)); edgePlanes[2].z -= dot(halfPixel[projectIndex], abs(edgePlanes[2].xy)); vec3 intersection[3]; intersection[0] = cross(edgePlanes[0], edgePlanes[1]); intersection[1] = cross(edgePlanes[1], edgePlanes[2]); intersection[2] = cross(edgePlanes[2], edgePlanes[0]); intersection[0] /= intersection[0].z; intersection[1] /= intersection[1].z; intersection[2] /= intersection[2].z; // calculate dilated triangle vertices float z[3]; z[0] = -(intersection[0].x * trianglePlane.x + intersection[0].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z; z[1] = -(intersection[1].x * trianglePlane.x + intersection[1].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z; z[2] = -(intersection[2].x * trianglePlane.x + intersection[2].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z; pos[0].xyz = vec3(intersection[0].xy, z[0]); pos[1].xyz = vec3(intersection[1].xy, z[1]); pos[2].xyz = vec3(intersection[2].xy, z[2]); vec4 voxelPos; ProjectPos = pos[0].xyz; gl_Position = pos[0]; voxelPos = viewProjectInverse[projectIndex] * gl_Position; FragPos = voxelPos.xyz; EmitVertex(); ProjectPos = pos[1].xyz; gl_Position = pos[1]; voxelPos = viewProjectInverse[projectIndex] * gl_Position; FragPos = voxelPos.xyz; EmitVertex(); ProjectPos = pos[2].xyz; gl_Position = pos[2]; voxelPos = viewProjectInverse[projectIndex] * gl_Position; FragPos = voxelPos.xyz; EmitVertex(); EndPrimitive();&#125; &emsp;&emsp;最后的最后，我们还需要在片元着色器剔除无关的片元，具体原因我已经在前面说了，如果不做这一步的剔除操作，将出现如下图12所示的情况。在片元着色器中，我们根据传入的三角形包围盒与当前的片元位置判断是否需要丢弃该片元。具体看下面代码的第19行、第20行。 图12 保守光栅化出现的边边角角 12345678910111213141516171819202122232425262728#version 430 corein vec3 FragPos;in vec3 ProjectPos;in vec4 BoundingBox;layout (std430, binding = 0) buffer CountBuffer&#123; int cnts[];&#125;;uniform bool conservate;uniform float step;uniform vec3 boxMin;uniform vec3 resolution;out vec4 color;void main()&#123; if(ProjectPos.x &lt; BoundingBox.x || ProjectPos.y &lt; BoundingBox.y || ProjectPos.x &gt; BoundingBox.z || ProjectPos.y &gt; BoundingBox.w) discard; uint x = uint((FragPos.x - boxMin.x)/step); uint y = uint((FragPos.y - boxMin.y)/step); uint z = uint((FragPos.z - boxMin.z)/step); uint index = uint(y * (resolution.z * resolution.x) + z * resolution.x + x); atomicAdd(cnts[index], 1); color = vec4(0.0,1.0,0.0,1.0);&#125; &emsp;&emsp;最终，修复了孔洞的效果的如下图，可以看到，对比前面的图9，孔洞基本都被“补”上了。 图13 保守光栅化出现的边边角角 &emsp;&emsp;下面就是一些模型的体素化效果。 参考资料：$[1]$ The Basics of GPU Voxelization $[2]$ 《GPU Gems 2》： Chapter 42. Conservative Rasterization $[3]$ https://blog.csdn.net/xiewenzhao123/article/details/79875855","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"https://yangwc.com/categories/Position-Based-Dynamics/"},{"name":"Voxelization","slug":"Voxelization","permalink":"https://yangwc.com/categories/Voxelization/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Voxelization","slug":"Voxelization","permalink":"https://yangwc.com/tags/Voxelization/"}]},{"title":"流体模拟Fluid Simulation：Position Based Fluid","slug":"PositionBasedFluid","date":"2019-06-04T07:43:02.767Z","updated":"2019-06-26T01:48:47.815Z","comments":true,"path":"2019/06/04/PositionBasedFluid/","link":"","permalink":"https://yangwc.com/2019/06/04/PositionBasedFluid/","excerpt":"本篇文章主要是关于Position Based Dynamics的流体模拟方法，这类方法依旧采用基于拉格朗日的视角，把流体看成由一个一个粒子组成，易于并行化，适用于实时的流体模拟。目前实现的只是CPU版本，考虑在后面利用cuda挪到GPU上做模拟计算。相关的完整代码请看这里。","text":"本篇文章主要是关于Position Based Dynamics的流体模拟方法，这类方法依旧采用基于拉格朗日的视角，把流体看成由一个一个粒子组成，易于并行化，适用于实时的流体模拟。目前实现的只是CPU版本，考虑在后面利用cuda挪到GPU上做模拟计算。相关的完整代码请看这里。 基于位置动力学的物理模拟 基于位置动力学的流体模拟 流体模拟算法实现 实现效果 参考资料 一、基于位置动力学的物理模拟&emsp;&emsp;传统的物理模拟方法都是基于力的方法，这类方法通过计算内部力（如流体内部的粘性力、压力）和外部力（如重力和碰撞力）的合力，然后根据牛顿第二定律计算出加速度，最后根据数值计算方法求出物体的速度和位置。这种方法基本上针对每一种动态物体，会由一个独立的求解器，各种求解器按照一定的顺序计算，从而得到模拟的结果，这样会带来大量冗余的工作。基于位置动力学（Position Based Dynamics）的方法将这些物理运动通过约束表达出来，这样只需要一个求解器即可，更加方便地进行物理模拟。 &emsp;&emsp;下图1是基于力和基于位置动力学的物体碰撞更新过程的对比，可以看到基于力的碰撞检测首先在穿透发生时更新物体的速度，然后更新物体的位置。而基于位置动力学的碰撞检测首先只检测是否发生穿透，然后移动位置使之不发生穿透，最后再据此更新物体的速度信息。 图1 两种碰撞更新过程对比 1、基于位置动力学的模拟算法&emsp;&emsp;基于位置动力学英文全称为Position Based Dynamics，以下简称为PBD。接下来我们介绍经典的PBD算法。在PBD算法中，运动的物体由$N$个顶点和$M$个约束组成。顶点$i\\in [1,…,N]$的质量为$m_i$，位置为$x_i$，速度为$v_i$，每个约束$j\\in [1,…,M]$有如下五个性质： 约束的基数为$n_j$，即第$j$个约束所影响的顶点数目为$n_j$个； 约束函数$C_j:\\ R^{3n_j}\\to R$； 受约束影响的顶点索引值集合$\\{i_1,…,i_{n_j}\\},i_k\\in [1,…N]$； 每个约束都有对应的刚度参数$k_j\\in [0,1]$，这里我们可以理解为约束的强度； 约束分为两种，一类是等式约束即$C_j(x_{i1},x_{i_2},…,x_{i_{n_j}})=0$，另一类是不等式约束$C_j(x_{i_1},x_{i_2},…,x_{i_{n_j}})\\geq 0$。 &emsp;&emsp;给定时间步长$\\Delta t$，PBD的运动物体模拟的算法伪代码如下所示： \\begin{align} &1.forall\\ \\ vertices\\ \\ i:\\\\ &2.\\ \\ \\ \\ initialize\\ \\ x_i=x_i^0,v_i=v_i^0,w_i=1/m_i\\\\ &3.endfor\\\\ &4.loop\\\\ &5.\\ \\ \\ \\ forall\\ \\ vertices\\ \\ i\\ \\ do\\ \\ v_i\\leftarrow v_i+\\Delta tw_if_{ext}(x_i)\\\\ &6.\\ \\ \\ \\ dampVelocities(v_1,...,v_N)\\\\ &7.\\ \\ \\ \\ forall\\ \\ vertices\\ \\ i\\ \\ do\\ \\ p_i\\leftarrow x_i+\\Delta t v_i\\\\ &8.\\ \\ \\ \\ forall\\ \\ vertices\\ \\ i\\ \\ do\\ \\ generateCollisionConstraints(x_i\\to p_i)\\\\ &9.\\ \\ \\ \\ loop\\ \\ solverIterations\\ \\ times\\\\ &10.\\ \\ \\ \\ \\ \\ \\ \\ projectConstraints(C1,...,C_{M+M_{coll}},p_1,...,p_N)\\\\ &11.\\ \\ \\ endloop\\\\ &12.\\ \\ \\ forall\\ \\ vertices\\ \\ i\\\\ &13.\\ \\ \\ \\ \\ \\ \\ \\ v_i\\leftarrow (p_i-x_i)\\Delta t\\\\ &14.\\ \\ \\ \\ \\ \\ \\ \\ x_i\\leftarrow p_i\\\\ &15.\\ \\ \\ endfor\\\\ &16.\\ \\ \\ velocityUpdate(v1,...,v_N)\\\\ &17.endloop\\\\ \\end{align}&emsp;&emsp;在上面的算法第1步到第3步中，我们首先对顶点的位置、速度和质量倒数进行初始化，其中质量的倒数$w_i=1/m_i$，除了可以避免冗余的除法操作外，还可以使用于静态的物体，对于静态的物体我们设为$w_i=0$，这样在后续的更新中都不会产生位置和速度的变化量。第5步中的$f_{ext}$代表不能转换成约束形式的力（如重力），我们根据$f_{ext}$进行一次数值计算预测在$f_{ext}$的作用下的速度$v_i$。紧接着在第6步中我们添加阻尼的作用，阻尼可以理解为物体在运动中发生了能量耗散，从而导致速度有所衰减。第8行主要是生成碰撞约束，物体会与周围的环境发生碰撞，例如布料落在地板上，水碰上一面墙等，这些碰撞约束在每个时间步长都发生改变，所以每一次都需要重新生成碰撞约束。有了内部约束（如不可压缩流体的密度约束）和外部约束（如流体与地面的碰撞约束）之后，我们需要根据这些约束做一个迭代求解，也就是上面伪代码中的第9行到第11行，这里我们称为约束投影步骤。从约束投影步骤我们得到服从给定约束的粒子位置，然后再第12行到第15行更新顶点粒子的速度和位置信息。最后在第16行根据摩擦系数（friction）和恢复系数（restitution）更新速度，如下图2所示。这样，一个完整的PBD物理模拟步骤就完成了。 图2 friction和restitution 2、约束投影步骤&emsp;&emsp;接下来我们就针对约束投影步骤详细展开相关的内容，约束投影是PBD中的最难理解的核心部分，涉及的数学内容比较多一点。设有一个基数为n（也就是前面提到的$n_j$，受到该约束影响的顶点数目或者说粒子数目）的约束，关联的粒子点为$p_1,…,p_n$，约束函数记为$C$，刚度系数（stiffness）为$k$。记$p=[p_1^T,…,p_n^T]^T$，则等式约束函数表示为： C(p)=0 \\tag {1}&emsp;&emsp;我们的目标是计算这样的一个位移偏移量$\\Delta p$，使得粒子顶点在$p+\\Delta p$处约束条件依然满足，即： C(p+\\Delta p)=0 \\tag {2}&emsp;&emsp;对约束函数$C$做一阶泰勒展开（或者导数的定义），则可得: C(p+\\Delta p)\\approx C(p)+\\nabla_pC(p)\\cdot\\Delta p=0 \\tag {3}&emsp;&emsp;为了使粒子在$p+\\Delta p$处依然满足约束条件，我们要求解方程$(3)$得到$\\Delta p$。PBD算法的一个巧妙之处在于它将$\\Delta p$的方向限制在约束函数的梯度方向$\\nabla_p C(p)$上。如下图3所示，约束$C$所涉及到的粒子位置会形成一个高维空间，下图为该空间中满足不同约束条件的粒子位置形成的二维等值线示意图，其中满足$C$约束条件的是黑色等值线。故当粒子处于下图的黑色点的位置时，不满足约束条件，如果我们沿着点所在的等值线（灰色曲线）移动，此时刚体模态（Rigid body modes）的方向与该等值线相同，新得到的位置仍然在该灰色等值线上，依然不在黑色曲线 $C=0$上，即不满足约束条件。这可以理解为，约束中存在的误差依然没有得到修正。以两个粒子形成的距离约束为例，就好比同时移动了两个粒子或者该约束绕自身旋转，但是存在的误差并没有得到更正。而且这样一来还会引入系统中不存在的一种外力，导致系统动量不守恒。所以，我们希望该点的位移方向与刚体模态方向垂直，从而保证系统动量守恒，即从黑点指向红点的方向$\\nabla C$。 图3 约束等值线 &emsp;&emsp;因此，我们令位移向量$\\Delta p$为约束函数的梯度向量$\\nabla_p C$再乘上一个标量缩放系数$\\lambda$： \\Delta p=\\lambda \\nabla_p C(p) \\tag {4}&emsp;&emsp;其中的标量缩放系数$\\lambda$我们称之为拉格朗日乘子（Lagrange multiplier）。联立公式$(3)$和$(4)$我们可得： \\lambda=-\\frac{C(p)}{|\\nabla_pC(p)|^2} \\tag {5}&emsp;&emsp;然后将$\\lambda$再代入公式$(4)$我们可得$\\Delta p$的表达式： \\Delta p=\\lambda \\nabla_pC(p)=-\\frac{C(p)}{|\\nabla_pC(p)|^2}\\nabla_pC(p) \\tag {6}&emsp;&emsp;具体到粒子$i$，约束投影后其对应的位移向量为： \\Delta p_i=-s\\nabla_{p_i}C(p_1,...,p_n) \\tag {7}&emsp;&emsp;其中的$s$为如下所示，$s$的值对于约束函数$C$作用范围内的所有点都一样。 s=\\frac{C(p_1,...,p_n)}{\\Sigma_j|\\nabla_{p_j}C(p_1,...,p_n)|^2} \\tag {8}&emsp;&emsp;前面我们假定所有的粒子质量都相同，现在考虑粒子质量不同的情况。记粒子$i$的质量为$m_i$，其质量的倒数为$w_i=1/m_i$，则公式$(4)$变为： \\Delta p_i=\\lambda w_i\\nabla_{p_i}C(p) \\tag {9}&emsp;&emsp;公式$(7)$和公式$(8)$变为： \\Delta p_i=-s w_i\\nabla_{p_i}C(p_1,...,p_n) \\tag {10} s=\\frac{C(p_1,...,p_n)}{\\Sigma_jw_j|\\nabla_{p_j}C(p_1,...,p_n)|^2} \\tag {11}&emsp;&emsp;为了便于理解，接下来我们举个简单的例子应用约束投影方法。如下图4所示。 图4 简单的约束例子 &emsp;&emsp;上面的约束可以表示为$C(p_1,p_2)=|p_1-p_2|-d$，位移向量记为$\\Delta p_i$。根据约束投影方法，我们首先约束函数$C(p_1,p_2)$关于$p_1$和$p_2$的梯度，也就是求偏导数。注意到$C(p_1,p_2)=|p_1-p_2|-d=(\\sqrt{(p_1-p_2)^2})-d$，我们可以求得以下的梯度向量表达式： \\nabla_{p_1}C(p_1,p_2)=\\frac{p_1-p_2}{|p_1-p_2|}\\\\ \\nabla_{p_2}C(p_1,p_2)=-\\frac{p_1-p_2}{|p_1-p_2|} \\tag {12}&emsp;&emsp;注意，上面求到的是一个矢量，也就是我们说的梯度向量。将公式$(12)$代入公式$(11)$可得： \\begin{align} s=&\\frac{C(p_1,...,p_n)}{\\Sigma_jw_j|\\nabla_{p_j}C(p_1,...,p_n)|^2}\\\\ =&\\frac{|p_1-p_2|-d}{w_1|\\nabla_{p_1}C(p_1,p_2)|^2+w_2|\\nabla_{p_2}C(p_1,p_2)|^2}\\\\ =&\\frac{|p_1-p_2|-d}{w_1+w_2} \\tag {13} \\end{align}&emsp;&emsp;最后，将公式$(13)$代入到公式$(10)$，可得约束投影计算得到的位移： \\begin{align} \\Delta p_1=&-\\frac{|p_1-p_2|-d}{w_1+w_2}w_1\\nabla_{p_1}C(p_1,p_2)\\\\ =&-\\frac{w_1}{w_1+w_2}(|p_1-p_2|-d)\\frac{p_1-p_2}{|p_1-p_2|} \\end{align}&emsp;&emsp;同理$\\Delta p_2$如下所示： \\Delta p_2=+\\frac{w_2}{w_1+w_2}(|p_1-p_2|-d)\\frac{p_1-p_2}{|p_1-p_2|}&emsp;&emsp;前面我们提到每个约束都有对应的刚度系数$k$，令$k’=1-(1-k)^{1/n_s}$去乘$\\Delta p$，这里$n_s$迭代之后误差为$\\Delta p(1-k’)^{n_s}=\\Delta p(1-k)$，与刚度系数成线性关系，而与迭代次数$n_s$无关。下一个时间步的位置如下所示： p_1^{t+1}=p_1^t+k'\\Delta p_1\\\\ p_2^{t+1}=p_2^t+k'\\Delta p_23、约束投影求解器&emsp;&emsp;前面的伪代码中我们可以看到约束投影的输入为$M+M_{coll}$个约束和$N个$点的预测位置$p1,…,p_N$，所需要求解的方程组是非线性非对称方程组或不等式组（碰撞约束产生的）。约束投影步骤的主要任务就是修正预测位置使新得到的校正位置满足所有约束。但是一般情况下很难找到一个适当的$\\Delta p=[\\Delta p_1^T,…,\\Delta p_n^T]^T$恰好使得所有的约束都能够同时得到满足，故我们通常采用迭代的方法按顺序依次对约束进行求解。 &emsp;&emsp;我们可以采用非线性高斯-赛德尔（Non-Linear Gauss-Seidel，简称NGS）迭代方法。高斯赛德尔（Gauss-Sedel，简称GS）迭代方法只能求解线性方程组，NGS在依次求解德基础上，加入了约束投影求解这一非线性操作。与雅可比迭代方法（Jacobi method）不同，NGS求解器在一次迭代中对于顶点位置的修正立即被应用到下一个约束求解中，这样的好处就是显著加快了收敛速度。 &emsp;&emsp;但是NGS虽然稳定且容易实现，但是该方法收敛速度依然不是很快，不宜并行化。 二、基于位置动力学的流体模拟&emsp;&emsp;前面部分主要介绍了Position Based Dynamics算法相关的内容，接下来我们就看看如何将其PBD算法应用到流体模拟当中，主要是如何针对流体的物理特性构建相应的约束函数。基于位置动力学的流体全称为Position Based Fluid，简称PBF。 1、不可压缩约束&emsp;&emsp;在不可压缩性的流体模拟中，我们需要使粒子$i$的密度$\\rho_i$尽量与静态的密度$\\rho_0$相同，即$\\rho_i=\\rho_0$。因此，我们需要对每一个流体粒子都施加一个常量密度约束，PBF的常量密度约束如下所示： C_i(p_1,...,p_n)=\\frac{\\rho_i}{\\rho_0}-1 \\tag {14}&emsp;&emsp;公式$(14)$中，我们记粒子$i$的位置为$p_i$，$p_1,…,p_n$是与粒子$i$相邻的粒子。可以看到当密度约束$C_i(p_1,…,p_n)=0$时有$\\rho_i=\\rho_0$，此时流体的体积即不压缩也不膨胀，从而保证了流体的不可压缩条件，这就是公式$(14)$的由来。流体粒子$i$的密度根据SPH（Smoothed Particle Hydrodynamics，光滑粒子流体动力学，简称SPH）方法的计算公式如下所示： \\rho_i=\\Sigma_jm_jW(p_i-p_j,h) \\tag {15}&emsp;&emsp;在公式$(15)$中，$m_j$是邻居粒子$j$的质量，$h$是指定的光滑核半径。$W$函数我们接下来会提到。将公式$(15)$代入公式$(14)$，我们有： C_i(p_1,...,p_n)=\\frac{\\Sigma_j m_jW(p_i-p_j,h)}{\\rho_0}-1 \\tag {16}&emsp;&emsp;在公式$(15)$的密度计算中，PBF方法采用了Poly6核函数： W_{poly6}(r,h)=\\frac{315}{64\\pi h^9} \\begin{cases} (h^2-|r|^2)^3\\ \\ \\ \\ 0\\leq|r|\\leq h\\\\ 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ otherwise \\end{cases} \\tag {17}&emsp;&emsp;但是在计算密度的梯度时，却又采用了Spiky核函数： W_{spiky}(r,h)=\\frac{15}{\\pi h^6} \\begin{cases} (h-|r|)^3\\ \\ \\ \\ 0\\leq|r|\\leq h\\\\ 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 0therwise \\end{cases} \\tag {18}&emsp;&emsp;对公式$(18)$求关于$r$的导数（注意，$|r|=\\sqrt{r^2}$，不能直接对$|r|$求导），从而流体粒子密度的梯度如下所示： \\nabla W_{spiky}(r,h)=-\\frac{45}{\\pi h^6} \\begin{cases} (h-|r|)^2\\frac{r}{|r|}\\ \\ \\ \\ 0\\leq|r|\\leq h\\\\ 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ otherwise \\end{cases} \\tag {19}&emsp;&emsp;因此，粒子$i$的约束函数$(16)$是一个关于$p_1,…,p_n$的非线性方程组$C_i(p_1,…,p_n)=0$，所有粒子$i$的约束组成了一个非线性方程组。在PBF方法中，我们只考虑粒子质量相同的情况，故我们可以省去公式$(15)$和公式$(16)$中的质量$m_j$，即： \\rho_i=\\Sigma_jW(p_i-p_j,h) \\tag {20} C_i(p_1,...,p_n)=\\frac{\\Sigma_j W(p_i-p_j,h)}{\\rho_0}-1 \\tag {21}&emsp;&emsp;然后求约束函数$C_i$关于$p_k$的梯度如下，其中$k\\in\\{1,2,…,n\\}$： \\nabla_{p_k}C_i=\\frac1\\rho_0\\Sigma_j\\nabla_{p_k}W(p_i-p_j,h) \\tag {22}&emsp;&emsp;显然，针对$k$的不同，分为两种情况。当$k=i$也就是粒子本身的时候，连加符号中的$W$均为关于$p_k$的函数；当$k=j$即邻居粒子的时候，只有$W(p_i-p_k,h)$才有意义，其他相对于$p_k$来说都是常量，故导数为0（注意用到了求导的链式法则）： \\nabla_{p_k}C_i=\\frac1\\rho_0 \\begin{cases} \\Sigma_j\\nabla_{p_k}W(p_i-p_j,h)\\ \\ \\ \\ if\\ \\ k=i\\\\ -\\nabla_{p_k}W(p_i-p_j,h)\\ \\ \\ \\ \\ if\\ \\ k=j \\end{cases} \\tag {23}&emsp;&emsp;既然求出了约束函数的梯度，我们就把它应用到前面提到的拉格朗日乘子的计算公式中，联立公式$(5)$和公式$(23)$，我们有： \\lambda_i=-\\frac{C_i(p_1,...,p_n)}{\\Sigma_k|\\nabla_{p_k}C_i|^2} \\tag {24}2、混合约束&emsp;&emsp;如果一个约束条件不能被违背，我们称之为硬约束；而能一定程度上被违背的约束称为软约束。在理想的情况下，我们都希望约束始终是硬约束，但是由于误差或者数值方法的不稳定等原因，我们有时不得不向软约束妥协。 &emsp;&emsp;在PBF中，当$|r|=h$，粒子$i$与粒子$j$之间的距离等于光滑核半径时，粒子$i$和粒子$j$处于即将分离的状态。注意观察公式$(19)$的密度梯度计算公式，此时$\\nabla W_{spiky}(r,h)=0$。若所有的邻居粒子与粒子$i$都处于这种状态，那么必将导致约束函数的梯度即公式$(22)$取值为0： \\nabla_{p_k}C_i=\\frac1\\rho_0\\Sigma_j\\nabla_{p_k}W(p_i-p_j,h) = 0&emsp;&emsp;从而导致公式$(24)$中的分母$\\Sigma_k|\\nabla_{p_k}C_i|^2$为0，出现除零错误，这将导致PBF方法出现潜在的不稳定性。为了解决这个问题，PBF采用混合约束的方法，使密度硬约束转变成软约束。具体的做法就是将根据密度函数求解得到的约束力再加入到原始的约束函数中，这里在PBF的常量密度约束中得到的拉格朗日乘子$\\lambda$有类似的作用，故将$\\lambda$加入到初始的约束方程（即公式$(3)$）： C(p+\\Delta p)\\approx C(p)+\\nabla C^T\\nabla C \\lambda + \\epsilon\\lambda=0 \\tag {25}&emsp;&emsp;公式$(25)$中的$\\epsilon$是松弛参数，可以由用户指定。引入公式$(25)$后，拉格朗日乘子的计算公式$(24)$就变为： \\lambda_i=-\\frac{C_i(p_1,...,p_n)}{\\Sigma_k|\\nabla_{p_k}C_i|^2+\\epsilon} \\tag {26}&emsp;&emsp;从而可得粒子$i$在经过上述约束投影后对应的位移向量（包括自身密度约束以及邻居粒子密度约束共同作用的结果。注意，这里对应的上面的公式$(4)$，结合公式$(23)$）： \\begin{align} \\Delta p_i&=\\lambda_i \\nabla_{p_i}C_i+\\Sigma_j\\lambda_j\\nabla_{p_j}C_i\\\\ &=\\frac1\\rho_0\\Sigma_j\\lambda_i\\nabla_{p_i}W(r,h)+(-\\frac1\\rho_0\\Sigma_j\\lambda_j\\nabla_{p_j}W(r,h))\\\\ &=\\frac1\\rho_0\\Sigma_j\\lambda_i\\nabla_{p_i}W(r,h)+\\frac1\\rho_0\\Sigma_j\\lambda_j\\nabla_{p_i}W(r,h)\\\\ &=\\frac{1}{\\rho_0}\\Sigma_j(\\lambda_i+\\lambda_j)\\nabla_{p_i}W(r,h) \\end{align} \\tag {27}3、拉伸不稳定性&emsp;&emsp;PBF采用SPH的方法来计算流体粒子的密度，但是该方法通常需要30~40个邻居粒子才能使密度求值结果趋于静态密度。在邻居粒子数量较少的情况下，通过该方法计算得到的流体密度低于静态密度，由此会造成流体内部压强为负数，原本粒子间的压力变为吸引力，使得流体粒子凝聚在一起，导致流体表面的模拟效果失真。PBF采用了一种人工排斥力的计算模型，当流体粒子距离过近时该排斥力会使它们分开，避免产生粒子聚集的现象。在公式$(24)$的基础上，加入一个排斥项（repulsive term）$s_{corr}$： \\Delta p_i=\\frac1\\rho_0\\Sigma_j(\\lambda_i+\\lambda_j+s_{corr})\\nabla_{p_i}W(p_i-p_j,h) \\tag {28}&emsp;&emsp;其中的$s_{corr}$计算方式如下： s_{corr}=-k(\\frac{W(p_i-p_j,h)}{W(\\Delta q,h)})^n \\tag {29}&emsp;&emsp;公式$(29)$中，$\\Delta q$表示到粒子$i$的一个固定距离，通常取$|\\Delta q|=0.1h,…,0.3h$，$h$即前面提到的光滑核半径。此外，公式中的$k$可以看作表面张力参数，取值$k=0.1$，而$n=4$。公式$(28)$中的排斥项会使得流体粒子的密度稍微低于静态密度，从而产生类似于表面张力的效果，使得流体表面的的粒子分布均匀。通过这个排斥项，我们不再需要硬性规定流体的邻居数量必须在30~40个，进一步提升算法的流体模拟效率。 4、涡轮控制和人工粘性&emsp;&emsp;由于数值耗散，PBD的方法会引入额外的阻尼，使得整个系统的能量损耗太快，导致本来应该由的一些涡流细节迅速消失。在这里，PBF通过涡轮控制方法向整个系统重新注入能量： f_i^{vorticity}=\\epsilon (N\\times \\omega_i) \\tag {30}&emsp;&emsp;上述的公式中，$N=\\frac{\\eta}{|\\eta|},\\ \\eta=\\nabla|\\omega|_i$，而流体粒子的旋度$\\omega_i$计算公式如下： \\omega_i=\\nabla\\times v=\\Sigma_j(v_j-v_i)\\times \\nabla_{p_j}W(p_i-p_j,h) \\tag {31}&emsp;&emsp;涡轮控制方法的基本思路就是：通过添加一个体积力$f_i^{vorticity}$（在算法的第一步），在旋度粒子（可直观理解为比周围粒子旋转快的粒子，旋度$\\omega_i$指向粒子$i$的旋转轴）处加速粒子的旋转运动，通过这种方式来增加系统的旋度细节。公式$(30)$中的$\\epsilon$用于控制涡轮控制力的强度。 &emsp;&emsp;最后，PBF方法采用XSPH的粘度方法直接更新速度，从而产生粘性阻尼。人工粘性除了可以增加模拟的数值稳定性，还可以消除非物理的流体振荡。拉格朗日流体模拟方法中，人工粘性本质上会对流体粒子的相对运动产生阻尼作用，使流体的动能转化为热能： v_i^{new}=v_i+c\\Sigma_j(v_i-v_j)\\cdot W(p_i-p_j,h) \\tag {32}&emsp;&emsp;在流体模拟中，我们取公式$(32)$中的$c=0.01$。 5、PBF算法&emsp;&emsp;PBF算法的总体框架就是按照前面提到的PBD算法，只是经典PBD算法采用了顺序高斯-赛德尔（Sequential Gauss-Seidel，SGS）迭代求解，而SGS不容易被GPU并行化，因此基于CUDA实现的PBF求解器使用了雅克比（Jacobi）迭代方法并行求解。 &emsp;&emsp;PBF的算法伪代码如下所示： \\begin{align} &1.\\ forall\\ \\ particles\\ \\ i\\ \\ do\\\\ &2.\\ \\ \\ \\ \\ apply\\ \\ force\\ \\ v_i\\leftarrow v_i+\\Delta tf_{ext}(x_i)\\\\ &3.\\ \\ \\ \\ \\ predict\\ \\ position\\ \\ x_i^*\\leftarrow x_i+\\Delta t v_i\\\\ &4.\\ endfor\\\\ &5.\\ forall\\ \\ particles\\ \\ i\\ \\ do\\\\ &6.\\ \\ \\ \\ \\ find\\ \\ neighboring\\ \\ particles\\ \\ N_i(x_i^*)\\\\ &7.\\ endfor\\\\ &8.\\ while\\ \\ iter\\ \\","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/tags/Fluid-Simulation/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"https://yangwc.com/tags/Position-Based-Dynamics/"}]},{"title":"光线追踪器Ray Tracer：进阶篇","slug":"RayTracer-Advance","date":"2019-05-23T13:08:38.407Z","updated":"2019-05-26T03:37:23.595Z","comments":true,"path":"2019/05/23/RayTracer-Advance/","link":"","permalink":"https://yangwc.com/2019/05/23/RayTracer-Advance/","excerpt":"本篇文章在前面的基础上，丰富光线追踪器的各种特性。本篇内容主要包含添加纹理映射、平面光源和球形光源、三角网格模型渲染、增加天空盒背景、构建BVH树、tbb多线程渲染加速、蒙特卡罗积分方法、重要性采样，后面部分涉及的高等数学和概率论内容较多。相关的全部代码在这里。","text":"本篇文章在前面的基础上，丰富光线追踪器的各种特性。本篇内容主要包含添加纹理映射、平面光源和球形光源、三角网格模型渲染、增加天空盒背景、构建BVH树、tbb多线程渲染加速、蒙特卡罗积分方法、重要性采样，后面部分涉及的高等数学和概率论内容较多。相关的全部代码在这里。 纹理映射 三角网格模型 添加光源 天空盒背景 构建BVH树 tbb多线程渲染 蒙特卡罗积分 重要性采样 MC光线追踪 程序效果 参考资料 一、纹理映射&emsp;&emsp;纹理映射对渲染的重要性不言而喻，为了丰富物体表面的细节，我们在这里创建一个纹理加载和采样的类。实际上，除了图片纹理，还有一些过程式产生的纹理。我们创建一个虚类$Texture$，并将$sample$类作为虚接口。然后创建子类$ImageTexture$，图片的加载我采用了stb_image库。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Texture&#123;public: typedef std::shared_ptr&lt;Texture&gt; ptr; Texture() = default; virtual ~Texture() = default; virtual Vector3D sample(const float &amp;u, const float &amp;v, const Vector3D &amp;p) const = 0;&#125;;class ImageTexture : public Texture&#123;private: unsigned char *m_data; int m_width, m_height, m_channel; public: typedef std::shared_ptr&lt;ImageTexture&gt; ptr; ImageTexture() = default; ImageTexture(const std::string &amp;path); virtual ~ImageTexture(); virtual Vector3D sample(const float &amp;u, const float &amp;v, const Vector3D &amp;p) const; &#125;;ImageTexture::ImageTexture(const std::string &amp;path)&#123; m_data = stbi_load(path.c_str(), &amp;m_width, &amp;m_height, &amp;m_channel, 0); if (m_data == nullptr) std::cout &lt;&lt; \"Failed to load the image-&gt;\" &lt;&lt; path.c_str() &lt;&lt; std::endl;&#125;Vector3D ImageTexture::sample(const float &amp;u, const float &amp;v, const Vector3D &amp;p) const&#123; int i = static_cast&lt;int&gt;(u * m_width); int j = static_cast&lt;int&gt;((1.0f - v)*m_height) - 0.001; if (i &lt; 0) i = 0; if (j &lt; 0) j = 0; if (i &gt; m_width - 1) i = m_width - 1; if (j &gt; m_height - 1) j = m_height - 1; int index = (j * m_width + i) * m_channel; float r = static_cast&lt;int&gt;(m_data[index + 0]) / 255.0f; float g = static_cast&lt;int&gt;(m_data[index + 1]) / 255.0f; float b = static_cast&lt;int&gt;(m_data[index + 2]) / 255.0f; return Vector3D(r, g, b);&#125; &emsp;&emsp;纹理坐标转换为像素数组下标要注意是否越界了，这里实现的纹理环绕方式是clamp。然后对于球体，我们要计算球体上每个点的纹理坐标，这里采用球面坐标的一个技巧。球体的上每一个点，都对应着一组唯一的方向角和天顶角$(\\theta,\\phi)$，我们把$(\\theta,\\phi)$映射到二维纹理坐标即可。映射方法如下： u = \\phi/(2\\pi) \\\\ v = \\theta/\\pi \\tag {1}&emsp;&emsp;那么如何根据一个球面的点计算它的方向角和天顶角呢？从球面坐标$(\\theta, \\phi)$转到笛卡尔坐标$(x,y,z)$，不难理解，有如下关系： x = cos(\\phi)cos(\\theta) \\\\ y = sin(\\phi)cos(\\theta) \\\\ z = sin(\\theta) \\tag {2}&emsp;&emsp;注意到$y/x=tan(\\phi)$，所以我们可以采用下面的方式得到球面上点的天顶角和方位角： \\phi=atan2(y,x)\\\\ \\theta=asin(2) \\tag {3}&emsp;&emsp;需要注意的是，$atan2$函数返回的角度范围是$[-\\pi,+\\pi]$，$asin$返回的角度范围是$[-\\pi/2,\\pi/2]$。 1234567static void getSphereUV(const Vector3D &amp;p, Vector2D &amp;tex)&#123; float phi = atan2(p.z, p.x); float theta = asin(p.y); tex.x = 1 - (phi + M_PI) / (2*M_PI); tex.y = (theta + M_PI/2) / M_PI;&#125; 二、三角网格模型&emsp;&emsp;除了像球体、圆柱、圆锥等等这类有显示数学表达式的几何体，我们接触到更多的是没有表达式的网格模型。有显示的数学表达式当然好，因为我们直接直接求交点的解析解，非常准确。这里我们构建一个通用的网格模型类，它由一个个三角形构成。obj模型的导入我不再赘述，这里重点讲述了射线与三角形求交的推导过程。 &emsp;&emsp;一个三角形由空间中的三个顶点$P_0$、$P_1$、$P_2$的位置表示，三角形所在平面的法向量$N$可由下式计算而得： N=(P_1-P_0)\\times(P_2-P_0) \\tag {4}&emsp;&emsp;平面与原点的距离$d$等于平面法向量$N$与平面中任意一点的内积的负数，这里选$P_0$，则$d$为： d = -N\\cdot P_0 \\tag {5}&emsp;&emsp;则三角形所在的平面可以用四维向量$(N, -N\\cdot P_0)$表示，实际上三角形所在平面的表达式为$N\\cdot(x,y,z)+d=0$，首先我们求射线与该平面的交点，然后再判断交点是否在三角形内部。将射线方程$P(t) = S+tV$带入平面的方程，则有： N\\cdot P(t)+d=0\\\\ \\to N\\cdot S +(N\\cdot V)t +d=0\\\\ \\to t=-\\frac{(N\\cdot S+d)}{N\\cdot V}&emsp;&emsp;通过以上的方程我们就可以得到射线在平面$L$上的交点处的$t$值。需要注意的是，当$N\\cdot V=0$时，射线与平面平行，不存在交点。然后我们把$t$值带入射线方程即可求出射线与平面的交点$P$。接下来的问题是判断点$P$是否位于三角形内部，通过计算点$P$对于三角形的三个顶点$P_0$、$P_1$、$P_2$的重心坐标可以完成该判断。重心坐标是三角形顶点加权平均值，由三个标量$\\omega_0$、$\\omega_1$和$\\omega_2$组成，有： P=\\omega_0 P_0+\\omega_1 P_1 + \\omega_2 P_2 \\tag {6}&emsp;&emsp;其中，$\\omega_0+\\omega_1+\\omega_2 =1$，用$1-\\omega_1-\\omega_2$代替$\\omega_0$，可得： P=(1-\\omega_1-\\omega_2)P_0+\\omega_1P_1+\\omega_2P_2 \\\\ =P_0+\\omega_1(P_1-P_0)+\\omega_2(P_2-P_0) \\tag {7}&emsp;&emsp;定义以下的等式： R=P-P_0\\\\ Q_1=P_1-P_0\\\\ Q_2=P_2-P_0 \\tag {8}&emsp;&emsp;将公式$(9)$带入公式$(8)$，可得： R=\\omega_1Q_1+\\omega_2Q_2 \\tag {9}&emsp;&emsp;分别给式$(10)$两边乘$Q_1$和$Q_2$可得以下两个方程： R\\cdot Q_1=\\omega_1Q_1^2+\\omega_2(Q_1\\cdot Q_2)\\\\ R\\cdot Q_2=\\omega_1(Q_1\\cdot Q_2)+\\omega_2Q_2^2 \\tag {10}&emsp;&emsp;写成矩阵形式如下： \\left[ \\begin{matrix} Q_1^2 & Q_1\\cdot Q_2\\\\ Q_1\\cdot Q_2 & Q^2_2 \\end{matrix} \\right] \\left[ \\begin{matrix} \\omega_1\\\\ \\omega_2 \\end{matrix} \\right] = \\left[ \\begin{matrix} R\\cdot Q_1\\\\ R\\cdot Q_2 \\end{matrix} \\right] \\tag {11}&emsp;&emsp;解以上关于$\\omega_1$和$\\omega_2$的方程，可得： \\left[ \\begin{matrix} \\omega_1\\\\ \\omega_2 \\end{matrix} \\right] = \\left[ \\begin{matrix} Q_1^2 & Q_1\\cdot Q_2\\\\ Q_1\\cdot Q_2 & Q^2_2 \\end{matrix} \\right]^{-1} \\left[ \\begin{matrix} R\\cdot Q_1\\\\ R\\cdot Q_2 \\end{matrix} \\right] \\\\ =\\frac1{Q^2_1Q^2_2-(Q_1\\cdot Q_2)^2} \\left[ \\begin{matrix} Q_2^2 & -Q_1\\cdot Q_2 \\\\ -Q_1\\cdot Q_2 & Q_1^2 \\end{matrix} \\right] \\left[ \\begin{matrix} R\\cdot Q_1\\\\ R\\cdot Q_2 \\end{matrix} \\right] \\tag {12}&emsp;&emsp;当且仅当$\\omega_0$、$\\omega_1$和$\\omega_2$三个权值均为非负值时，点$R$位于三角形内部，由于$\\omega_0=1-\\omega_1-\\omega_2$，则此时应有$\\omega_1+\\omega_2\\leq 1$且$\\omega1 \\geq 0\\ and\\ \\omega_2\\geq0$。若顶点$P_0$、$P_1$和$P_2$上关联有一些属性信息，如颜色、法向量或者纹理坐标，则可以利用权值$\\omega_0$、$\\omega_1$和$\\omega_2$对这些属性信息进行插值。 123456789101112131415161718192021222324252627282930313233343536bool MeshHitable::triangleHit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret, const Vertex &amp;p0, const Vertex &amp;p1, const Vertex &amp;p2, const Vector3D &amp;normal) const&#123; float n_dot_dir = normal.dotProduct(ray.getDirection()); // no intersection. if (equal(n_dot_dir, 0.0)) return false; float d = -normal.dotProduct(p0.m_position); float t = -(normal.dotProduct(ray.getOrigin()) + d) / n_dot_dir; if (t &lt; t_min || t &gt; t_max) return false; ret.m_t = t; ret.m_position = ray.pointAt(t); ret.m_material = m_material; // judge inside or not. Vector3D r = ret.m_position - p0.m_position; Vector3D q1 = p1.m_position - p0.m_position; Vector3D q2 = p2.m_position - p0.m_position; float q1_squaredLen = q1.getSquaredLength(); float q2_squaredLen = q2.getSquaredLength(); float q1_dot_q2 = q1.dotProduct(q2); float r_dot_q1 = r.dotProduct(q1); float r_dot_q2 = r.dotProduct(q2); float determinant = 1.0f / (q1_squaredLen * q2_squaredLen - q1_dot_q2 * q1_dot_q2); float omega1 = determinant * (q2_squaredLen * r_dot_q1 - q1_dot_q2 * r_dot_q2); float omega2 = determinant * (-q1_dot_q2 * r_dot_q1 + q1_squaredLen * r_dot_q2); if (omega1 + omega2 &gt; 1.0f || omega1 &lt; 0.0f || omega2 &lt; 0.0f) return false; ret.m_normal = p0.m_normal * (1.0f - omega1 - omega2) + p1.m_normal * omega1 + p2.m_normal * omega2; ret.m_texcoord = p0.m_texcoord * (1.0f - omega1 - omega2) + p1.m_texcoord * omega1 + p2.m_texcoord * omega2; if (ret.m_normal.dotProduct(ray.getDirection()) &gt; 0.0f) ret.m_normal = -ret.m_normal; return true;&#125; &emsp;&emsp;既然模型是由一个个三角形组成，那么在判断射线与当前的模型是否存在交点时，我们就遍历所有的三角形，一个一个三角形与射线做相交判断： 1234567891011121314151617181920212223bool MeshHitable::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const&#123; HitRecord tmpRec; bool hitAny = false; float closestSoFar = t_max; for (int x = 0; x &lt; m_indices.size(); x += 3) &#123; int index1 = m_indices[x + 0]; int index2 = m_indices[x + 1]; int index3 = m_indices[x + 2]; if (triangleHit(ray, t_min, closestSoFar, tmpRec, m_vertices[index1], m_vertices[index2], m_vertices[index3], m_faceNormal[x / 3])) &#123; hitAny = true; closestSoFar = tmpRec.m_t; ret = tmpRec; &#125; &#125; return hitAny;&#125; 三、添加光源&emsp;&emsp;光源是一种特殊的物体，一般情况下它不反射、折射光线，而是自身发射光线。因此，为了实现一个光源，当我们的射线碰撞到光源表面时，我们直接返回光源的碰撞点的颜色，不再做折射和反射。我们将发光的逻辑放到材质中，并将发光这一行为抽象为$emitted$函数。对于非光源物体，我们可以看成发出的光rgb均为0。 123456789101112131415161718192021222324252627282930313233343536373839404142class Material&#123;public: typedef std::shared_ptr&lt;Material&gt; ptr; Material() = default; virtual ~Material() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, ScatterRecord &amp;srec) const &#123; return false; &#125; virtual float scattering_pdf(const Ray &amp;in, const HitRecord &amp;rec, const Ray &amp;scattered) const &#123; return 1.0f; &#125; virtual Vector3D emitted(const Ray &amp;in, const HitRecord &amp;rec, const float &amp;u, const float &amp;v, const Vector3D &amp;p) const &#123; return Vector3D(0.0f, 0.0f, 0.0f); &#125;&#125;;class DiffuseLight : public Material&#123;private: unsigned int m_emitTex; public: typedef std::shared_ptr&lt;DiffuseLight&gt; ptr; DiffuseLight(unsigned int a) : m_emitTex(a) &#123; &#125; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, ScatterRecord &amp;srec) const &#123; return false; &#125; virtual Vector3D emitted(const Ray &amp;in, const HitRecord &amp;rec, const float &amp;u, const float &amp;v, const Vector3D &amp;p) const;&#125;;Vector3D DiffuseLight::emitted(const Ray &amp;in, const HitRecord &amp;rec, const float &amp; u, const float &amp; v, const Vector3D &amp; p) const&#123; return TextureMgr::getSingleton()-&gt;getTexture(m_emitTex)-&gt;sample(u, v, p);&#125; &emsp;&emsp;这样，对于任意的物体，我们都可以把它当作一个光源，只要给这个物体赋予的材质为$DiffuseLight$即可，同时要注意给发光材质设置一个纹理。 四、天空盒背景&emsp;&emsp;之前在光线投射到背景中时，我们是直接返回设定的背景颜色（或通过插值、或直接指定背景）。同样，我们可以通过天空盒来丰富我们的场景细节。天空盒的相关原理比较简单，不再赘述。一个天空盒用边长为1的立方体表示，一个立方体我采用多个三角形构成立方体网格。这里有个问题，就是如何实现天空盒永远无法靠近的效果。在实时渲染时我们直接移除视图矩阵的位移，在光追这里我们直接将光源的出发点设为原点，方向保持不变，这样的一条射线再与天空盒立方体做求交并采样纹理即可。 1234567891011121314151617181920212223242526272829Vector3D Skybox::sampleBackground(const Ray &amp;ray)&#123; HitRecord rec; Ray r(Vector3D(0,0,0), ray.getDirection()); TextureMgr::ptr texMgr = TextureMgr::getSingleton(); int index = -1; for (int x = 0; x &lt; m_indices.size(); x += 3) &#123; int index1 = m_indices[x + 0]; int index2 = m_indices[x + 1]; int index3 = m_indices[x + 2]; if (triangleHit(r, 0.001f, FLT_MAX, rec, m_vertices[index1], m_vertices[index2], m_vertices[index3], m_vertices[index1].m_normal)) &#123; index = x; break; &#125; &#125; if(index != -1) &#123; int map = index / 6; return texMgr-&gt;getTexture(m_cubemap[map]) -&gt;sample(rec.m_texcoord.x, rec.m_texcoord.y, rec.m_position); &#125; else return Vector3D(0.0,0.0,0.0);&#125; 五、构建BVH树&emsp;&emsp;在整个光线追踪算法的渲染过程中，计算量最大的就是光线与场景图元的求交过程。如果不采用一些特殊的数据结构而只是用线性表存储场景物体的话，那么每一条射线都需要对这个存储场景物体的线性表遍历一次，这个射线碰撞检测的算法时间复杂度是$O(n)$的，当$n$比较大时，那么射线碰撞检测需要耗费绝大部分的光线追踪算法时间。射线相交检测的时间是目前光线追踪算法从理论到大规模实际应用过渡的主要瓶颈。为此，我们需要一些特殊的场景管理数据结构来加速这个过程，BVH树（全称为bounding volume hierachy，即层次包围体）是光线追踪领域常用的一种3D场景管理数据结构。它的启发思路就是通过一个简单的包围盒把物体包围起来，射线和场景中的物体求交之前，会先和这个包围盒进行求交，如果该射线没有碰到该包围盒，表明该直线一定不会和包围盒里的物体相交；如果该射线碰到该包围盒，那么再来计算射线是否和包围盒中的物体相交。我们采用包围体是AABB包围盒（即axis-aligned minimum bounding box，轴对齐的最小包围盒，简称轴向包围盒）。 &emsp;&emsp;BVH树本质上是对空间做分割，然后采用二分搜索快速判断射线会与哪些包围盒发生碰撞，从而使得算法的时间复杂度从$O(n)$降到了$O(log(n))$，这是一个非常明显的算法效率的提升，特别是当$n$数量逐渐增大的时候。每一次的判断过程如下列伪代码所示。如果射线与父节点的包围盒有交点，则进一步判断子节点与射线的相交情况，否则直接退出。 1234if (ray hits bounding object) return whether ray hits bounded objectselse return false &emsp;&emsp;BVH树全称是层次包围盒，故名思意，它是一个树形的层次结构，父节点的包围盒包围全部子节点所在的空间，正如下图11所示。蓝色和红色的包围盒被包含在紫色的大包围盒中，它们可能重叠，并且它们不是有序的，它们只是单纯地被包含在内部。 图1 层次包围盒 &emsp;&emsp;对于图1，检测的伪代码如下： 123456if (hits purple) hit0 = hits blue enclosed objects hit1 = hits red enclosed objectsif (hit0 or hit1) return true and info of closer hitreturn false 1、射线与包围盒相交判断&emsp;&emsp;我们采用的紧凑的包围盒是AABB包围盒，计算出了包围盒之后，我们还需要一个判断射线是否与包围盒相交的办法，不需要求出射线与包围盒的交点，只需判断是否存在交点即可！我们采用一种常见的”slab“方法，它是基于AABB包围盒的。三维的AABB包围盒由三个轴的区间表示，假设分别为$[x_0,x_1]$、$[y_0,y_1]$、$[z_0,z_1]$。 &emsp;&emsp;对于每一个区间，我们首先判断射线在边界的投影交点情况。三维空间中，$x=x_0$和$x=x_1$是一个平面，射线在这两个平面上的交点的$x$值可以通过将$x=x_0$和$x=x_1$带入射线的方程$P(t)=S+tV$的$x$分量得到： x_0=S_x+t_0*V_x \\\\ x_1=S_x+t_1*V_x \\tag {13} 图2 射线与边界的交点 &emsp;&emsp;从而可以求出$t_0$和$t_1$如下所示： t_0=\\frac{x_0-S_x}{V_x} \\\\ t_1=\\frac{x_1-S_x}{V_x} \\tag {14}&emsp;&emsp;关于$y$轴和$z$轴同理，我们求出了每条轴的交点分量，那么如何快速判断射线与包围盒区域是否存在相交的情况呢？为了便于理解，我们以二维的情况为例，则射线与二维的包围区域相交由如下三种情况： 图3 射线与边界相交的三种情况 &emsp;&emsp;我们求得$t$值是关于射线上的电到射线原点的距离，通过仔细观察上面的三张图片，我们可以发现在二维的情况下，当$max(t_0,t_2)&gt;min(t_1,t_3)$时，射线一定和区域存在交点，即射线与每个轴区间的左端点中的最大$t$值大于射线与每个轴区域间的右端点中的最小$t$值。 123456789101112131415161718192021222324252627282930313233343536bool hit(const Ray &amp;ray, float tmin, float tmax) const&#123; float t0, t1, invD; // x invD = 1.0f / ray.getDirection().x; t0 = (m_min.x - ray.getOrigin().x) * invD; t1 = (m_max.x - ray.getOrigin().x) * invD; if (invD &lt; 0.0f) std::swap(t0, t1); tmin = t0 &gt; tmin ? t0 : tmin; tmax = t1 &lt; tmax ? t1 : tmax; if (tmax &lt;= tmin) return false; // y invD = 1.0f / ray.getDirection().y; t0 = (m_min.y - ray.getOrigin().y) * invD; t1 = (m_max.y - ray.getOrigin().y) * invD; if (invD &lt; 0.0f) std::swap(t0, t1); tmin = t0 &gt; tmin ? t0 : tmin; tmax = t1 &lt; tmax ? t1 : tmax; if (tmax &lt;= tmin) return false; // z invD = 1.0f / ray.getDirection().z; t0 = (m_min.z - ray.getOrigin().z) * invD; t1 = (m_max.z - ray.getOrigin().z) * invD; if (invD &lt; 0.0f) std::swap(t0, t1); tmin = t0 &gt; tmin ? t0 : tmin; tmax = t1 &lt; tmax ? t1 : tmax; if (tmax &lt;= tmin) return false; return true;&#125; 2、BVH树的构建&emsp;&emsp;首先我们要考虑如何构建一颗BVH树，BVH数据结构本质就是一颗二叉树。每个树节点右两个子节点，当然子节点之间不存在空间上的顺序关系。树的内部节点都不存储实际的场景物体，仅存储一个包围盒，叶子节点才存储真正的场景物体。构建BVH树的工作考虑的是如何构造一棵可以有效描述当前场景信息的二叉树。这当中的关键是如何对毫无规律地散落在场景中的众物体进行划分，即决定哪些物体该划分到左子树上，哪些物体该划分到右子树上。我们可以把这个问题抽象成一个”划分策略“——我们总会按照某种”策略“划分场景的，待会再考虑具体有哪些策略。另外，由于我们是在3D空间中工作，为了将问题简化，用分而治之的角度看，我们可以首先建立一个”原则“：即决定在哪根轴（x,y,z）上进行划分。”原则“与”策略“的不同之处在于，不管用何种”策略“，总是遵守同一种”原则“。 &emsp;&emsp;决定在哪根轴（x,y,z）上进行划分，取决于场景中的物体在各个轴上分布的“散度”。如果这些物体沿着某根轴分布得最为“松散”（即落在该轴上靠一侧最近的物体与另一侧最近的物体，二者距离为最大），那么就沿该轴进行划分。还有一种方式，即采用随机的方式选取划分的轴，这样当场景物体分散的很随机时，实现的效果还不错。这里我采用随机选取一个轴的方法进行划分。 &emsp;&emsp;确定了以哪根轴进行划分，接下来就要考虑“怎么划分”。我们目前暂时实现按终点划分的策略，顾名思义，取中点划分的意思就是在先前选取的轴上取其中点作为划分点，中点以左划分到左子树，中点以右划分到右子树。这种划分的实现方式最为简单，但往往效果不是太好：因为物体的分布往往不是均匀的。其中一种糟糕的情况（a）是，某侧子树上可能会拥挤过多的物体，而另一侧子树上却太少，这对查找效率影响很大。另外还有一种糟糕的情况（b），就是包围盒之间互相“重叠”（overlapped）的情况。如果两棵子树对应的包围盒“重叠”的越多，那么一条射线穿过该区域时同时击中两子树的概率也就越大，这就意味着两棵子树都得进行相交测试。当然我们目前实现的BVH树没有考虑那么多。 1234567891011121314151617181920212223242526272829BVHNode::BVHNode(std::vector&lt;Hitable *&gt; &amp;list, int start, int end)&#123; // sort it randomly depend on int axis = static_cast&lt;int&gt;(3 * drand48()); if (axis == 0) sort(&amp;list[start], &amp;list[end], boxCompareX); else if (axis == 1) sort(&amp;list[start], &amp;list[end], boxCompareY); else if (axis == 2) sort(&amp;list[start], &amp;list[end], boxCompareZ); int length = end - start; if (length == 1) m_left = m_right = list[start]; else if (length == 2) &#123; m_left = list[start]; m_right = list[start + 1]; &#125; else &#123; m_left = new BVHNode(list, start, start + length / 2); m_right = new BVHNode(list, start + length / 2, end); &#125; // bounding box. AABB boxLeft, boxRight; if (!m_left-&gt;boundingBox(0, 0, boxLeft) || !m_right-&gt;boundingBox(0, 0, boxRight)) std::cerr &lt;&lt; \"no bounding box in BVHNode constructor\\n\"; m_box = AABB::surroundingBox(boxLeft, boxRight);&#125; 3、BVH树的遍历&emsp;&emsp;遍历BVH差不多是件直截了当的事情。在遍历的过程中，当发现射线与某个子节点相交的话，那么有无必要再检测下与另一子节点是否相交？答案是要的。因为两个节点无法保证完全“不重叠”，如下图所示，很有可能在检测另一子节点时发现了更近的交点。 123456789101112131415161718192021222324252627282930313233bool BVHNode::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const&#123; if (m_box.hit(ray, t_min, t_max)) &#123; HitRecord leftRec, rightRec; bool hitLeft = m_left-&gt;hit(ray, t_min, t_max, leftRec); bool hitRight = m_right-&gt;hit(ray, t_min, t_max, rightRec); // both hit. if (hitLeft &amp;&amp; hitRight) &#123; if (leftRec.m_t &lt; rightRec.m_t) ret = leftRec; else ret = rightRec; return true; &#125; // only left child. else if (hitLeft) &#123; ret = leftRec; return true; &#125; else if (hitRight) &#123; ret = rightRec; return true; &#125; else return false; &#125; else return false;&#125; 六、tbb多线程渲染&emsp;&emsp;到目前为止我们实现的光追渲染逻辑都是串行的，只能利用单核cpu运行我们的渲染程序。对于简单的场景来说，渲染的速度还是挺快的。但是当我们渲染复杂的模型时，单核光追的渲染速度慢到爆炸，渲染时间随着模型的面片数迅速增长，渲染时间动不动就数十小时！为此，我们迫切需要加速渲染程序。我们可以看到，每个像素着色之间是没有联系的，一个像素的着色值与其周围的像素计算无关，所以像素的着色计算是可以并行计算的。我们首先实现在cpu上利用多核加速我们的渲染程序。直接操纵原生的线程API不是非常好，因为这样的话我们必须知道当前电脑的核心数，并据此将循环做一个分割，以便充分利用每个cpu核心。Intel开发的TBB是非常有用的线程库，它屏蔽了底层的线程细节，自动根据我们给定的工作量做线程分割，充分利用电脑的全部cpu资源，而且使用起来也非常简单。这里利用多核线程的渲染速度加速比大致是当前电脑的核心数，也就是说，电脑的cpu核心越多，渲染速度越快。tbb的官方网站请看这里。 &emsp;&emsp;tbb的全称是Thread Building Blocks，这里我们只用了tbb的parallel_for接口，它对一个给定的for循环做划分，然后每个划分并行计算。我采用的parallel_for接口函数如下所示： 12template&lt;typename Range, typename Body&gt;void parallel_for( const Range&amp; range, const Body&amp; body, const auto_partitioner&amp; partitioner ) &emsp;&emsp;可以看到出现了三个参数：range、body和partitioner。range就是我们要做并行的for循环下标范围，通常采用一维的迭代器blocked_range指定。这里我把二重循环展开成一重循环。然后body就是函数执行体，这里我通过c++11的lambda表达式指定。最后的partitioner是线程的划分方法，通常直接采用auto_partitioner。并行版的光追渲染如下所示： 12345678910111213141516171819202122232425262728void Tracer::parallelThreadRender(Hitable *scene)&#123; parallel_for(blocked_range&lt;size_t&gt;(0, m_config.m_height * m_config.m_width, 10000), [&amp;](blocked_range&lt;size_t&gt;&amp; r) &#123; for (size_t i = r.begin(); i != r.end(); ++i) &#123; Vector4D color; size_t col = i % m_config.m_width; size_t row = i / m_config.m_width; for (int x = 0; x &lt; m_config.m_samplings; ++x) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = m_config.m_camera-&gt;getRay(u, v); color += deNan(tracing(ray, scene, &amp;m_samplingList,0)); &#125; color /= static_cast&lt;float&gt;(m_config.m_samplings); color.w = 1.0f; // gamma correction. color = Vector4D(sqrt(color.x), sqrt(color.y), sqrt(color.z), color.w); if(color.x &gt; 1.0f) color.x = 1.0f; if(color.y &gt; 1.0f) color.y = 1.0f; if(color.z &gt; 1.0f) color.z = 1.0f; drawPixel(col, row, color); &#125; &#125;, auto_partitioner());&#125; 七、蒙特卡罗积分&emsp;&emsp;蒙特卡罗积分方法（Monte Carlo method）是数值分析中的一个重要分支，它的核心概念是使用随机性来解决确定性的问题。大数定律告诉我们，对于满足某个概率分布的随机变量，其数学期望所描述的积分可以使用这个随机变量随机抽样的样本均值来近似，因此在一定的误差范围内，我们能够使用大量的随机数来近似积分运算的结果。在计算机图形学中， 蒙特卡罗方法主要被应用于物理模拟以及光照传输中的积分运算，在离线渲染领域， 渲染方程几乎只能使用蒙特卡洛方法来进行计算。为了深入理解蒙特卡罗方法，我们首先要复习概率论相关的一些基础内容。以下的内容主要参考秦春林的那本书《全局光照技术：从离线到实时渲染》。 1、概率密度函数、概率分布函数&emsp;&emsp;概率密度函数（probability density function, 简称PDF）用于描述连续型随机变量所服从的概率分布，对于连续随机变量$X$，其概率密度函数$p(x)$是通过落于$x$附近的区间$[x,x+dx]$内的随机数的概率$p(x)dx$来定义的，然而这种定义方式并不直观，所以连续随机变量的概率分布一般通过更直观的称为概率分布函数或者累积分布函数（cumulative distribution function, 简称CDF）来定义，连续随机变量$X$的累积分布函数用大写字母$P$表示，其定义如下： P(y)=Pr\\{x\\leq y\\}=\\int_{-\\infty}^yp(x)dx \\tag {15}&emsp;&emsp;可以看到，概率分布函数$P(y)$定义的是所有随机数的值中小于或等于$y$的随机变量的概率的积分，即理解成对于一个随机数$x$，其小于等于$y$的概率。因此，概率分布函数是一个递增函数。连续随机变量的概率密度函数$p(x)$具有以下的属性： \\forall x:p(x) \\geq 0 \\tag {16} \\int _{-\\infty}^{+\\infty}p(x)dx = 1 \\tag {17} p(x)=\\frac{dP(x)}{dx} \\tag {18}&emsp;&emsp;其中，式$(8)$说明了$p(x)$和$P(x)$的关系，前者是后者的导数。那么给定一个随机变量的区间范围$[a,b]$，随机变量的值$x$落在这个区间的概率计算如下： Pr\\{a\\leq x\\leq b\\}=Pr(x\\leq b)-Pr(x\\leq a)\\\\ =P(b)-P(a)=\\int_a^b p(z)dz \\tag {19}&emsp;&emsp;注意，这里的$Pr$函数是概率函数，而不是概率分布函数。直观来讲，概率密度函数$p(x)$给定的并不是随机变量取值$x$的概率，概率密度函数与轴围成的面积才是给定的概率。如下所示，图(a)是概率分布函数，而图$(b)$则是概率密度函数，给定区间的$[a,b]$的概率就是下图(b)中的面积，这也对应了公式$(19)$中的积分形式（积分的几何意义就是面积）。 &emsp;&emsp;在这里，我们要特别关注的一个分布，那就是均匀分布！对于$[a,b]$区间上的均匀分，其概率密度函数为常数$\\frac{1}{b-a}$，它表示随机抽样结果落于区间$[x,x+dx]$的概率在每个$x$处都相同。均匀分布的随机变量是整个蒙特卡罗方法的基础，在计算机模拟中，通过都是由系统提供的random()函数生成某个区间内的均匀分布，然后通过一些方法将均匀分布的随机变量转换为具有任意概率密度分布的随机变量。 2、数学期望&emsp;&emsp;对于离散随机变量$X$，假设其值$x_i$对应的抽样概率为$p_i$，则该随机变量$X$的数学期望，或称为均值，为： E[X]=\\Sigma_{i=1}^np_ix_i \\tag {20}&emsp;&emsp;数学期望代表的是对一个随机变量$X$进行抽样的平均结果。例如，对于骰子的例子，它的数学期望为： E[X_{die}]=\\Sigma_{i=1}^6p_i x_i\\\\ =\\Sigma_{i=1}^6\\frac16x_i=\\frac16(1+2+3+4+5+6)=3.5 \\tag {21}&emsp;&emsp;相应地，对于连续随机变量$X$，其期望值为随机变量值$x$与其概率密度函数$p(x)$的乘积在全定义域上的积分： E[X]=\\int_{-\\infty}^{+\\infty}xp(x)dx \\tag {22}&emsp;&emsp;连续随机变量$X$的数学期望为什么上面的公式$(22)$形式呢？这其实可以通过离散划分连续随便变量的定义域，然后按照离散数学期望得到一个近似的公式，当划分数趋向于无穷大且划分区间趋向于无穷小时，就是公式$(22)$的积分定义。如下所示： E[X]\\approx\\frac{b-a}{n}\\Sigma_{i=1}^{n}x_ip(x_i) \\\\ n\\to+\\infty,\\ \\frac{b-a}{n}\\Sigma_{i=1}^{n}x_ip(x_i)=\\int_a^bxp(x)dx=E[X]&emsp;&emsp;通常我们对随机变量的函数更感兴趣。考虑以随机变量$X$为自变量的函数$Y=g(X)$，我们只知道随机变量$X$的概率分布，怎样求出随机变量$Y$的数学期望值呢？我们可以通过无意识的统计规律（law of the unconsicious statistician）来求随机变量函数的数学期望：设$Y$是随机变量$X$的函数$Y=g(X)$，且函数$g$是连续函数。若$X$是离散型随机变量，它的概率函数为$P\\{X=x_i\\}=p_i,i=1,2,…$，则有： E[Y]=E[g(X)]=\\Sigma_{i=1}^{\\infty}g(x_i)p_i \\tag {23}&emsp;&emsp;若$X$是连续型随机变量，它的概率密度函数为$p(x)$，则有： E[Y]=E[g(X)]=\\int_{-\\infty}^{+\\infty}g(x)p(x)dx \\tag {24}&emsp;&emsp;该方法的重要意义在于：当求$E[Y]$时，我们不必求出$Y$的分布律或概率密度函数，只需利用$X$的分布律或概率密度即可。 3、大数定律&emsp;&emsp;在统计学中，很多问题涉及对大量独立的随机变量抽样$x_i$的和进行处理，这些随机变量拥有相同的概率密度函数$p(x)$，这样的随机变量称为独立同分布的随机变量。当这些随机变量抽样的和被除以这些随机变量抽样的数量$N$时，我们就得到该随机变量的期望值的一个估计： E[X]\\approx\\overline X=\\frac1N\\Sigma_{i=1}^Nx_i \\tag {25}&emsp;&emsp;随着抽象数量$N$的增大，该估计的方差逐渐减小。当$N$的值足够大时，该估计的值就能够充分接近实际数学期望的值，这样我们就能够将统计方法用于解决确定性问题。大数定律（law of large numbers）告诉我们，当$N\\to\\infty$时，我们可以确定随机变量的统计平均值趋近于数学期望的值，即： P\\{E[X]=lim_{N\\to \\infty}\\frac1N\\Sigma_{i=1}^Nx_i\\} = 1 \\tag {26}&emsp;&emsp;因此，随机变量的数学期望可以通过对随机变量执行大量的重复抽样来近似计算得到。 4、蒙特卡罗积分&emsp;&emsp;假设我们要计算一个一维函数的积分，如$\\int_a^bf(x)dx$，数值分析方法通常采用一些近似方法来计算积分。一种最简单的求积分的方法就是采用梯形法，它通过将被积函数沿作用域上划分成多个区域，然后计算这些区域面积的和。这种方法不适用于多维积分的计算，计算机图形学领域用的最多的还是蒙特卡罗方法。大数定律用于对数学期望的积分公式进行估计，即对积分$\\int_{-\\infty}^{+\\infty}xf(x)dx$进行估计。但是通常情况下我们要求的积分公式是对任意的一个函数积分，假设函数$g(x)$的定义域为$x\\in S$（可以是一个多维空间），我们希望计算如下的积分： I=\\int_{x\\in S}g(x)dx \\tag {27}&emsp;&emsp;现在先不管公式$(27)$。由前面我们知道，给定任意一个关于随机变量的实数函数$f$以及服从$p(x)$概率密度函数的随机变量$x$，我们可以采用如下的公式来近似计算随机变量函数$f(x)$的数学期望： E[f(x)]=\\int_{x\\in S}f(x)p(x)dx\\approx\\frac1N\\Sigma_{i=1}^Nf(x_i) \\tag {28}&emsp;&emsp;现在我们令公式$(27)$的被积函数$g(x)=f(x)p(x)$，则$f(x)=\\frac{g(x)}{p(x)}$，那么公式$(28)$即可转变对公式$(27)$的形式，如下所示： \\int_{x\\in S}f(x)p(x)dx=\\int_{x\\in S}g(x)dx\\approx\\frac1N\\Sigma_{i=1}^N\\frac{g(x_i)}{p(x_i)} \\tag {29}&emsp;&emsp;可以看到通过这个变换，我们巧妙地转换成我们要求的积分公式，这就是蒙特卡洛方法求积分的核心思想。公式$(29)$的期望值为： E[\\frac1N\\Sigma_{i=1}^N\\frac{g(x_i)}{p(x_i)}]=\\frac1N\\Sigma_{i=1}^NE[\\frac{g(x_i)}{p(x_i)}]\\\\ =\\frac1NN\\int\\frac{g(x)}{p(x)}p(x)dx=\\int g(x)dx \\tag {30}&emsp;&emsp;而公式$(29)$的估计方差为： \\sigma^2=\\frac1N\\int(\\frac{g(x)}{p(x)}-I)^2p(x)dx \\tag {31}&emsp;&emsp;可以看到，随着$N$的增大，公式$(31)$的方差随之降低（成反比），这就是一般蒙特卡罗方法的特点。实际上蒙特卡罗方法最大的问题就是估计逼近正确结果的速度非常慢。理论上，公式$(29)$的$p(x)$函数的选择可以是任意的，这也是蒙特卡罗方法的优点，因为通常很难生成与被积函数具有一致分布的随机数。从公式$(31)$也可以看出，通过使$g(x_i)$和$p(x_i)$的比值尽可能地小也可以减少估计误差，在实践上通常我们尽可能地使$p(x)$的分布接近于$g(x)$。综上，蒙特卡洛积分方法计算任意函数的积分步骤如下： 首先对一个满足某种概率分布的随机数进行抽样； 使用该抽样值计算$\\frac{g(x_i)}{p(x_i)}$的值，这称为该样本的贡献值； 最后对所有抽样点计算的结果求平均值。 &emsp;&emsp;上面的步骤中，最困难的就是怎么样对一个具有任意分布函数的随机变量进行抽样。 5、随机抽样&emsp;&emsp;首先定义什么是抽样。给定一个定于域空间$\\Omega_0$及其概率密度函数$p(x)$，其中$x\\in \\Omega_0$，则应有： \\int_{\\Omega_0}p(x)dx=1 \\tag {32}&emsp;&emsp;抽样是这样的一个算法，它能够从$p(x)$对应的随机变量$X$中产生一系列随机数$X1,X2,…$，使得对任意的$\\Omega \\in \\Omega_0$满足如下： P\\{X_k\\in\\Omega\\}=\\int_{\\Omega}p(x)dx\\leq 1 \\tag {33}&emsp;&emsp;在实现中我们并不能直接从$p(x)$产生随机数，在计算机程序中这个过程必须要求首先具有某些基础随机数的一个序列。我们通常采用均匀随机数random来产生一个均匀分布的随机数，然后用来作为抽象所需的基础随机数。目前抽象方法根据不同情况有不同的方法，这里目前只介绍逆变换算法。 &emsp;&emsp;逆变换算法的定义为：设$X$是连续随机变量，其概率分布函数为$P_X$，若随机变量$Y$是一个$[0,1]$上的均匀分布，则随机变量$P_X^{-1}(Y)$具有和$X$一样的概率分布。即我们通过概率分布函数的反函数来获取服从$p(x)$概率密度函数的随机变量，注意是概率分布函数$P(x)$的反函数，而不是概率密度函数$p(x)$的反函数。有时我们不知道概率分布函数，这时我们可以通过概率密度函数来求它的概率分布函数。 &emsp;&emsp;逆变换算法从一个概率密度函数$p(x)$产生随机数$X_i$的步骤如下： 首先计算$p(x)$的概率分布函数：$P(x)=\\int_0^xp(t)dt$； 其次计算累计分布函数的反函数：$P^{-1}(x)$； 然后从一个$[0,1]$上的均匀分布产生一个随机数$\\phi$； 最后将随机数$\\phi$代入$P^{-1}(x)$求出服从$p(x)$分布的随机数：$X_i=P^{-1}(\\phi)$。 八、重要性采样&emsp;&emsp;重要性采样（importance sampling）是蒙特卡罗方法中最重要的方差缩减方法，它通过选择对一个与目标概率分布具有相似形状的分布函数进行抽样来减少方差。重要性采样试图在被积函数中贡献较多的区域放置更多的采样点，以体现这部分区域的重要性。给定一个概率密度函数$p(x)$以及根据该概率密度函数抽样得到的$N$个随机数$x_i$，根据蒙特卡洛方法，被积函数$f(x)$的积分$I$（即前面的公式（27），被积函数换成$f(x)$）可以通过以下公式来近似估计： I_{N}=\\frac1N\\Sigma_{i=1}^N\\frac{f(x_i)}{p(x_i)} \\tag {34}&emsp;&emsp;一个理想的估计的方差应该为$0$，即： \\sigma^2=\\frac1N\\int(\\frac{f(x)}{p(x)}-I)^2p(x)dx=0 \\tag {35}&emsp;&emsp;注意到公式$(35)$中，被积函数部分的$p(x)&gt;0$，故应有$(\\frac{f(x)}{p(x)}-I)^2=0$，从而有如下的推导： p(x)=\\frac{|f(x)|}{I} \\tag {36}&emsp;&emsp;若我们采用公式$(36)$得到的概率密度函数进行采样，那么方差就会被完全消除。但是公式$(36)$要求我们首先计算$I$的值，而这正是我们尝试去求解的，因而行不通。但是我们可以通过选取与被积函数$f(x)$具有相似形状的概率密度函数来减少方差。选择用于抽样的概率密度函数非常重要，尽管蒙特卡罗方法本身没有限制对概率密度函数的选择，但是选择不好的概率密度函数会大大增加蒙特卡罗估计的方差。 &emsp;&emsp;直观来讲，重要性采样就是根据被积函数$f(x)$的值来调整$p(x)$的概率分布。$f(x)$值大的地方，就多采样几个点；$f(x)$值小的地方，就少采样一些点。$p(x)$概率密度函数越是接近$f(x)$，蒙特卡罗方法估算的结果就越精确。 1、复合重要性采样&emsp;&emsp;在实际的情景中，计算机图形学中的被积函数通常非常复杂，它们可能是不连续的，通常在少数区间拥有奇点或者一些较大的值，所以很难找到一个简单的与被积函数相似的分布来做重要性采样。例如，我们考虑渲染中最普通的直接光源的计算公式，如下所示： L_o(p,v)=\\int_{\\Omega}f_r(l,v)\\times L_i(p,l)cos\\theta_id\\omega_i \\tag {37}&emsp;&emsp;我们可以选取$L_i$或者$f_r$来做重要性采样，但是这种方式表现效果并不佳。考虑一个接近镜面的BRDF表面被一个球形面积光照亮的例子。如下所示： &emsp;&emsp;若将面积光源的分布$L_i$作为重要性采样概率密度函数，因为物体表面几乎是镜面的，所以除了沿镜面反射光方向$\\omega_i$，大部分光源上的采样对在最终的光照贡献都为0，因此估计的方差会非常大；而若采用BRDF分布作为重要性采样分布，那么对于小面积光源，依然会导致很大的方差。 &emsp;&emsp;因此，我们通常使用更复杂的采样方式，从而降低估算的方差。通常是根据被积函数的分布特征对其进行区域划分，然后在不同特征的区域上使用不同的分布函数进行采样，最后将这些结果以某种方式进行混合。复合重要性采样就是这一类的采样方法，它提供了一个策略使得可以从多个不同的分布中采样，然后对这些不同的采样结果进行加权组合。复合重要性采样可以简单地分成以下几步： 首先，选取一系列的重要性分布$p1,…,p_n$，使得对于被积函数$f$的每一个函数值比较大的区域$\\Omega_i$，在这个区域$\\Omega_i$，分布函数$p_i$近似为被积函数$f$。通常一个复杂的被积函数是多个不相关的简单分布的乘积形式，所以这些重要性分布来源于这些简单分布。 然后，从每个分布$p_i$产生$n_i$个随机数$X_{i,1},…,X_{i,n_i}$； 最后，将所有的分布估算结果通过加权组合起来。 &emsp;&emsp;复合重要性采样加权组合公式如下所示： I_N=\\Sigma_{i=1}^n\\frac1n_i\\Sigma_{j=1}^{n_i}\\omega_i(X_{i,j})\\frac{f(X_{i,j})}{p_i(X_{i,j})} \\tag {38}&emsp;&emsp;公式$(38)$中的$\\Sigma_{i=1}^n$表明结果是由$n$个采样技术的叠加，$\\frac1n_i\\Sigma_{j=1}^{n_i}\\omega_i(X_{i,j})\\frac{f(X_{i,j})}{p_i(X_{i,j})}$即表示一种特定的采样分布$p_i$的蒙特卡罗估算结果。可以看到，这里还乘上了一个权重系数$\\omega_i(X_{i,j})$，$w_i(x)$可以在每个$x$处的值不一样，只要保证对于给定的$x$值，满足$\\Sigma_{i=1}^n\\omega_i(x)=1$即可。 2、平衡启发式&emsp;&emsp;现在我们需要确定公式$(38)$中的权重系数计算方法。假设我们采用两个采样分布$p_1(x)$和$p_2(x)$，两个采样分布单独的采样估算结果分别为$\\frac1{n_1}\\Sigma\\frac{f(x)}{p_1(x)}$和$\\frac{1}{n_2}\\Sigma\\frac{f(x)}{p_2(x)}$，它们各自的方差都很大，所以我们给它们各自乘上一个系数进行加权组合。为了尽可能地发挥每个采样分布的优势，我们往往尽可能地保证在每个区域贡献较大的采样分布拥有更大的权值系数。考虑如下的权重系数函数： \\omega_i(x)=\\frac{c_ip_i(x)}{\\Sigma_j^nc_jp_j(x)} \\tag {39}&emsp;&emsp;其中$c_i$是每个采样分布$p_i$对应的采样数量占比，即$c_i=\\frac{n_i}{N}$，故$\\Sigma_ic_i=1$，$c_i$在采样之前我们就可以确定得到。公式$(39)$被称为平衡启发式，将$c_i=\\frac{n_i}{N}$和公式$(39)$代入到公式$(38)$，可以推导出如下的标准的蒙特卡洛估算方法（做一些消去）： I_N= \\Sigma_{i=1}^n\\frac1n_i\\Sigma_{j=1}^{n_i}\\omega_i(X_{i,j})\\frac{f(X_{i,j})}{p_i(X_{i,j})}\\\\ =\\Sigma_{i=1}^n\\Sigma_{j=1}^{n_i}\\frac1n_i\\frac{c_ip_i(X_{i,j})}{\\Sigma_j^nc_jp_j(X_{i,j})}\\frac{f(X_{i,j})}{p_i(X_{i,j})}\\\\ =\\Sigma_{i=1}^n\\Sigma_{j=1}^{n_i}\\frac1n_i\\frac{\\frac{n_i}{N}p_i(X_{i,j})}{\\Sigma_j^nc_jp_j(X_{i,j})}\\frac{f(X_{i,j})}{p_i(X_{i,j})}\\\\ =\\frac{1}{N}\\Sigma_{i=1}^n\\Sigma_{j=1}^{n_i}\\frac{f(X_{i,j})}{\\Sigma_j^nc_jp_j(X_{i,j})} \\\\ =\\frac{1}{N}\\Sigma_{i=1}^n\\Sigma_{j=1}^{n_i}\\frac{f(X_{i,j})}{\\overline p(X_{i,j})} \\tag {40}&emsp;&emsp;其中，$\\overline p(x)$又被称为联合抽样分布，其数学公式如下所示。总的采样数$N$，每个分布$p_i$采集$n_i$个随机数$X_{i,j}$。以上就是平衡启发式的核心思想，一种很自然地组合多种采样分布的方式。我们采用一个单一的与$i$无关的分布$\\overline p(x)$来表述这种组合方式。 \\overline p(x)=\\Sigma_{i=1}^nc_ip_i(x) \\tag {41}九、MC光线追踪&emsp;&emsp;了解了相关的原理，接下来我们就实现一个MC（Monte Carlo，蒙特卡罗）光线追踪，主要的参考资料是Peter Shirley的《Ray Tracing_ the Rest of Your Life.pdf》。采样方法是复合重要性采样，复合的采样分布为Lambertian材质BRDF采样分布加上光源采样分布。 1、立体角&emsp;&emsp;在球面坐标中，一个方向向量我们通常采用$(\\theta, \\phi)$来唯一地表示，分别是天顶角和方位角。在衡量发光强度和辐射辐射度量学中，立体角有着广泛的应用。立体角描述了站在某一点的观察者观测到的物体大小的尺度，它被定义为球表面截取的面积微分与球半径平方之比，单位为球面度，写作$sr$。显然，立体角是二维圆心角的三维扩展： d\\omega=\\frac{dA}{r^2} \\tag {42}&emsp;&emsp;更一般的情况，立体角通常转换为$(\\theta, \\phi)$来表示，在单位球体上，$d\\omega=dA$，我们转换成用$(\\theta,\\phi)$求微分面积$dA$。我们知道二维的弧长公式为：圆心角弧度数*半径（注意圆心角要换成弧度制）。如下所示，$\\theta$和$\\phi$对应的弧长为： s_{\\theta}=\\theta * r_{\\theta} \\\\ s_{\\phi}=\\phi * r_{\\phi} \\tag {43} 图4 求弧长 &emsp;&emsp;公式$(43)$中的$r_\\theta$其实就是球体半径，$r_\\phi$与$r_\\theta$的关系为：$r_\\phi=r_\\theta * sin\\theta$。微分面积$dA$可以看成是一个矩形，宽和高分别为对应的弧长$d_{r_\\phi}$和$d_{r_\\theta}$，根据公式$(43)$我们可知$d_{r_\\phi}$、$d_{r_\\theta}$计算方法如下： d_{s_\\theta}=r_{\\theta}d\\theta\\\\ d_{s_\\phi}=r_{\\phi}d\\phi \\tag {44}&emsp;&emsp;对于单位球体，$r_\\theta=r=1,r_\\phi=r_\\theta*sin\\theta=sin\\theta$，从而立体角微分可转换成如下表示： d\\omega=dA=d_{s_\\theta}*d_{s_\\phi}=sin\\theta d\\theta d\\phi \\tag {45}2、Lambertian材质BRDF采样&emsp;&emsp;对于Lambertian材质我们假定其光线的散射分布与$cos\\theta$成正比，这里的$\\theta$是光线与表面法向量的夹角，也就是说在靠近法线的方向光线散射得比较多。当光线与表面法线夹角大于90度时，不发生光线散射。我们记得光线得散射概率密度函数pdf为$C*cos\\theta$，其中$C$为某个常数。对于概率密度函数，我们必须保证其在全定义域上的（这里就是整个半球方向）积分为1，即有（涉及到了立体角转球面坐标表示形式和求定积分）： \\int_{\\Omega}C*cos\\theta d\\omega=C\\int_0^{2\\pi}d\\phi\\int_0^{\\frac\\pi2}cos\\theta sin\\theta d\\theta\\\\ =C*2\\pi\\int_0^{\\frac\\pi2}sin\\theta d(\\sin\\theta)=C*2\\pi * \\frac12=1\\\\ \\to C=\\frac{1}{\\pi} \\tag {46}&emsp;&emsp;从而，Lambertian材质的光线散射概率密度函数PDF，记为$pS(direction)$，如下所示： pS(direction)=\\frac{cos\\theta}{\\pi} \\tag {47}&emsp;&emsp;现在我们要根据这个概率密度函数生成服从该分布的随机半球向量，根据前面随机抽样部分，我们首先要求出它的概率分布函数。根据定义，概率分布函数就是对概率密度函数积分： P=\\int_{\\Omega}\\frac{cos\\theta}{\\pi}d\\omega =\\int_0^{2\\pi}d\\phi\\int_0^\\theta \\frac{cos\\ t}{\\pi}sin\\ t\\ dt\\\\ =2\\pi * \\frac1\\pi \\int_0^{\\theta}sin\\ td(sin\\ t)=sin^2\\theta=1-cos^2\\theta \\tag {48}&emsp;&emsp;根据逆变换算法，我们要取概率分布函数的反函数。这里有个小技巧，我们不需要调用反三角函数得到反函数，我们只需得到$cos\\theta$即可。因为即便调用反三角函数得到$\\theta$，后面我们将$(\\theta,\\phi)$转换成笛卡尔坐标向量的时候还是要调用三角函数$cos$，我们直接避免这个比较费时的过程。所以，任取一个$[0,1]$上均匀随机数$r_2$： cos\\theta =\\sqrt{1-r_2} \\tag {49}&emsp;&emsp;公式$(49)$只得到随机方向向量的$\\theta$，我们还需要$\\phi$。对于Lamberatian材质，光线在方向角上是均匀分布的，故其概率密度函数为$\\frac{1}{2\\pi}$，概率分布函数为$\\frac{\\phi}{2\\pi}$。故对$\\phi$的随机采样如下，任取一个$[0,1]$上的均匀随机数$r_1$： \\frac{\\phi}{2\\pi}=r_1 \\to \\phi=r_1*2\\pi \\tag {50}&emsp;&emsp;采样得方向向量的$(\\theta,\\phi)$，我们还要将其转换到笛卡尔坐标系的形式，这个过程不难理解，仔细观察图4，不再赘述。从而，服从公式$(47)$采样方向向量的代码如下所示： 12345678910 static Vector3D randomCosineDir()&#123; float r1 = drand48(); float r2 = drand48(); float z = sqrt(1-r2); float phi = 2 * M_PI * r1; float x = cos(phi) * 2 * sqrt(r2); float y = sin(phi) * 2 * sqrt(r2); return Vector3D(x,y,z);&#125; &emsp;&emsp;值得注意的是，我们的采样是以物体表面的切线和法线构成的坐标轴为参考系的，其中z轴方向是表面的法线向量。因此，通过上面的代码采样的得到方向向量还要转到该局部坐标系下。这个过程可以构建矩阵，也可以直接将方向向量三个分量与轴向量相乘，最后相加得到。我们采用了后者，首先构建一个局部坐标的正交基（Ortho-normal Bases）： 123456789101112131415161718192021222324252627282930313233343536373839class ONB&#123;private: Vector3D m_axis[3];public: ONB() = default; Vector3D u() const &#123; return m_axis[0]; &#125; Vector3D v() const &#123; return m_axis[1]; &#125; Vector3D w() const &#123; return m_axis[2]; &#125; Vector3D operator[](int i) const &#123; return m_axis[i]; &#125; Vector3D local(float a, float b, float c) const &#123; return u() * a + v() * b + w() * c; &#125; Vector3D local(const Vector3D &amp;a) const &#123; return u() * a.x + v() * a.y + w() * a.z; &#125; void buildFromW(const Vector3D &amp;n);&#125;;void ONB::buildFromW(const Vector3D &amp;n)&#123; m_axis[2] = n; m_axis[2].normalize(); Vector3D a; if(fabs(w().x) &gt; 0.9f) a = Vector3D(0,1,0); else a = Vector3D(1,0,0); m_axis[1] = w().crossProduct(a); m_axis[1].normalize(); m_axis[0] = w().crossProduct(v()); m_axis[0].normalize();&#125; &emsp;&emsp;再构建一个PDF虚类，将PDF函数的函数值和采样抽线为$value$接口和$generate$接口。并继承它创建CosinePDF类，可以看到CosinePDF的$value$是按照公式$(47)$计算的： 123456789101112131415161718192021222324252627282930313233343536class PDF&#123;public: virtual float value(const Vector3D &amp;driection) const = 0; virtual Vector3D generate() const = 0;&#125;;class CosinePDF : public PDF&#123;private: ONB uvw;public: CosinePDF(const Vector3D &amp;w) &#123; uvw.buildFromW(w); &#125; virtual float value(const Vector3D &amp;driection) const; virtual Vector3D generate() const;&#125;;float CosinePDF::value(const Vector3D &amp;direction) const&#123; Vector3D dir = direction; dir.normalize(); float cosine = dir.dotProduct(uvw.w()); if(cosine &gt; 0.0f) return cosine / M_PI; else return 0.0f;&#125;Vector3D CosinePDF::generate() const&#123; return uvw.local(Vector3D::randomCosineDir());&#125; 3、直接光源采样&emsp;&emsp;显然在靠近光源的方向上，光照值对物体表面的颜色贡献更大，因此直接对光源采样对减少蒙特卡洛积分的方差有非常重要的作用。直接光源采样需要我们首先求采样分布的概率密度函数，目前我们先讨论一个最简单的光源例子，即矩形光源。假设矩形光源的面积为A，那么这个矩形光源的直接均匀采样的概率密度函数PDF为$\\frac1A$，但是通常我们采样的单位是立体角微分，如下所示， 图5 直接光源采样 &emsp;&emsp;$d\\omega$与$dA$存在着一个对应关系，实际上我们可以通过前面提到的立体角定义（即公式$(42)$）得到$d\\omega$与$dA$的关系如下所示，这个公式不难理解。其中$\\alpha$夹角是采样方向向量与矩形表面的法线向量的夹角，$dAcos\\alpha$实际上是将矩形的微分面积$dA$投影到采样方向$pq$上，这是因为从$pq$方向看去只能看到$dAcos\\alpha$这个大小的面积，然后再比上半径长度的平方$||pq||^2$，这是立体角的定义。 d\\omega=\\frac{dA*cos\\alpha}{||pq||^2} \\tag {51}&emsp;&emsp;现在对$dA$的采样概率为$\\frac{dA}{A}$，在球体方向上对立体角$d\\omega$采样的概率为$p(direction)d\\omega$，其中$p(direction)$是我们假定的对光源直接采样的概率密度函数。理论上来说，$\\frac{dA}{A}$应该等于$p(direction)d\\omega$，即有： p(direction)*\\frac{dA*cos\\alpha}{||pq||^2}=\\frac{dA}{A}\\\\ \\to p(direction)=\\frac{||pq||^2}{Acos\\alpha} \\tag {52}&emsp;&emsp;公式$(52)$推导出了我们要找的直接光源采样的概率密度函数。根据逆变换算法，我们还要求它的概率分布函数从而生成服从公式$(52)$概率密度函数的随机采样方向，但是这里其实没有必要。我们直接在矩形光源上随机采样一个点，然后将这个采样点与物体表面上的点连接起来就是我们的直接光源采样方向，通过这个方法省去了比较复杂的高数推导过程。 &emsp;&emsp;有了以上的理论基础，我们接下来就实现矩形的直接光源采样。我这里的定义的一个矩形平面是由两个三角形组成，默认是在xz平面上的边长为2的正方形。 1234567891011121314151617181920212223242526272829Vector3D Plane::random(const Vector3D &amp;o) const&#123; Vector3D center = m_transformation.translation(); Vector3D leftCorner; float width = m_transformation.scale().x * 2.0f; float height = m_transformation.scale().z * 2.0f; leftCorner.x = center.x - m_transformation.scale().x; leftCorner.z = center.z - m_transformation.scale().z; leftCorner.y = center.y; Vector3D random_point(leftCorner.x + drand48() * width, leftCorner.y, leftCorner.z + drand48() * height); return random_point - o;&#125;float Plane::pdfValue(const Vector3D &amp;o, const Vector3D &amp;v) const&#123; HitRecord rec; if(this-&gt;hit(Ray(o,v), 0.001f, FLT_MAX, rec)) &#123; float area = m_transformation.scale().x * 2.0f * m_transformation.scale().z * 2.0f; float distance_squared = v.getSquaredLength(); float cosine = fabs(v.dotProduct(rec.m_normal) / v.getLength()); float ret = distance_squared / (cosine * area); return ret; &#125; else return 0.0f;&#125; &emsp;&emsp;除了矩形区域光源，我们接下来还添加一个对球形区域光源的重要性采样。我们采取的坐标系依然是球形光源的局部坐标，而且依然是对光源区域做均匀采样。设想我们从物体表面上的一点望向一个球形区域光源，我们能够看到的区域就是我们要做均匀采样的区域，采样方法依然是围绕$(\\theta,\\phi)$展开，其中$\\theta$是采样方向向量与物体表面的点与球心构成的方向向量的夹角。 &emsp;&emsp;显然方位角$\\phi$依然是$[0,2\\pi]$的范围，不然我们不可能看到一个圆形。而$\\theta$则需要做一些限制，它现在有个上界，如下图6所示，P是物体表面上的一点，C为球形光源的球心，R是球形光源的半径。 图6 球形区域光源采样 &emsp;&emsp;由图6可知，$sin(\\theta_{max})=\\frac{R}{||C-P||}$，相应的$\\theta_{max}$的余弦值如下所示： cos(\\theta_{max})=\\sqrt{1-\\frac{R^2}{||C-P||^2}} \\tag {53}&emsp;&emsp;然后我们是对$\\theta$和$\\phi$做均匀采样，$\\phi$的采样与前面Lambertian采样一样，这里不再赘述。对于$\\theta$，因为是均匀采样，那么它的概率密度函数必然也是一个常数，我们设为$C$，那么其概率分布函数计算如下： P=\\int_{\\Omega}Cd\\omega =\\int_0^{2\\pi}d\\phi \\int_0^{\\theta}Csint\\ dt\\\\ =2\\pi C(1-cos\\theta) \\tag {54}&emsp;&emsp;根据逆变换算法，取$[0,1]$上的均匀随机数$r_2$，并结合公式$(54)$的反函数，可得采样的$cos\\theta$如下： cos(\\theta)=1-\\frac{r_2}{2\\pi C} \\tag {55}&emsp;&emsp;现在有个问题就是$C$这个具体是多少？我们已经知道$\\theta$的上界$\\theta_{max}$，当$\\theta=\\theta_{max}$时，应该取概率分布函数值$P(\\theta_{max})$为1，也就是$r_2=1$。故将其代入公式$(55)$我们可以得到$C$的具体表达式： C=\\frac{1}{2\\pi (1-cos\\theta_{max})} \\tag {56}&emsp;&emsp;然后再将公式$(56)$和公式$(53)$代入公式$(55)$，可得： cos(\\theta)=1+r_2(cos(\\theta_{max})-1)\\\\ =1+r_2(\\sqrt{1-\\frac{R^2}{||C-P||^2}}-1) \\tag {57}&emsp;&emsp;公式$(56)$j就是我们所需的概率密度函数，可以看起来不是很直观，这里我稍微解释一下。公式$(56)$其实就是我们从物体表面上的一点观测到的球形光源所占的立体角的倒数（注意，这里的立体角是以物体表面上的一点为球心而不是球形光源的球面上的立体角）！立体角的几何意义是就是单位球体上的面积，然后做一个倒数是因为我们是做均匀随机采样。立体角的求法如下所示： SolidAngle =\\int_0^{2\\pi}d\\phi\\int_0^{\\theta_{max}}sin\\theta d\\theta=2\\pi(1-cos(\\theta_{max})) \\tag {58}&emsp;&emsp;可以看到公式$(58)$求得的结果就是公式$(56)$中的分母。取球形光源上的随机一点采样算法如下，就是公式$(57)$的实现。 12345678910static Vector3D randomToSphere(float radius, float distance_squared)&#123; float r1 = drand48(); float r2 = drand48(); float z = 1 + r2 * (sqrt(1 - radius * radius/distance_squared) - 1); float phi = 2 * M_PI * r1; float x = cos(phi) * sqrt(1 - z * z); float y = sin(phi) * sqrt(1 - z * z); return Vector3D(x, y, z);&#125; &emsp;&emsp;对球形光源的随机采样以及求取概率密度函数的值如下所示： 123456789101112131415161718192021float Sphere::pdfValue(const Vector3D &amp;o, const Vector3D &amp;v) const&#123; HitRecord rec; if(this-&gt;hit(Ray(o,v), 0.001f, FLT_MAX, rec)) &#123; float cos_theta_max = sqrt(1- m_radius * m_radius/(m_center - o).getSquaredLength()); float solid_angle = 2 * M_PI * (1 - cos_theta_max); return 1.0f / solid_angle; &#125; else return 0.0f;&#125;Vector3D Sphere::random(const Vector3D &amp;o) const&#123; Vector3D dir = m_center - o; float distance_squared = dir.getSquaredLength(); ONB uvw; uvw.buildFromW(dir); return uvw.local(Vector3D::randomToSphere(m_radius, distance_squared));&#125; 2、复合重要性采样&emsp;&emsp;上面我们分别讨论了Lambertian采样和直接光源采样，然后我们要把它复合到一起。场景中通常有多个光源，所以直接光源采样应该对多个光源进行采样，我们采取均匀随机的策略，对于一束光线，它采样哪个光源由均匀的随机数决定，这样就能雨露均沾。复合的权重套用前面提到的平衡启发式，Lambertian采样和直接光源采样的权值各0.5，也就是各占一半。 123456789101112131415161718192021class MixturePDF : public PDF&#123;private: PDF* m_pdf[2];public: MixturePDF(PDF *p0, PDF *p1) &#123; m_pdf[0] = p0;m_pdf[1] = p1; &#125; virtual float value(const Vector3D &amp;direction) const &#123; return 0.5f * m_pdf[0]-&gt;value(direction) + 0.5f * m_pdf[1]-&gt;value(direction); &#125; virtual Vector3D generate() const &#123; if(drand48() &lt; 0.5f) return m_pdf[0]-&gt;generate(); else return m_pdf[1]-&gt;generate(); &#125;&#125;; &emsp;&emsp;对于多个光源的直接采样，我们采取均匀随机的策略，那么PDF值也应该是这些直接光源采样概率密度函数的平均值。 1234567891011121314float HitableList::pdfValue(const Vector3D &amp;o, const Vector3D &amp;v) const&#123; float weight = 1.0f / m_list.size(); float sum = 0; for(int x = 0;x &lt; m_list.size();++ x) sum += m_list[x]-&gt;pdfValue(o, v); return sum * weight;&#125;Vector3D HitableList::random(const Vector3D &amp;o) const&#123; int index = static_cast&lt;int&gt;(drand48() * m_list.size()); return m_list[index]-&gt;random(o);&#125; &emsp;&emsp;最后在光线追踪递归函数中加上我们的复合重要性采样。 12345678910111213141516171819202122232425262728293031323334353637383940414243Vector4D Tracer::tracing(const Ray &amp;r, Hitable *world, Hitable *light, int depth)&#123; HitRecord rec; if (world-&gt;hit(r, 0.001f, FLT_MAX, rec)) &#123; ...... if (depth &lt; m_config.m_maxDepth &amp;&amp; material-&gt;scatter(r, rec, srec)) &#123; if(srec.m_isSpecular) &#123; return srec.m_attenuation * tracing(srec.m_scatterRay, world, light, depth + 1); &#125; else &#123; Vector3D dir; float pdf_val; if(!m_samplingList.isEmpty()) &#123; HitablePDF light_pdf(light, rec.m_position); MixturePDF mix_pdf(&amp;light_pdf, srec.m_pdf.get()); dir = mix_pdf.generate(); pdf_val = mix_pdf.value(dir); &#125; else &#123; dir = srec.m_pdf-&gt;generate(); pdf_val = srec.m_pdf-&gt;value(dir); &#125; Ray scattered = Ray(rec.m_position, dir); return emitted + srec.m_attenuation * material-&gt;scattering_pdf(r, rec, scattered) * tracing(scattered, world, light, depth + 1) / pdf_val; &#125; &#125; else return emitted; &#125; else &#123; // background color. ...... &#125;&#125; 程序效果 参考资料$[1$ http://www.thegibook.com/ $[2]$ Peter Shirley. Ray Tracing in One Weekend. Amazon Digital Services LLC, January 26, 2016. $[3]$ https://software.intel.com/en-us/node/506045?_ga=2.114625223.1582767698.1558613799-2057498546.1558613799 $[4]$ https://blog.csdn.net/zoufeiyy/article/details/1887579 $[5]$ https://www.jianshu.com/p/b570b1ba92bb $[6]$ https://blog.csdn.net/libing_zeng/article/details/74989755 $[7]$ https://www.qiujiawei.com/solid-angle/","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/tags/Ray-Tracer/"}]},{"title":"光线追踪器Ray Tracer：入门篇","slug":"RayTracer-Basis","date":"2019-05-08T14:12:14.121Z","updated":"2019-07-17T03:40:11.410Z","comments":true,"path":"2019/05/08/RayTracer-Basis/","link":"","permalink":"https://yangwc.com/2019/05/08/RayTracer-Basis/","excerpt":"光线追踪技术是计算机图形学的一类全局光照算法，目前的影视行业大多都采用光线追踪做离线渲染。本章开始构建一个光线追踪离线渲染器（路径追踪），深入理解光线追踪的技术原理。主要参考资料为Peter Shirley的《Ray Tracing in One Weekend》。数学库沿用之前自己写的3D数学库，这方面的东西不再赘述。相关的完全代码在这里。","text":"光线追踪技术是计算机图形学的一类全局光照算法，目前的影视行业大多都采用光线追踪做离线渲染。本章开始构建一个光线追踪离线渲染器（路径追踪），深入理解光线追踪的技术原理。主要参考资料为Peter Shirley的《Ray Tracing in One Weekend》。数学库沿用之前自己写的3D数学库，这方面的东西不再赘述。相关的完全代码在这里。 光线追踪纵览 实现光线追踪渲染器 程序结果 一、光线追踪纵览&emsp;&emsp;光线追踪 (Ray Tracing) 算法是一种基于真实光路模拟的计算机三维图形渲染算法，相比其它大部分渲染算法，光线追踪算法可以提供更为真实的光影效果。此算法由 Appel 在 1968 年初步提出，1980 年由Whitted 改良为递归算法并提出全局光照模型。直到今天，光线追踪算法仍是图形学的热点，大量的改进在不断涌现。基于对自然界光路的研究, 光线追踪采取逆向计算光路来还原真实颜色。追踪的过程中涵盖了光的反射、折射、吸收等特性 (精确计算)， 并辅以其它重要渲染思想 (进一步模拟)。 其中包含了重要方法，诸如冯氏光照模型 (Phong Shading)、辐射度(Radiosity)、光子映射 (Photon Mapping)、蒙特卡罗方法 (Monte Carlo) 等等。鉴于光线追踪算法对场景仿真程度之高，其被普遍认为是计算机图形学的核心内容， 以及游戏设计、电影特效等相关领域的未来方向。 近年来由于硬件系统的迅速改良， 基于分布式、GPU， 甚至实时渲染的光线追踪显卡也纷纷出现（本人就是入手了一块实时光追显卡rtx2070）。 &emsp;&emsp;光线追踪算法是一种非常自然的技术，相比于光栅化的方法，它更加简单、暴力、真实。与光栅化根据物体计算所在的像素的方式不同，光线路径追踪的方法是一个相反的过程，它在于用眼睛去看世界而不是世界如何到达眼中。如下图所示，从视点出发向屏幕上每一个像素发出一条光线View Ray，追踪此光路并计算其逆向光线的方向，映射到对应的像素上。通过计算光路上颜色衰减和叠加，即可基本确定每一个像素的颜色。 图1 光线追踪示意图 &emsp;&emsp;可以看到光线追踪是一个递归的过程。发射一束光线到场景，求出光线和几何图形间最近的交点，如果该交点的材质是反射性或折射性的，可以在该交点向反射方向或折射方向继续追踪，如此递归下去，直到设定的最大递归深度或者射线追踪到光源处（或者背景色），如此便计算处一个像素的着色值。 &emsp;&emsp;基本的光线追踪tracing()递归算法如下所示： &emsp;&emsp;Algorithm 1: 光线追踪递归算法 &emsp;&emsp;Input: 射线ray &emsp;&emsp;Output: 反向光颜色 &emsp;&emsp;Function tracing(): &emsp;&emsp;if no intersection with any object then&emsp;&emsp;&emsp;&emsp;return background color&emsp;&emsp;else&emsp;&emsp;&emsp;&emsp;obj $\\leftarrow$ find nearest object from the ray;&emsp;&emsp;&emsp;&emsp;reflect ray $\\leftarrow$getReflectRay(obj);&emsp;&emsp;&emsp;&emsp;refract ray $\\leftarrow$ getRefractRay(obj);&emsp;&emsp;&emsp;&emsp;main color $\\leftarrow$ the radiance of obj;&emsp;&emsp;&emsp;&emsp;reflect color $\\leftarrow$ tracing(reflect ray);&emsp;&emsp;&emsp;&emsp;refract color $\\leftarrow$ tracing(refract ray); &emsp;&emsp;&emsp;&emsp;return mix(main color, reflect color, refract color); 二、实现光线追踪渲染器&emsp;&emsp;采用C++语言不借助第三方图形渲染API实现一个简易的光线追踪器，为了将最后的结果显示出来，我采用stb_image将计算得到的像素矩阵保存为png图片。本篇实现的光线追踪只包含求交运行、计算光线反射和折射向量、反走样、景深等较为初级的方面，而实现的材质包含磨砂材质、玻璃材质和金属材质。 1、摄像机&emsp;&emsp;与光栅化的空间变换过程相反，光线追踪大部分操作都是在世界空间中进行，因而需要将屏幕空间的像素坐标变换到世界空间中，并相应地发射出一条射线。在这里我们不再构建矩阵，直接求解出摄像机的三个坐标轴，然后根据视锥体的视域fov和屏幕的宽高比aspect得到每个像素发射出来的射线。 &emsp;&emsp;首先我们创建一个射线类$Ray$，射线通常用一个射线原点$m_origin$和射线方向$m_direction$表示，射线上的每个点则表示为$p(t)=m_origin+t*m_direction$，射线上每一个独立的点都有一个自己唯一的$t$值。因而创建的$Ray$类如下所示，其中$pointAt$函数根据给定的$t$值返回相应的射线上的点： 12345678910111213141516171819202122class Ray&#123;private: Vector3D m_origin; Vector3D m_direction;public: // ctor/dtor. Ray() = default; ~Ray() = default; Ray(const Vector3D &amp;org, const Vector3D &amp;dir) :m_origin(org), m_direction(dir) &#123; m_direction.normalize(); &#125; // Getter. Vector3D getOrigin() const &#123; return m_origin; &#125; Vector3D getDirection() const &#123; return m_direction; &#125; // p(t) = origin + t*dir; Vector3D pointAt(const float &amp;t)const &#123; return m_origin + m_direction * t; &#125;&#125;; &emsp;&emsp;我们实现的基于cpu的光线追踪核心渲染流程是对给定分辨率的像素矩阵，逐行逐列地遍历每个像素坐标，如下所示： 1234567891011unsigned char *RayTracing::render()&#123; for(int row = 0;row &lt; m_height;++ row) &#123; for(int col = 0;col &lt; m_width;++ col) &#123; ...... &#125; &#125; return m_image;&#125; &emsp;&emsp;因而对于每个给定的像素坐标$(x,y)$，我们需要获取这个像素坐标对应的发射出去的射线，首先我们把值域为$[0,m_width]$和$[0,m_height]$的像素坐标映射到$[0,1]$，正如如下所示： 12float u = static_cast&lt;float&gt;(col) / static_cast&lt;float&gt;(m_config.m_width);float v = static_cast&lt;float&gt;(row) / static_cast&lt;float&gt;(m_config.m_height); &emsp;&emsp;接下来我们根据$u$和$v$获取射线方向向量，这涉及到两个方面，一个摄像机的坐标系统，另一个是关于视锥的大小设置。摄像机的坐标轴决定了当前的朝向，视锥的大小设定决定了当前视域的大小。为此，我把摄像机与视锥合并一起，坐标系类型依然是右手坐标系。创建的摄像机类如下所示： 12345678910111213141516171819202122232425262728293031class Camera&#123;public: Vector3D m_pos; Vector3D m_target; Vector3D m_lowerLeftCorner; Vector3D m_horizontal; Vector3D m_vertical; float m_fovy, m_aspect; Vector3D m_axisX, m_axisY, m_axisZ; Camera(const Vector3D &amp;cameraPos, const Vector3D &amp;target,float vfov, float aspect); // Getter. Ray getRay(const float &amp;s, const float &amp;t) const; Vector3D getPosition() const &#123; return m_pos; &#125; Vector3D getTarget() const &#123; return m_target; &#125; Vector3D getAxisX() const &#123; return m_axisX; &#125; Vector3D getAxisY() const &#123; return m_axisY; &#125; Vector3D getAxisZ() const &#123; return m_axisZ; &#125; // Setter. void setPosition(const Vector3D &amp;pos) &#123; m_pos = pos; update(); &#125; void setTarget(const Vector3D &amp;_tar) &#123; m_target = _tar; update(); &#125; void setFovy(const float &amp;fov) &#123; m_fovy = fov; update(); &#125; void setAspect(const float &amp;asp) &#123; m_aspect = asp; update(); &#125;private: void update();&#125;; &emsp;&emsp;其中$m_pos$即摄像机的世界坐标位置，$m_target$即目标位置，而$m_lowerLeftCorner$表示视锥近平面的左下角位置，$m_horizontal$表示近平面在摄像机坐标系下水平方向的跨度，$m_vertical$则是近平面在摄像机坐标系下垂直方向的跨度。$m_fovy$和$m_aspect$分别是视锥的垂直视域和屏幕的宽高比。初始时我们传入摄像机坐标、目标点以及垂直视域和视口宽高比，然后我们根据这些计算摄像机的三个坐标轴，以及近平面的位置： 123456789101112131415161718192021void Camera::update()&#123; const Vector3D worldUp(0.0f, 1.0f, 0.0f); // frustum. float theta = radians(m_fovy); float half_height = static_cast&lt;float&gt;(tan(theta * 0.5f)); float half_width = m_aspect * half_height; // camera coordinate system. m_axisZ = m_pos - m_target; m_axisZ.normalize(); m_axisX = worldUp.crossProduct(m_axisZ); m_axisX.normalize(); m_axisY = m_axisZ.crossProduct(m_axisX); m_axisY.normalize(); // view port. m_lowerLeftCorner = m_pos - m_axisX * half_width - m_axisY * half_height - m_axisZ; m_horizontal = m_axisX * 2.0f * half_width; m_vertical = m_axisY * 2.0f * half_height;&#125; &emsp;&emsp;然后我们对于给定在$[0,1]$的$u$和$v$，就可以计算出一条对应的射线向量了。 1234Ray Camera::getRay(const float &amp;s, const float &amp;t) const&#123; return Ray(m_pos , m_lowerLeftCorner + m_horizontal * s + m_vertical * t - m_pos );&#125; 12345678910for (int row = m_config.m_height - 1; row &gt;= 0; --row)&#123; for (int col = 0; col &lt; m_config.m_width; ++col) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = camera.getRay(u, v); ...... &#125;&#125; 2、物体求交&emsp;&emsp;射线发射出去之后要与物体进行求交运行，对于这类能够被射线碰撞到的物体我们把它抽象为$Hitable$，并用一个虚函数$Hit$作为所有的碰撞求交的接口，创建$Hitable$虚类如下： 12345678910111213141516class Material;struct HitRecord&#123; float m_t; Vector3D m_position; Vector3D m_normal; Material *m_material;&#125;;class Hitable&#123;public: Hitable() = default; virtual ~Hitable() &#123;&#125; virtual bool hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const = 0;&#125;; &emsp;&emsp;可以看到我们还创建了一个$HitRecord$结构体，它包含一次射线碰撞求交的结果记录，其中$m_t$是射线方程的参数$t$，$m_position$是交点的位置，$m_normal$是交点的法向量，而$m_material$则是交点所在物体的材质，求交之后我们需要根据这些记录来计算物体的折射、反射。 &emsp;&emsp;$Hitable$中的$hit$接口以一条射线$ray$作为输入参数，以一个$Hitable$的引用$ret$作为求交的结果记录，函数返回布尔值以表示是否发生了射线碰撞。此外，值得一提的是我们还输入了两个参数，分别是$t_min$和$t_max$，这个是我们自己对射线线段长度做的一个限制，可以分别去掉太近和太远的物体。 &emsp;&emsp;然后我们需要向场景中添加物体，光线追踪器的一个”Hello, world!”是球体。我们知道，一个球体的数学表达式为如下所示： (x-cx)^2+(y-cy)^2+(z-cz)^2=R^2 \\tag {1}&emsp;&emsp;其中$c=(cx,cy,cz)$是球体的球心，$R$为球体半径。我们现在要求的就是，对于射线上的一点$P(t)=S+tV$，使得$(x,y,z)=P(t)=S+tV$带入公式$(1)$成立，公式$(1)$可以写成点乘的形式如下： (P-c)\\cdot (P-c) = R^2 \\tag {2}&emsp;&emsp;将$P=P(t)=S+tV$带入公式$(2)$可得： (V\\cdot V)t^2+2(V\\cdot(S-c))t+(S-c)\\cdot(S-c)-R^2=0 \\tag {3}&emsp;&emsp;可以看到公式$(3)$中只有$t$未知，它是一个一元二次方程。对于任意的一元二次方程$at^2+bt+c=0$，其解有如下形式： t=\\frac{-b\\pm \\sqrt{b^2-4ac}}{2a} \\tag {4}&emsp;&emsp;其中根号内的$D=b^2-4ac$称为根的判别式，它可以反应多项式根的数量。若$D&gt;0$则有两个实数根，若$D=0$则只有一个实数根，若$D&lt;0$则没有实数根。我们首先可以根据判别式判断是否存在交点，然后再求出具体的交点坐标。下面的$Hit$函数，我们首先求出多项式方程的常数项$a$、$b$和$c$，然后求判别式，最后再有解的情况下求取交点。注意，在有两个交点的情况下，我们首先取较近的点，不符合再取较远的那个点。只有一个交点的情况下（如下图2所示），我们不当作射线发生了碰撞（擦边而过）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Sphere : public Hitable &#123; public: float m_radius; Vector3D m_center; Material *m_material; Sphere(const Vector3D &amp;cen, const float r, Material *mat) :m_center(cen), m_radius(r), m_material(mat) &#123;&#125; ~Sphere() &#123; if (m_material)delete m_material; m_material = nullptr; &#125;; virtual bool hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const; &#125;; bool Sphere::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const &#123; Vector3D oc = ray.getOrigin() - m_center; float a = ray.getDirection().dotProduct(ray.getDirection()); float b = oc.dotProduct(ray.getDirection()); float c = oc.dotProduct(oc) - m_radius * m_radius; // discriminant float discriminant = b * b - a * c; if (discriminant &gt; 0) &#123; float temp = (-b - sqrt(b * b - a * c)) / a; if (temp &gt; t_min &amp;&amp; temp &lt; t_max) &#123; ret.m_t = temp; ret.m_position = ray.pointAt(ret.m_t); ret.m_normal = (ret.m_position - m_center) / m_radius; ret.m_material = m_material; return true; &#125; temp = (-b + sqrt(b * b - a * c)) / a; if (temp &gt; t_min &amp;&amp; temp &lt; t_max) &#123; ret.m_t = temp; ret.m_position = ray.pointAt(ret.m_t); ret.m_normal = (ret.m_position - m_center) / m_radius; ret.m_material = m_material; return true; &#125; &#125; return false; &#125; 图2 射线与球体的相交情况 &emsp;&emsp;当场景中有多个物体时，当前的做法是在每次求交时遍历所有的物体，我们需要一个$HitableList$来存储这些物体。我们令$HitableList$继承自$Hitable$，这样$HitableList$就表现得好像只有一个很大的物体一样，并在实现$hit$函数中对场景得所有物体遍历调用他们的$Hit$方法： 1234567891011121314151617181920212223242526272829303132333435363738class HitableList : public Hitable&#123;public: std::vector&lt;Hitable*&gt; m_list; HitableList() = default; ~HitableList() = default; void addHitable(Hitable *target) &#123; m_list.push_back(target); &#125; void clearHitable() &#123; for (int x = 0; x &lt; m_list.size(); ++x) &#123; delete m_list[x]; m_list[x] = nullptr; &#125; &#125; virtual bool hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const;&#125;;bool HitableList::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const&#123; HitRecord tempRec; bool hitAny = false; double closestSoFar = t_max; for (unsigned int x = 0; x &lt; m_list.size(); ++x) &#123; if (m_list[x]-&gt;hit(ray, t_min, closestSoFar, tempRec)) &#123; hitAny = true; closestSoFar = tempRec.m_t; ret = tempRec; &#125; &#125; return hitAny;&#125; &emsp;&emsp;此外，值得一提的是，在$HitableList$的$hit$函数中我们需要做一个类似于深度测试的步骤，我们从摄像机发射的射线只能跟最靠近摄像机的那个交点做反射、折射，一条射线发射出去可能会与多个物体相交，我们必须取最近的交点。这个距离我们用射线方程中的$t$来描述，显然$t$越大则交点越远，因此用$closestSoFar$来记录当前获取的交点的最小$t$，以此作为$t$的上限，这样最终求出来的必然就是最近的交点。 3、物体材质&emsp;&emsp;现在我们的一个问题就是求出交点之后，光线在交点上做什么样的反射和折射？这取决于物体的材质。若物体的材质是透明的玻璃，那么光线一般做折射；而若物体是光滑的镜面，则光线做完美的反射。针对不同物体的材质，光线的散射情况各不相同，为此我们创建一个虚类$Material$，并把光线散射的这一过程抽象为$sactter$函数接口。 123456789class Material&#123;public: Material() = default; virtual ~Material() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const = 0;&#125;; &emsp;&emsp;可以看到，$scatter$函数接收入射射线$Ray$以及求交获得的$HitRecord$，计算散射光线的向量，返回的结果表示是否发生了散射。其中的$attenuation$本质上是物体自身的反射颜色，之所以叫$attenuation$是因为光线照射到物体上，物体一般会吸收光线中的大部分颜色，然后仅反射自身颜色的部分，这个过程使得光线在反射过程中不断衰减。 3.1 Lambertian反射材质&emsp;&emsp;首先我们要实现的是Lambertian反射的材质，Lambertian反射也叫理想散射。Lambertian表面是指在一个固定的照明分布下从所有的视场方向上观测都具有相同亮度的表面，Lambertian表面不吸收任何入射光。Lambertian反射也叫散光反射，不管照明分布如何，Lambertian表面在所有的表面方向上接收并发散所有的入射照明，结果是每一个方向上都能看到相同数量的能量。这是一种理想情况，现实中不存在完全漫反射，但Lambertian可以用来近似的模拟一些粗糙表面的效果，比如纸张。 图3 Lambertian反射 &emsp;&emsp;为了实现Lambertian表面的均匀反射现象，我们令射线碰撞到表面之后，在交点的半球方向上随机地反射，只要随机性够均匀，我们就能模拟出理想散射的情况。为此，我们取一个正切于交点$P$表面的单位球体，在这个球体内随机取一个点$S$，则反射的向量就为$S-P$。这个正切于交点$P$表面的单位球体不难求得，设交点$P$的单位法向量为$N$，那么该正切球体的球心为$P+N$。我们首先在球心为原点的单位球内随机求得一个方向向量，然后将这个方向向量加上正切球体的球心即可得出反射的方向向量。（$drand48$是生成$[0,1)$之间的均匀随机数函数，一般linux下才有这个内建函数，windows下没有，所以我们就自己写了。） 123456789101112131415161718192021222324252627282930313233343536373839404142434445#define rndm 0x100000000LL#define rndc 0xB16#define rnda 0x5DEECE66DLL static unsigned long long seed = 1; inline double drand48(void) &#123; seed = (rnda * seed + rndc) &amp; 0xFFFFFFFFFFFFLL; unsigned int x = seed &gt;&gt; 16; return ((double)x / (double)rndm); &#125;=============================================================== static Vector3D randomInUnitSphere() &#123; Vector3D pos; do &#123; pos = Vector3D(drand48(), drand48(), drand48()) * 2.0f - Vector3D(1.0, 1.0, 1.0); &#125; while (pos.getSquaredLength() &gt;= 1.0); return pos; &#125;=============================================================== class Lambertian : public Material &#123; private: Vector3D m_albedo; public: Lambertian(const Vector3D &amp;a) : m_albedo(a) &#123;&#125; virtual ~Lambertian() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const; &#125;; bool Lambertian::scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const &#123; Vector3D target = rec.m_position + rec.m_normal + Vector3D::randomInUnitSphere(); scattered = Ray(rec.m_position, target - rec.m_position); attenuation = m_albedo; return true; &#125; &emsp;&emsp;其中的$m_albedo$为物体自身的反射颜色。 3.2 金属镜面反射材质&emsp;&emsp;金属的表面比较光滑，因而不会呈现出光线随机散射的情况。对于一个完美镜面的材质来说，入射光线和反射光线遵循反射定律，即光射到镜面上时，反射线跟入射线和法线在同一平面内，反射线和入射线分居法线两侧，并且与界面法线的夹角（分别叫做入射角和反射角）相等。反射角等于入射角。 &emsp;&emsp;求反射向量如下图4所示，比较简单，不再赘述。 图4 反射向量 R = I-2(N\\cdot I)N \\tag {5}1234static Vector3D reflect(const Vector3D &amp;ray, const Vector3D &amp;normal)&#123; return ray - normal * (ray.dotProduct(normal)) * 2.0f;&#125; &emsp;&emsp;对于一个完美镜面的金属材质来说，我们只需求出反射向量，然后按照这个反射向量递归下去就行了。但是有些金属并没有那么光滑，它的高光反射并没有那么锐利，为此我们对求出的反射向量做一定的扰动，使反射向量在一定的波瓣内随机，这个波瓣有多大由用户决定（波瓣越大则金属越粗糙）。废话不多说直接上图就明白了。 &emsp;&emsp;我们在反射向量的终点上取一个给定半径的球体，在这个球体内随机选一个点作为新的反射向量的终点即可。这个半径的大小我们用$m_fuzz$变量存储，交给用户决定。 12345678910111213141516171819202122class Metal : public Material&#123;private: float m_fuzz; Vector3D m_albedo;public: Metal(const Vector3D &amp;a, const float &amp;f) : m_albedo(a), m_fuzz(f) &#123; if (f &gt; 1.0f)m_fuzz = 1.0f; &#125; virtual ~Metal() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const;&#125;;bool Metal::scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const&#123; Vector3D reflectedDir = Vector3D::reflect(in.getDirection(), rec.m_normal); scattered = Ray(rec.m_position, reflectedDir + Vector3D::randomInUnitSphere() * m_fuzz); attenuation = m_albedo; return (scattered.getDirection().dotProduct(rec.m_normal) &gt; 0.0f);&#125; 3.3 透明玻璃折射材质&emsp;&emsp;对于水、玻璃和钻石等等物体的材质，光线照射到它们的表面时，它会把光线分成折射（也叫透射）光线和反射光线两部分。我们实现的材质采用随机的策略， 就是在折射和反射两个部分中随机选取一种。首先我们要根据入射向量、法线以及入射介质系数和折射介质系数计算折射方向向量，相比反射向量，推导计算的过程稍微有点复杂。折射表面有折射系数属性，根据Snell定律，如图5所示，入射角$\\theta _L$和折射角$\\theta _T$之间的关系有： \\eta _Lsin\\theta _L=\\eta _rsin\\theta _r \\tag {6} 图5 折射向量的计算 &emsp;&emsp;其中，$\\eta _L$时光线离开的介质的折射系数，$\\eta _r$是光线进入的介质的折射系数。空气的折射系数通常定位$1.00$，折射系数越大，则在两种不同介质之间光线弯曲效果越明显。$N$和$L$都是单位方向向量。折射向量$T$可为与法向量平行的向量$-Ncos\\theta_T$和垂直的向量$-Gsin\\theta _T$，$G$是上图所示的单位向量。而$perp_NL$与$G$向量平行，且$||perp_NL=sin\\theta_L||$，故有： G=\\frac{perp_NL}{sin\\theta_L}=\\frac{L-(N\\cdot L)N}{sin\\theta_L} \\tag {7}&emsp;&emsp;折射向量$T$可以表示为： T=-Ncos\\theta_T-Gsin\\theta_T\\\\ =-Ncos\\theta_T-\\frac{sin\\theta_T}{sin\\theta_L}[L-(N\\cdot L)N] \\tag {8}&emsp;&emsp;利用公式$(6)$，我们可以将上式中的正弦商替换为$\\eta _L/\\eta _T$，可得： T=-Ncos\\theta_T-\\frac{\\eta _L}{\\eta _T}[L-(N\\cdot L)N] \\tag {9}&emsp;&emsp;注意到公式$(9)$中的$cos\\theta_T$未知，用$\\sqrt{1-sin^2\\theta_T}$替换$cos\\theta_T$，再用$(\\eta_L/\\eta_r)sin\\theta_L$代替$sin\\theta_T$，可得： T=-N\\sqrt{1-\\frac{\\eta^2_L}{\\eta^2_T}sin^2\\theta_L}-\\frac{\\eta_L}{\\eta_T}[L-(N\\cdot L)N] \\tag {10}&emsp;&emsp;最后再用$1-cos^2\\theta_L=1-(N\\cdot L)^2$代替$sin^2\\theta_L$，得到最终的表达式为： T=(\\frac{\\eta_L}{\\eta_T}N\\cdot L-\\sqrt{1-\\frac{\\eta^2_L}{\\eta^2_T}[1-(N\\cdot L)^2]}\\ )N-\\frac{\\eta_L}{\\eta_T}L \\tag {11}&emsp;&emsp;如果$\\eta_L&gt;\\eta_T$，则上式平方根里的数值可能为负，这种情况发生在当光线从一个大折射率的介质进入一个小折射率的介质时，此时光线与表面之间的入射角较大。特别的，若仅当$sin\\theta_L\\leq \\eta_r/\\eta_L$时，公式$(11)$有效，如果平方根里的数值为负，则会出现所谓的全内反射现象，也就是光线不被折射，仅在介质内部反射。此外，需要注意的是，我们在程序实现中的入射向量与图5中$L$是相反的，所以需要将公式中的$(11)$的入射向量取反，如下所示： T=\\frac{\\eta_L}{\\eta_T}(L-(N\\cdot L)N)-N\\sqrt{1-\\frac{\\eta^2_L}{\\eta^2_T}[1-(N\\cdot L)^2]}\\ \\tag {12}123456789101112131415static bool refract(const Vector3D &amp;ray, const Vector3D &amp;normal, float niOvernt, Vector3D &amp;refracted)&#123; Vector3D uv = ray; uv.normalize(); float dt = uv.dotProduct(normal); float discriminant = 1.0f - niOvernt * niOvernt * (1.0f - dt * dt); if (discriminant &gt; 0.0f) &#123; refracted = (uv - normal * dt) * niOvernt - normal * sqrt(discriminant); return true; &#125; else return false;&#125; &emsp;&emsp;然后创建一个$Dielectric$类，它有一个私有变量$refIdx$，它表面该物体的材质折射系数。在实现玻璃材质物体的散射函数$scatter$时，我们需要判断当前射线是从外部折射到内部还是从内部折射到外部，这可以通过计算入射向量与法向量的夹角余弦值来判断（通常法向量朝外），然后相应地将法向量的方向扭正。这里用$ni-over-nt$变量来记录$\\frac{\\eta_L}{\\eta_T}$，我们知道空气的折射系数为$1.00$，所以从外面折射入物体内部时其取值等于$1.0/refIdx$，从内部折射到外部时取值为$refIdx$。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354 class Dielectric : public Material &#123; private: float refIdx; public: Dielectric(float ri) : refIdx(ri) &#123;&#125; virtual ~Dielectric() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const; &#125;; bool Dielectric::scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const &#123; Vector3D outward_normal; Vector3D reflected = Vector3D::reflect(in.getDirection(), rec.m_normal); float ni_over_nt; attenuation = Vector3D(1.0f, 1.0f, 1.0f); Vector3D refracted; float reflect_prob; float cosine; // from inside to outside. if (in.getDirection().dotProduct(rec.m_normal) &gt; 0.0f) &#123; outward_normal = -rec.m_normal; ni_over_nt = refIdx; cosine = refIdx * in.getDirection().dotProduct(rec.m_normal) / in.getDirection().getLength(); &#125; // from outside to inside. else &#123; outward_normal = rec.m_normal; ni_over_nt = 1.0 / refIdx; cosine = -in.getDirection().dotProduct(rec.m_normal) / in.getDirection().getLength(); &#125; if (Vector3D::refract(in.getDirection(), outward_normal, ni_over_nt, refracted)) &#123; reflect_prob = schlick(cosine, refIdx); &#125; else &#123; scattered = Ray(rec.m_position, reflected); reflect_prob = 1.0f; &#125; if (drand48() &lt; reflect_prob) scattered = Ray(rec.m_position, reflected); else scattered = Ray(rec.m_position, refracted); return true; &#125;&#125; &emsp;&emsp;这里还要引入一个菲涅尔反射现象（仅对电介质和非金属表面有定义）。生活中，当我们以垂直的视角观察时，任何物体或者材质表面都有一个基础反射率(Base Reflectivity)，但是如果以一定的角度往平面上看的时候所有反光都会变得明显起来。你可以自己尝试一下，用垂直的视角观察你自己的木制桌面，此时一定只有最基本的反射性。但是如果你从近乎与法线成90度的角度观察的话反光就会变得明显的多。如果从理想的90度的视角观察，所有的平面理论上来说都能完全的反射光线。这种现象因菲涅尔而闻名，并体现在了菲涅尔方程之中。菲涅尔方程是一个相当复杂的方程式，不过幸运的是菲涅尔方程可以用Fresnel-Schlick近似法求得近似解： F_{schlick(h,v,F_0)}=F_0+(1-F_0)(1-(h\\cdot v))^5 \\tag {13}&emsp;&emsp;这里的$F_0$y由物体的折射系数得到，$h$是入射向量的负向量（因为我们定义的入射向量方向朝向交点），$v$则是交点处的法向量$v$，我们实现一个$schlick$函数如下： 123456float schlick(float cosine, float ref_idx) const&#123; float r0 = (1.0f - ref_idx) / (1.0f + ref_idx); r0 = r0 * r0; return r0 + (1.0f - r0) * pow((1.0f - cosine), 5.0f);&#125; &emsp;&emsp;我们还定义了一个reflect_prob变量，它介于0~1之间。我们根据reflect_prob与介于$[0,1)$的随机数做比较确定选择反射还是折射，这个还是很合理的，为什么呢？因为我们做了100次采样！那么我们可以理直气壮的说，我们的透明电介质真正做到了反射和折射的混合（除了全反射现象），而且这样符合光线照射透明电介质时，它会分裂为反射光线和折射光线的物理现象。（在程序中，教程作者在从内部折射到外部的时候将$cosine$值还乘上了个$refIdx$，这个操作没明白作者的意图，不乘上$refIdx$好像也没有发现渲染结果有明显的错误）。 &emsp;&emsp;最后，我们实现的玻璃球球内图像是颠倒的，这属于正常现象，原因如下图所示。光线经过两次折射最终导致了图像的颠倒。 4、抗锯齿&emsp;&emsp;为了减少光线追踪方法的噪声点和锯齿，我们需要做一些抗锯齿处理。方法就是在计算一个像素坐标的像素值时，发射很多条射线，射线的取值范围在一个像素之内，然后将所有光线获取的像素值累加起来，最后除以总的采样数。代码如下： 123456789101112131415161718192021int samples = 100;for (int row = m_config.m_height - 1; row &gt;= 0; --row)&#123; for (int col = 0; col &lt; m_config.m_width; ++col) &#123; Vector4D color; for (int sps = 0; sps &lt; samples; ++sps) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = camera.getRay(u, v); color += tracing(ray, world, 0); &#125; color /= static_cast&lt;float&gt;(samples); color.w = 1.0f; // gamma correction. color = Vector4D(sqrt(color.x), sqrt(color.y), sqrt(color.z), color.w); drawPixel(col, row, color); &#125;&#125; &emsp;&emsp;这里还提到了gamma矫正，关于gamma矫正请看这里)。我们对计算得到的像素做了一个简单的gamma矫正，gamma矫正系数取为$2.0$。不进行gamma矫正的话，渲染出来的图片明显偏暗。 5、景深&emsp;&emsp;关于现实生活中摄像机的景深原理，我不再详细说明。在光线追踪中实现景深并不复杂。实现的方法：首先是射线的出发点视点，我们的眼睛（或者相机）不再是一个点而是眼睛所在的周围圆盘上的随机点，因为实际的相机是有摄像镜头的，摄像镜头是一个大光圈（很大一个镜片），并不是针孔类的东东，所以，我们要模拟镜头，就要随机采针孔周围的光圈点。 &emsp;&emsp;此外还有一个焦距的问题，我们一开始假设成像平面在摄像机坐标系的$z=-1$上，为了实现摄像机的景深效果，现在我们要引入现实摄像机的焦距概念。简单的说焦距是焦点到面镜的中心点之间的距离。因此我们提供了一个焦距的参数给用户调整，以确定所需的景深效果。通常情况下焦距$focusDist$等于$length(target-cameraPos)$。这个时候我们将成像平面挪到了摄像机坐标系的$z=-focusDist$上，相应地需要调整计算成像平面的$halfHeight$（在前面的基础上再乘上个$focusDist$）。 12345678910111213141516171819202122232425262728293031323334Camera::Camera(const Vector3D &amp;cameraPos, const Vector3D &amp;target, float vfov, float aspect, float aperture, float focus_dist)&#123; m_pos = cameraPos; m_target = target; m_fovy = vfov; m_aspect = aspect; m_lensRadius = aperture * 0.5f; m_focusDist = focus_dist; update();&#125;void Camera::update()&#123; const Vector3D worldUp(0.0f, 1.0f, 0.0f); // frustum. float theta = radians(m_fovy); float half_height = static_cast&lt;float&gt;(tan(theta * 0.5f)) * m_focusDist; float half_width = m_aspect * half_height; // camera coordinate system. m_axisZ = m_pos - m_target; m_axisZ.normalize(); m_axisX = worldUp.crossProduct(m_axisZ); m_axisX.normalize(); m_axisY = m_axisZ.crossProduct(m_axisX); m_axisY.normalize(); // view port. m_lowerLeftCorner = m_pos - m_axisX * half_width - m_axisY * half_height - m_axisZ * m_focusDist; m_horizontal = m_axisX * 2.0f * half_width; m_vertical = m_axisY * 2.0f * half_height;&#125; 6、递归光线追踪&emsp;&emsp;最后，我们实现的光线追踪器$Tracer$如下，追踪器的核心实现主要在$tracing$函数和$render$函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168class Hitable;class Vector3D;class Vector4D;class Tracer&#123;private: class Setting &#123; public: int m_maxDepth; int m_width, m_height, m_channel; Setting():m_maxDepth(50), m_channel(4) &#123;&#125; &#125;; Setting m_config; unsigned char *m_image;public: Tracer(); ~Tracer(); void initialize(int w, int h, int c = 4); unsigned char *render(); int getWidth() const &#123; return m_config.m_width; &#125; int getHeight() const &#123; return m_config.m_height; &#125; int getChannel() const &#123; return m_config.m_channel; &#125; int getRecursionDepth() const &#123; return m_config.m_maxDepth; &#125; unsigned char *getImage() const &#123; return m_image; &#125; void setRecursionDepth(int depth); void setCamera(const Vector3D &amp;cameraPos, const Vector3D &amp;target, const Vector3D &amp;worldUp, float fovy, float aspect, float aperture, float focus_dist);private: Hitable *randomScene(); Vector4D tracing(const Ray &amp;r, Hitable *world, int depth); float hitSphere(const Vector3D &amp;center, const float &amp;radius, const Ray &amp;ray); void drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color);&#125;;void Tracer::initialize(int w, int h, int c)&#123; m_config.m_width = w; m_config.m_height = h; if (m_image != nullptr) delete m_image; m_image = new unsigned char[m_config.m_width * m_config.m_height * m_config.m_channel];&#125;unsigned char *Tracer::render()&#123; // viewport Vector3D lower_left_corner(-2.0, -1.0, -1.0); Vector3D horizontal(4.0, 0.0, 0.0); Vector3D vertical(0.0, 2.0, 0.0); Vector3D origin(0.0, 0.0, 0.0); // scene Hitable* world = randomScene(); // camera Vector3D lookfrom(3, 4, 10); Vector3D lookat(0, 0, 0); float dist_to_focus = 10.0f; float aperture = 0.0f; Camera camera(lookfrom, lookat, 45, static_cast&lt;float&gt;(m_config.m_width) / m_config.m_height, aperture, dist_to_focus); int samples = 100; for (int row = m_config.m_height - 1; row &gt;= 0; --row) &#123; for (int col = 0; col &lt; m_config.m_width; ++col) &#123; Vector4D color; for (int sps = 0; sps &lt; samples; ++sps) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = camera.getRay(u, v); color += tracing(ray, world, 0); &#125; color /= static_cast&lt;float&gt;(samples); color.w = 1.0f; // gamma correction. color = Vector4D(sqrt(color.x), sqrt(color.y), sqrt(color.z), color.w); drawPixel(col, row, color); &#125; &#125; reinterpret_cast&lt;HitableList*&gt;(world)-&gt;clearHitable(); delete world; return m_image;&#125;void Tracer::drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color)&#123; if (x &lt; 0 || x &gt;= m_config.m_width || y &lt; 0 || y &gt;= m_config.m_height) return; unsigned int index = (y * m_config.m_width + x) * m_config.m_channel; m_image[index + 0] = static_cast&lt;unsigned char&gt;(255 * color.x); m_image[index + 1] = static_cast&lt;unsigned char&gt;(255 * color.y); m_image[index + 2] = static_cast&lt;unsigned char&gt;(255 * color.z); m_image[index + 3] = static_cast&lt;unsigned char&gt;(255 * color.w);&#125;Hitable *Tracer::randomScene()&#123; int n = 500; HitableList *list = new HitableList(); list-&gt;addHitable(new Sphere(Vector3D(0, -1000.0, 0), 1000, new Lambertian(Vector3D(0.5, 0.5, 0.5)))); for (int a = -11; a &lt; 11; ++a) &#123; for (int b = -11; b &lt; 11; ++b) &#123; float choose_mat = drand48(); Vector3D center(a + 0.9*drand48(), 0.2, b + 0.9*drand48()); if ((center - Vector3D(4, 0.2, 0)).getLength() &gt; 0.9) &#123; // diffuse. if (choose_mat &lt; 0.4f) list-&gt;addHitable(new Sphere(center, 0.2, new Lambertian (Vector3D(drand48()*drand48(), drand48()*drand48(), drand48()*drand48())))); // metal else if (choose_mat &lt; 0.6f) list-&gt;addHitable(new Sphere(center, 0.2, new Metal (Vector3D(0.5f*(1.0f + drand48()), 0.5f*(1.0f + drand48()), 0.5f*(1.0f + drand48())), 0.5f*drand48()))); // glass else list-&gt;addHitable(new Sphere(center, 0.2, new Dielectric (1.5f))); &#125; &#125; &#125; list-&gt;addHitable(new Sphere(Vector3D(0, 1, 0), 1.0, new Dielectric(1.5f))); list-&gt;addHitable(new Sphere(Vector3D(-4, 1, 0), 1.0, new Lambertian(Vector3D(0.4, 0.2, 0.1)))); list-&gt;addHitable(new Sphere(Vector3D(4, 1, 0), 1.0, new Metal(Vector3D(0.7, 0.6, 0.5), 0.0f))); return list;&#125;Vector4D Tracer::tracing(const Ray &amp;r, Hitable *world, int depth)&#123; HitRecord rec; if (world-&gt;hit(r, 0.001f, FLT_MAX, rec)) &#123; Ray scattered; Vector3D attenuation; if (depth &lt; m_config.m_maxDepth &amp;&amp; rec.m_material-&gt;scatter(r, rec, attenuation, scattered)) return attenuation * tracing(scattered, world, depth + 1); else return Vector4D(0.0f, 0.0f, 0.0f, 1.0f); //return backgroundColor(Ray(rec.m_position, target - rec.m_position), world) * 0.5f; //return rec.normal * 0.5f + Vector3D(0.5f, 0.5f, 0.5f); &#125; else &#123; float t = 0.5f * (r.getDirection().y + 1.0f); Vector4D ret = Vector3D(1.0f, 1.0f, 1.0f) * (1.0f - t) + Vector3D(0.5f, 0.7f, 1.0f) * t; ret.w = 1.0f; return ret; &#125;&#125; 三、程序结果 参考资料$[1]$ https://www.cnblogs.com/jerrycg/p/4941359.html $[2]$ https://blog.csdn.net/baishuo8/article/details/81476422 $[3]$ https://blog.csdn.net/silangquan/article/details/8176855 $[4]$ Peter Shirley. Ray Tracing in One Weekend. Amazon Digital Services LLC, January 26, 2016. $[5]$ https://learnopengl-cn.github.io/07%20PBR/01%20Theory/ $[6]$ https://www.cnblogs.com/lv-anchoret/p/10223222.html","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/tags/Ray-Tracer/"}]},{"title":"软渲染器Soft Renderer：光照着色篇（完结）","slug":"SoftRenderer-Shading","date":"2019-05-05T12:39:50.871Z","updated":"2021-04-12T03:31:11.530Z","comments":true,"path":"2019/05/05/SoftRenderer-Shading/","link":"","permalink":"https://yangwc.com/2019/05/05/SoftRenderer-Shading/","excerpt":"本渲染器已完全重构，与之前版本大不相同，详情移步至TinySoftRenderer。","text":"本渲染器已完全重构，与之前版本大不相同，详情移步至TinySoftRenderer。 obj模型导入 Blinn-Phong光照着色 虚拟场景漫游 程序结果 结语 一、obj模型导入&emsp;&emsp;obj模型文件（这里不是指c++编译得到的.o中间文件）是一种格式简单、清晰的模型文件，这种模型的格式非常容易解析。目前有一个非常流行的开源的模型导入库Assimp，封装了各种各样模型文件的加载，省去很多麻烦。而我因为一方面为了尽量避免引入第三方库，另一方面obj模型的导入不难，所以自己实现了一个obj加载类$ObjModel$。实现obj模型加载并不难，只需简单了解一下obj文件的格式即可。 &emsp;&emsp;obj文件格式有类数据，一类一行，分别以v、vt、vn和f开头。用记事本打开一个简单的obj文件，如下所示： &emsp;&emsp;以v（即vertex的缩写）开头的一行分别为模型顶点的$x$、$y$、$z$坐标，以vt（即vertex texcoord的缩写）开头的一行分别为纹理坐标的$u$、$v$值，以vn（即vertex normal的缩写）开头的一行分别是法向量的$x$、$y$、$z$值。而f（即face的缩写）格式为v/vt/vn，其中对应的是各自的索引值，一个v/vt/vn描述了一个三角形顶点的顶点坐标、纹理坐标、法线向量，通常以f的一行有三列v/vt/vn，组成一个三角形面片。所以我们读取的时候按照这些开头标记读取即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394class ObjModel : public Mesh&#123;private: Vector3D minPoint, maxPoint; // Bounding box.public: // ctor/dtor. ObjModel(const std::string &amp;path); virtual ~ObjModel(); // Size setting. Vector3D setSizeToVector(float sx, float sy, float sz) const; Matrix4x4 setSizeToMatrix(float sx, float sy, float sz) const;private: // Obj file loader. void loadObjFile(const std::string &amp;path);&#125;;void ObjModel::loadObjFile(const std::string &amp;path)&#123; // obj loader. ifstream in; in.open(path, ifstream::in); if(in.fail()) &#123; std::cout &lt;&lt; \"Fail to load obj-&gt;\" &lt;&lt; path &lt;&lt; endl; &#125; string line; minPoint = Vector3D(+10000000000,+10000000000,+10000000000); maxPoint = Vector3D(-10000000000,-10000000000,-10000000000); vector&lt;Vector3D&gt; vertices; vector&lt;Vector3D&gt; normals; vector&lt;Vector2D&gt; texcoords; while(!in.eof()) &#123; getline(in, line); istringstream iss(line.c_str()); char trash; //vertex if(!line.compare(0, 2, \"v \")) &#123; iss &gt;&gt; trash; Vector3D vertex; iss &gt;&gt; vertex.x; iss &gt;&gt; vertex.y; iss &gt;&gt; vertex.z; vertices.push_back(vertex); if(minPoint.x &gt; vertex.x)minPoint.x = vertex.x; if(minPoint.y &gt; vertex.y)minPoint.y = vertex.y; if(minPoint.z &gt; vertex.z)minPoint.z = vertex.z; if(maxPoint.x &lt; vertex.x)maxPoint.x = vertex.x; if(maxPoint.y &lt; vertex.y)maxPoint.y = vertex.y; if(maxPoint.z &lt; vertex.z)maxPoint.z = vertex.z; &#125; // normal else if(!line.compare(0, 3, \"vn \")) &#123; iss &gt;&gt; trash &gt;&gt; trash; Vector3D normal; iss &gt;&gt; normal.x; iss &gt;&gt; normal.y; iss &gt;&gt; normal.z; normal.normalize(); normals.push_back(normal); &#125; // texcoord else if(!line.compare(0, 3, \"vt \")) &#123; iss &gt;&gt; trash &gt;&gt; trash; Vector2D texcoord; iss &gt;&gt; texcoord.x; iss &gt;&gt; texcoord.y; texcoords.push_back(texcoord); &#125; // face else if(!line.compare(0, 2, \"f \")) &#123; iss &gt;&gt; trash; int index[3]; while(iss &gt;&gt; index[0] &gt;&gt; trash &gt;&gt; index[1] &gt;&gt; trash &gt;&gt; index[2]) &#123; Vertex data; data.position = vertices[index[0] - 1]; data.texcoord = texcoords[index[1] - 1]; data.normal = normals[index[2] - 1]; data.color = Vector4D(1.0,1.0,1.0,1.0); m_indices.push_back(m_vertices.size()); m_vertices.push_back(data); &#125; &#125; &#125; in.close();&#125; &emsp;&emsp;可以看到这里继承了父类$Mesh$，这样读进来就作为一个网格类，能够传进渲染管线中渲染。测试读取了几个模型文件，效果如下： 二、Blin-Phong光照着色&emsp;&emsp;之前我们的着色器一直都是直接传输数据，没有做一些着色器计算，这里我们给渲染出来的模型加上光照着色。采用的光照模型是Blinn-Phong光照模型，并实现了两种着色器方法，分别是Gouraud着色、Phong着色。注意别混淆了光照模型和着色模型，光照模型是一种理论模型，着色模型则是具体的实现方式。Gouraud着色和Phong着色都是采用Blinn-Phong光照模型，差别在于两者在何处实现光照计算。 &emsp;&emsp;网上的LearnOpenGL教程很详细地介绍了Phong光照模型以及Blinn-Phong光照（Phong和Blinnn的差别只在于高光计算的一小部分），我就不再说太多这些方面的东西了，想具体了解的朋友请看这里)和这里)。概括起来，Phong光照模型包含环境光、漫反射光和镜面高光，其计算方式如下： I=K_aI_a+k_dI_ecos\\alpha+k_sI_scos^n\\lambda \\tag {1}&emsp;&emsp;其中的$k_a$、$k_d$和$k_s$分别为物体的环境光颜色、漫反射颜色和镜面高光颜色数，$n$是物体的高光读，而$I_a$、$I_e$和$I_s$是光源的环境光颜色、漫反射照亮的颜色和镜面反射的颜色。针对物体材质和光照的种类，我们创建一个$Material$和虚类$Light$，并把光照的计算过程抽象为一个函数$lighting$： 123456789101112131415161718192021222324252627282930313233class Material&#123;public: Material() = default; ~Material() = default; double m_shininess; Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_reflect; void setMaterial(Vector3D _amb, Vector3D _diff, Vector3D _spec, double _shin) &#123; m_shininess = _shin; m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; &#125;&#125;;class Light&#123;public: Light() = default; virtual ~Light() = default; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const = 0;&#125;; &emsp;&emsp;根据光源的种类不同，通常有平行光、点光源和聚束光三类（关于这类光，请看LearnOpenGL的这篇)）。平行光的特点就是光线束都是平行的，因而只需记录平行光的方向即可： 12345678910111213141516171819202122232425262728293031323334353637383940414243class DirectionalLight : public Light&#123;public: Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_direction; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const; void setDirectionalLight(Vector3D _amb, Vector3D _diff, Vector3D _spec, Vector3D _dir) &#123; m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; m_direction = _dir; m_direction.normalize(); &#125;&#125;;void DirectionalLight::lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D &amp;ambient, Vector3D &amp;diffuse, Vector3D &amp;specular) const&#123; float diff = max(normal.dotProduct(-this-&gt;m_direction), 0.0f); Vector3D halfwayDir = eyeDir + this-&gt;m_direction; halfwayDir.normalize(); float spec = pow(max(eyeDir.dotProduct(halfwayDir), 0.0f), material.m_shininess); ambient = m_ambient; diffuse = m_diffuse * diff; specular = m_specular * spec;&#125; &emsp;&emsp;点光源则需要记录光源的位置，用以计算光照的方向。与平行光还有一点不同的是，点光源通常有个照明区域范围，光照的强度随着距离的增加而削弱，且这类减弱不是线性的。因此我们还要衰减因子，把计算得到的光照颜色再乘上这个衰减因子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class PointLight : public Light&#123;public: Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_position; Vector3D m_attenuation; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const; void setPointLight(Vector3D _amb, Vector3D _diff, Vector3D _spec, Vector3D _pos, Vector3D _atte) &#123; m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; m_position = _pos; m_attenuation = _atte; &#125;&#125;;void PointLight::lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D &amp;ambient, Vector3D &amp;diffuse, Vector3D &amp;specular) const&#123; // ambient ambient = this-&gt;m_ambient; // diffuse Vector3D lightDir = (this-&gt;m_position - position); lightDir.normalize(); float diff = max(normal.dotProduct(lightDir), 0.0f); diffuse = this-&gt;m_diffuse * diff; // specular Vector3D halfwayDir = eyeDir + lightDir; halfwayDir.normalize(); float spec = pow(max(eyeDir.dotProduct(halfwayDir), 0.0f), material.m_shininess); specular = this-&gt;m_specular * spec; // attenuation float distance = (this-&gt;m_position - position).getLength(); float attenuation = 1.0 / (m_attenuation.x + m_attenuation.y * distance + m_attenuation.z * (distance * distance)); ambient *= attenuation; diffuse *= attenuation; specular *= attenuation;&#125; &emsp;&emsp;聚束光是一种比较特殊的光源（例如手电筒光、舞台灯光），它的特点就是只有在聚光方向的特定半径内的物体才会被照亮，其它的物体都会保持黑暗。我们采用一个光源位置、照明方向和切光角来描述一个聚光灯： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class SpotLight : public Light&#123;public: double m_cutoff, m_outcutoff; Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_position; Vector3D m_direction; Vector3D m_attenuation; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const; void setSpotLight(Vector3D _amb, Vector3D _diff, Vector3D _spec, double _cut, Vector3D _pos, Vector3D _dir, Vector3D _atte) &#123; m_cutoff = cos(_cut * M_PI/180.0); m_outcutoff = cos((_cut + 10.0) * M_PI/180.0); m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; m_position = _pos; m_direction = _dir; m_attenuation = _atte; m_direction.normalize(); &#125;&#125;;void SpotLight::lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D &amp;ambient, Vector3D &amp;diffuse, Vector3D &amp;specular) const&#123; // ambient ambient = this-&gt;m_ambient; // diffuse Vector3D lightDir = this-&gt;m_position - position; lightDir.normalize(); float diff = max(normal.dotProduct(lightDir), 0.0f); diffuse = this-&gt;m_diffuse * diff ; // specular Vector3D halfwayDir = eyeDir + lightDir; halfwayDir.normalize(); float spec = pow(max(eyeDir.dotProduct(halfwayDir), 0.0f), material.m_shininess); specular = this-&gt;m_specular * spec; // spotlight (soft edges) float theta = lightDir.dotProduct(-this-&gt;m_direction); float epsilon = (this-&gt;m_cutoff - this-&gt;m_outcutoff); float intensity = (theta - this-&gt;m_outcutoff) / epsilon; if(intensity &lt; 0.0f)intensity = 0.0f; if(intensity &gt; 1.0f)intensity = 1.0f; diffuse *= intensity; specular *= intensity; // attenuation float distance = (this-&gt;m_position - position).getLength(); float attenuation = 1.0 / (m_attenuation.x + m_attenuation.y * distance + m_attenuation.z * (distance * distance)); ambient *= attenuation; diffuse *= attenuation; specular *= attenuation;&#125; &emsp;&emsp;然后我们就需要把光照计算集成到着色器中，这里提供了两种方式：光照计算集成到顶点着色器，即Gouraud着色方法，逐顶点光照，然后靠线性插值得到每个像素的光照颜色；光照计算集成到片元着色器，即Phong着色法，逐像素光照，根据插值得到的法向量做相应的计算。显然前者计算量少了很多，但是后者更为真实。我们建立一个$Gouraud$着色类如下： 123456789101112131415161718192021222324252627282930313233class GouraudShader : public BaseShader&#123;private: // Those are not created by shader. const Light *m_light; // Light.(just only one) const Material *m_material; // Mesh material. const Texture2D *m_unit; // Texture unit. Vector3D m_eyePos; // Observer's position. Matrix4x4 m_modelMatrix; // Model matrix. Matrix4x4 m_viewMatrix; // View matrix. Matrix4x4 m_projectMatrix; // Projection matrix. Matrix4x4 m_invModelMatrix; // Inverse of model matrix for normal.public: // ctor/dtor. GouraudShader(); virtual ~GouraudShader() = default; // Shader stage. virtual VertexOut vertexShader(const Vertex &amp;in); virtual Vector4D fragmentShader(const VertexOut &amp;in); // Shader setting. virtual void bindShaderUnit(Texture2D *unit)&#123;m_unit = unit;&#125; virtual void setModelMatrix(const Matrix4x4 &amp;world) &#123;m_modelMatrix = world;m_invModelMatrix = m_modelMatrix.getInverseTranspose();&#125; virtual void setViewMatrix(const Matrix4x4 &amp;view)&#123;m_viewMatrix = view;&#125; virtual void setProjectMatrix(const Matrix4x4 &amp;project)&#123;m_projectMatrix = project;&#125; virtual void setMaterial(const Material *material)&#123;m_material = material;&#125; virtual void setLight(const Light *light)&#123;m_light = light;&#125; virtual void setEyePos(const Vector3D eye)&#123;m_eyePos = eye;&#125;&#125;; &emsp;&emsp;这里提一下关于顶点法向量的变换矩阵。我们目前已经有顶点的model矩阵，但是顶点做变换之后的法向量却不能直接乘上model矩阵获得。我们知道顶点的切线与法线相互垂直，因而它们的点乘为$0$，即有： N\\cdot T = N^T*T = 0 \\tag {2}&emsp;&emsp;顶点切线必然随着模型矩阵的变换而变换，即模型矩阵为$M$，因而变换后的切线$T’=M\\cdot T$。我们记变换后的法向量为$N’$，其正确的法线变换为$Q$，则$N’=Q\\cdot N$，那么变换后$N’$和$T’$应该依旧保持垂直关系，依旧有$N’\\cdot T’=(Q\\cdot N)\\cdot (M\\cdot T)=(Q\\cdot N)^T\\cdot (M\\cdot T)=N^T\\cdot (Q^T\\cdot M)\\cdot T$，与公式$(2)$对比，我们只要令$Q^T\\cdot M = I$结果为单位矩阵，则有$N’\\cdot T’=N\\cdot T = 0$。从而可得法线的变换矩阵为： Q= (N^{-1})^T \\tag {3}123456789101112131415161718192021222324252627282930313233343536373839404142434445VertexOut GouraudShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = m_modelMatrix * in.position; result.posH = m_projectMatrix * m_viewMatrix * result.posTrans; result.color = in.color; result.texcoord = in.texcoord; result.normal = m_invModelMatrix * Vector4D(in.normal); // Gouraud shading. if(m_unit) result.color = m_unit-&gt;sample(result.texcoord); Vector3D _amb, _diff, _spec; if(m_light) &#123; Vector3D eyeDir = m_eyePos - result.posTrans; eyeDir.normalize(); m_light-&gt;lighting(*m_material, result.posTrans, result.normal, eyeDir, _amb, _diff, _spec); result.color.x *= (_amb.x + _diff.x + _spec.x); result.color.y *= (_amb.y + _diff.y + _spec.y); result.color.z *= (_amb.z + _diff.z + _spec.z); result.color.w = 1.0f; &#125; // oneDivZ to correct lerp. result.oneDivZ = 1.0 / result.posH.w; result.posTrans *= result.oneDivZ; result.texcoord *= result.oneDivZ; result.color *= result.oneDivZ; return result;&#125;Vector4D GouraudShader::fragmentShader(const VertexOut &amp;in)&#123; Vector4D litColor = in.color; return litColor;&#125; &emsp;&emsp;Phong着色方式则在$fragmentShader$中实现光照计算，原理简单，不再赘述。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class PhongShader : public BaseShader&#123;private: // Those are not created by shader. const Light *m_light; // Light.(just only one) const Material *m_material; // Mesh material. const Texture2D *m_unit; // Texture unit. Vector3D m_eyePos; // Observer's position. Matrix4x4 m_modelMatrix; // Model matrix. Matrix4x4 m_viewMatrix; // View matrix. Matrix4x4 m_projectMatrix; // Projection matrix. Matrix4x4 m_invModelMatrix; // Inverse of model matrix for normal.public: // ctor/dtor PhongShader(); virtual ~PhongShader() = default; // Shader stage. virtual VertexOut vertexShader(const Vertex &amp;in); virtual Vector4D fragmentShader(const VertexOut &amp;in); // Shader setting. virtual void bindShaderUnit(Texture2D *unit)&#123;m_unit = unit;&#125; virtual void setModelMatrix(const Matrix4x4 &amp;world) &#123;m_modelMatrix = world;m_invModelMatrix = m_modelMatrix.getInverseTranspose();&#125; virtual void setViewMatrix(const Matrix4x4 &amp;view)&#123;m_viewMatrix = view;&#125; virtual void setProjectMatrix(const Matrix4x4 &amp;project)&#123;m_projectMatrix = project;&#125; virtual void setMaterial(const Material *material)&#123;m_material = material;&#125; virtual void setLight(const Light *light)&#123;m_light = light;&#125; virtual void setEyePos(const Vector3D eye)&#123;m_eyePos = eye;&#125;&#125;;VertexOut PhongShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = m_modelMatrix * in.position; result.posH = m_projectMatrix * m_viewMatrix * result.posTrans; result.color = in.color; result.texcoord = in.texcoord; result.normal = m_invModelMatrix * Vector4D(in.normal); // oneDivZ to correct lerp. result.oneDivZ = 1.0 / result.posH.w; result.posTrans *= result.oneDivZ; result.texcoord *= result.oneDivZ; result.color *= result.oneDivZ; return result;&#125;Vector4D PhongShader::fragmentShader(const VertexOut &amp;in)&#123; Vector4D litColor = in.color; // Gouraud shading. if(m_unit) litColor = m_unit-&gt;sample(in.texcoord); Vector3D _amb, _diff, _spec; if(m_light) &#123; Vector3D eyeDir = m_eyePos - in.posTrans; eyeDir.normalize(); m_light-&gt;lighting(*m_material, in.posTrans, in.normal, eyeDir, _amb, _diff, _spec); litColor.x *= (_amb.x + _diff.x + _spec.x); litColor.y *= (_amb.y + _diff.y + _spec.y); litColor.z *= (_amb.z + _diff.z + _spec.z); litColor.w = 1.0f; &#125; return litColor;&#125; &emsp;&emsp;下图分别为Phong着色方式的平行光、点光源、聚束光效果： 三、虚拟场景漫游&emsp;&emsp;虚拟场景漫游是一个三维程序必不可少的，我们比较常用的虚拟摄像机有两类：第一人称摄像机、第三人生摄像机。第三人称摄像机又称为半上帝视角，一般的rpg游戏都是采用的第三人称视角。摄像机一般都是相应键盘按键、鼠标移动、鼠标滚轮事件，为了方便描述，我们创建一个$Camera3D$虚类如下： 12345678910111213141516171819202122class Camera3D&#123;public: // Local axis. // Here LocalForward should (0,0,-1). static const Vector3D LocalForward; static const Vector3D LocalUp; static const Vector3D LocalRight; // ctor/dtor. Camera3D() = default; virtual ~Camera3D()&#123;&#125; // Getter. virtual Matrix4x4 getViewMatrix() = 0; virtual Vector3D getPosition() = 0; // Key/Mouse reaction. virtual void onKeyPress(char key) = 0; virtual void onWheelMove(double delta) = 0; virtual void onMouseMove(double deltaX, double deltaY, std::string button) = 0;&#125;; 1、第一人称相机&emsp;&emsp;LearnOpenGl的这篇)对第一人称相机的构建做了的很详细的描述。不同的是，我不再采用欧拉角来描述渲染，而是采用了四元数（关于四元数，请看知乎的这篇）。理解了四元数，采用欧拉角反而比较繁琐。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class FPSCamera : public Camera3D&#123;private: mutable bool m_dirty; // Should update or not. Vector3D m_translation; // Camera's translation. Quaternion m_rotation; // Camera's rotation. Matrix4x4 m_viewMatrix; // View Matrix.public: // ctor/dtor FPSCamera(Vector3D _pos); virtual ~FPSCamera() = default; // Getter. virtual Vector3D getPosition() &#123;return m_translation;&#125; virtual Matrix4x4 getViewMatrix(); // Key/Mouse reaction. virtual void onKeyPress(char key); virtual void onWheelMove(double delta); virtual void onMouseMove(double deltaX, double deltaY, std::string button); // Transform camera's axis. void translate(const Vector3D &amp;dt); void rotate(const Vector3D &amp;axis, float angle); void setTranslation(const Vector3D &amp;t); void setRotation(const Quaternion &amp;r); // Query for camera's axis. Vector3D forward() const; Vector3D up() const; Vector3D right() const;&#125;;void FPSCamera::onKeyPress(char key)&#123; switch(key) &#123; case 'W': this-&gt;translate(forward() * 0.2f); break; case 'S': this-&gt;translate(-forward() * 0.2f); break; case 'A': this-&gt;translate(-right() * 0.2f); break; case 'D': this-&gt;translate(+right() * 0.2f); break; case 'Q': this-&gt;translate(up() * 0.2f); break; case 'E': this-&gt;translate(-up() * 0.2f); break; default: break; &#125;&#125;void FPSCamera::onWheelMove(double delta)&#123; // nothing now.&#125;void FPSCamera::onMouseMove(double deltaX, double deltaY, std::string button)&#123; double speed = 0.1f; deltaX *= speed; deltaY *= speed; this-&gt;rotate(LocalUp, -deltaX); this-&gt;rotate(right(), -deltaY);&#125; 2、第三人称摄像机&emsp;&emsp;第三人称有一个固定的目标，这个目标通常就是玩家操控的物体。摄像机可以拉远拉近、围绕目标在$xz$平面旋转、绕$x$轴上下旋转，而且摄像机永远在玩家的上方（即俯视）。为此，我们用$distance$（摄像机到玩家的距离）、$pitch$（绕$x$轴的旋转角）、$yaw$（绕$y$轴的旋转角）来获取摄像机的位置，最后获取了摄像机的位置后我们就可以直接用$LookAt$矩阵获得视图矩阵。更多关于第三人称摄像机方面的细节请看youtube上的这个视频。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596class TPSCamera : public Camera3D&#123;private: mutable bool m_dirty; // Should update or not. Vector3D m_cameraPos; // Camera's position. Transform3D m_player; // Player's transformation. Matrix4x4 m_viewMatrix; // View matrix. double m_yaw, m_pitch, m_distance; // yaw, pitch and distance to player's space.public: // ctor/dtor. TPSCamera(Vector3D target); virtual ~TPSCamera() = default; // Getter. Matrix4x4 getPlayerMatrix(); virtual Matrix4x4 getViewMatrix(); virtual Vector3D getPosition() &#123;update();return m_cameraPos;&#125; // Key/Mouse reaction. virtual void onKeyPress(char key); virtual void onWheelMove(double delta); virtual void onMouseMove(double deltaX, double deltaY, std::string button);private: // Update view matrix. void update();&#125;;void TPSCamera::onKeyPress(char key)&#123; double speed = 2.0f; switch(key) &#123; case 'W': m_dirty = true; m_player.translate(-m_player.forward() * 0.1f); break; case 'S': m_dirty = true; m_player.translate(+m_player.forward() * 0.1f); break; case 'A': m_dirty = true; m_player.rotate(m_player.up(), +speed); break; case 'D': m_dirty = true; m_player.rotate(m_player.up(), -speed); break; &#125;&#125;void TPSCamera::onWheelMove(double delta)&#123; m_dirty = true; double speed = 0.01; m_distance += -speed * delta; if(m_distance &gt; 35.0)m_distance = 35.0; if(m_distance &lt; 5.00)m_distance = 5.0;&#125;void TPSCamera::onMouseMove(double deltaX, double deltaY, std::string button)&#123; double speed = 0.2; if(button == \"RIGHT\") &#123; m_dirty = true; m_pitch += speed * deltaY; if(m_pitch &lt; 0.0)m_pitch = 0.0; if(m_pitch &gt; 89.9)m_pitch = 89.9; &#125; else if(button == \"LEFT\") &#123; m_dirty = true; m_yaw += -speed * deltaX; fmod(m_yaw, 360.0); &#125;&#125;void TPSCamera::update()&#123; if(m_dirty) &#123; m_dirty = false; Vector3D target = m_player.translation(); float height = m_distance * sin(radians(m_pitch)); float horizon = m_distance * cos(radians(m_pitch)); Vector3D _playerRot = m_player.rotation().eulerAngle(); _playerRot.y = fmod(_playerRot.y, 360); m_cameraPos.y = target.y + height; m_cameraPos.x = target.x + horizon * sin(radians(m_yaw)); m_cameraPos.z = target.z + horizon * cos(radians(m_yaw)); m_viewMatrix.setLookAt(m_cameraPos, m_player.translation(), LocalUp); &#125;&#125; 四、程序结果 五、结语&emsp;&emsp;软渲染器的搭建就此告一段落，不借助任何图形库从零开始搭建这么一个渲染管线的初衷是为了更加深入地了解当前三维渲染的整个流程，很多理论东西需要实践才能彻底地理解。这么几天关于搭建软渲染器的折腾让我收获不少，这为以后的图形学道路打下了深厚的基础。目前我实现的软渲染管线已经包含了一个传统固定管线的基本功能，我借助一些工具统计得软渲染管线的核心代码（不包括空行、注释）共2838行。不再打算加入更多的功能特性如透明融合、阴影等等，因为没必要了。相关的全部源代码已经提交到github上，请点这里。 &emsp;&emsp;由于本人的知识水平有限，若发现任何bug或者博文叙述错误，欢迎指正，感谢！ 参考资料$[1]$ https://learnopengl-cn.github.io/02%20Lighting/02%20Basic%20Lighting/ $[2]$ https://learnopengl-cn.github.io/01%20Getting%20started/09%20Camera/ $[3]$ https://www.youtube.com/watch?v=PoxDDZmctnU&amp;list=PLRIWtICgwaX0u7Rf9zkZhLoLuZVfUksDP&amp;index=19 $[4]$ https://github.com/ssloy/tinyrenderer/wiki","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/tags/Soft-Renderer/"},{"name":"3D pipeline","slug":"3D-pipeline","permalink":"https://yangwc.com/tags/3D-pipeline/"}]},{"title":"软渲染器Soft Renderer：进击三维篇","slug":"SoftRenderer-3DPipeline","date":"2019-05-02T06:26:22.186Z","updated":"2021-04-12T03:34:33.972Z","comments":true,"path":"2019/05/02/SoftRenderer-3DPipeline/","link":"","permalink":"https://yangwc.com/2019/05/02/SoftRenderer-3DPipeline/","excerpt":"本渲染器已完全重构，与之前版本大不相同，详情移步至TinySoftRenderer。","text":"本渲染器已完全重构，与之前版本大不相同，详情移步至TinySoftRenderer。 进入三维世界 裁剪、剔除优化 透视纹理映射、采样 程序结果 一、进入三维世界&emsp;&emsp;尽管二维的屏幕只能显示二维的像素，但是我们可以通过将三维的物体变换到二维的屏幕上，从而渲染出三维空间的一个投影面。这与我们人类的视觉系统类似，视网膜上最终获取的也只是三维空间某个角度下的投影。为了让三维物体正确地显示到屏幕上，我们需要借助一系列的坐标空间变换。 1、坐标系统&emsp;&emsp;在渲染管线中，三维物体的顶点在最终转换为屏幕坐标之前会被变换到多个坐标系统，这其中有几个过渡性的坐标系，使得整个变换流程逻辑清晰、便于理解。此外在某些特定情况下在这些特定的坐标系中，一些操作更加容易、方便和灵活。通常，渲染管线有$5$个不同的坐标系统，分别是局部空间、世界空间、视觉空间、裁剪空间和屏幕空间，以下是LearnOpenGL CN)的原话： 局部坐标是对象相对于局部原点的坐标，也是物体起始的坐标。 下一步是将局部坐标变换为世界空间坐标，世界空间坐标是处于一个更大的空间范围的。这些坐标相对于世界的全局原点，它们会和其它物体一起相对于世界的原点进行摆放。 接下来我们将世界坐标变换为观察空间坐标，使得每个坐标都是从摄像机或者说观察者的角度进行观察的。 坐标到达观察空间之后，我们需要将其投影到裁剪坐标。裁剪坐标会被处理至-1.0到1.0的范围内，并判断哪些顶点将会出现在屏幕上。 最后，我们将裁剪坐标变换为屏幕坐标，我们将使用一个叫做视口变换(Viewport Transform)的过程。视口变换将位于-1.0到1.0范围的坐标变换到由glViewport函数所定义的坐标范围内。最后变换出来的坐标将会送到光栅器，将其转化为片段 &emsp;&emsp;通过以上的几个步骤，三维的物体坐标最终变换到了屏幕的坐标上，其中视图矩阵和投影矩阵的构建较为复杂一点，前面我的博文软渲染器Soft Renderer：3D数学篇已经推导过这两个矩阵，这里就不再赘述了。若想查看更多关于坐标系统的内容，请查看LearnOpenGL CN的这篇文章：坐标系统)。坐标变换是一般发生在顶点着色器以及顶点着色器输出到光栅化这一阶段，视口变换在顶点着色器输出之后，不在着色器中进行（视口变换已经在前面的光栅化篇提到过了）。所以为了实现坐标变换，我们的着色器要存储$model$、$view$、$project$这三个矩阵，在$SimpleShader$中添加相关的成员变量及方法： 1234567891011121314151617181920212223242526272829class SimpleShader : public BaseShader&#123;private: Matrix4x4 m_modelMatrix; Matrix4x4 m_viewMatrix; Matrix4x4 m_projectMatrix;public: ...... virtual void setModelMatrix(const Matrix4x4 &amp;world); virtual void setViewMatrix(const Matrix4x4 &amp;view); virtual void setProjectMatrix(const Matrix4x4 &amp;project);&#125;;void SimpleShader::setModelMatrix(const Matrix4x4 &amp;world)&#123; m_modelMatrix = world;&#125;void SimpleShader::setViewMatrix(const Matrix4x4 &amp;view)&#123; m_viewMatrix = view;&#125;void SimpleShader::setProjectMatrix(const Matrix4x4 &amp;project)&#123; m_projectMatrix = project;&#125; &emsp;&emsp;这样外部要渲染时，应该向着色器输入这三个矩阵。然后在我们的顶点着色器中填入相关的逻辑： 12345678910VertexOut SimpleShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = m_modelMatrix * in.position; result.posH = m_projectMatrix * m_viewMatrix * result.posTrans; result.color = in.color; result.normal = in.normal; result.texcoord = in.texcoord; return result;&#125; &emsp;&emsp;$VertexOut$是前面文章定义的顶点着色器输出的类，它存储投影后的顶点$posH$、世界空间中的顶点$posTrans$、物体的颜色、顶点法线以及纹理坐标。接着在视口变换并送入光栅化部件之前执行透视除法，即直接将裁剪空间的顶点坐标除以它的第四个分量$w$即可。然后我们在外部的渲染循环中设置模型矩阵、视图矩阵已经投影矩阵，就能显示出三维的立体感了，以我们前一章画的三角形为例（gif录制的好像有bug，出现绿色它就给我录制成这个模糊的鬼样，实际上是非常清晰，不是渲染的锅）。 &emsp;&emsp;进入3D世界，怎么能少了3D渲染的”hello world!”——立方体呢？在$Mesh.h$手动创建一个立方体的网格数据，然后用立方体替换掉上面丑陋的三角形： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154void Mesh::asBox(double width, double height, double depth)&#123; vertices.resize(24); indices.resize(36); float halfW = width * 0.5f; float halfH = height * 0.5f; float halfD = depth * 0.5f; //front vertices[0].position = Vector3D(halfW, halfH, halfD); vertices[0].normal = Vector3D(0.f, 0.f, 1.f); vertices[0].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[0].texcoord = Vector2D(1.f, 1.f); vertices[1].position = Vector3D(-halfW, halfH, halfD); vertices[1].normal = Vector3D(0.f, 0.f, 1.f); vertices[1].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[1].texcoord = Vector2D(0.f, 1.f); vertices[2].position = Vector3D(-halfW,-halfH, halfD); vertices[2].normal = Vector3D(0.f, 0.f, 1.f); vertices[2].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[2].texcoord = Vector2D(0.f, 0.f); vertices[3].position = Vector3D(halfW, -halfH, halfD); vertices[3].normal = Vector3D(0.f, 0.f, 1.f); vertices[3].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[3].texcoord = Vector2D(1.f, 0.f); //left vertices[4].position = Vector3D(-halfW, +halfH, halfD); vertices[4].normal = Vector3D(-1.f, 0.f, 0.f); vertices[4].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[4].texcoord = Vector2D(1.f, 1.f); vertices[5].position = Vector3D(-halfW, +halfH, -halfD); vertices[5].normal = Vector3D(-1.f, 0.f, 0.f); vertices[5].color = Vector4D(1.f, 1.f, 0.f, 1.f); vertices[5].texcoord = Vector2D(0.f, 1.f); vertices[6].position = Vector3D(-halfW, -halfH, -halfD); vertices[6].normal = Vector3D(-1.f, 0.f, 0.f); vertices[6].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[6].texcoord = Vector2D(0.f, 0.f); vertices[7].position = Vector3D(-halfW, -halfH, halfD); vertices[7].normal = Vector3D(-1.f, 0.f, 0.f); vertices[7].color = Vector4D(1.f, 1.f, 1.f, 1.f); vertices[7].texcoord = Vector2D(1.f, 0.f); //back vertices[8].position = Vector3D(-halfW, +halfH, -halfD); vertices[8].normal = Vector3D(0.f, 0.f, -1.f); vertices[8].color = Vector4D(1.f, 0.f, 1.f, 1.f); vertices[8].texcoord = Vector2D(0.f, 0.f); vertices[9].position = Vector3D(+halfW, +halfH, -halfD); vertices[9].normal = Vector3D(0.f, 0.f, -1.f); vertices[9].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[9].texcoord = Vector2D(1.f, 0.f); vertices[10].position = Vector3D(+halfW, -halfH, -halfD); vertices[10].normal = Vector3D(0.f, 0.f, -1.f); vertices[10].color = Vector4D(1.f, 1.f, 0.f, 1.f); vertices[10].texcoord = Vector2D(1.f, 1.f); vertices[11].position = Vector3D(-halfW, -halfH, -halfD); vertices[11].normal = Vector3D(0.f, 0.f, -1.f); vertices[11].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[11].texcoord = Vector2D(0.f, 1.f); //right vertices[12].position = Vector3D(halfW, +halfH, -halfD); vertices[12].normal = Vector3D(1.f, 0.f, 0.f); vertices[12].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[12].texcoord = Vector2D(0.f, 0.f); vertices[13].position = Vector3D(halfW, +halfH, +halfD); vertices[13].normal = Vector3D(1.f, 0.f, 0.f); vertices[13].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[13].texcoord = Vector2D(1.f, 0.f); vertices[14].position = Vector3D(halfW, -halfH, +halfD); vertices[14].normal = Vector3D(1.f, 0.f, 0.f); vertices[14].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[14].texcoord = Vector2D(1.f, 1.f); vertices[15].position = Vector3D(halfW, -halfH, -halfD); vertices[15].normal = Vector3D(1.f, 0.f, 0.f); vertices[15].color = Vector4D(1.f, 0.f, 1.f, 1.f); vertices[15].texcoord = Vector2D(0.f, 1.f); //top vertices[16].position = Vector3D(+halfW, halfH, -halfD); vertices[16].normal = Vector3D(0.f, 1.f, 0.f); vertices[16].color = Vector4D(0.f, 0.f, 0.f, 1.f); vertices[16].texcoord = Vector2D(0.f, 0.f); vertices[17].position = Vector3D(-halfW, halfH, -halfD); vertices[17].normal = Vector3D(0.f, 1.f, 0.f); vertices[17].color = Vector4D(1.f, 1.f, 0.f, 1.f); vertices[17].texcoord = Vector2D(1.f, 0.f); vertices[18].position = Vector3D(-halfW, halfH, halfD); vertices[18].normal = Vector3D(0.f, 1.f, 0.f); vertices[18].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[18].texcoord = Vector2D(1.f, 1.f); vertices[19].position = Vector3D(+halfW, halfH, halfD); vertices[19].normal = Vector3D(0.f, 1.f, 0.f); vertices[19].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[19].texcoord = Vector2D(0.f, 1.f); //down vertices[20].position = Vector3D(+halfW, -halfH, -halfD); vertices[20].normal = Vector3D(0.f, -1.f, 0.f); vertices[20].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[20].texcoord = Vector2D(0.f, 0.f); vertices[21].position = Vector3D(+halfW, -halfH, +halfD); vertices[21].normal = Vector3D(0.f, -1.f, 0.f); vertices[21].color = Vector4D(1.f, 1.f, 1.f, 1.f); vertices[21].texcoord = Vector2D(1.f, 0.f); vertices[22].position = Vector3D(-halfW, -halfH, +halfD); vertices[22].normal = Vector3D(0.f, -1.f, 0.f); vertices[22].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[22].texcoord = Vector2D(1.f, 1.f); vertices[23].position = Vector3D(-halfW, -halfH, -halfD); vertices[23].normal = Vector3D(0.f, -1.f, 0.f); vertices[23].color = Vector4D(1.f, 0.f, 1.f, 1.f); vertices[23].texcoord = Vector2D(0.f, 1.f); //front indices[0] = 0; indices[1] = 1; indices[2] = 2; indices[3] = 0; indices[4] = 2; indices[5] = 3; //left indices[6] = 4; indices[7] = 5; indices[8] = 6; indices[9] = 4; indices[10] = 6; indices[11] = 7; //back indices[12] = 8; indices[13] = 9; indices[14] = 10; indices[15] = 8; indices[16] = 10; indices[17] = 11; //right indices[18] = 12; indices[19] = 13; indices[20] = 14; indices[21] = 12; indices[22] = 14; indices[23] = 15; //top indices[24] = 16; indices[25] = 17; indices[26] = 18; indices[27] = 16; indices[28] = 18; indices[29] = 19; //down indices[30] = 20; indices[31] = 21; indices[32] = 22; indices[33] = 20; indices[34] = 22; indices[35] = 23;&#125; &emsp;&emsp;结果我们就得到一个如下面所示的奇怪的立方体： &emsp;&emsp;这的确有点像是一个立方体，但又有种说不出的奇怪。立方体的某些本应被遮挡住的面被绘制在了这个立方体其他面之上。出现这样结果的原因是因为我们的软渲染器是对一个一个三角形进行绘制的，而且计算像素时时直接覆盖而不管这个像素是否已经有其他值了，所以一个像素的值完全取决于最后赋予它的$RGBA$。除非渲染管线自动按照从远到近的顺序（这类算法有画家算法、空间分割BSP树算法）绘制三角形，否则直接覆盖的方法获取不了正确的像素值。正确渲染结果应该是像素的$RGBA$值为最靠近视点的片元值，一种常用的技术是借助第三维信息——深度来对每个相同位置的不同片元做深度的比较，并且取深度较低的那一个。 2、深度测试&emsp;&emsp;为了获取正确的三维渲染结果，我们采用一种深度缓冲的技术。深度缓冲存储深度信息，它的分辨率应该与颜色缓冲一致，深度值存储在每个片段里面（作为片段的z值），当片段想要输出它的颜色时，我们将它的深度值和z缓冲进行比较，如果当前的片段在其它片段之后，它将会被丢弃，否则将会覆盖。这个过程称为深度测试。在OpenGL和DirectX这些渲染API中，深度缓冲会自动执行而无需用户操作。在我们的软渲染器中，我们自己实现一个这样的深度测试，算法原理很简单，但是效果非常不错！ &emsp;&emsp;深度缓冲通常和颜色缓冲一起，作为帧缓冲的附件，我们在帧缓冲类中增加深度缓冲相关的变量、方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class FrameBuffer&#123;private: ...... std::vector&lt;double&gt; m_depthBuffer;public: ...... void clearColorAndDepthBuffer(const Vector4D &amp;color); double getDepth(const unsigned int &amp;x, const unsigned int &amp;y)const; void drawDepth(const unsigned int &amp;x, const unsigned int &amp;y, const double &amp;value);&#125;;void FrameBuffer::clearColorAndDepthBuffer(const Vector4D &amp;color)&#123; // fill the color buffer and depth buffer. unsigned char red = static_cast&lt;unsigned char&gt;(255*color.x); unsigned char green = static_cast&lt;unsigned char&gt;(255*color.y); unsigned char blue = static_cast&lt;unsigned char&gt;(255*color.z); unsigned char alpha = static_cast&lt;unsigned char&gt;(255*color.w); for(unsigned int row = 0;row &lt; m_height;++ row) &#123; for(unsigned int col = 0;col &lt; m_width;++ col) &#123; m_depthBuffer[row*m_width+col] = 1.0f; ...... &#125; &#125;&#125;double FrameBuffer::getDepth(const unsigned int &amp;x, const unsigned int &amp;y) const&#123; if(x &lt; 0 || x &gt;= m_width || y &lt; 0 || y &gt;= m_height) return 0.0f; return m_depthBuffer[y*m_width+x];&#125;void FrameBuffer::drawDepth(const unsigned int &amp;x, const unsigned int &amp;y, const double &amp;value)&#123; if(x &lt; 0 || x &gt;= m_width || y &lt; 0 || y &gt;= m_height) return; unsigned int index = y*m_width + x; m_depthBuffer[index] = value;&#125; &emsp;&emsp;然后我们对于每一个片元，我们获取深度缓冲中相应的数值并进行比较。在这之前，我们还要简单回顾一下在透视投影矩阵中深度值的非线性映射，在前面的数学篇中我们知道透视投影矩阵有如下形式： M_{projection}= \\left( \\begin{matrix} \\frac{1}{aspect*tan(fovy/2)}&0&0&0\\\\ 0&\\frac{1}{tan(fovy/2)}&0&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right)&emsp;&emsp;因而视图空间中的深度信息$z_e$和标准化设备空间中的深度信息$z_n$关系为： z_n=(-\\frac{f+n}{f-n}z_e-\\frac{2fn}{f-n})/{-z_e} =\\frac{2fn}{z_e(f-n)}+\\frac{f+n}{f-n} \\tag {1}&emsp;&emsp;可以看到$z_e$d到$z_n$是一种从$[-f, -n]$到$[-1,1]$的非线性映射。当$z_e$比较小的时候，公式$(1)$有很高的精度；当$z_e$比较大的时候，公式$(1)$应为取值精度降低。这个关系可以直观地从下图的函数曲线看出来： &emsp;&emsp;可以看到，深度值很大一部分是由很小的z值所决定的，这给了近处的物体很大的深度精度。$z_n$取值为$[-1,1]$，我们最后将其简单地映射到$[0,1]$，这一步我放在透视除法后。 123456789void Pipeline::perspectiveDivision(VertexOut &amp;target)&#123; target.posH.x /= target.posH.w; target.posH.y /= target.posH.w; target.posH.z /= target.posH.w; target.posH.w = 1.0f; // map from [-1,1] to [0,1] target.posH.z = (target.posH.z+1.0f) * 0.5f;&#125; &emsp;&emsp;在写入深度缓冲之前应该要清除上一帧的深度缓冲，全部置$1.0f$即可，我把这一步和清除颜色缓冲放一起了，即前面的帧缓冲类的$clearColorAndDepthBuffer$方法。在光栅化步骤，获取每个片元的屏幕位置，查找深度缓并比较，若小于当前深度缓冲中获取的值，则通过深度测试并写入深度缓冲。 12345678910111213141516171819202122232425262728void Pipeline::scanLinePerRow(const VertexOut &amp;left, const VertexOut &amp;right)&#123; // scan the line from left to right. VertexOut current; int length = right.posH.x - left.posH.x + 1; for(int i = 0;i &lt;= length;++i) &#123; // linear interpolation double weight = static_cast&lt;double&gt;(i)/length; current = lerp(left, right, weight); current.posH.x = left.posH.x + i; current.posH.y = left.posH.y; // depth testing. double depth = m_backBuffer-&gt;getDepth(current.posH.x, current.posH.y); if(current.posH.z &gt; depth) continue;// fail to pass the depth testing. m_backBuffer-&gt;drawDepth(current.posH.x,current.posH.y,current.posH.z); double w = 1.0/current.oneDivZ; current.posTrans *= w; current.color *= w; current.texcoord *= w; // fragment shader m_backBuffer-&gt;drawPixel(current.posH.x, current.posH.y, m_shader-&gt;fragmentShader(current)); &#125;&#125; &emsp;&emsp;然后就可以根据深度信息正确地渲染出三维的立体感了。 3、裁剪、剔除优化&emsp;&emsp;目前目前我们已经构建出三维的渲染管线，但是这还不够，因为图形渲染计算量很大，通常我们需要做一些优化。常见的嵌入在渲染管线中的优化算法有几何裁剪、背面剔除。 几何裁剪&emsp;&emsp;注意在坐标系统的变换过程中，位于视锥体内的顶点坐标各分量都会被映射到$[-1,1]$的范围内，超出视锥体的顶点则被映射到超出$[-1,1]$的范围。我们在这个基础上的做相关的裁剪，注意在透视除法之前各分量实际上是处于$[-w,w]$的范围内的，这里的$w$就是该顶点坐标的第四个分量$w$。针对线框模式渲染和填充模式渲染，我们有两种不同的裁剪算法。 Cohen-Sutherland线条裁剪算法&emsp;&emsp;一条线段在视口内的情况有如下所示的四种。其中端点完全在视口内和一端在视口内而另一端是在视口外的情况很好判断，但是线段完全在视口外就没那么简单了。可以看到线段$GH$的端点都在视口外部，但是线段的一部分却在视口的内部，这是如果直接根据两个端点是否在视口外做剔除的话会导致在边缘部分的线段直接消失，得到错误的结果。一种暴力的解法就是计算线段与视口窗口的交点，但是这并不高效。 &emsp;&emsp;Cohen-Sutherland提出了一种基于编码的判断算法，通过简单的移位、与或逻辑运算就可以判断一条线段处于哪种情况。对于每一个端点$(x,y)$，我们定义一个outcode——$b_0b_1b_2b_3$，视口所处的范围用$x_{min}$、$x_{max}$、$y_{min}$、$y_{max}$表示。每个端点$(x,y)$的outcode的计算方法如下： &emsp;&emsp;$b_0 = 1\\ if \\ y &gt; y_{max},\\ 0\\ otherwiose$ &emsp;&emsp;$b_1 = 1\\ if \\ y &lt; y_{min},\\ 0\\ otherwiose$ &emsp;&emsp;$b_2 = 1\\ if \\ x &gt; x_{min},\\ 0\\ otherwiose$ &emsp;&emsp;$b_3 = 1\\ if \\ x &lt; x_{max},\\ 0\\ otherwiose$ &emsp;&emsp;可以看出outcode将屏幕空间分成了$9$个部分： &emsp;&emsp;观察上面的$9$个区域，对于两个端点outcode1和outcode2，做如下的判断策略，其中的$OR$和$AND$是逻辑按位运算： &emsp;&emsp;若$(outcode1\\ OR\\ outcode2)==0$，那么线段就完全在视口内部； &emsp;&emsp;若$(outcode1\\ AND\\ outcode2)!=0$，那么线段就完全在视口外部； &emsp;&emsp;若$(outcode1\\ AND\\ outcode2)==0$，那么线段就可能部分在视口外部，部分在内部，还需要做进一步的判断（这里我进一步判断用了包围盒，因为比较常见和简单，就不过多描述了）。 &emsp;&emsp;这里我的实现就是只裁剪掉肯定完全在视口外部的线段，若还想裁剪掉部分外视口外部的线段则需要进一步的求交运算。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950bool Pipeline::lineCliping(const VertexOut &amp;from, const VertexOut &amp;to)&#123; // return whether the line is totally outside or not. float vMin = -from.posH.w, vMax = from.posH.w; float x1 = from.posH.x, y1 = from.posH.y; float x2 = to.posH.x, y2 = to.posH.y; int tmp = 0; int outcode1 = 0, outcode2 = 0; // outcode1 calculation. tmp = (y1&gt;vMax)?1:0; tmp &lt;&lt;= 3; outcode1 |= tmp; tmp = (y1&lt;vMin)?1:0; tmp &lt;&lt;= 2; outcode1 |= tmp; tmp = (x1&gt;vMax)?1:0; tmp &lt;&lt;= 1; outcode1 |= tmp; tmp = (x1&lt;vMin)?1:0; outcode1 |= tmp; // outcode2 calculation. tmp = (y2&gt;vMax)?1:0; tmp &lt;&lt;= 3; outcode2 |= tmp; tmp = (y2&lt;vMin)?1:0; tmp &lt;&lt;= 2; outcode2 |= tmp; tmp = (x2&gt;vMax)?1:0; tmp &lt;&lt;= 1; outcode2 |= tmp; tmp = (x2&lt;vMin)?1:0; outcode2 |= tmp; if((outcode1 &amp; outcode2) != 0) return true; // bounding box judge. Vector2D minPoint,maxPoint; minPoint.x = min(from.posH.x, to.posH.x); minPoint.y = min(from.posH.y, to.posH.y); maxPoint.x = max(from.posH.x, to.posH.x); maxPoint.y = max(from.posH.y, to.posH.y); if(minPoint.x &gt; vMax || maxPoint.x &lt; vMin || minPoint.y &gt; vMax || maxPoint.y &lt; vMin) return true; return false;&#125; 三角形裁剪&emsp;&emsp;齐次空间三角形精准裁剪见此链接。 背面剔除&emsp;&emsp;背面剔除网上的这篇博客已经讲得非常详细了，原理也很简单，我就不过多描述。我们定义顶点逆时针的环绕顺序正面，然后通过三角形的三个顶点计算出法线，将顶点与视线做点乘并判断其符号即可。 123456789101112131415bool Pipeline::backFaceCulling(const Vector4D &amp;v1, const Vector4D &amp;v2, const Vector4D &amp;v3)&#123; // back face culling. if(m_mode == RenderMode::wire) return true; Vector4D tmp1 = v2 - v1; Vector4D tmp2 = v3 - v1; Vector3D edge1(tmp1.x, tmp1.y, tmp1.z); Vector3D edge2(tmp2.x, tmp2.y, tmp2.z); Vector3D viewRay(m_eyePos.x - v1.x, m_eyePos.y - v1.y, m_eyePos.z - v1.z); Vector3D normal = edge1.crossProduct(edge2); return normal.dotProduct(viewRay) &gt; 0;&#125; &emsp;&emsp;然后背面剔除应该放在渲染管线的顶点着色器输出之后，如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849void Pipeline::drawIndex(RenderMode mode)&#123; // renderer pipeline. bool line1 = false, line2 = false, line3 = false; m_mode = mode; if(m_indices.empty())return; for(unsigned int i = 0;i &lt; m_indices.size();i += 3) &#123; //! assembly to triangle primitive. Vertex p1,p2,p3; &#123; ...... &#125; //! vertex shader stage. VertexOut v1,v2,v3; &#123; ...... &#125; //! back face culling. &#123; if(!backFaceCulling(v1.posTrans, v2.posTrans, v3.posTrans)) continue; &#125; //! geometry cliping. &#123; ...... &#125; //! perspective division. &#123; ...... &#125; //! view port transformation. &#123; ...... &#125; //! rasterization and fragment shader stage. &#123; ...... &#125; &#125;&#125; 二、透视纹理映射、采样&emsp;&emsp;纹理映射是丰富三维物体细节的一个非常重要的方法，简单、廉价、快速，只需计算好的纹理坐标、纹理图片即可实现物体的多姿多彩。通常纹理图片的制作（除了过程式纹理的生成）由设计师完成，无需我们关心。而纹理坐标的计算则需要非常注意，送入渲染管线的纹理坐标只是逐顶点的纹理坐标，在光栅化阶段我们还要将纹理坐标做插值操作，最后根据插值后得到的纹理坐标对纹理图片采样获取片元的像素值。 1、透视纹理映射&emsp;&emsp;在光栅化阶段，我们是根据屏幕空间的$x$值和$y$值做线性插值操作获取片元的位置，而片元的纹理坐标如果也这么获得的话（这种方法叫做仿射纹理映射），将会导致严重的纹理扭曲。这是因为仿射纹理映射是基于这样的一个假设：物体空间的纹理坐标与屏幕空间的顶点坐标呈线性管线。 &emsp;&emsp;我们知道纹理坐标是定义在物体的顶点上面的，当我们根据屏幕空间的顶点坐标插值时，就默认了纹理坐标的变化与屏幕空间顶点坐标的变化是呈线性、均匀的关系的。但是问题在于：默认的屏幕空间上的线性关系，还原到世界空间中，就不是那么回事了。除了纹理坐标，所有定义在世界空间线性关系下的属性插值都需要进行透视矫正，例如深度、法线向量、世界空间顶点坐标等等。 &emsp;&emsp;关于透视矫正插值见此链接。 2、双线性纹理采样&emsp;&emsp;定义的纹理坐标都是$[0.0f,1.0f]$的浮点数，为了采样纹理我们需要把它乘上纹理的宽高转成整数的下标取访问纹理的像素矩阵。乘上纹理的宽高之后我们得到的依然应该是一个浮点数，为了获取像素下标，一个简单的方法就是向下取整（这种采样方法对应于OpenGL的GL_NEAREST纹理过滤方法）。如下所示： 12345678910double trueU = texcoord.x * (m_width - 1);double trueV = texcoord.y * (m_height - 1);x = static_cast&lt;unsigned int&gt;(trueU);y = static_cast&lt;unsigned int&gt;(trueV);int index[0] = (x * m_width + y) * m_channel;Vector3D texels;// INV_SCALE is 1.0/255texels.x = static_cast&lt;float&gt;(m_pixelBuffer[index + 0]) * INV_SCALE;texels.y = static_cast&lt;float&gt;(m_pixelBuffer[index + 1]) * INV_SCALE;texels.z = static_cast&lt;float&gt;(m_pixelBuffer[index + 2]) * INV_SCALE; &emsp;&emsp;问题就出在这里，这样直接抛弃小数点以后的值导致采样出的相邻纹理并不连续，那么用float采样行吗？答案是：不行！这边实现的采样函数是从数组取值，纹理坐标转为数组下标，数组下标不能用float只能用int，那么就没办法了吗？并不是，可以对周围纹理进行采样然后按照各自比例进行混合，这样能够提高显示效果。混合的方法就是双线性插值。所谓双线性插值，就是先后线性插值一次，共两次。即横向线性插值一次，然后根据前面一次的插值结果竖向插值一次，二维纹理是有两个维度，所以做双线性插值。 &emsp;&emsp;除了采样之外，还有一个纹理坐标溢出的问题。纹理坐标超过的$[0,1]$通常由两种处理方式，一种是$clamp$，超过$[0,1]$的地方的像素都获取边上的像素，这样效果就是拉伸。一种是$repeat$，故名思议，即重复平铺。这里我实现的是重复平铺，在计算真正的纹理下标之前做相应的判断和处理即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108class Texture2D&#123;private: int m_width; int m_height; int m_channel; unsigned char *m_pixelBuffer;public: Texture2D():m_width(0), m_height(0), m_channel(0), m_pixelBuffer(nullptr)&#123;&#125; ~Texture2D(); bool loadImage(const std::string &amp;path); Vector4D sample(const Vector2D &amp;texcoord) const;&#125;;bool Texture2D::loadImage(const std::string &amp;path)&#123; if(m_pixelBuffer)delete m_pixelBuffer; m_pixelBuffer = nullptr; m_pixelBuffer = stbi_load(path.c_str(), &amp;m_width, &amp;m_height, &amp;m_channel, 0); if(m_pixelBuffer == nullptr) &#123; qDebug() &lt;&lt; \"Failed to load image-&gt;\" &lt;&lt; QString::fromStdString(path); &#125; return m_pixelBuffer != nullptr;&#125;Vector4D Texture2D::sample(const Vector2D &amp;texcoord) const&#123; // just for rgb and rgba format. Vector4D result(0.0,0.0,0.0,1.0); if(m_pixelBuffer == nullptr) return result; unsigned int x = 0, y = 0; // for bilinear interpolation. double factorU = 0, factorV = 0; // calculate the corresponding coordinate. if(texcoord.x &gt;= 0.0f &amp;&amp; texcoord.x &lt;= 1.0f &amp;&amp; texcoord.y &gt;= 0.0f &amp;&amp; texcoord.y &lt;= 1.0f) &#123; double trueU = texcoord.x * (m_width - 1); double trueV = texcoord.y * (m_height - 1); x = static_cast&lt;unsigned int&gt;(trueU); y = static_cast&lt;unsigned int&gt;(trueV); factorU = trueU - x; factorV = trueV - y; &#125; else &#123; // repeating way. float u = texcoord.x,v = texcoord.y; if(texcoord.x &gt; 1.0f) u = texcoord.x - static_cast&lt;int&gt;(texcoord.x); else if(texcoord.x &lt; 0.0f) u = 1.0f - (static_cast&lt;int&gt;(texcoord.x) - texcoord.x); if(texcoord.y &gt; 1.0f) v = texcoord.y - static_cast&lt;int&gt;(texcoord.y); else if(texcoord.y &lt; 0.0f) v = 1.0f - (static_cast&lt;int&gt;(texcoord.y) - texcoord.y); double trueU = u * (m_width - 1); double trueV = v * (m_height - 1); x = static_cast&lt;unsigned int&gt;(trueU); y = static_cast&lt;unsigned int&gt;(trueV); factorU = trueU - x; factorV = trueV - y; &#125; // texel fetching. Vector3D texels[4]; int index[4]; index[0] = (x * m_width + y) * m_channel; index[1] = (x * m_width + y + 1) * m_channel; index[2] = ((x + 1) * m_width + y + 1) * m_channel; index[3] = ((x + 1) * m_width + y) * m_channel; // left bottom texels[0].x = static_cast&lt;float&gt;(m_pixelBuffer[index[0] + 0]) * INV_SCALE; texels[0].y = static_cast&lt;float&gt;(m_pixelBuffer[index[0] + 1]) * INV_SCALE; texels[0].z = static_cast&lt;float&gt;(m_pixelBuffer[index[0] + 2]) * INV_SCALE; //return texels[0]; // left top texels[1].x = static_cast&lt;float&gt;(m_pixelBuffer[index[1] + 0]) * INV_SCALE; texels[1].y = static_cast&lt;float&gt;(m_pixelBuffer[index[1] + 1]) * INV_SCALE; texels[1].z = static_cast&lt;float&gt;(m_pixelBuffer[index[1] + 2]) * INV_SCALE; // right top texels[2].x = static_cast&lt;float&gt;(m_pixelBuffer[index[2] + 0]) * INV_SCALE; texels[2].y = static_cast&lt;float&gt;(m_pixelBuffer[index[2] + 1]) * INV_SCALE; texels[2].z = static_cast&lt;float&gt;(m_pixelBuffer[index[2] + 2]) * INV_SCALE; // right bottom texels[3].x = static_cast&lt;float&gt;(m_pixelBuffer[index[3] + 0]) * INV_SCALE; texels[3].y = static_cast&lt;float&gt;(m_pixelBuffer[index[3] + 1]) * INV_SCALE; texels[3].z = static_cast&lt;float&gt;(m_pixelBuffer[index[3] + 2]) * INV_SCALE; // bilinear interpolation. // horizational texels[0] = texels[0] * (1.0 - factorU) + texels[3] * factorU; texels[1] = texels[1] * (1.0 - factorU) + texels[2] * factorU; //vertical result = texels[0] * (1.0 - factorV) + texels[1] *factorV; return result;&#125; &emsp;&emsp;加载图片我的用的stb_image，一个简单使用的头文件，因为加载图片不是我们的重点，所以就不造这方面的轮子了。 三、程序结果&emsp;&emsp;目前的帧率还不错hhh。 参考资料$[1]$ https://learnopengl.com/Advanced-OpenGL/Depth-testing $[2]$ https://www.cnblogs.com/pbblog/p/3484193.html $[3]$ https://learnopengl.com/Getting-started/Coordinate-Systems $[4]$ http://www.songho.ca/opengl/gl_projectionmatrix.html $[5]$ https://blog.csdn.net/popy007/article/details/5570803 $[6]$ https://learnopengl-cn.github.io/01%20Getting%20started/06%20Textures/","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/tags/Soft-Renderer/"},{"name":"3D pipeline","slug":"3D-pipeline","permalink":"https://yangwc.com/tags/3D-pipeline/"}]},{"title":"软渲染器Soft Renderer：光栅化篇","slug":"SoftRenderer-Rasterization","date":"2019-05-01T02:37:54.047Z","updated":"2021-04-12T03:34:07.533Z","comments":true,"path":"2019/05/01/SoftRenderer-Rasterization/","link":"","permalink":"https://yangwc.com/2019/05/01/SoftRenderer-Rasterization/","excerpt":"本渲染器已完全重构，与之前版本大不相同，详情移步至TinySoftRenderer。","text":"本渲染器已完全重构，与之前版本大不相同，详情移步至TinySoftRenderer。 渲染管线框架 光栅化算法 一、渲染管线框架&emsp;&emsp;渲染管线的搭建主要包含像素显示、网格数据封装、渲染循环、帧率fps计算、帧缓冲、着色器、渲染逻辑、光栅化等等，其中光栅化作为重点对象抽出来放在后面。当然我们不会一下子就完成渲染管线的基本功能，我们现在是要搭建一个框架，大部分的内容不用写入或者仅仅是做简单的处理，这样后面完善软渲染器的时候只需在相应的位置填写相应的代码逻辑即可。本章目标就是搭建一个渲染管线，用光栅化算法画三角形。当然，如果仅仅是画一个三角形，当然不用这么麻烦，但是我的目标是实现三维的软渲染器，深入理解三维渲染的整个流程，得从基础一步一步慢慢来。 1、像素显示的画布&emsp;&emsp;渲染器最终渲染出来的是一个像素矩阵，我们要把这个像素矩阵显示出来。显示的方法有很多，因人而异，这里我采用自己最熟悉的$Qt$来实现。显示的窗口继承一个普通的$QWidget$父类，然后我们通过重写它的$paintEvent$函数，将渲染出来的像素画到$QWidget$上。但是采用$QPainter$直接画上去的方式效率非常低，我通过查询资料得知，若想要快速地绘制给定的像素矩阵，可以利用$QImage$来实现。话不多说，上代码： 123456789101112131415class Window : public QWidget&#123; Q_OBJECTpublic: explicit Window(QWidget *parent = nullptr); ~Window();private: void paintEvent(QPaintEvent *) override;private: Ui::Window *ui; QImage *canvas;&#125;; &emsp;&emsp;接收到一帧的像素之后，在重绘事件里面利用$QImage$绘制给定的像素数组（记得调用$update$触发重绘事件）。由于篇幅原因，我不会讲太多细节方面的东西，代码也不会全部放出来，那样没意义。想看完整源代码的朋友直接去本人的github上看。 12345678910111213141516void Window::receiveFrame(unsigned char *image)&#123; if(canvas) delete canvas; canvas = new QImage(image, width(), height(), QImage::Format_RGBA8888); update();&#125;void Window::paintEvent(QPaintEvent *event)&#123; if(canvas) &#123; QPainter painter(this); painter.drawImage(0, 0, *canvas); &#125; QWidget::paintEvent(event);&#125; 2、帧缓冲类&emsp;&emsp;帧缓冲通常包含基本的颜色缓冲附件、深度缓冲附件等，这里我们暂且只实现颜色缓冲附件（四通道，格式为$RGBA$，各占一个字节），深度缓冲附件后面再加上。渲染管线最终的渲染结果是写入帧缓冲的，我们采用一个一维的单字节数组作为帧缓冲的颜色缓冲。帧缓冲的最基本的功能就是清楚缓冲区、写入像素： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162class FrameBuffer&#123;private: int m_width, m_height, m_channel; std::vector&lt;unsigned char&gt; m_colorBuffer;public: FrameBuffer(int width, int height); ~FrameBuffer() = default; int getWidth()&#123;return m_width;&#125; int getHeight()&#123;return m_height;&#125; unsigned char *getColorBuffer() &#123;return m_colorBuffer.data();&#125; void clearColorBuffer(const Vector4D &amp;color); void drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color);&#125;;FrameBuffer::FrameBuffer(int width, int height) :m_channel(4), m_width(width), m_height(height)&#123; m_colorBuffer.resize(m_width*m_height*m_channel, 255);&#125;void FrameBuffer::clearColorBuffer(const Vector4D &amp;color)&#123; // fill the color buffer. unsigned char red = static_cast&lt;unsigned char&gt;(255*color.x); unsigned char green = static_cast&lt;unsigned char&gt;(255*color.y); unsigned char blue = static_cast&lt;unsigned char&gt;(255*color.z); unsigned char alpha = static_cast&lt;unsigned char&gt;(255*color.w); for(int row = 0;row &lt; m_height;++ row) &#123; for(int col = 0;col &lt; m_width;++ col) &#123; m_colorBuffer[row*m_width*m_channel+col*m_channel + 0] = red; m_colorBuffer[row*m_width*m_channel+col*m_channel + 1] = green; m_colorBuffer[row*m_width*m_channel+col*m_channel + 2] = blue; m_colorBuffer[row*m_width*m_channel+col*m_channel + 3] = alpha; &#125; &#125;&#125;void FrameBuffer::drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color)&#123; if(x &lt; 0 || x &gt;= m_width || y &lt; 0 || y &gt;= m_height) return; unsigned char red = static_cast&lt;unsigned char&gt;(255*color.x); unsigned char green = static_cast&lt;unsigned char&gt;(255*color.y); unsigned char blue = static_cast&lt;unsigned char&gt;(255*color.z); unsigned char alpha = static_cast&lt;unsigned char&gt;(255*color.w); unsigned int index = y*m_width*m_channel + x*m_channel; m_colorBuffer[index + 0] = red; m_colorBuffer[index + 1] = green; m_colorBuffer[index + 2] = blue; m_colorBuffer[index + 3] = alpha;&#125; 3、网格顶点数据&emsp;&emsp;三维的渲染程序中的顶点数据通常包含顶点位置、顶点颜色、纹理坐标、顶点法线，然后在此基础上利用一组给定顺序的顶点数据表示一个网格，渲染时网格的数据将被送入管线进行处理。为此，有必要对顶点数据做一定的封装。 1234567891011121314class Vertex&#123;public: Vector4D position; Vector4D color; Vector2D texcoord; Vector3D normal; Vertex() = default; Vertex(Vector4D _pos, Vector4D _color, Vector2D _tex, Vector3D _normal) :position(_pos),color(_color),texcoord(_tex),normal(_normal) &#123;&#125; Vertex(const Vertex &amp;rhs) :position(rhs.position),color(rhs.color),texcoord(rhs.texcoord),normal(rhs.normal)&#123;&#125;&#125;; &emsp;&emsp;顶点数据经过顶点着色器的处理之后，会被送到下一个渲染管线的阶段处理。顶点着色器的顶点数据输出与输入有些差异，为此我们也定义一个类表示为顶点着色器的输出，这对于构建渲染管线尤为重要。 12345678910111213141516171819class VertexOut&#123;public: Vector4D posTrans; //世界变换后的坐标 Vector4D posH; //投影变换后的坐标 Vector2D texcoord; //纹理坐标 Vector3D normal; //法线 Vector4D color; //颜色 double oneDivZ; //1/z用于深度测试 VertexOut() = default; VertexOut(Vector4D _posT, Vector4D _posH, Vector2D _tex, Vector3D _normal, Vector4D _color, double _oneDivZ) :posTrans(_posT),posH(_posH),texcoord(_tex), normal(_normal),color(_color),oneDivZ(_oneDivZ) &#123;&#125; VertexOut(const VertexOut&amp; rhs) :posTrans(rhs.posTrans), posH(rhs.posH), texcoord(rhs.texcoord), normal(rhs.normal), color(rhs.color), oneDivZ(rhs.oneDivZ) &#123;&#125;&#125;; &emsp;&emsp;然后就是关于网格的表示，为了节省空间（特别是对于很大的模型），我们直接采用索引来组织网格。若想详细了解OpenGL的顶点索引概念请看这里。一个网格有两个数组，分别是$Vertex$数组和$Index$数组。下面的代码中，有一个$asTriangle$方法，这是一个三角形网格，调用这个方法之后网格存储的就是一个三角形，用于后面的光栅化调试，光栅化的基本单元就是三角形。通常情况，所有的网格模型都可以用一定数量的三角形构成，因而我们实现的软渲染器的基本图元就是三角形。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class Mesh&#123;public: std::vector&lt;Vertex&gt; vertices; std::vector&lt;unsigned int&gt; indices; Mesh() = default; ~Mesh() = default; Mesh(const Mesh&amp; mesh) :vertices(mesh.vertices), indices(mesh.indices)&#123;&#125; Mesh&amp; operator=(const Mesh&amp; mesh) &#123; if (&amp;mesh == this) return *this; vertices = mesh.vertices; indices = mesh.indices; return *this; &#125; void setVertices(Vertex* _vs, int count) &#123; vertices.resize(count); new(&amp;vertices[0])std::vector&lt;Vertex&gt;(_vs, _vs + count); &#125; void setIndices(int* _es, int count) &#123; indices.resize(count); new(&amp;indices)std::vector&lt;unsigned int&gt;(_es, _es + count); &#125; void asBox(double width, double height, double depth); void asTriangle(const Vector3D p1, const Vector3D p2, const Vector3D p3);&#125;;void Mesh::asTriangle(Vector3D p1, Vector3D p2, Vector3D p3)&#123; vertices.resize(3); indices.resize(3); vertices[0].position = p1; vertices[0].normal = Vector3D(0.f, 0.f, 1.f); vertices[0].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[0].texcoord = Vector2D(0.f, 0.f); vertices[1].position = p2; vertices[1].normal = Vector3D(0.f, 0.f, 1.f); vertices[1].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[1].texcoord = Vector2D(1.f, 0.f); vertices[2].position = p3; vertices[2].normal = Vector3D(0.f, 0.f, 1.f); vertices[2].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[2].texcoord = Vector2D(0.5f, 1.f); indices[0] = 0; indices[1] = 1; indices[2] = 2;&#125; 4、简单的着色器&emsp;&emsp;着色器方面时软渲染中较为高级的内容，目前我们只是搭建一个框架，因而着色器不需要什么复杂的操作，只需简单地传递数据就行了。博主实现的软渲染器只包含必不可少的顶点着色器和片元着色器，目前的顶点着色器将顶点原封不动地输出，片元着色器也是如此，这样我们后面要实现光照效果的时候直接在着色器里写上就行了。为了更加有条理，我们设计一个着色器的虚类，这样实现不同效果的着色器时我们直接继承这个虚类即可。 123456789101112class BaseShader&#123;public: BaseShader() = default; virtual ~BaseShader() = default; virtual VertexOut vertexShader(const Vertex &amp;in) = 0; virtual Vector4D fragmentShader(const VertexOut &amp;in) = 0; virtual void setModelMatrix(const Matrix4x4 &amp;world) = 0; virtual void setViewMatrix(const Matrix4x4 &amp;view) = 0; virtual void setProjectMatrix(const Matrix4x4 &amp;project) = 0;&#125;; 12345678910111213141516171819202122232425262728293031323334353637383940414243class SimpleShader : public BaseShader&#123;public: SimpleShader() = default; virtual ~SimpleShader() = default; virtual VertexOut vertexShader(const Vertex &amp;in); virtual Vector4D fragmentShader(const VertexOut &amp;in); virtual void setModelMatrix(const Matrix4x4 &amp;world); virtual void setViewMatrix(const Matrix4x4 &amp;view); virtual void setProjectMatrix(const Matrix4x4 &amp;project);&#125;;VertexOut SimpleShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = in.position; result.posH = in.position; result.color = in.color; result.normal = in.normal; result.oneDivZ = 1.0; result.texcoord = in.texcoord; return result;&#125;Vector4D SimpleShader::fragmentShader(const VertexOut &amp;in)&#123; Vector4D litColor; litColor = in.color; return litColor;&#125;void SimpleShader::setModelMatrix(const Matrix4x4 &amp;world)&#123;&#125;void SimpleShader::setViewMatrix(const Matrix4x4 &amp;view)&#123;&#125;void SimpleShader::setProjectMatrix(const Matrix4x4 &amp;project)&#123;&#125; &emsp;&emsp;可以看到$SimpleShader$仅仅是将顶点数据直接输出，不进行任何处理。 5、搭建基本的渲染管线&emsp;&emsp;目前我们已经有了一些渲染管线的基本组件，现在就需要把这些组件串起来。首先是渲染循环的问题，$Qt$有它自己的事件循环，而且主线程的事件循环要尽量避免大量的运算（否则UI控件会陷入未响应），因此将渲染循环放到子线程里是一个不错的渲染，这样也可以避免我们的软渲染逻辑与$Qt$的接口耦合得太高。 渲染线程&emsp;&emsp;$Qt$提供了$QThread$类构建线程，我采用的方式为：渲染循环类继承$QObject$，然后调用$moveToThread$番方法挂到子线程上运行，最后将线程的启动信号与$loop$渲染循环关联即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class RenderLoop : public QObject&#123; Q_OBJECTpublic: explicit RenderLoop(int w, int h, QObject *parent = nullptr); ~RenderLoop(); void stopIt() &#123;stoped = true;&#125; void setFpsZero()&#123;fps = 0;&#125; int getFps()&#123;return fps;&#125;signals: void frameOut(unsigned char *image);public slots: void loop();private: bool stoped; int fps; int width, height, channel;&#125;;RenderLoop::RenderLoop(int w, int h, QObject *parent) : QObject(parent), width(w), height(h), channel(4)&#123; fps = 0; stoped = false;&#125;RenderLoop::~RenderLoop()&#123;&#125;void RenderLoop::loop()&#123; // pipeline initialization ...... // fps counting. fps = 0; while(!stoped) &#123; // render logic ...... ++ fps; &#125;&#125; &emsp;&emsp;然后在主窗口中创建$RenderLoop$对象，挂到$QThread$上启动。此外还有一点要注意的是在子线程中最好不用使用$QTimer$类，因此我在主窗口中创建$QTimer$类，设定为每秒触发，触发时主线程读取子线程的$fps$，这样就达到了显示帧率的目的了。 12345678910111213141516171819202122232425262728293031在Window类声明处：private: QTimer *timer; QThread *loopThread; RenderLoop *loop;在Window类构造函数处： loop = new RenderLoop(width(), height(), nullptr); loopThread = new QThread(this); // fps counting. timer = new QTimer(); connect(timer,&amp;QTimer::timeout,this,&amp;Window::fpsTimeOut); // render thread. loop-&gt;moveToThread(loopThread); connect(loopThread,&amp;QThread::finished,loop, &amp;RenderLoop::deleteLater); connect(loopThread,&amp;QThread::started,loop,&amp;RenderLoop::loop); connect(loop,&amp;RenderLoop::frameOut,this,&amp;Window::receiveFrame); // begin the thread. loopThread-&gt;start(); timer-&gt;start(1000);Window的其他函数：void Window::fpsTimeOut()&#123; int fps = loop-&gt;getFps(); loop-&gt;setFpsZero(); this-&gt;setWindowTitle(QString(\" fps: %1\").arg(fps));&#125; 渲染流程&emsp;&emsp;回顾一下$OpenGL$的渲染流程（这里只考虑一般的情况，即不包含几何着色器、细分着色器等），首先外部处理网格，将网格顶点数据和网格顶点索引送入渲染管线，设置基本图元（如三角形）、渲染方式（如线框模式）。渲染管线的第一阶段为顶点着色器阶段（在这之前还有个缓冲清理阶段），顶点着色器对网格数据逐顶点处理（包含坐标空间变换、投影变换等等），随之输出。然后渲染管线对输出的顶点数据进行裁剪，送入光栅化部件，计算几何图元覆盖的像素点，其中进行了大量的线性插值操作。接着片元着色器获取光栅化后的像素，对每个像素做颜色计算等，然后输出颜色数据、深度数据，最后根据这些缓冲数据做深度测试。 &emsp;&emsp;所以一个最基本的渲染管线应该有如下几个步骤： &emsp;&emsp;初始化（如缓冲区创建）$\\to$输入顶点缓冲、索引缓冲$\\to$清除缓冲区$\\to$设置着色器、渲染方式$\\to$绘制$\\to$交换双缓冲$\\to$输出。根据这些步骤，创建$Pipeline$类如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192class Pipeline&#123;private: int m_width, m_height; // width and height of viewport. BaseShader *m_shader; // shaders including vertex shader and fragment shader. FrameBuffer *m_frontBuffer; FrameBuffer *m_backBuffer; Matrix4x4 viewPortMatrix; // viewport transformation matrix. std::vector&lt;Vertex&gt; m_vertices; // vertex buffer. std::vector&lt;unsigned int&gt; m_indices;// index buffer.public: Pipeline(int width, int height); ~Pipeline(); void initialize(); void clearBuffer(const Vector4D &amp;color, bool depth = false); void setVertexBuffer(const std::vector&lt;Vertex&gt; &amp;vertices)&#123;m_vertices = vertices;&#125; void setIndexBuffer(const std::vector&lt;unsigned int&gt; &amp;indices)&#123;m_indices = indices;&#125; void setShaderMode(ShadingMode mode); void drawIndex(RenderMode mode); void swapBuffer(); unsigned char *output()&#123;return m_frontBuffer-&gt;getColorBuffer();&#125;&#125;;Pipeline::Pipeline(int width, int height) :m_width(width),m_height(height) ,m_shader(nullptr),m_frontBuffer(nullptr) ,m_backBuffer(nullptr)&#123;&#125;Pipeline::~Pipeline()&#123; if(m_shader)delete m_shader; if(m_frontBuffer)delete m_frontBuffer; if(m_backBuffer)delete m_backBuffer; m_shader = nullptr; m_frontBuffer = nullptr; m_backBuffer = nullptr;&#125;void Pipeline::initialize()&#123; if(m_frontBuffer) delete m_frontBuffer; if(m_backBuffer) delete m_backBuffer; if(m_shader) delete m_shader; viewPortMatrix.setViewPort(0,0,m_width,m_height); m_frontBuffer = new FrameBuffer(m_width, m_height); m_backBuffer = new FrameBuffer(m_width, m_height); m_shader = new SimpleShader();&#125;void Pipeline::drawIndex(RenderMode mode)&#123; 输入顶点着色器; 光栅化; 输入片元着色器; 写入缓冲区;&#125;void Pipeline::clearBuffer(const Vector4D &amp;color, bool depth)&#123; (void)depth; m_backBuffer-&gt;clearColorBuffer(color);&#125;void Pipeline::setShaderMode(ShadingMode mode)&#123; if(m_shader)delete m_shader; if(mode == ShadingMode::simple) m_shader = new SimpleShader(); else if(mode == ShadingMode::phong) ;&#125;void Pipeline::swapBuffer()&#123; FrameBuffer *tmp = m_frontBuffer; m_frontBuffer = m_backBuffer; m_backBuffer = tmp;&#125; &emsp;&emsp;注意到我创建了帧缓冲，分别是$m_frontBuffer$和$m_backBuffer$，前者存储着当前显示的像素，后者缓冲区用于写入像素。这就是著名的双缓冲原理，可以避免画面的闪烁、撕裂等现象。除此之外，还有一个值得特别说明的就是视口变换矩阵$viewPortMatrix$，这个一般很少见到，因为被内嵌在了渲染管线里面了。经过投影变换、透视除法操作之后，顶点数据都在标准化设备空间中，即$x$轴、$y$轴、$z$轴取值范围为$[-1,1]$。但是屏幕的像素坐标范围并非如此，通常屏幕的$x$轴坐标范围为$[0,width]$，$y$轴坐标范围为$[0,height]$，屏幕像素坐标原点在左上角，$x$轴正向朝右，$y$轴正向朝下，所以我们还要把标准化设备坐标顶点数据变换到屏幕的坐标范围中，这就是视口变换（$z$轴一般保持不变）。视口变换矩阵的构造并没有难度，因为这仅仅是简单的线性映射，因此不再赘述。视口变换矩阵如下所示： viewPortMatrix= \\left[ \\begin{matrix} \\frac{w}{2}&0&0&s_x+\\frac{w}{2}\\\\ 0&-\\frac{h}{2}&0&s_y+\\frac{h}{2}\\\\ 0&0&1&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {1}&emsp;&emsp;其中$(s_x,s_y)$是视口左上角的坐标，$(w,h)$为屏幕的宽度和高度。 12345678void Matrix4x4::setViewPort(int left, int top, int width, int height)&#123; loadIdentity(); entries[0] = static_cast&lt;float&gt;(width)/2.0f; entries[5] = -static_cast&lt;float&gt;(height)/2.0f; entries[12] = static_cast&lt;float&gt;(left)+static_cast&lt;float&gt;(width)/2.0f; entries[13] = static_cast&lt;float&gt;(top)+static_cast&lt;float&gt;(height)/2.0f;&#125; &emsp;&emsp;$Pipeline$还有个非常重要的函数$drawIndex$，它是渲染管线的核心部分，涉及到了图元装配、顶点着色器调度、光栅化、片元着色器调度、写入帧缓冲这几个重要的步骤。我们实现的软渲染器几何图元默认为三角形，所以图元装配就是每三个顶点装成一个图元。 123456789101112131415161718192021222324252627282930313233343536373839void Pipeline::drawIndex(RenderMode mode)&#123; if(m_indices.empty())return; for(unsigned int i = 0;i &lt; m_indices.size()/3;++ i) &#123; //! vertices assembly to triangle primitive Vertex p1,p2,p3; &#123; p1 = m_vertices[3*i+0]; p2 = m_vertices[3*i+1]; p3 = m_vertices[3*i+2]; &#125; //! vertex shader stage. VertexOut v1,v2,v3; &#123; v1 = m_shader-&gt;vertexShader(p1); v2 = m_shader-&gt;vertexShader(p2); v3 = m_shader-&gt;vertexShader(p3); &#125; //! rasterization and fragment shader stage. &#123; v1.posH = viewPortMatrix * v1.posH; v2.posH = viewPortMatrix * v2.posH; v3.posH = viewPortMatrix * v3.posH; if(mode == RenderMode::wire) &#123; // bresenham rasterization &#125; else if(mode == RenderMode::fill) &#123; // edge walking rasterization &#125; &#125; &#125;&#125; &emsp;&emsp;有了以上的$Pipeline$函数，我们的渲染循环逻辑的一般形式如下： 1234567891011while(!stoped)&#123; pipeline-&gt;clearBuffer(Vector4D(0.502f,0.698f,0.800f,1.0f)); pipeline-&gt;drawIndex(RenderMode::fill); pipeline-&gt;swapBuffer(); emit frameOut(pipeline-&gt;output()); ++ fps;&#125; 二、光栅化算法&emsp;&emsp;顶点着色器处理的还是一个个离散的几何顶点，在顶点着色器之后我们还需要进行光栅化操作，将几何覆盖的屏幕像素计算出来，送入片元着色器计算每个点的像素数据。光栅化一般有两种模式：一种是线框模式，即只描绘几何的边；二是填充模式，即将几何的面片全部填充完。Bresenham算法是经典的描线算法，它采用迭代的形式将所需的算术操作降低到最少。除此之外还有DDA描线算法，效率上不如Bresenham算法，所以我没有实现。 1、Bresenham描线算法&emsp;&emsp;我们要描绘的是从$(x_0,y_0)$到$(x_1,y_1)$的一条直线线段。一些数学符号标记如下： \\Delta x= x_1-x_0>0,\\ \\Delta y=y_1-y_0>0,\\ m=\\frac{\\Delta y}{\\Delta x}&emsp;&emsp;其中$m$即直线线段的斜率，为了便于讨论，我们假设$|m|\\leq 1$，其他情况很容易推广。 &emsp;&emsp;在如上的情况下，Bresenham算法从$x=x_0$开始，每次将$x$坐标值加一，然后推算相应的$y$坐标值。记第$i$次迭代获得的点为$(x_i,y_i)$。那么第$i+1$次迭代时获取的点就在$(\\overline x_i+1,\\overline y_i)$和$(\\overline x_i+1,\\overline y_i+1)$这两个中选取。那如何判断应该选哪个呢？即选择这两个点之一的判断标准是什么？直观上，我们应该选取距离的直线线段在该$y$轴上的交点最近的点，如下图1所示。 图1 判别标准&emsp;&emsp;直线的一般表达式为$y=mx+B$，$m$为直线的斜率，那么$(x_{i+1},y_{i+1})$表示为如下（注意$y_{i+1}$表示的是直线在$x_{i+1}$上真正的$y$值）： x_{i+1}=x_i+1\\\\ y_{i+1}=mx_{i+1}+B=m(x_i+1)+B \\tag {2} 图2 交点到右边的点、右上的点的距离 &emsp;&emsp;故$d_{upper}$和$d_{lower}$的取值如下： d_{upper}=\\overline y_i+1-\\overline y_{i+1}=\\overline y_i+1-m\\overline x_{i+1}-B\\\\ d_{lower}=y_{i+1}-\\overline y_i=mx_{i+1}+B-\\overline y_i \\tag {3}&emsp;&emsp;显然，如果$d_{lower}-d_{upper}&gt;0$，则应该取右上方的点；如果$d_{lower}-d_{upper}0$的符号。 d_{lower}-d_{upper}=m(x_i+1)+B-\\overline y_i-(\\overline y_i+1-m(x_i+1)-B)\\\\ =2m(x_i+1)-2\\overline y_i+2B-1 \\tag {4}&emsp;&emsp;式$(4)$中的$m$是直线的斜率，因此将式$(4)$作为判断标准需要做非常昂贵的浮点数除法运算。为了消去除法，注意到$m=\\frac{\\Delta y}{\\Delta x}$，两边同时乘上$\\Delta x&gt;0$，正负符号不变。 p_i=\\Delta x\\cdot (d_{lower}-d_{upper}) =2\\Delta y\\cdot(x_i+1)-2\\Delta x\\cdot \\overline y_i+(2B-1)\\Delta x\\\\ =2\\Delta y\\cdot x_i-2\\Delta x\\cdot\\overline y_i+c\\\\ where \\ \\ c=(2B-1)\\Delta x+2\\Delta y \\tag {5}&emsp;&emsp;所以可以用$p_i$的符号作为选取的标准。但是，式$(5)$的计算能够进一步简化，考虑$p_i$和$p_{i+1}$（注意我们根据$p_i$的符号来选取$\\overline y_{i+1}$）： p_{i+1}-p_{i} = (2\\Delta y\\cdot x_{i+1}-2\\Delta x\\cdot\\overline y_{i+1}+c) - (2\\Delta y\\cdot x_i-2\\Delta x\\cdot\\overline y_i+c) \\\\= 2\\Delta y-2\\Delta x(\\overline y_{i+1}-\\overline y_i) \\tag {6}&emsp;&emsp;若$p_i\\leq 0$，那么选择右边的点，此时$\\overline y_{i+1}=\\overline y_i$，那么有： p_{i+1}=p_i+2\\Delta y \\tag {7}&emsp;&emsp;若$p_i&gt;0$，那么选择右上角的点，此时$\\overline y_{i+1}=\\overline y_i+1$，那么有： p_{i+1}=p_i+2\\Delta y-2\\Delta x \\tag {8}&emsp;&emsp;所以我们可以根据$p_i$的符号快速计算出$p_{i+1}$的符号，如此迭代下去： Bresenham Algorithm: $draw (x_0, y_0);$ Calculate $\\Delta x$,$\\Delta y$,$2\\Delta y$,$2\\Delta y-2\\Delta x$,$p_0=2\\Delta y-\\Delta x$; for $x$ from $x_0$ to $x_1$: &emsp;&emsp;if $p_i\\leq 0$ &emsp;&emsp;&emsp;&emsp;draw $(x_{i+1},\\overline y_{i+1})=(x_i+1,\\overline y_i)$ ; &emsp;&emsp;&emsp;&emsp;compute $p_{i+1}=p_i+2\\Delta y$; &emsp;&emsp;if $p_i &gt; 0$ &emsp;&emsp;&emsp;&emsp;draw $(x_{i+1},\\overline y_{i+1})=(x_i+1,\\overline y_i+1)$ ; &emsp;&emsp;&emsp;&emsp;compute $p_{i+1}=p_i+2\\Delta y-2\\Delta x$; &emsp;&emsp;$x += 1;$ &emsp;&emsp;上面我们讨论的都是$|m|1$的情况呢？其实这是对称的，这时把$x$看成$y$，把$y$看成$x$即可。另外，当$\\Delta x &lt;0$时，我们的$x$不是递增$1$，而是递减$1$，具体实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465void Pipeline::bresenhamLineRasterization(const VertexOut &amp;from, const VertexOut &amp;to)&#123; int dx = to.posH.x - from.posH.x; int dy = to.posH.y - from.posH.y; int stepX = 1, stepY = 1; // judge the sign if(dx &lt; 0) &#123; stepX = -1; dx = -dx; &#125; if(dy &lt; 0) &#123; stepY = -1; dy = -dy; &#125; int d2x = 2*dx, d2y = 2*dy; int d2y_minus_d2x = d2y - d2x; int sx = from.posH.x; int sy = from.posH.y; VertexOut tmp; // slope &lt; 1. if(dy &lt;= dx) &#123; int flag = d2y - dx; for(int i = 0;i &lt;= dx;++ i) &#123; // linear interpolation tmp = lerp(from, to, static_cast&lt;double&gt;(i)/dx); // fragment shader m_backBuffer-&gt;drawPixel(sx,sy,m_shader-&gt;fragmentShader(tmp)); sx += stepX; if(flag &lt;= 0) flag += d2y; else &#123; sy += stepY; flag += d2y_minus_d2x; &#125; &#125; &#125; // slope &gt; 1. else &#123; int flag = d2x - dy; for(int i = 0;i &lt;= dy;++ i) &#123; // linear interpolation tmp = lerp(from, to, static_cast&lt;double&gt;(i)/dy); // fragment shader m_backBuffer-&gt;drawPixel(sx,sy,m_shader-&gt;fragmentShader(tmp)); sy += stepY; if(flag &lt;= 0) flag += d2x; else &#123; sx += stepX; flag -= d2y_minus_d2x; &#125; &#125; &#125;&#125; 2、Edge-Walking三角形填充算法&emsp;&emsp;三角形光栅化填充对输入给定的三个三角形顶点，计算这个三角区域覆盖的所有像素。三角形填充的光栅化算法有很多种，这里仅实现了Edge-Walking算法，此外还有Edge-Equation算法。关于Edge-Walking算法的前世今生我不再赘述了，这个算法的思路比较简单，但是实现起来比较麻烦一点。 &emsp;&emsp;话不多少，直接上伪代码（懒得自己写了伪代码了）： &emsp;&emsp;大致的思想就是从上往下（或从下往上）扫描，获取每对$X_L$、$X_R$，然后在$[X_L,X_R]$范围内从左到右扫描。显然就是双重循环。一般，我们的三角形光栅化对象有如下四种情况： 图3 四类三角形 &emsp;&emsp;先来看平底三角形的情况，如下图4所示。显然，平底三角形很容易地实现从下往上扫面，竖直方向上仅需考虑左右两条边。当然这里有个问题，就是如何确定$X_L$和$X_R$？如果直接采用算法伪代码中的利用$dx/dy$迭代获取$X$值，因为$X$值是整数，而$dx/dy$是浮点数，当$dx/dy&lt;1$时，把$dx/dy$加到$X$上面计算机对整数类型坐标自动向下取整，结果相当于没加。（即便是浮点数类型，最终也要取整，因为屏幕空间的像素坐标必须是整数） 图4 平底三角形&emsp;&emsp;一种解决方案就是线性插值，算法从下往上扫描时，$y-=1$，我们根据当前的$y$值来获取$x$值： X_L = (1.0f-\\frac{y1-y}{y1-y0})*x1+\\frac{y1-y}{y1-y0}*x0 \\\\ X_y = (1.0f-\\frac{y2-y}{y2-y0})*x2+\\frac{y2-y}{y2-y0}*x0&emsp;&emsp;平顶的三角形光栅化亦类似，不再赘述。那么除了平底和平顶的情况之外，我们该如何处理其余的情况？一个技巧就是将其他情况的三角形分割乘一个平底三角形、一个平顶三角形，如下图所示： 图5 三角形分割&emsp;&emsp;这样我们通过调用平底三角形光栅化方法、平顶三角形光栅化方法即可实现一般情况的三角形光栅化： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114void Pipeline::scanLinePerRow(const VertexOut &amp;left, const VertexOut &amp;right)&#123; VertexOut current; int length = right.posH.x - left.posH.x + 1; for(int i = 0;i &lt;= length;++i) &#123; // linear interpolation double weight = static_cast&lt;double&gt;(i)/length; current = lerp(left, right, weight); current.posH.x = left.posH.x + i; current.posH.y = left.posH.y; // fragment shader m_backBuffer-&gt;drawPixel(current.posH.x, current.posH.y, m_shader-&gt;fragmentShader(current)); &#125;&#125;void Pipeline::rasterTopTriangle(VertexOut &amp;v1, VertexOut &amp;v2, VertexOut &amp;v3)&#123; VertexOut left = v2; VertexOut right = v3; VertexOut dest = v1; VertexOut tmp, newleft, newright; if(left.posH.x &gt; right.posH.x) &#123; tmp = left; left = right; right = tmp; &#125; int dy = left.posH.y - dest.posH.y + 1; for(int i = 0;i &lt; dy;++i) &#123; double weight = 0; if(dy != 0) weight = static_cast&lt;double&gt;(i)/dy; newleft = lerp(left, dest, weight); newright = lerp(right, dest, weight); newleft.posH.y = newright.posH.y = left.posH.y - i; scanLinePerRow(newleft, newright); &#125;&#125;void Pipeline::rasterBottomTriangle(VertexOut &amp;v1, VertexOut &amp;v2, VertexOut &amp;v3)&#123; VertexOut left = v1; VertexOut right = v2; VertexOut dest = v3; VertexOut tmp, newleft, newright; if(left.posH.x &gt; right.posH.x) &#123; tmp = left; left = right; right = tmp; &#125; int dy = dest.posH.y - left.posH.y + 1; for(int i = 0;i &lt; dy;++i) &#123; double weight = 0; if(dy != 0) weight = static_cast&lt;double&gt;(i)/dy; newleft = lerp(left, dest, weight); newright = lerp(right, dest, weight); newleft.posH.y = newright.posH.y = left.posH.y + i; scanLinePerRow(newleft, newright); &#125;&#125;void Pipeline::edgeWalkingFillRasterization(const VertexOut &amp;v1, const VertexOut &amp;v2, const VertexOut &amp;v3)&#123; // split the triangle into two part VertexOut tmp; VertexOut target[3] = &#123;v1, v2,v3&#125;; if(target[0].posH.y &gt; target[1].posH.y) &#123; tmp = target[0]; target[0] = target[1]; target[1] = tmp; &#125; if(target[0].posH.y &gt; target[2].posH.y) &#123; tmp = target[0]; target[0] = target[2]; target[2] = tmp; &#125; if(target[1].posH.y &gt; target[2].posH.y) &#123; tmp = target[1]; target[1] = target[2]; target[2] = tmp; &#125; // bottom triangle if(equal(target[0].posH.y,target[1].posH.y)) &#123; rasterBottomTriangle(target[0],target[1],target[2]); &#125; // top triangle else if(equal(target[1].posH.y,target[2].posH.y)) &#123; rasterTopTriangle(target[0], target[1], target[2]); &#125; // split it. else &#123; double weight = static_cast&lt;double&gt;(target[1].posH.y-target[0].posH.y)/(target[2].posH.y-target[0].posH.y); VertexOut newPoint = lerp(target[0],target[2],weight); newPoint.posH.y = target[1].posH.y; rasterTopTriangle(target[0], newPoint, target[1]); rasterBottomTriangle(newPoint,target[1],target[2]); &#125;&#125; 三、程序结果&emsp;&emsp;最终，不借用任何图形接口通过自己实现的光栅化算法画出了三角形： 参考资料$[1]$ https://blog.csdn.net/cppyin/article/details/6232453 $[2]$ https://blog.csdn.net/y1196645376/article/details/78937614 $[3]$ https://blog.csdn.net/y1196645376/article/details/78907914","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/tags/Soft-Renderer/"},{"name":"Rasterization","slug":"Rasterization","permalink":"https://yangwc.com/tags/Rasterization/"}]},{"title":"软渲染器Soft Renderer：3D数学篇","slug":"SoftRenderer-Math","date":"2019-05-01T02:37:49.425Z","updated":"2021-04-12T03:33:36.739Z","comments":true,"path":"2019/05/01/SoftRenderer-Math/","link":"","permalink":"https://yangwc.com/2019/05/01/SoftRenderer-Math/","excerpt":"本章开始博主将手动搭建一个渲染管线，深入理解3D渲染的整个流程。线性代数中的向量和矩阵是计算机图形学的常客，深入理解和掌握对于图形渲染有着非常重要的意义，本节主要是关于3D数学库的内容。","text":"本章开始博主将手动搭建一个渲染管线，深入理解3D渲染的整个流程。线性代数中的向量和矩阵是计算机图形学的常客，深入理解和掌握对于图形渲染有着非常重要的意义，本节主要是关于3D数学库的内容。 向量 矩阵 一、向量&emsp;&emsp;$n$维向量本质就是一个$n$元组，从几何意义上来说，向量是有大小和方向的有向线段。向量的大小就是向量的长度（模）向量有非负的长度，而向量的方向描述了空间中向量的指向。向量的相关内容高中就已涉及，因此不再赘述。若想要重新深入了解相关内容，可以查看这个地址。 &emsp;&emsp;图形渲染中通常使用的向量为$2$到$4$维，如下分别是$2$维、$3$维、$4$维向量类的常用方法，主要是运算操作符重载以及点乘、叉乘、模、标准化、线性插值等基本操作。向量的内容简单，没什么要特别说明的。 1、2D向量类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Vector2D&#123;public: float x,y; // constructors Vector2D():x(0.0f), y(0.0f) &#123;&#125; Vector2D(float newX, float newY):x(newX), y(newY)&#123;&#125; Vector2D(const float * rhs):x(*rhs), y((*rhs)+1) &#123;&#125; Vector2D(const Vector2D &amp; rhs):x(rhs.x), y(rhs.y)&#123;&#125; ~Vector2D() = default; // setter,getter void set(float newX, float newY)&#123;x=newX;y=newY; &#125; void setX(float newX) &#123;x = newX;&#125; void setY(float newY) &#123;y = newY;&#125; float getX() const &#123;return x;&#125; float getY() const &#123;return y;&#125; // normalization void normalize(); Vector2D getNormalize()const; // length float getLength() const &#123; return static_cast&lt;float&gt;(sqrt(x*x + y*y));&#125; float getSquaredLength()const&#123;return static_cast&lt;float&gt;(x*x + y*y);&#125; // overloaded operators Vector2D operator+(const Vector2D &amp;rhs) const &#123;return Vector2D(x + rhs.x, y + rhs.y);&#125; Vector2D operator-(const Vector2D &amp;rhs) const &#123;return Vector2D(x - rhs.x, y - rhs.y);&#125; Vector2D operator*(const float rhs) const &#123;return Vector2D(x*rhs, y*rhs);&#125; Vector2D operator/(const float rhs) const &#123;return (rhs==0) ? Vector2D(0.0f, 0.0f) : Vector2D(x / rhs, y / rhs);&#125; bool operator==(const Vector2D &amp;rhs) const &#123;return (equal(x,rhs.x) &amp;&amp; equal(y,rhs.y));&#125; bool operator!=(const Vector2D &amp;rhs) const &#123;return !((*this)==rhs);&#125; void operator+=(const Vector2D &amp;rhs)&#123;x+=rhs.x; y+=rhs.y;&#125; void operator-=(const Vector2D &amp;rhs)&#123;x-=rhs.x; y-=rhs.y;&#125; void operator*=(const float rhs)&#123;x*=rhs;y*=rhs;&#125; void operator/=(const float rhs)&#123;if(!equal(rhs, 0.0))&#123;x/=rhs;y/=rhs;&#125;&#125; Vector2D operator-() const &#123;return Vector2D(-x, -y);&#125; Vector2D operator+() const &#123;return *this;&#125; // interpolation Vector2D lerp(const Vector2D &amp;v2,const float factor)const &#123;return (*this)*(1.0f - factor) + v2*factor;&#125; Vector2D quadraticInterpolate(const Vector2D &amp; v2, const Vector2D &amp; v3, const float factor) const &#123;return (*this)*(1.0f-factor)*(1.0f-factor) + v2*2.0f*factor*(1.0f-factor) + v3*factor*factor;&#125;&#125;; 2、3D向量类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Vector3D&#123;public: float x,y,z; // constructors Vector3D():x(0.0f), y(0.0f), z(0.0f)&#123;&#125; Vector3D(float newX, float newY, float newZ):x(newX), y(newY), z(newZ)&#123;&#125; Vector3D(const float * rhs):x(*rhs), y(*(rhs+1)), z(*(rhs+2))&#123;&#125; Vector3D(const Vector3D &amp;rhs):x(rhs.x), y(rhs.y), z(rhs.z)&#123;&#125; ~Vector3D() = default; // setter,getter void set(float newX, float newY, float newZ)&#123;x=newX;y=newY;z=newZ;&#125; void setX(float newX) &#123;x = newX;&#125; void setY(float newY) &#123;y = newY;&#125; void setZ(float newZ) &#123;z = newZ;&#125; float getX() const &#123;return x;&#125; float getY() const &#123;return y;&#125; float getZ() const &#123;return z;&#125; // normalization void normalize(); Vector3D getNormalized() const; // length caculation float getLength() const &#123;return static_cast&lt;float&gt;(sqrt(x*x+y*y+z*z));&#125; float getSquaredLength() const &#123;return x*x+y*y+z*z;&#125; // product float dotProduct(const Vector3D &amp;rhs) const &#123;return x*rhs.x + y*rhs.y + z*rhs.z;&#125; Vector3D crossProduct(const Vector3D &amp;rhs) const &#123;return Vector3D(y*rhs.z - z*rhs.y, z*rhs.x - x*rhs.z, x*rhs.y - y*rhs.x);&#125; // linear interpolation Vector3D lerp(const Vector3D &amp;v2, float factor) const &#123;return (*this)*(1.0f-factor) + v2*factor;&#125; Vector3D QuadraticInterpolate(const Vector3D &amp;v2, const Vector3D &amp;v3, float factor) const &#123;return (*this)*(1.0f-factor)*(1.0f-factor) + v2*2.0f*factor*(1.0f-factor) + v3*factor*factor;&#125; // overloaded operators Vector3D operator+(const Vector3D &amp;rhs) const &#123;return Vector3D(x + rhs.x, y + rhs.y, z + rhs.z);&#125; Vector3D operator-(const Vector3D &amp;rhs) const &#123;return Vector3D(x - rhs.x, y - rhs.y, z - rhs.z);&#125; Vector3D operator*(const float rhs) const &#123;return Vector3D(x*rhs, y*rhs, z*rhs);&#125; Vector3D operator/(const float rhs) const &#123;return (equal(rhs,0.0f))?Vector3D(0.0f, 0.0f, 0.0f):Vector3D(x/rhs, y/rhs, z/rhs);&#125; bool operator==(const Vector3D &amp;rhs) const &#123;return (equal(x,rhs.x) &amp;&amp; equal(y,rhs.y) &amp;&amp; equal(z,rhs.z));&#125; bool operator!=(const Vector3D &amp;rhs) const &#123;return !((*this)==rhs);&#125; void operator+=(const Vector3D &amp;rhs) &#123;x+=rhs.x;y+=rhs.y;z+=rhs.z;&#125; void operator-=(const Vector3D &amp; rhs) &#123;x-=rhs.x;y-=rhs.y;z-=rhs.z;&#125; void operator*=(const float rhs)&#123;x*=rhs;y*=rhs;z*=rhs;&#125; void operator/=(const float rhs)&#123;if(!equal(rhs,0.0f))&#123;x/=rhs; y/=rhs; z/=rhs;&#125;&#125; Vector3D operator-() const &#123;return Vector3D(-x, -y, -z);&#125; Vector3D operator+() const &#123;return *this;&#125;&#125;; 3、4D向量类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Vector4D&#123;public: float x,y,z,w; // constructors Vector4D():x(0.0f), y(0.0f), z(0.0f), w(0.0f)&#123;&#125; Vector4D(float newX, float newY, float newZ, float newW):x(newX), y(newY), z(newZ), w(newW)&#123;&#125; Vector4D(const float * rhs):x(*rhs), y(*(rhs+1)), z(*(rhs+2)), w(*(rhs+3))&#123;&#125; Vector4D(const Vector4D &amp;rhs):x(rhs.x), y(rhs.y), z(rhs.z), w(rhs.w)&#123;&#125; Vector4D(const Vector3D &amp; rhs): x(rhs.x), y(rhs.y), z(rhs.z), w(1.0f)&#123;&#125; ~Vector4D() = default; // setter,getter void set(float newX, float newY, float newZ, float newW)&#123;x=newX;y=newY;z=newZ;w=newW;&#125; void setX(float newX) &#123;x = newX;&#125; void setY(float newY) &#123;y = newY;&#125; void setZ(float newZ) &#123;z = newZ;&#125; void setW(float newW) &#123;w = newW;&#125; float getX() const &#123;return x;&#125; float getY() const &#123;return y;&#125; float getZ() const &#123;return z;&#125; float getW() const &#123;return w;&#125; // product float dotProduct(const Vector4D &amp;rhs) const &#123;return x*rhs.x + y*rhs.y + z*rhs.z + w*rhs.w;&#125; // linear interpolation Vector4D lerp(const Vector4D &amp;v2, float factor) const &#123;return (*this)*(1.0f-factor) + v2*factor;&#125; Vector4D QuadraticInterpolate(const Vector4D &amp;v2, const Vector4D &amp;v3, float factor) const &#123;return (*this)*(1.0f-factor)*(1.0f-factor)+v2*2.0f*factor*(1.0f-factor)+v3*factor*factor;&#125; // overloaded operators Vector4D operator+(const Vector4D &amp;rhs) const &#123;return Vector4D(x+rhs.x, y+rhs.y, z+rhs.z, w+rhs.w);&#125; Vector4D operator-(const Vector4D &amp;rhs) const &#123;return Vector4D(x-rhs.x, y-rhs.y, z-rhs.z, w-rhs.w);&#125; Vector4D operator*(const float rhs) const &#123;return Vector4D(x*rhs, y*rhs, z*rhs, w*rhs);&#125; Vector4D operator/(const float rhs) const &#123;return (equal(rhs,0.0f))?Vector4D(0.0f, 0.0f, 0.0f, 0.0f):Vector4D(x/rhs, y/rhs, z/rhs, w/rhs);&#125; bool operator==(const Vector4D &amp;rhs) const &#123;return (equal(x,rhs.x)&amp;&amp;equal(y,rhs.y)&amp;&amp;equal(z,rhs.z)&amp;&amp;equal(w,rhs.w));&#125; bool operator!=(const Vector4D &amp;rhs) const &#123;return !((*this)==rhs);&#125; void operator+=(const Vector4D &amp;rhs) &#123;x+=rhs.x;y+=rhs.y;z+=rhs.z;w+=rhs.w;&#125; void operator-=(const Vector4D &amp; rhs) &#123;x-=rhs.x;y-=rhs.y;z-=rhs.z;w-=rhs.w;&#125; void operator*=(const float rhs)&#123;x*=rhs;y*=rhs;z*=rhs;w*=rhs;&#125; void operator/=(const float rhs)&#123;if(!equal(rhs,0.0f))&#123;x/=rhs; y/=rhs; z/=rhs; w/=rhs;&#125;&#125; Vector4D operator-() const &#123;return Vector4D(-x, -y, -z, -w);&#125; Vector4D operator+() const &#123;return *this;&#125;&#125;; 二、矩阵&emsp;&emsp;矩阵本质就是向量的进一步扩展的，一个$n\\times m$的矩阵可看成$n$个$m$维行向量组成或者$m$个$n$维列向量组成，关于矩阵的基本概念、操作请看这里。通常我们采用方阵来描述线性变换。所谓线性变换，即变换之后保留了直线而不被弯曲，平行线依然平行，原点没有变化，但其他的几何性质如长度、角度、面积和体积可能被变换改变了。直观来说，线性变换可能“拉伸”坐标系，但不会“弯曲”或“卷折”坐标系。 &emsp;&emsp;矩阵在计算机中有行主序存储、列主序存储两种方式，行主序存储即按照顺序逐行存储，列主序存储则按照顺序逐列存储。图形学渲染中我们通常采用的是列主序的方式，以下的讨论都是列主序的矩阵存储方式。那么矩阵是如何变换向量的？ &emsp;&emsp;向量在几何上能被解释成一系列与轴平行的位移，一般来说，任意向量$\\vec v$都能写成如下的形式： \\vec v=\\left[\\begin{matrix}x\\\\y\\\\z\\end{matrix}\\right]=\\left[\\begin{matrix}x\\\\0\\\\0\\end{matrix}\\right]+\\left[\\begin{matrix}0\\\\y\\\\0\\end{matrix}\\right]+\\left[\\begin{matrix}0\\\\0\\\\z\\end{matrix}\\right]=x\\left[\\begin{matrix}1\\\\0\\\\0\\end{matrix}\\right]+y\\left[\\begin{matrix}0\\\\1\\\\0\\end{matrix}\\right]+z\\left[\\begin{matrix}0\\\\0\\\\1\\end{matrix}\\right] \\tag {1}&emsp;&emsp;公式$(1)$右边的单位向量就是$x$、$y$、$z$轴方向的向量，向量的每个坐标都表明了平行于相应坐标轴的有向位移。我们记$\\vec p$、$\\vec q$、$\\vec r$分别为公式$(1)$中右边的$x$、$y$、$z$轴的单位列向量，则有： \\vec v=x\\vec p+y\\vec q+z\\vec r=\\left[\\begin{matrix}\\vec p &\\vec q&\\vec r\\end{matrix}\\right]\\left[\\begin{matrix}x \\\\y\\\\z\\end{matrix}\\right] \\tag {2}&emsp;&emsp;向量$\\vec v$就变成了向量$\\vec p$、$\\vec q$、$\\vec r$的线性表示，向量$\\vec p$、$\\vec q$、$\\vec r$称作基向量。以上仅仅讨论的是笛卡尔坐标系，但更通用的情况是，一个$3$维坐标系能用任意$3$个线性无关的基向量表示，以列向量$\\vec p$、$\\vec q$、$\\vec r$构建$3\\times 3$的矩阵$M$： M=\\left[\\begin{matrix}\\vec p &\\vec q&\\vec r\\end{matrix}\\right]=\\left[\\begin{matrix}p_x &q_x&r_x\\\\p_y &q_y&r_y\\\\p_z &q_z&r_z\\end{matrix}\\right] \\tag {3}&emsp;&emsp;结合公式$(2)$和公式$(3)$，即有： \\vec v=M\\left[\\begin{matrix}x \\\\y\\\\z\\end{matrix}\\right] \\tag{4}&emsp;&emsp;坐标系变换矩阵的每一列（如果是行主序，就是每一行）都是该坐标系的基向量，一个点$v$右乘该矩阵就相当于执行了一次坐标系转换。求解线性变换矩阵的关键就是根据当前的坐标系求解变换之后的坐标系的基向量，然后将基向量填入向量位置！ &emsp;&emsp;一个矩阵类通常有如下方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class Matrix4x4&#123;public: float entries[16]; // constructors Matrix4x4()&#123;loadIdentity();&#125; Matrix4x4(float e0, float e1, float e2, float e3, float e4, float e5, float e6, float e7, float e8, float e9, float e10,float e11, float e12,float e13,float e14,float e15); Matrix4x4(const float *rhs); Matrix4x4(const Matrix4x4 &amp;rhs); ~Matrix4x4() = default; // setter,getter void setEntry(int position, float value); float getEntry(int position) const; Vector4D getRow(int position) const; Vector4D getColumn(int position) const; void loadIdentity(); void loadZero(); // overloaded operators Matrix4x4 operator+(const Matrix4x4 &amp; rhs) const; Matrix4x4 operator-(const Matrix4x4 &amp; rhs) const; Matrix4x4 operator*(const Matrix4x4 &amp; rhs) const; Matrix4x4 operator*(const float rhs) const; Matrix4x4 operator/(const float rhs) const; bool operator==(const Matrix4x4 &amp; rhs) const; bool operator!=(const Matrix4x4 &amp; rhs) const; void operator+=(const Matrix4x4 &amp; rhs); void operator-=(const Matrix4x4 &amp; rhs); void operator*=(const Matrix4x4 &amp; rhs); void operator*=(const float rhs); void operator/=(const float rhs); Matrix4x4 operator-() const; Matrix4x4 operator+() const &#123;return (*this);&#125; Vector4D operator*(const Vector4D rhs) const; // inverse, transpose void inverted(); Matrix4x4 getInverse() const; void transpose(); Matrix4x4 getTranspose() const; void invertTranspose(); Matrix4x4 getInverseTranspose() const; // operation on space void setTranslation(const Vector3D &amp; translation); void setScale(const Vector3D &amp; scaleFactor); void setRotationAxis(const double angle, const Vector3D &amp; axis); void setRotationX(const double angle); void setRotationY(const double angle); void setRotationZ(const double angle); void setRotationEuler(const double angleX, const double angleY, const double angleZ); void setPerspective(float fovy, float aspect, float near, float far); void setPerspective(float left, float right, float bottom, float top, float near, float far); void setOrtho(float left, float right, float bottom, float top, float near, float far);&#125;; 1、线性变换、仿射变换&emsp;&emsp;满足$F(a+b)=F(a)+F(b)$和$F(ka)=kF(a)$的映射$F(a)$就是线性的。对于映射$F(a)=Ma$，当$M$为任意方阵时，也可以说明$F$映射是一个线性变换。在计算机图形学中，缩放、旋转的变换操作都是线性的，但是平移不是线性变换。 &emsp;&emsp;具有$v’=Mv’+b$形式的变换都是仿射变换。平移作为最常用的变换之一，然而却不是线性变换；所以为了包括平移变换提出了仿射变换。仿射变换是指线性变换后接着平移。因此，仿射变换的集合是线性变换的超集，任何线性变换都是仿射变换，但不是所有的仿射变换都是线性变换。为了统一用矩阵表示低维度的仿射变换，我们可以通过高维度的线性变换来完成，为此引入了$4$维齐次坐标。（当然引入第$4$维$w$还有其他的用途，如当$w=0$时，可解释为无穷远的“点”，其意义是描述方向）。 &emsp;&emsp;从而，对于高维度来说只是经历了一次切变+投影变换就可以实现低维度的平移，在$3D$渲染中，我们采用$4\\times 4$的矩阵做相应的变换。关于平移和缩放不再赘述： 123456789101112131415void Matrix4x4::setTranslation(const Vector3D &amp;translation)&#123; loadIdentity(); entries[12] = translation.x; entries[13] = translation.y; entries[14] = translation.z;&#125;void Matrix4x4::setScale(const Vector3D &amp;scaleFactor)&#123; loadIdentity(); entries[0] = scaleFactor.x; entries[5] = scaleFactor.y; entries[10] = scaleFactor.z;&#125; 2、绕任意轴旋转&emsp;&emsp;在3D中，绕坐标轴旋转，而不是绕点旋转，此时首先需要定义的是何为旋转正方向： 左手坐标系中定义此方向的规则为左手法则。首先，要明确旋转轴指向哪个方向。当然，旋转轴在理论上是无限延伸的，但我们还是要认为它有正端点和负端点。与笛卡尔坐标轴定义坐标系相同，左手法则是这样的:伸出左手，大拇指向上，其余手指弯曲。大拇指指向旋转轴的正方向，此时，四指弯曲的方向就是旋转的正方向。右手坐标系则根据右手法则利用右手判断旋转正方向，本文讨论的是常见的右手坐标系。 &emsp;&emsp;在旋转变换中，一个常见的特殊情况就是绕$x$轴、绕$y$轴、绕$z$轴旋转，这类的旋转矩阵求解比较简单，只需牢牢记住列主序矩阵的列向量就是变换后的坐标系的基向量即可快速推导出相应的旋转矩阵： R_x(\\theta)=\\left[ \\begin{matrix} 1&0&0\\\\ 0&cos\\theta&-sin\\theta\\\\ 0&sin\\theta&cos\\theta \\end{matrix}\\right] \\\\ R_y(\\theta)=\\left[\\begin{matrix}cos\\theta&0&sin\\theta\\\\0&1&0\\\\-sin\\theta&0&cos\\theta \\end{matrix}\\right]\\\\ R_z(\\theta)=\\left[\\begin{matrix}cos\\theta&-sin\\theta&0\\\\ sin\\theta&cos\\theta&0\\\\0&0&1\\end{matrix}\\right] \\tag {5}1234567891011121314151617181920212223242526void Matrix4x4::setRotationX(const double angle)&#123; loadIdentity(); entries[5] = static_cast&lt;float&gt;(cos(M_PI*angle/180)); entries[6] = static_cast&lt;float&gt;(sin(M_PI*angle/180)); entries[9] = -entries[6]; entries[10] = entries[5];&#125;void Matrix4x4::setRotationY(const double angle)&#123; loadIdentity(); entries[0] = static_cast&lt;float&gt;(cos(M_PI*angle/180)); entries[2] = -static_cast&lt;float&gt;(sin(M_PI*angle/180)); entries[8] = -entries[2]; entries[10] = entries[0];&#125;void Matrix4x4::setRotationZ(const double angle)&#123; loadIdentity(); entries[0] = static_cast&lt;float&gt;(cos(M_PI*angle/180)); entries[1] = static_cast&lt;float&gt;(sin(M_PI*angle/180)); entries[4] = -entries[1]; entries[5] = entries[0];&#125; &emsp;&emsp;但是更一般的情况是绕任意轴进行旋转，构建这样的矩阵稍微有点麻烦，我们接下来就做一些绕任意轴旋转的矩阵构建推到。在这里我们不考虑平移，因而围绕旋转的轴一定是通过原点的。如下图1所示，将$\\vec v$旋转到$\\vec v ‘$，任意轴用单位向量$\\vec n$表示，绕$\\vec n$旋转$\\theta$角度的矩阵记为$R(\\vec n, \\theta)$，$\\vec v’$是向量绕轴$\\vec n$旋转后的向量，即$\\vec v’=R(\\vec n,\\theta)\\vec v$。 图1 绕任意轴旋转&emsp;&emsp;我们的目标就是用$\\vec v$、$\\vec n$和$\\theta$来表示$\\vec v’$，从而构造出$R(\\vec n, \\theta)$。首先将$\\vec v$分解成平行于$\\vec n$的向量$\\vec v_{||}$和垂直于$\\vec n$的分量$\\vec v_{⊥}$，而$\\vec v’_{⊥}$是垂直于$\\vec n$的分向量。注意，$\\vec n$是单位向量，但$\\vec v$不是单位向量，可得$\\vec v$在$\\vec n$方向的投影向量$\\vec v_{||}$为： \\vec v_{||}=(\\vec v\\cdot\\vec n)\\vec n \\tag {6}&emsp;&emsp;从而根据$\\vec v_{||}$和$\\vec v$可知$\\vec v_{⊥}$和$w$，$w$是垂直于$\\vec n$和$\\vec v_{⊥}$的向量： \\vec v_{⊥}=\\vec v-\\vec v_{||} \\tag {7} w=\\vec n \\times \\vec v_{⊥} = \\vec n\\times (\\vec v-\\vec v_{||})\\\\ =\\vec n\\times\\vec v-\\vec n\\times\\vec v_{||}=\\vec n\\times\\vec v-0=\\vec n\\times \\vec v \\tag{8}&emsp;&emsp;$\\vec w$和$\\vec v_{⊥}$相互垂直，$\\vec w$、$\\vec v_{⊥}$和$\\vec v’_{⊥}$在同一个平面上，$\\vec v’_{⊥}$和$\\vec v_{⊥}$的夹角为$\\theta$，从而$\\vec v’_{⊥}$可由$\\vec w$和$\\vec v_{⊥}$线性表示为： \\vec v'_{⊥}=cos\\theta\\vec v_{⊥}+sin\\theta\\vec w\\\\ =cos\\theta(\\vec v-(\\vec v\\cdot\\vec n)\\vec n)+sin\\theta(\\vec n\\times\\vec v)\\tag {9}&emsp;&emsp;最后，根据公式$(6)$和公式$(9)$我们已知$\\vec v_{||}$和$\\vec v’_{⊥}$，从而可以得出$\\vec v’$： \\vec v'=\\vec v_{||}+\\vec v'_{⊥}\\\\ =cos\\theta(\\vec v-(\\vec v\\cdot\\vec n)\\vec n)+sin\\theta(\\vec n\\times\\vec v)+(\\vec v\\cdot\\vec n)\\vec n \\tag {10}&emsp;&emsp;由公式$(10)$可知，我们已经用$\\vec v$、$\\vec n$和$\\theta$表示$\\vec v’$，那如何根据上述的公式$(10)$构建旋转矩阵$R(\\vec n, \\theta)$？还是那个思路：列主序变换矩阵的列向量就是变换后的坐标系的基向量。我们只需求出笛卡尔坐标系的$\\vec x$、$\\vec y$、$\\vec z$三个轴方向上的基向量按照公式$(10)$旋转之后的基向量$\\vec x’$、$\\vec y’$、$\\vec z’$，然后填入矩阵$R(\\vec n, \\theta)$即可，以$\\vec x=[1\\ \\ 0 \\ \\ 0]^T$为例： \\vec x'=cos\\theta(\\vec x-(\\vec x\\cdot\\vec n)\\vec n)+sin\\theta(\\vec n\\times\\vec x)+(\\vec x\\cdot\\vec n)\\vec n =\\left[\\begin{matrix} n^2_x(1-cos\\theta)+cos\\theta \\\\n_xn_y(1-cos\\theta)+n_zsin\\theta \\\\n_xn_z(1-cos\\theta)-n_ysin\\theta) \\end{matrix}\\right] \\tag {11}&emsp;&emsp;$\\vec y=[0\\ \\ 1\\ \\ 0]^T$和$\\vec z=[0\\ \\ 0\\ \\ 1]^T$同理： \\vec y' =\\left[\\begin{matrix} n_xn_y(1-cos\\theta)-n_zsin\\theta \\\\n^2_y(1-cos\\theta)+cos\\theta \\\\n_yn_z(1-cos\\theta)+n_xsin\\theta \\end{matrix}\\right] \\tag {12} \\vec z' =\\left[\\begin{matrix} n_xn_z(1-cos\\theta)+n_ysin\\theta \\\\n_yn_z(1-cos\\theta)-n_xsin\\theta \\\\n^2_z(1-cos\\theta)+cos\\theta \\end{matrix}\\right] \\tag {13}&emsp;&emsp;将$\\vec x’$、$\\vec y’$、$\\vec z’$合并到$R(\\vec n, \\theta)$中： R(\\vec n, \\theta) =\\left[\\begin{matrix} \\vec x'&\\vec y'&\\vec z' \\end{matrix}\\right] \\\\=\\begin{bmatrix} {n_x}^2(1-cos\\theta)+cos\\theta&n_xn_y(1-cos\\theta)-n_zsin\\theta&n_xn_z(1-cos\\theta)+n_ysin\\theta \\\\n_xn_y(1-cos\\theta)+n_zsin\\theta&n^2_y(1-cos\\theta)+cos\\theta&n_yn_z(1-cos\\theta)-n_xsin\\theta \\\\n_xn_z(1-cos\\theta)-n_ysin\\theta)&n_yn_z(1-cos\\theta)+n_xsin\\theta&n^2_z(1-cos\\theta)+cos\\theta \\end{bmatrix} \\tag {14}12345678910111213141516171819202122void Matrix4x4::setRotationAxis(const double angle, const Vector3D &amp;axis)&#123; Vector3D u = axis.getNormalized(); float sinAngle = static_cast&lt;float&gt;(sin(M_PI*angle/180)); float cosAngle = static_cast&lt;float&gt;(cos(M_PI*angle/180)); float oneMinusCosAngle = 1.0f - cosAngle; loadIdentity(); entries[0] = (u.x)*(u.x) + cosAngle*(1-(u.x)*(u.x)); entries[4] = (u.x)*(u.y)*(oneMinusCosAngle) - sinAngle*u.z; entries[8] = (u.x)*(u.z)*(oneMinusCosAngle) + sinAngle*u.y; entries[1] = (u.x)*(u.y)*(oneMinusCosAngle) + sinAngle*u.z; entries[5] = (u.y)*(u.y) + cosAngle*(1-(u.y)*(u.y)); entries[9] = (u.y)*(u.z)*(oneMinusCosAngle) - sinAngle*u.x; entries[2] = (u.x)*(u.z)*(oneMinusCosAngle) - sinAngle*u.y; entries[6] = (u.y)*(u.z)*(oneMinusCosAngle) + sinAngle*u.x; entries[10] = (u.z)*(u.z) + cosAngle*(1-(u.z)*(u.z));&#125; 3、透视投影、正交投影&emsp;&emsp;$3D$空间中的物体最终都要通过投影显示到$2D$的屏幕上，这一过程就是投影变换。投影变换矩阵将视图空间中的顶点数据变换到裁剪空间，裁剪空间中的顶点最后通过透视除法被变换到标准化设备坐标（$NDC$）。通常由两类投影：透视投影、正交投影。 透视投影矩阵&emsp;&emsp;关于透视投影矩阵的前世今生我不过多说，直接上透视投影矩阵的推导过程。一个视锥体我们目前用六个参数表示：$left$，$right$，$bottom$，$top$，$near$，$far$，简写为$l$、$r$、$b$、$t$、$n$和$f$，即视锥体的六个面。我们的目标就是将视图空间中在视锥体内的点变换到标准化设备坐标中的立方体内。即$x$轴方向从$[l,r]$映射到$[-1,1]$，$y$轴方向从$[b,t]$映射到$[-1,1]$，$z$轴方向从$[-n,-f]$映射到$[-1,1]$。 &emsp;&emsp;可能你会觉得奇怪，$z$轴方向为什么是从$[-n,-f]$映射到$[-1,1]$？这是因为摄像机空间的坐标系是右手坐标系，在视图空间中摄像机是朝向视图坐标系的$z$轴的负方向，如下图左边所示，$+Y$、$+Z$、$+X$标准摄像机坐标系的三个轴，而摄像机的观察视锥体是朝向$-Z$方向的。而$NDC$又是左手坐标系，朝向$+Z$方向，所以我们要取负。 图2 透视投影视锥和标准化设备坐标 图3 从-Y方向看去的视锥横截面 图4 从-X方向看去的视锥横截面&emsp;&emsp;在视锥体中的顶点$(x_e,y_e,z_e)$被投影到视锥体的近平面，近平面上的点我们记为$(x_p,y_p,-n)$。如图3和图4所示，根据三角形相似的原理，我们有： \\frac{x_p}{x_e}=\\frac{-n}{z_e}\\ \\rightarrow\\ x_p=\\frac{-n\\cdot x_e}{z_e}=\\frac{n\\cdot x_e}{-z_e} \\tag {15} \\frac{y_p}{y_e}=\\frac{-n}{y_e}\\ \\rightarrow\\ y_p=\\frac{-n\\cdot y_e}{z_e}=\\frac{n\\cdot y_e}{-z_e} \\tag {16}&emsp;&emsp;注意到公式$(15)$和$(16)$中分母都是一个$-z_e$，这与我们将裁剪空间中的顶点做透视除法相对应，透视投影然后做透视除法如下公式$(17)$、$(18)$所示： \\left( \\begin{matrix} x_{clip}\\\\ y_{clip}\\\\ z_{clip}\\\\ w_{clip} \\end{matrix} \\right) =M_{projection}\\cdot \\left( \\begin{matrix} x_{eye}\\\\ y_{eye}\\\\ z_{eye}\\\\ w_{eye} \\end{matrix} \\right) \\tag {17} \\left( \\begin{matrix} x_{ndc}\\\\ y_{ndc}\\\\ z_{ndc} \\end{matrix} \\right) = \\left( \\begin{matrix} x_{clip}/w_{clip}\\\\ y_{clip}/w_{clip}\\\\ z_{clip}/w_{clip} \\end{matrix} \\right) \\tag {18}&emsp;&emsp;为了便于构建矩阵（$x_e$和$y_e$均与$-z_e$相除，不好构建矩阵），我们令裁剪空间中的$w_{clip}$为$-z_e$，将除以$-z_e$的这一步挪到了透视除法去做。故目前的透视矩阵就变为： \\left( \\begin{matrix} x_{c}\\\\ y_{c}\\\\ z_{c}\\\\ w_{c} \\end{matrix} \\right) = \\left( \\begin{matrix} .&.&.&.\\\\ .&.&.&.\\\\ .&.&.&.\\\\ 0&0&-1&0 \\end{matrix} \\right) \\left( \\begin{matrix} x_{e}\\\\ y_{e}\\\\ z_{e}\\\\ w_{e} \\end{matrix} \\right) \\tag {19}&emsp;&emsp;其中”$.$”均表示未知。得到在近平面的$x_p$和$y_p$之后，我们还要将$x_p$映射到$[-1,1]$范围，同理$y_p$也是。以$x_p$为例，我们知道其值域为$[l,r]$。为了将$x_p$其映射到$[-1,1]$，我们首先将其映射到$[0,1]$，不难得到如下式子： \\frac{x_p-l}{r-l}\\in[0,1] \\tag {20}&emsp;&emsp;式$(20)$乘上一个$2$再减去$1$就映射到了$[-1,1]$，映射之后记为$x_n$： x_n=2\\frac{x_p-l}{r-l}-1=\\frac{2x_p}{r-l}-\\frac{r+l}{r-l}\\in[-1,1] \\tag {21}&emsp;&emsp;同理$y_p$到$y_n$的映射： y_n=\\frac{2y_p}{r-l}-\\frac{t+b}{t-b}\\in[-1,1] \\tag {22}&emsp;&emsp;然后将公式$(15)$中的$x_p$带入公式$(21)$，将公式$(16)$中的$y_p$带入公式$(22)$，以$x_p$为例： x_n=\\frac{2x_p}{r-l}-\\frac{r+l}{r-l} =\\frac{2\\frac{n\\cdot x_e}{-z_e}}{r-l}-\\frac{r+l}{r-l}\\\\ =\\frac{2n\\cdot x_e}{(r-l)(-z_e)}-\\frac{r+l}{r-l} =\\frac{\\frac{2n}{r-l}\\cdot x_e}{-z_e}-\\frac{r+l}{r-l}\\\\ =\\frac{\\frac{2n}{r-l}\\cdot x_e}{-z_e}+\\frac{\\frac{r+l}{r-l}\\cdot z_e}{-z_e} =\\underbrace{(\\frac{2n}{r-l}\\cdot x_e+\\frac{r+l}{r-l}\\cdot z_e)}_{x_c}/-z_e \\tag {23}&emsp;&emsp;其中$x_c$即公式$(19)$中的裁剪空间中的$x$轴坐标值。$y_p$同理可得$y_c$: y_n =\\underbrace{(\\frac{2n}{t-b}\\cdot y_e+\\frac{t+b}{t-b}\\cdot z_e)}_{y_c}/-z_e \\tag {24}&emsp;&emsp;现在我们已经知道了$x_c$和$y_c$分辨关于$x_e$、$y_e$以及$z_e$的表达形式，我们可以填充式$(19)$中的投影矩阵第一行与第二行： \\left( \\begin{matrix} x_{c}\\\\ y_{c}\\\\ z_{c}\\\\ w_{c} \\end{matrix} \\right) = \\left( \\begin{matrix} \\frac{2n}{r-l}&0&\\frac{r+l}{r-l}&0\\\\ 0&\\frac{2n}{t-b}&\\frac{t+b}{t-b}&0\\\\ 0&0&A&B\\\\ 0&0&-1&0 \\end{matrix} \\right) \\left( \\begin{matrix} x_{e}\\\\ y_{e}\\\\ z_{e}\\\\ w_{e} \\end{matrix} \\right) \\tag {25}&emsp;&emsp;现在我们还剩下投影矩阵的第三行还不知道。因为我们知道$z$的投影与$x_e$和$y_e$无关，只与$z_e$、$w_e$有关，故可以假设投影矩阵的第三行如上式$(25)$所示，$A$和$B$就是我们假设的要求解的未知表达式。此外，在视图空间中的$w_e$是等于$1$的，$w_c$即前面提到的$-z_e$，从而有： z_n=z_c/w_c=\\frac{Az_e+Bw_e}{-z_e}=\\frac{Az_e+B}{-z_e} \\tag {26}&emsp;&emsp;为了求出公式$(26)$中的$A$和$B$，我们取两个极端的例子：在$-n$处的$z$值被映射到$-1$，在$-f$处的$z$值被映射到$1$，将$(z_n,z_e)=(-1,-n)$和$(z_n,z_e)=(1,-f)$带入式$(26)$中，可得方程组： \\begin{cases} \\frac{-An+B}{n}=-1\\\\ \\frac{-Af+B}{f}=1\\\\ \\end{cases}\\ \\rightarrow\\ \\begin{cases} {-An+B}=-n\\\\ {-Af+B}=f\\\\ \\end{cases} \\tag {27}&emsp;&emsp;求解方程$(27)$，可得$A$与$B$如下所示： A=-\\frac{f+n}{f-n}\\\\ B=-\\frac{2fn}{f-n} \\tag {28}&emsp;&emsp;将公式$(28)$带入公式$(26)$中： z_n=\\underbrace{(-\\frac{f+n}{f-n}z_e-\\frac{2fn}{f-n})}_{z_c}/{-z_e} \\tag {29}&emsp;&emsp;我们最终得到了$z_c$关于$z_e$的表达式，将$A$与$B$填入式$(25)$的投影矩阵即可，$M_{projection}$就是我们一直在寻求的透视投影矩阵： M_{projection}= \\left( \\begin{matrix} \\frac{2n}{r-l}&0&\\frac{r+l}{r-l}&0\\\\ 0&\\frac{2n}{t-b}&\\frac{t+b}{t-b}&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right) \\tag {30}&emsp;&emsp;公式$(30)$中的透视投影矩阵只是一个通用的形式，在视图空间中的视锥体通常都是关于$x$轴和$y$轴对称的，从而有$r=-l$、$t=-b$，将式$(30)$简化成如下形式： M_{projection}= \\left( \\begin{matrix} \\frac{2n}{r-l}&0&0&0\\\\ 0&\\frac{2n}{t-b}&0&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right) \\tag {31}&emsp;&emsp;&emsp;但是通常我们传入构建透视矩阵函数的参数是$fovy$（$y$轴方向的视域角）、$aspect$（屏幕的宽高比）、$near$（近平面）以及$far$（远平面），如何根据这些参数构造式$(31)$的透视投影矩阵呢？注意到$r-l=width$即近平面宽度，$t-b=height$即近平面的高度，我们可以根据$fovy$和$aspect$得出$width$和$height$，具体细节不再赘述： r-l=width=2*near*aspect*tan(fovy/2)\\\\ t-b=height=2*near*tan(fovy/2) M_{projection}= \\left( \\begin{matrix} \\frac{1}{aspect*tan(fovy/2)}&0&0&0\\\\ 0&\\frac{1}{tan(fovy/2)}&0&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right) \\tag {32}123456789101112void Matrix4x4::setPerspective(float fovy, float aspect, float near, float far)&#123; loadZero(); // convert fov from degrees to radians float rFovy = fovy*M_PI/180; const float tanHalfFovy = tanf(static_cast&lt;float&gt;(rFovy*0.5f)); entries[0] = 1.0f/(aspect*tanHalfFovy); entries[5] = 1.0f/(tanHalfFovy); entries[10] = -(far+near)/(far-near); entries[11] = -1.0f; entries[14] = (-2.0f*near*far)/(far-near);&#125; 正交投影矩阵&emsp;&emsp;理解了透视投影矩阵的构造之后，正交投影就简单太多了，正交投影只需做简单的线性映射就行了。只需将$x$轴方向从$[l,r]$映射到$[-1,1]$，$y$轴方向从$[b,t]$映射到$[-1,1]$，$z$轴方向从$[-n,-f]$映射到$[-1,1]$，而这个映射的过程很简单，正如前面公式$(20)$和$(21)$那样，先映射到$[0,1]$，再映射到$[0,2]$，最后映射到$[-1,1]$，这个过程我也不细说了，直接上结果： M_{projection}= \\left( \\begin{matrix} \\frac{2}{r-l}&0&0&-\\frac{r+l}{r-l}\\\\ 0&\\frac{2}{t-b}&0&-\\frac{t+b}{t-b}\\\\ 0&0&\\frac{-2}{f-n}&-\\frac{f+n}{f-n}\\\\ 0&0&0&1 \\end{matrix} \\right) \\tag {33}&emsp;&emsp;然后又因为视锥体关于$x$轴、$y$轴对称，简化的正交投影矩阵就为： M_{projection}= \\left( \\begin{matrix} \\frac{2}{r-l}&0&0&0\\\\ 0&\\frac{2}{t-b}&0&0\\\\ 0&0&\\frac{-2}{f-n}&-\\frac{f+n}{f-n}\\\\ 0&0&0&1 \\end{matrix} \\right) \\tag {33}12345678910void Matrix4x4::setOrtho(float left, float right, float bottom, float top, float near, float far)&#123; loadIdentity(); entries[0] = 2.0f/(right-left); entries[5] = 2.0f/(top-bottom); entries[10] = -2.0f/(far-near); entries[12] = -(right+left)/(right-left); entries[13] = -(top+bottom)/(top-bottom); entries[14] = -(far+near)/(far-near);&#125; 4、lookAt函数构造视图矩阵&emsp;&emsp;视图矩阵的工作目标是将世界坐标系中的所有物体的顶点的坐标从世界坐标系转换到摄像机坐标系。这是因为摄像机坐标系的原点不一定与世界坐标系重合，同时由于自身的旋转，坐标轴也一定不与世界坐标系的坐标轴平行。为完成工作任务，需要分为两步走：首先整体平移，将摄像机平移至世界坐标系原点，然后将顶点从世界坐标系变换至摄像机坐标系。 &emsp;&emsp;lookAt函数的输入参数分别为：$eye$摄像机的位置，$target$摄像机目标点，$up$世界空间的上向量,。首先我们要根据这些参数确定摄像机坐标系的三个轴向量，其中需要非常注意的就是变换到视图空间中时摄像机是朝向视图空间的$-Z$方向的，所以求视图空间中的$Z$轴时是摄像机的位置减去目标点的位置： Z = normalize(eye - target)\\\\ X = normalize(cross(up, Z))\\\\ Y = normalize(cross(Z,X))&emsp;&emsp;通过以上的方式我们就求出了视图空间的三条轴向量，再加上摄像机的位置我们就可以求出将世界坐标变换到与视图坐标重合的矩阵了，记为$M=T\\cdot R$，其中$T$是平移到摄像机位置$eye$的变换矩阵，$R$是旋转到摄像机坐标轴方向的旋转矩阵： M=T\\cdot R= \\left[ \\begin{matrix} 1&0&0&eye_x\\\\ 0&1&0&eye_x\\\\ 0&0&1&eye_x\\\\ 0&0&0&1 \\end{matrix} \\right] \\cdot \\left[ \\begin{matrix} X_x&Y_x&Z_x&0\\\\ X_y&Y_y&Z_y&0\\\\ X_z&Y_z&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {34}&emsp;&emsp;然而公式$(34)$并不是我们要求的视图矩阵，上式中的矩阵$M$仅仅是将世界坐标轴变换到摄像机坐标轴。摄像机只是一个虚拟的物品，我们不能将上述的矩阵$M$作用于摄像机，因为摄像机根本不存在！我们视图矩阵最终作用的世界空间中的物体，这就涉及到了一个相对运动的概念！ &emsp;&emsp;当我们向前移动摄像机的时候，可以看成是摄像机不动，而物体朝着与摄像机朝向相反的方向移动。当我们向右旋转摄像机时，相当于摄像机不动而物体朝着摄像机的左边移动。摄像机的构造得益于相对于运动的理论，计算机图形学中的虚拟$3D$摄像机实际上是通过物体的移动来实现的，所以我们要构造的视图矩阵是公式$(34)$中的逆矩阵。 viewMatrix = M^{-1}=(T\\cdot R)^{-1}=R^{-1}\\cdot T^{-1} = \\left[ \\begin{matrix} X_x&Y_x&Z_x&0\\\\ X_y&Y_y&Z_y&0\\\\ X_z&Y_z&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} \\cdot \\left[ \\begin{matrix} 1&0&0&eye_x\\\\ 0&1&0&eye_x\\\\ 0&0&1&eye_x\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} \\tag {35}&emsp;&emsp;由上式可知，构造视图矩阵涉及到$R$和$T$的求逆，其中的平移矩阵$T$的求逆则是直接取平移量的相反数即可： T^{-1}= \\left[ \\begin{matrix} 1&0&0&eye_x\\\\ 0&1&0&eye_x\\\\ 0&0&1&eye_x\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} = \\left[ \\begin{matrix} 1&0&0&-eye_x\\\\ 0&1&0&-eye_x\\\\ 0&0&1&-eye_x\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {36}&emsp;&emsp;至于旋转矩阵$R$，我们知道旋转矩阵都是正交矩阵，正交矩阵的一个特点就是它的逆等于它的转置： R^{-1}= \\left[ \\begin{matrix} X_x&Y_x&Z_x&0\\\\ X_y&Y_y&Z_y&0\\\\ X_z&Y_z&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} = \\left[ \\begin{matrix} X_x&X_y&X_z&0\\\\ Y_x&Y_y&Y_z&0\\\\ Z_x&Z_y&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {37}&emsp;&emsp;最后，我们得到视图矩阵： viewMatrix=R^{-1}\\cdot T^{-1}= \\left[ \\begin{matrix} X_x&X_y&X_z&0\\\\ Y_x&Y_y&Y_z&0\\\\ Z_x&Z_y&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\cdot \\left[ \\begin{matrix} 1&0&0&-eye_x\\\\ 0&1&0&-eye_x\\\\ 0&0&1&-eye_x\\\\ 0&0&0&1 \\end{matrix} \\right] \\\\= \\left[ \\begin{matrix} X_x&X_y&X_z&-(\\vec X\\cdot \\vec {eye})\\\\ Y_x&Y_y&Y_z&-(\\vec Y\\cdot \\vec {eye})\\\\ Z_x&Z_y&Z_z&-(\\vec Z\\cdot \\vec {eye})\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {38}1234567891011121314151617181920212223242526void Matrix4x4::setLookAt(Vector3D cameraPos, Vector3D target, Vector3D worldUp)&#123; Vector3D zAxis = cameraPos - target; zAxis.normalize(); Vector3D xAxis = worldUp.crossProduct(zAxis); xAxis.normalize(); Vector3D yAxis = zAxis.crossProduct(xAxis); yAxis.normalize(); loadIdentity(); entries[0] = xAxis.x; entries[4] = xAxis.y; entries[8] = xAxis.z; entries[1] = yAxis.x; entries[5] = yAxis.y; entries[9] = yAxis.z; entries[2] = zAxis.x; entries[6] = zAxis.y; entries[10] = zAxis.z; entries[12] = -(xAxis.dotProduct(cameraPos)); entries[13] = -(yAxis.dotProduct(cameraPos)); entries[14] = -(zAxis.dotProduct(cameraPos));&#125; 参考资料$[1]$ http://www.songho.ca/opengl/gl_projectionmatrix.html $[2]$ https://blog.csdn.net/zsq306650083/article/details/8773996 $[3]$ https://blog.csdn.net/y1196645376/article/details/78463248 $[4]$ https://www.cnblogs.com/J1ac/p/9340622.html $[5]$ https://learnopengl-cn.github.io/01%20Getting%20started/08%20Coordinate%20Systems/","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/tags/Soft-Renderer/"},{"name":"3D Math","slug":"3D-Math","permalink":"https://yangwc.com/tags/3D-Math/"}]},{"title":"流体模拟Fluid Simulation：流体模拟基础","slug":"fluidSimulation","date":"2019-05-01T02:37:38.127Z","updated":"2021-02-19T07:02:33.533Z","comments":true,"path":"2019/05/01/fluidSimulation/","link":"","permalink":"https://yangwc.com/2019/05/01/fluidSimulation/","excerpt":"本文主要参考文献《FLUID SIMULATION SIGGRAPH 2007 Course Notes》，结合我的理解单纯地讲述一下流体渲染的一些基础知识，本人水平有限，如有错误，欢迎指出。本文只是单纯针对流体模拟领域，可能一些地方不太严谨，但是对于虚拟模拟来说是可行的。即便如此，本文涉及到大量的数学方法。","text":"本文主要参考文献《FLUID SIMULATION SIGGRAPH 2007 Course Notes》，结合我的理解单纯地讲述一下流体渲染的一些基础知识，本人水平有限，如有错误，欢迎指出。本文只是单纯针对流体模拟领域，可能一些地方不太严谨，但是对于虚拟模拟来说是可行的。即便如此，本文涉及到大量的数学方法。 矢量微积分 Naiver-Stokes偏微分方程组 N-S方程的分步求解 对流算法 一、矢量微积分&emsp;&emsp;高等数学中太多数讨论的是一维的微积分，而矢量微积分则是一维微积分的高维扩展。矢量微积分的三个基础算子：梯度（符号为$∇$），散度（符号为$∇\\cdot$)，旋度（符号为$∇\\times$），在此基础上流体力学中经常用到的还有拉普拉斯算子。 1、梯度（Gradient）&emsp;&emsp;梯度实际上就是矢量的空间偏导数，且结果依然是一个矢量，$2$维的梯度如下： ∇f(x,y)=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}) \\tag {1.1}&emsp;&emsp;依此类推，$3$维的梯度有如下形式： ∇f(x,y,z)=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y},\\frac{\\partial f}{\\partial z}) \\tag {1.2}&emsp;&emsp;有时也会采用如下形式来表示梯度： ∇f=\\frac{\\partial f}{\\partial \\vec x} \\tag {1.3}&emsp;&emsp;梯度通常用来近似计算函数值（实际上就是一维形式的推广)： f(\\vec x+\\Delta \\vec x)\\approx f(\\vec x)+∇f(\\vec x)\\cdot \\Delta \\vec x \\tag {1.4}&emsp;&emsp;同样的，多个函数的梯度就构成了一个矩阵： ∇\\vec F=∇(f,g,h)=\\left( \\begin{matrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} & \\frac{\\partial f}{\\partial z} \\\\ \\frac{\\partial g}{\\partial x} & \\frac{\\partial g}{\\partial y} & \\frac{\\partial g}{\\partial z} \\\\ \\frac{\\partial h}{\\partial x} & \\frac{\\partial h}{\\partial y} & \\frac{\\partial h}{\\partial z} \\\\ \\end{matrix} \\right) =\\left( \\begin{matrix}∇f\\\\ ∇g\\\\ ∇h\\\\ \\end{matrix} \\right) \\tag {1.5}2、散度（Divergence）&emsp;&emsp;散度算子仅仅应用于向量场，它衡量在某一点出相应的矢量聚集或者发散程度，测量方向为径向，结果为标量。$2$维、$3$维形式的散度算子如下所示： ∇\\cdot \\vec u=∇\\cdot (u,v)=\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y} ∇\\cdot \\vec u=∇\\cdot (u,v,w)=\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}+\\frac{\\partial w}{\\partial z} \\tag {1.6}&emsp;&emsp;输入是矢量，而输出为标量。类比梯度，散度符号$∇\\cdot \\vec u$可以理解为梯度$∇$与矢量$\\vec u$的点乘： ∇\\cdot \\vec u=(\\frac{\\partial}{\\partial x},\\frac{\\partial}{\\partial y},\\frac{\\partial}{\\partial z})\\cdot (u,v,w)=\\frac{\\partial}{\\partial x}u+\\frac{\\partial}{\\partial y}v+\\frac{\\partial}{\\partial z}w \\tag {1.7}&emsp;&emsp;若矢量场散度为$0$，则称该矢量场无散度。 3、旋度（Curl）&emsp;&emsp;旋度衡量围绕某一点的旋转速度，测量方向为切向，三维形式的旋度是一个向量： ∇\\times \\vec u=∇\\times (u,v,w) =(\\frac{\\partial w}{\\partial y}-\\frac{\\partial v}{\\partial z}, \\frac{\\partial u}{\\partial z}-\\frac{\\partial w}{\\partial x}, \\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y}) \\tag {1.8}&emsp;&emsp;倒推到$2$维，我们取上式中的$w=0$，即矢量场为$(u,v,0)$，$2$维向量场的旋度是一个标量： ∇\\times \\vec u=∇\\times (u,v)=\\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y} \\tag {1.9}&emsp;&emsp;同样地，旋度符号$∇\\times \\vec u$我们可以理解为梯度$∇$与矢量场$\\vec u$的叉乘： ∇\\times \\vec u= (\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z})\\times(u,v,w) \\tag {1.10}&emsp;&emsp;若矢量场旋度为$0$，则称该矢量场无旋度。 4、拉普拉斯算子（Laplacian）&emsp;&emsp;拉普拉斯算子定义为梯度的散度，符号表示为$∇\\cdot∇$，显然$∇\\cdot$是散度，而后面的$∇$则为梯度，故拉普拉斯算子即梯度的散度，是一个二阶微分算子。$2$维、$3$维形式分别如下： ∇\\cdot∇f=\\frac{\\partial^2f}{\\partial x^2}+\\frac{\\partial^2f}{\\partial y^2} ∇\\cdot∇f=\\frac{\\partial^2f}{\\partial x^2}+\\frac{\\partial^2f}{\\partial y^2}+\\frac{\\partial^2f}{\\partial z^2} \\tag {1.11}&emsp;&emsp;简言之，拉普拉斯算子定义如下： ∇\\cdot∇f=\\Sigma_{i=1}^n\\frac{\\partial^2f}{\\partial x_i^2} \\tag {1.12}&emsp;&emsp;偏微分方程$∇\\cdot ∇f=0$被称为拉普拉斯方程；而如果右边为某个非$0$常数，即$∇\\cdot ∇f=q$，我们称之为泊松方程。更一般地，如果梯度再乘上一个标量$a$（如$1/\\rho$)，即$∇\\cdot (a∇f)=q$，我们依旧称之为泊松问题。 二、$Naiver-Stokes$偏微分方程组&emsp;&emsp;流体模拟器的构建主要围绕著名的不可压缩$Navier-Stokes$方程展开，它是一个流体力学领域的偏微分方程，方程形式如下： \\frac{\\partial \\vec u}{\\partial t}+\\vec u\\cdot ∇\\vec u+\\frac1\\rho∇p=\\vec g+\\nu∇\\cdot∇\\vec u \\tag {2.1} ∇\\cdot\\vec u=0 \\tag {2.2}&emsp;&emsp;这个方程组看起非常地复杂，接下来我们就把它剖析成一个个比较简单的部分来理解。 1、符号标记&emsp;&emsp;我们有必要定义一些物理量的符号用以标记： &emsp;&emsp;符号$\\vec u$在流体力学中通常表示为流体的速度矢量，记$3$维的速度矢量$\\vec u=(u,v,w)$； &emsp;&emsp;希腊字符$\\rho$是流体的密度，对于水，该值大约为$1000kg/m^3$，而空气则大约为$1.3kg/m^3$； &emsp;&emsp;字符$p$代表压力，流体对任何物体施加的单位面积力； &emsp;&emsp;字符$\\vec g$则是我们熟悉的重力加速度，通常取$(0,-9.81,0)m/s^2$。我们约定$y$轴向上，而$x$轴和$z$轴在水平面上。另外补充一点，我们把其他的一些类似的力都累加到$\\vec g$上，也就是我们统一用$\\vec g$表示所有类似力之和，这类力我们称之为体积力（称之为体积力是因为它们的力是作用到整个流体而不只是流体的表面）； &emsp;&emsp;希腊字符$\\nu$是流体的运动粘度，它衡量流体的黏滞性。糖蜜之类的流体有非常高的粘度，而像酒精之类的流体有很低的粘度； &emsp;&emsp;其它一些矢量微积分的符号算子前面已经提到过，不再赘述。 2、动量方程&emsp;&emsp;偏微分方程$(2.1)$我们称之为动量方程，它本质上就是我们熟悉的牛顿定律$\\vec F=m\\vec a$的形式，描述了施加在流体上的力是如何影响流体的运动。 &emsp;&emsp;假设我们用粒子系统来模拟流体，每个粒子代表流体的一小滴，每个粒子有各自的质量$m$、体积$V$和速度$\\vec u$。为了让整个粒子系统运作起来，我们必须弄清楚每个粒子所承受的力的作用。牛顿第二定律告诉我们：$\\vec F=m\\vec a$，而根据加速度定义，我们有： \\vec a=\\frac{D\\vec u}{Dt} \\tag {2.3}&emsp;&emsp;符号$D$是指物质导数，所谓物质导数，就是对流体质点求导数，而且是针对流体质点（在这里就是流体粒子）而不是空间的固定点。因而牛顿第二定律就变成： m\\frac{D\\vec u}{Dt}=\\vec F \\tag {2.4}&emsp;&emsp;那么流体粒子承受的力有哪些呢？一个最简单的力就是重力：$m\\vec g$。而其他的流体质点（或其他流体粒子）也会对当前的流体粒子产生作用力。流体内部的相互作用力之一便是压力，较大压力的区域会向较低压力区域产生作用力。值得注意的是，我们只关注施加在粒子上的压力的净合力。例如，若施加在粒子上压力在每个方向上都相等，那么它的压力的合力便为0。我们用压力的负梯度（取负是因为方向是由压力大的区域指向压力小的区域）来衡量在当前流体粒子处压力的不平衡性，即取$-∇p$。那么流体粒子所承受的压力就是对$-∇p$在整个流体粒子的体积上进行积分，为了简化，我们简单地将$V$与$-∇p$相乘，故粒子压力部分为$-V∇p$。 &emsp;&emsp;其他的流体相互作用力则是由流体的黏性产生的，我们直观地把这种力理解为尽可能使得粒子以周围区域的平均速度移动的力，也就是使得粒子的速度与周围区域粒子速度的差距最小化。拉普拉斯算子是衡量一个量与之周围区域该量平均值之差的算符，因而$∇\\cdot∇\\vec u$是当前粒子速度矢量与周围区域平均速度矢量之差。为了计算粘滞力，我们同样对$∇\\cdot∇\\vec u$在整个粒子体积$V$上进行积分，与前面类似，我们简单取$V∇\\cdot∇\\vec u$。除此之外，我们还引进一个称为动力粘度系数的物理量，符号为$\\mu$。因而粘滞力为$V\\mu∇\\cdot∇\\vec u$。 &emsp;&emsp;把重力、压力和粘滞力综合一起，我们可得： m\\frac{D\\vec u}{Dt}=\\vec F=m\\vec g-V∇p+V\\mu∇\\cdot∇\\vec u \\tag {2.5}&emsp;&emsp;当粒子系统中的粒子数量趋于无穷大，而每个粒子大小趋于$0$时，会产生一个问题：此时每个粒子的质量$m$和体积$V$变为$0$，此时上式变得没有意义。为此，我们把$(2.5)$式调整一下，两边同除以体积$V$，又因$\\rho=m/V$，故有： \\rho\\frac{D\\vec u}{Dt}=\\rho\\vec g-∇p+\\mu∇\\cdot∇\\vec u \\tag {2.6}&emsp;&emsp;两边同除以$\\rho$，移项调整： \\frac{D\\vec u}{Dt}+\\frac1\\rho∇p=\\vec g+\\frac\\mu\\rho∇\\cdot∇\\vec u \\tag {2.7}&emsp;&emsp;为了进一步简化，定义运动粘度为$\\nu=\\mu/\\rho$，式$(2.7)$变为： \\frac{D\\vec u}{Dt}+\\frac1\\rho∇p=\\vec g+\\nu∇\\cdot∇\\vec u \\tag {2.8}&emsp;&emsp;我们已经快把动量方程推导出来，现在我们要把物质导数$\\frac{D\\vec u}{Dt}$弄清楚，为此，我们需要了解两种描述方法：拉格朗日描述和欧拉描述。 3、拉格朗日描述与欧拉描述&emsp;&emsp;当我们尝试研究流体或可变形固体的运动的时候，通常有两种方法来描述：拉格朗日描述（ Lagrangian viewpoint）、欧拉描述（Eulerian viewpoint）。 &emsp;&emsp;拉格朗日描述方法是我们比较熟悉的方法，这种描述方法把物体看成是由类似于粒子系统的形式组成，固体或流体的每个点看作一个独立的粒子，粒子有各自相应的位置$\\vec x$和速度$\\vec u$。我们可以把粒子理解为组成物体的分子。对于我们通常采用拉格朗日描述法进行建模模拟，即用一系列离散的粒子集来构建，粒子之间通过网格相联系。 &emsp;&emsp;欧拉描述方法则采用了完全不同的角度，它常被用于流体力学中。与拉格朗日描述追踪每个物体粒子的方法不同，欧拉描述关注点是空间中的一个固定点，并考察在这个固定点上流体性质（如密度、速度、温度等）是如何随着时间变化的。流体流动经过这个固定点可能会导致这个固定点的物理性质发生一些变化（如一个温度较高的流体粒子流经这个固定点，后面紧跟着一个温度较低的流体粒子流过固定点，那么这个固定点的温度会降低，但是并没有任何一个流体粒子的温度发生了变化）。 &emsp;&emsp;用天气测量举个简单的例子：拉格朗日描述方法就是你乘坐在一个随风而飘的热气球上，测量周围空气的压力、密度和浑浊度等天气指标；而欧拉描述方法就是你固定在地面上，测量流过的空气的天气指标。 &emsp;&emsp;欧拉描述法似乎看起来带来了一些不必要的复杂度，但是目前大多数的流体模拟器都是基于欧拉描述法，这是因为欧拉描述法相比于拉格朗日描述法有一些不可比拟的优点：欧拉描述法能够更加方便地计算一些物理量的空间导数（例如压力梯度和粘度）；而如果用粒子方法的话（即拉格朗日描述法），那么计算物理量相对于空间位置的变化是比较难的。 &emsp;&emsp;把拉格朗日描述法和欧拉描述法联系起来的关键点就是物质导数。首先从拉格朗日描述法出发，假设有一群粒子，每个粒子都有各自的位置$\\vec x$和速度$\\vec u$。记$q$为通用的物理量（如密度、速度和温度等），每个粒子有其对应的$q$值。方程$q(t,\\vec x)$描述在时间点$t$而位置为$\\vec x$的粒子对应的物理量值$q$。则一个粒子的物理量$q$随时间$t$的变化率是多少？这是一个拉格朗日描述角度下的问题，我们取对时间$t$的导数（注意用到了求导链式法则，以及$\\frac{\\partial q}{\\partial \\vec x}=∇q$和$\\vec u=\\frac{d\\vec x}{dt}）$： \\frac d{dt}q(t,\\vec x)=\\frac{\\partial q}{\\partial t}+∇q\\cdot\\frac{d\\vec x}{dt}=\\frac{\\partial q}{\\partial t}+∇q\\cdot\\vec u\\equiv\\frac{Dq}{Dt} \\tag {2.9}&emsp;&emsp;这就是物质导数。把式$(2.9)$代入式$(2.8)$我们就得到了流体动量方程$(2.1)$。物质导数针对的是流体质点（在这里就是流体粒子）而不是空间的固定点。式$(2.9)$写完整一点就是： \\frac{Dq}{Dt}=\\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}+v\\frac{\\partial q}{\\partial y}+w\\frac{\\partial q}{\\partial z} \\tag {2.10}&emsp;&emsp;对于给定的速度场$\\vec u$， 流体的物理性质如何在这个速度场$\\vec u$下变化的计算我们称之为对流（advection）。一个最简单的对流方程，就是其物理量的物质导数为$0$，如下所示： \\frac{Dq}{Dt}=0\\implies\\frac{\\partial q}{\\partial t}+\\vec u\\cdot ∇q = 0 \\tag {2.11}&emsp;&emsp;公式$(2.11)$的意义即在拉格朗日视角观察下，每个流体粒子的物理量保持不变。 4、不可压缩性&emsp;&emsp;关于流体的压缩性在此不做过多的物理细节描述，只需知道一点：通常情况下流体的体积变化非常小（除开一些极端的情况，而且这些极端情况我们日常生活中较少出现）。可压缩流体的模拟涉及到非常复杂的情况，往往需要昂贵的计算资源开销，为此在计算机流体模拟中我们通常把所有的流体当作是不可压缩的，即它们的体积不会发生变化。 &emsp;&emsp;任取流体的一部分，设其体积为$\\Omega$而其边界闭合曲面为$\\partial\\Omega$，我们可以通过围绕边界曲面$\\partial\\Omega$对流体速度$\\vec u$在曲面法线方向上的分量进行积分来衡量这块部分流体的体积变化速率： \\frac d{dt}Volume(\\Omega)=\\int\\int_{\\partial\\Omega}\\vec u\\cdot n \\tag{2.12}&emsp;&emsp;对于不可压缩的流体，其体积保持为某个常量，故其体积变化速率为$0$： \\int\\int_{\\partial\\Omega}\\vec u\\cdot n=0 \\tag {2.13}&emsp;&emsp;由高斯散度定理，我们可以把式$(2.13)$转换为体积分： \\int\\int_{\\partial\\Omega}\\vec u\\cdot n=\\int\\int\\int_\\Omega∇\\cdot \\vec u=0 \\tag{2.14}&emsp;&emsp;式$(13)$应该对任意的$\\Omega$成立，意即无论$\\Omega$取何值，积分值均为$0$。这种情况下只有令积分函数值取$0$方可成立，即对$0$积分无论$\\Omega$取何值结果均为$0$。所以有： ∇\\cdot \\vec u=0 \\tag{2.15}&emsp;&emsp;这就是$Navier-Stokes$方程中的不可压缩条件$(2.2)$。满足不可压缩条件的速度场被称为是无散度的，即在该速度场下流体体积既不膨胀也不坍缩，而是保持在一个常量。模拟不可压缩流体的关键部分就是使得流体的速度场保持无散度的状态，这也是流体内部压力的来源。 &emsp;&emsp;为了把压力与速度场的散度联系起来，我们在动量方程$(2.1)$两边同时取散度： ∇\\cdot\\frac{\\partial \\vec u}{\\partial t}+∇\\cdot(\\vec u\\cdot ∇\\vec u)+∇\\cdot\\frac1\\rho∇p=∇\\cdot(\\vec g+\\nu∇\\cdot∇\\vec u) \\tag {2.16}&emsp;&emsp;对于上式$(2.16)$第一项，我们转变一下求导次序： \\frac {\\partial}{\\partial t}∇\\cdot\\vec u \\tag {2.17}&emsp;&emsp;如果满足流体不可压缩条件，那么式$(2.17)$取值$0$（因为无散度），然后我们调整一下式$(2.16)$可得关于压力的方程： ∇\\cdot\\frac1\\rho∇p=∇\\cdot(-\\vec u\\cdot ∇\\vec u+\\vec g+\\nu∇\\cdot∇\\vec u) \\tag{2.18}5、丢弃粘度项&emsp;&emsp;在某些流体如蜂蜜、小水珠等的模拟中，粘滞力起着非常重要的作用。但是在大多数流体动画模拟中，粘滞力的影响微乎其微，为此秉持着方程组越简单越好的原则，我们常常丢弃粘度项。当然这也不可避免地带来一些误差，事实上，在计算流体力学中尽可能地减少丢弃粘度项带来的误差是一个非常大的挑战。下面的叙述都是基于丢弃粘度项的前提。 &emsp;&emsp;丢弃了粘度项的$Navier-Stokes$方程被称为欧拉方程，而这种理想的流体则是无粘度的。丢弃了粘度项的欧拉方程如下： \\frac{D\\vec u}{Dt}+\\frac1\\rho∇p=\\vec g \\tag {2.19} ∇\\cdot\\vec u=0 \\tag{2.20}&emsp;&emsp;大多数的流体模拟的计算方程都是欧拉方程。 6、边界条件&emsp;&emsp;目前为止我们讨论的都是流体内部的情况，然而边界部分也是流体模拟非常关键的部分。在流体模拟中我们仅仅关注两种边界条件：固体墙（solid walls）、自由面（free surfaces）。 &emsp;&emsp;固体墙顾名思义就是流体与固体接触的边界，用速度来描述很简单：流体既不会流进固体内部也不会从固体内部流出，因此流体在固体墙法线方向上的分量为$0$： \\vec u\\cdot n=0 \\tag {2.21}&emsp;&emsp;当然，上述是固体自身不移动的情况下。通常来说，流体速度在法线方向上的分量与固体的移动速度在法线方向上的分量应该保持一致： \\vec u\\cdot n=\\vec u_{solid}\\cdot n \\tag{2.22}&emsp;&emsp;上述的两个公式都是仅对流体速度在法线方向上的分量做了限制，对于无粘度的流体，切线方向上的流体速度与固体的移动速度无必然的联系。 &emsp;&emsp;自由面是另外一个非常重要的边界条件，它通常就是与另外一种流体相接壤的边界部分。例如在模拟水花四溅时，水流表面不与固体接触的都是自由面（如与空气这种流体接触）。因空气密度远小于水导致空气对水体的仿真影响非常小，为了简化模拟，我们将空气所占的空间设为某个固定大气压的区域，设为$0$是最方便的方案，此时自由面就是压强$p=0$的水体表面。 &emsp;&emsp;在小规模的流体仿真中，自由面的表面张力占据着非常重要的地位。在微观分子层面下，表面张力的存在是因为不同的分子相互吸引产生的力。从几何的角度来解释就是，表面张力就是促使流体的表面积尽可能小的一种力。物理学上，两种不同的流体之间实际上存在着与表面平均曲率成正比的压力骤变： [p]=\\lambda k. \\tag {2.23}&emsp;&emsp;公式$(2.23)$中的$[p]$记为压力之差。$\\lambda$是表面张力系数，可以根据模拟的流体类型查找对应的张力系数（例如空气与水在室温下张力系数为$\\lambda \\approx 0.073N/m$）。而$k$就是平均曲率，单位为$m^{-1}$。又因为我们常常设空气的压力为$0$，因此水与空气交界的自由面的压力为： p=\\lambda k \\tag {2.24}​ 三、N-S方程的分步求解&emsp;&emsp;有了对以上对$Navier-Stokes$方程的理论支撑，接下来我们就要如何用计算机来对该组偏微分方程进行离散化求解。为了程序的松耦合性以及使计算尽可能地高效、简单，在流体模型领域，我们将流体方程分成几个独立的步骤，然后按顺序先后推进。对于不可压缩的无粘度流体方程（即前面的欧拉方程$(2.19)$和$(2.20)$，我们将其离散化成对流项（advection）如公式$(3.1)$、体积力项（body force）如公式$(3.2)$、压力/不可压缩项如公式$(3.3)$： \\frac{Dq}{Dt}=0 \\tag {3.1} \\frac{\\partial \\vec u}{\\partial t}=\\vec g \\tag {3.2} \\begin{cases} \\frac{\\partial \\vec u}{\\partial t}+\\frac{1}{\\rho}∇p=0\\\\ ∇\\cdot\\vec u=0 \\end{cases} \\tag {3.3}&emsp;&emsp;需要注意的是，在对流项公式$(3.1)$中我们用了一个通用量的符号$q$是因为我们不仅仅要对流体的速度进行对流，还需要对其他物理量进行对流。我们记对流项公式$(3.1)$的对流计算算法为$advect(\\vec u, \\Delta t, q)$，即对于给定的时间步长$\\Delta t$和速度场$\\vec u$，对物理量q进行对流。 &emsp;&emsp;对于体积力项$(3.2)$，我们采用简单的前向欧拉法即可：$\\vec u \\leftarrow \\vec u + g\\Delta t$。 &emsp;&emsp;对于压力/不可压缩项$(3.3)$，我们用一个称为$project(\\Delta t, \\vec u)$的算法，通过$project(\\Delta t, \\vec u)$计算出正确的压力以确保速度场$\\vec u$的无散度性质。欧拉方案不会着重研究具体粒子间的作用力，因而不会正向去求解$\\frac{1}{\\rho}∇p$，它是利用流体不可压缩的特性，将速度场$\\vec u$投影到散度为$0$的空间上，间接地解算了压力项。这种思想相当于，已知一个中间量$\\vec u_{temp}$，对这个中间量的唯一一个操作（如正向求解压力$\\frac{1}{\\rho}∇p$）不可行，但是直到最终量$\\vec u_{fianl}$符号的一个性质（散度为$0$），于是只要将$\\vec u_{temp}$投影到符合散度为$0$的特性平面上，即可间接地还原正向求解压力的操作，得到最终的速度场$\\vec u_{temp}$。 &emsp;&emsp;对流项$advect(\\vec u, \\Delta t, q)$的输入速度场$\\vec u$要确保为无散度的状态，投影项$project(\\Delta t, \\vec u)$确保了流体体积保持不变，因而投影项输出的速度场必然是无散度的。所以我们只要确保投影项$project(\\Delta t, \\vec u)$输出的速度场$\\vec u$作为对流项$advect(\\vec u, \\Delta t, q)$的输入即可，这时我们的分步求解流体方程的优势就体现出来了，其伪代码如下所示。 算法1 Fluid Simulation($\\vec u_n$, $\\Delta t$): 1: 初始化速度场$\\vec u_n$,使得$\\vec u_n$无散度 2: 对于每个时间步$n = 0,1,2,…$ 3: &emsp;&emsp;决定一个合理的时间步长$\\Delta t = t_{n+1}-t_n$ 4: &emsp;&emsp;对流项计算$\\vec u_A=advect(\\vec u_n,\\Delta t,\\vec q)$ 5: &emsp;&emsp;体积力项计算$\\vec u_B=\\vec u_A+\\Delta t\\vec g$ 6: &emsp;&emsp;无散度投影$\\vec u_{n+1}=project(\\Delta t,\\vec u_B)$ 1、时间步长&emsp;&emsp;在流体模拟算法中，确定适当的时间步长是算法的第一步。因为计算流体模拟的最后结果是呈现在屏幕上的，所以$\\Delta t$的选取与屏幕的刷新率有重要的关系。若选取的$\\Delta t$有$t_n+\\Delta t &gt; t_{frame}$，那么必须做一个截断使$\\Delta t=t_{frame}-t_n$。此外，流体模拟的三个步骤即对流项、体积力项、无散度投影项对时间步长$\\Delta t$的要求不尽相同，要选择一个满足所有要求的最小时间步长能确保计算的收敛性。此外，一方面为了流体模拟的真实性，我们可能需要选取一个足够小的时间步长来复现流体的高质量细节。另一方面，有时高性能的需求又使得我们不能选取太小的时间步长去渲染一帧。假设一帧至少要进行三个时间步的模拟，那么$\\Delta t$应该至少设成帧间隔时间的三分之一。 2、网格结构&emsp;&emsp;欧拉法的整个流程都是基于网格的，所以合理的网格结构是算法高效的关键点。$Harlow$和$Welch$提出了一种经典的$MAC$（marker and cell）网格结构，许多不可压缩流体模拟的算法都在这个网格结构上呈现出了良好的效率。$MAC$网格是一种交叉排列的网格，不同类型的物理量被存储于网格的不同位置。以二维的网格为例，如图3-1左图所示，流体粒子的压力数据存储于网格的中心点$P_{i,j}$，而速度则沿着笛卡尔坐标被分成了两部分。水平方向的$u$成分被存储在了网格单元竖直边的中心处，例如网格单元$(i,j)$和$(i+1,j)$之间的水平速度记为$u_{i+1/2,j}$。垂直方向的$v$成分则被存储在了网格单元水平面的中心上。这样的存储方案十分有利于估算流体流进/流出某个网格单元的量。 图3-1 MAC网格,左图二维,右图三维&emsp;&emsp;扩展到三维的情况，$MAC$网格同样是交错排列的结构网格，如图3-1右图所示。压力数值存储在立方体网格单元的中心，三个速度分量分别被记录在立方体网格单元的三个表面的中心点上。在数值计算时，这样的分配方式使得我们可以准确地采用中心差分法计算压力梯度和速度的散度，同时克服了中心差分法的一个普遍的缺点。一维的情况为例，在网格顶点位置$…,q_{i-1},q_i,q_{i+1}…$上估算量场$q$的导数，为了无偏（所谓无偏，就是不偏向左边或者右边）估计网格顶点$i$处的$\\frac{\\partial q}{\\partial x}$，一种比较自然的方式就是采用一阶中心差分法： (\\frac{\\partial q}{\\partial x})_i\\approx \\frac{q_{i+1}-q_{i-1}}{2\\Delta x} \\tag {3.4}&emsp;&emsp;公式$(3.4)$是无偏的，且精确度为$O(\\Delta x^2)$。而前向欧拉差分法偏向右边且精确度只有$O(\\Delta x)$： (\\frac{\\partial q}{\\partial x})_i\\approx \\frac{q_{i+1}-q_i}{\\Delta x} \\tag {3.5}&emsp;&emsp;然而，公式$(3.4)$存在着一个非常严重的问题：网格点$i$的估算导数完全忽略了$q_i$的值。数学上，只有常数函数的一阶导数为零。但是公式$(3.4)$遇到了锯齿函数如$q_i=(-1)^i$时，它错误地将该类函数的导数估算为$0$，这种问题被称为零空间问题（null-space problem）。 &emsp;&emsp;交叉错排的$MAC$网格完美地克服了中心差分法的零空间问题，同时也保持了它的无偏二阶精度。在$MAC$网格上运用中心差分法，网格点$i$处的估算导数公式如下所示： (\\frac{\\partial q}{\\partial x})_i\\approx\\frac{q_{i+1/2}-q_{i-1/2}}{\\Delta x} \\tag {3.6}&emsp;&emsp;$MAC$网格确实给流体的压力计算和不可压缩性的处理带来了很大的便利，但与此同时也带来了一些其他方面的麻烦。如果我们要估算某个地方的速度向量，即便采样点恰好在网格点上我们也要做一些插值才能获取相应的速度向量。在网格点处，我们通常采用平均法，以二维为例： \\vec u_{i,j}=(\\frac{u_{i-1/2,j}+u_{i+1/2,j}}{2},\\frac{v_{i,j-1/2}+v_{i,j+1/2}}{2}),\\\\ \\vec u_{i+1/2,j}=(u_{i+1/2,j},\\frac{v_{i,j-1/2}+v_{i,j+1/2}+v_{i+1,j-1/2}+v_{i+1,j+1/2}}{4}),\\\\ \\vec u_{i,j+1/2}=(\\frac{u_{i-1/2,j}+u_{i+1/2,j}+u_{i-1/2,j+1}+u_{i+1/2,j+1}}{4},v_{i,j+1/2}).\\tag {3.7}&emsp;&emsp;最后，在实现中下标索引一般没有浮点数之说，前面直接采用$i+1/2$的记法是为了便于叙述。一般约定如下： p(i,j,k)=p_{i,j,k},\\\\ u(i,j,k)=u_{i-1/2,j,k},\\\\ v(i,j,k)=v_{i,j-1/2,k},\\\\ w(i,j,k)=w_{i,j,k-1/2}. \\tag{3.8}&emsp;&emsp;因而对于$nx\\times ny\\times nz$分辨率的网格，压力数值存储在$nx\\times ny\\times nz$的数组中，速度的$u$成分存储在$(nx+1)\\times ny\\times nz$数组中，速度的$v$成分存储在$nx\\times (ny+1)\\times nz$数组中，速度的$w$成分存储在$nx\\times ny\\times (nz+1)$数组中。 四、对流算法&emsp;&emsp;求解如下所示的对流方程是流体模拟的关键一步： \\frac{Dq}{Dt}=0 \\tag {4.1}&emsp;&emsp;我们把这个对流数值计算的算法记为： q^{n+1}=advect(\\vec u,\\Delta t,q^n) \\tag {4.2}&emsp;&emsp;公式$(4.2)$中的各个符号含义： &emsp;&emsp;$\\vec u$：在$MAC$网格上的离散化的速度场； &emsp;&emsp;$\\Delta t$：时间步长； &emsp;&emsp;$q^n$：当前的物理量场$q$（如流体密度、速度、燃烧物浓度等）； &emsp;&emsp;$q^{n+1}$：经过对流后得到的新的量场。 &emsp;&emsp;在这里要特别注意，输入对流算法的速度场$\\vec u$必须是无散度的，否则模拟结果会出现一些奇怪的失真现象。 1、半拉格朗日对流算法（Semi-Lagrangian Advection）&emsp;&emsp;一维情况下，对流方程$(4.1)$写成偏微分的形式如下： \\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}=0 \\tag {4.3}&emsp;&emsp;分别采用前向欧拉差分法计算对时间的偏导和中心差分法计算对空间的偏导，我们有： \\frac{q^{n+1}_{i}-q^n_i}{\\Delta t}+u^n_i\\frac{q^n_{i+1}-q^n_{i-1}}{2\\Delta x}=0 \\tag {4.4}&emsp;&emsp;转成以$q^{n+1}_i$为计算目标的显式公式，得： q^{n+1}_i=q^n_i-\\Delta t u^n_i\\frac{q^n_{i+1}-q^n_{i-1}}{2\\Delta x} \\tag {4.5}&emsp;&emsp;公式$(4.5)$看起来没什么问题，但是却存在非常严重的漏洞。首先，前向欧拉法被证明是无条件不稳定的空间离散方法：无论取多么小Δ𝑡，随着时间步的推进，累积误差终将发散。即使使用更稳定的时间积分方法来取代前向欧拉方法，解决了时间上的PDE（Partial Differential Equation，偏微分方程）计算，空间上的PDE计算还是会带来重大的麻烦。标准中心差分方法不可避免地会出现的零空间问题，具有高频震荡性质的速度场对空间的导数被错误地计算为$0$或几乎为$0$，低离速度分量被分离出来，从而导致模拟效果中出现许多奇怪的高频摆动和震荡。 &emsp;&emsp;针对这些问题，研究者们提出了一个解然不同的、更加简单和更具物理直观意义的半拉格朗日法。之所以叫半拉格朗日法，是因为这种方法是以拉格朗日视角去解决欧拉视角的对流方程（“半”字的由来）。假设我们的目标是求解网格点$\\vec x_G$的在第$n+1$个时间步时关于物理量$q$的新值，记为$q^{n+1}_G$。在拉格朗日的视角下，我们可以寻找在第$n+1$时间步之前，是空间中的哪一个点上的流体粒子在速度场$\\vec u$的作用下“流向”了$\\vec x_G$，我们记这个粒子在第$n$个时间步时的网格位置为$\\vec x_P$，则第$n+1$个时间步时$\\vec x_G$的$q^{n+1}_G$即为第$n$个时间步时$\\vec x_P$的$q^{n}_P$。如下图4-1为半拉格朗日对流法的示意图。 图4-1 半拉格朗日对流法&emsp;&emsp;半拉格朗日对流法的第一步就是要找出$\\vec x_P$，为此我们根据$\\vec x_G$做反向的追踪。粒子位置对时间的导数就是速度场： \\frac{d\\vec x}{dt}=\\vec u(\\vec x) \\tag {4.6}&emsp;&emsp;经过一个时间步长$\\Delta t$之后，粒子由$\\vec x_P$移动到$\\vec x_G$。为了得到$\\vec x_P$，最简单的方法就是采用前向欧拉法进行倒推： \\vec x_P=\\vec x_G-\\Delta t\\vec u(\\vec x_G) \\tag {4.7}&emsp;&emsp;然而前向欧拉法只有一阶的精度，若在不改变$\\Delta t$的情况下提高精度，我们可以采用高阶的龙格库塔法（Runge-Kutta method）。采用二阶的龙格库塔法如下所示： \\vec x_{mid}=\\vec x_G-\\frac12\\Delta t\\vec u(\\vec x_G),\\\\ \\vec x_P=\\vec x_G-\\Delta t\\vec u(\\vec x_{mid}). \\tag {4.7}&emsp;&emsp;倒推得到$\\Delta t$之前的网格位置$\\vec x_P$一般不会恰好在网格顶点上，为此我们需要做些插值。三维模拟通常采用三线性插值，而二维的则采用双线性插值。 q^{n+1}_G=interpolate(q_n,\\vec x_P) \\tag {4.8}2、边界情况&emsp;&emsp;若我们倒推得到的$\\vec x_P$仍然在流体的内部，那么做插值是完全没问题的。但若$\\vec x_P$在流体的边界之外呢？这种情况的出现的原因通常有两个：一个是$\\vec x_P$确确实实在流体的外部且即将流入流体内部，另一个是由前向欧拉法或龙格库塔法的数值计算方法带来的误差导致。 &emsp;&emsp;在一种情况下，我们应该知道当流体流入时其携带的物理量，此时我们将这个外部流入的物理量作为返回值即可。例如，第$n$个时间步时的外部流体以速度$\\vec U$和温度$T$在第$n+1$个时间步时注入流体内部$\\vec x_G$的位置，那么$\\vec T^{n+1}_G$的值就为$T$。 &emsp;&emsp;在第二种由误差导致的情况下，一个适当的策略就是根据边界上的最近点外推出所求得物理量。在模拟某些流体时，外推变得很简单。例如，在模拟烟雾时我们简单地假设烟雾流体外部即空气的速度风场为某个常数$\\vec U$（可能为$0$），这样边界上的速度场都取$\\vec U$。但还有一些必须根据流体内部的已知量外推出未知量，这时情况就变得比较复杂了。具体如何外推将在后面介绍，目前我们只需要知道大概的步骤：首先寻找边界上的最近点，然后在最近点的领域内插值获取相应的物理量场。 3、时间步长大小&emsp;&emsp;对任何一种数值计算方法的主要的考虑点就是它是否稳定。幸运的是，半拉格朗日对流法已经被证明是一种无条件稳定的算法：无论$\\Delta t$取多大，它永远不会出现数值爆炸的现象。因为每一个新值$q$的确定，都是通过对旧值得插值，无论是线性插值、双线性插值还是三线性插值，$q$的大小都是处于插值点之间，不会得到比原来插值点更大或者更小的值，因而$q$是有上下界的。这使得我们可以尽情地根据所需的模拟质量和模拟效率去调整时间步长。 &emsp;&emsp;但是在实践中，时间步长的大小也不能选得太过极端，否则会产生一些奇观的现象。Foster和Fekiw提出了一个对$\\Delta t$的限制：流体粒子在$\\Delta t$内的倒推轨迹最多经过某个常数个网格单元为宜，例如5个： \\Delta t \\leq \\frac{5\\Delta x}{u_{max}} \\tag {4.9}&emsp;&emsp;公式$(4.9)$中，$u_{max}$是速度场的最大值，我们可以简单地取 存储在网格中的最大速度值。一个更鲁棒的方法考虑了体积力（如重力、浮力等）对最大速度的影响： u_{max}=max(|u^n|)+\\Delta t|g| \\tag {4.10}&emsp;&emsp;将不等式$(4.9)$的最大值带入公式$(4.10)$，我们有： u_{max}=max(|u^n|)+\\frac{5\\Delta x}{u_{max}}|g| \\tag {4.11}&emsp;&emsp;取一个简单的速度上界（简化了公式$(4.11)$），$u_{max}$： u_{max}=max(|u^n|)+\\sqrt{5\\Delta xg} \\tag {4.12}&emsp;&emsp;这样确保了$u_{max}$始终为正，且避免公式$(4.9)$的除$0$错误。 &emsp;&emsp;关于时间步长的讨论离不开$CFL$（以Courant、Friedrichs、Lewy三人的名字命名）条件。$CFL$条件是一个简单而直观的判断计算是否收敛的必要条件。它的直观物理解释就是时间推进求解的速度必须大于物理扰动传播的速度，只有这样才能将物理上所有的扰动俘获到。满足$CFL$条件意味着当$\\Delta x$和$\\Delta t$趋于取极限$0$时，数值计算所求的解就会收敛到原微分方程的解。 &emsp;&emsp;对于半拉格朗日对流法，其满足$CFL$条件当且仅当在极限情况下，追踪得到的粒子轨迹足够逼近真实的轨迹。足够逼近的意思是经过正确的网格插值能够得到正确的依赖域（即差分格式的依赖域包含了原微分方程的依赖域），追踪的轨迹就会收敛到正确真实的轨迹。 &emsp;&emsp;因而，对于采用标准的显式有限差分法的对流方程求解，为了保证收敛，我们要求$q^{n+1}$的新值是由以当前网格点为中心、以$C\\Delta x$（$C$是一个小的整数常量）为半径的邻域范围内插值得到： \\Delta t \\leq C\\frac{\\Delta x}{|\\vec u|} \\tag {4.13}&emsp;&emsp;公式$(4.13)$中的$C$被称为$CFL$数，因而不等式$(4.9)$可以看成是公式$(4.13)$取$CFL$数为$5$得到。 4、数值耗散&emsp;&emsp;对流算法在对流获取新的物理量场$q^{n+1}_i$时会进行一些插值操作，插值不可避免地会平滑物理量场，这带来了一些数值耗散。一次两次的数值耗散不会由太大的影响，但是在流体模拟中我们会在每个时间步都进行对流运算，反反复复的平滑操作将数值耗散不断扩大，损失大量的流体细节。 &emsp;&emsp;以一维的对流项计算为例，流体速度为常量$u&gt;0$： \\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}=0 \\tag {4.14}&emsp;&emsp;假设$\\Delta t &lt; \\frac{\\Delta x}{u}$，即单个时间步长内粒子追踪轨迹长度小于单个网格单元的大小。我们的目标点是$x_i$，则倒推得到的粒子位置就落在了$[x_{i-1},x_i]$上的$x_i-\\Delta tu$，然后进行线性插值得到$q^{n+1}_i$： q^{n+1}=\\frac{\\Delta tu}{\\Delta x}q^n_{i-1}+(1-\\frac{\\Delta tu}{\\Delta x})q^n_i \\tag {4.15}&emsp;&emsp;将公式$(4.15)$整理一下，有： q^{n+1}_i=q^n_i-\\Delta tu\\frac{q^n_i-q^n_{i-1}}{\\Delta x} \\tag {4.16}&emsp;&emsp;公式$(4.16)$实际上正好就是采用时间上的前向欧拉差分法和空间上的单向有限差分法的欧拉方案，把$q^n_i$看成是$q^n$关于$x_i$的函数，对$q^n_{i-1}$进行泰勒级数展开： q^n_{i-1}=q^n_i-(\\frac{\\partial q}{\\partial x})^n_i\\Delta x+(\\frac{\\partial^2q}{\\partial x^2})^n_i\\frac{\\Delta x^2}{2}+O(\\Delta x^3) \\tag {4.17}&emsp;&emsp;将公式$(4.17)$代入公式$(4.16)$，并做一些变量消去，可得： q^{n+1}_i=q^n_i-\\Delta tu(\\frac{\\partial q}{\\partial x})^n_i+\\Delta tu\\Delta x(\\frac{\\partial^2q}{\\partial x^2})^n_i+O(\\Delta x^2) \\tag {4.18}&emsp;&emsp;在二阶截断误差的情况下，结合公式$(4.18)$和公式$(4.14)$，有： \\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}=u\\Delta x(\\frac{\\partial^2q}{\\partial x^2}) \\tag {4.19}&emsp;&emsp;右边就是对流方程计算时引入的额外类似粘度乘上系数$u\\Delta x$的项。这也就是说，当我们采用简单的半拉格朗日法去求解无粘度的对流方程时，模拟的结果却看起来我们像时在模拟有粘度的流体。这就是数值耗散！当然，当$\\Delta x\\to 0$时，这个数值耗散系数也会趋于$0$，所以取时间步无穷小时能够得到正确的模拟结果，但这需要耗费巨额的计算资源开销。我们通常模拟的流体大多数都是无粘度的，所以如何减少这个数值耗散是个至关重要的难题。 &emsp;&emsp;一个简单有效的修复数值耗散的方法就是采用更加锐利的插值方法，从而尽可能地减少由插值带来的数值耗散。在一维的情况时，我们采用三次插值（cubic interpolant）如下公式$(4.21)$，而不是简单的一次线性插值$(4.20)$： q\\approx(1-s)x_i+sx_{i+1} \\tag {4.20} q\\approx[-\\frac13s+\\frac12s^2-\\frac16s^3]q_{i-1}+[1-s^2+\\frac12(s^3-s)]q_i\\\\ +[s+\\frac12(s^2-s^3)]q_{i+1}+[\\frac16(s^3-s)]q_{i+2} \\tag {4.21}&emsp;&emsp;扩展到二维或者三维就是双三次插值（bicubic interpolation）或三三次插值（tricubic interpolation）。以二维情况为例，我们可以先沿着$x$轴做第一遍的三次插值如公式$(4.22)$，然后再沿着$y$轴做第二遍插值如公式$(4.23)$： q_{j-1}=w_{-1}(s)q_{i-1,j-1}+w_0(s)+q_{i,j-1}+w_1(s)q_{i+1,j-1}+w_2(s)q_{i+2,j-1},\\\\ q_{j}=w_{-1}(s)q_{i-1,j}+w_0(s)+q_{i,j}+w_1(s)q_{i+1,j}+w_2(s)q_{i+2,j},\\\\ q_{j+1}=w_{-1}(s)q_{i-1,j+1}+w_0(s)+q_{i,j+1}+w_1(s)q_{i+1,j+1}+w_2(s)q_{i+2,j+1},\\\\ q_{j+2}=w_{-1}(s)q_{i-1,j+2}+w_0(s)+q_{i,j+2}+w_1(s)q_{i+1,j+2}+w_2(s)q_{i+2,j+2}. \\tag {4.22} q=w_{-1}(t)q_{j-1}+w_0(t)q_j+w_1(t)q_{j+1}+w_2(t)q_{j+2} \\tag {4.23}&emsp;&emsp;当然也可以先沿着$y$轴，然后再沿着$x$轴做插值操作。","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/tags/Fluid-Simulation/"},{"name":"Naiver-Stokes Equations","slug":"Naiver-Stokes-Equations","permalink":"https://yangwc.com/tags/Naiver-Stokes-Equations/"},{"name":"Advection","slug":"Advection","permalink":"https://yangwc.com/tags/Advection/"}]}]}