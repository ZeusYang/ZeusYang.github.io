{"meta":{"title":"YangWC's Blog","subtitle":null,"description":"Personal blog website.","author":"WC Yang","url":"http://yoursite.com","root":"/"},"pages":[{"title":"404 Not Found","date":"2019-04-27T07:26:21.624Z","updated":"2019-04-27T07:26:21.624Z","comments":true,"path":"404.html","permalink":"http://yoursite.com/404.html","excerpt":"","text":"**404 Not Found** **�ܱ�Ǹ�������ʵ�ҳ�治����** �����������ַ�����õ�ַ�ѱ�ɾ��"},{"title":"所有分类","date":"2019-04-27T08:54:04.778Z","updated":"2019-04-27T08:54:04.778Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"大佬的博客","date":"2019-04-27T10:52:08.692Z","updated":"2019-04-27T10:52:08.692Z","comments":true,"path":"friends/index.html","permalink":"http://yoursite.com/friends/index.html","excerpt":"","text":"名称： YangWC’s Blog头像： https://cdn.jsdelivr.net/gh/ZeusYang/CDN-for-yangwc.com@1.1.4//globalImage/avator.jpg网址： https://yangwc.com"},{"title":"所有标签","date":"2019-04-27T08:02:52.306Z","updated":"2019-04-27T08:02:52.306Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"关于","date":"2019-04-27T10:20:23.860Z","updated":"2019-04-27T10:20:23.860Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"[](https://github.com/ZeusYang) 中山大学本科四年级 计算机科学与技术专业 准研究生，研究方向计算机图形学 现居广州大学城 关于本站欢迎来到 YangWC 的博客！本站会记录自己的一些学习内容，如若有错，欢迎指正，感谢！ 关于主题本站的主题风格是：Material X有任何问题请留言。"}],"posts":[{"title":"光线追踪器Ray Tracer：入门篇","slug":"RayTracer-Basis","date":"2019-05-08T14:12:14.121Z","updated":"2019-05-23T12:43:04.861Z","comments":true,"path":"2019/05/08/RayTracer-Basis/","link":"","permalink":"http://yoursite.com/2019/05/08/RayTracer-Basis/","excerpt":"光线追踪技术是计算机图形学的一类全局光照算法，目前的影视行业大多都采用光线追踪做离线渲染。本章开始构建一个光线追踪离线渲染器（路径追踪），深入理解光线追踪的技术原理。主要参考资料为Peter Shirley的《Ray Tracing in One Weekend》。数学库沿用之前自己写的3D数学库，这方面的东西不再赘述。相关的完全代码在这里。","text":"光线追踪技术是计算机图形学的一类全局光照算法，目前的影视行业大多都采用光线追踪做离线渲染。本章开始构建一个光线追踪离线渲染器（路径追踪），深入理解光线追踪的技术原理。主要参考资料为Peter Shirley的《Ray Tracing in One Weekend》。数学库沿用之前自己写的3D数学库，这方面的东西不再赘述。相关的完全代码在这里。 光线追踪纵览 实现光线追踪渲染器 程序结果 一、光线追踪纵览&emsp;&emsp;光线追踪 (Ray Tracing) 算法是一种基于真实光路模拟的计算机三维图形渲染算法，相比其它大部分渲染算法，光线追踪算法可以提供更为真实的光影效果。此算法由 Appel 在 1968 年初步提出，1980 年由Whitted 改良为递归算法并提出全局光照模型。直到今天，光线追踪算法仍是图形学的热点，大量的改进在不断涌现。基于对自然界光路的研究, 光线追踪采取逆向计算光路来还原真实颜色。追踪的过程中涵盖了光的反射、折射、吸收等特性 (精确计算)， 并辅以其它重要渲染思想 (进一步模拟)。 其中包含了重要方法，诸如冯氏光照模型 (Phong Shading)、辐射度(Radiosity)、光子映射 (Photon Mapping)、蒙特卡罗方法 (Monte Carlo) 等等。鉴于光线追踪算法对场景仿真程度之高，其被普遍认为是计算机图形学的核心内容， 以及游戏设计、电影特效等相关领域的未来方向。 近年来由于硬件系统的迅速改良， 基于分布式、GPU， 甚至实时渲染的光线追踪显卡也纷纷出现（本人就是入手了一块实时光追显卡rtx2070）。 &emsp;&emsp;光线追踪算法是一种非常自然的技术，相比于光栅化的方法，它更加简单、暴力、真实。与光栅化根据物体计算所在的像素的方式不同，光线路径追踪的方法是一个相反的过程，它在于用眼睛去看世界而不是世界如何到达眼中。如下图所示，从视点出发向屏幕上每一个像素发出一条光线View Ray，追踪此光路并计算其逆向光线的方向，映射到对应的像素上。通过计算光路上颜色衰减和叠加，即可基本确定每一个像素的颜色。 图1 光线追踪示意图 &emsp;&emsp;可以看到光线追踪是一个递归的过程。发射一束光线到场景，求出光线和几何图形间最近的交点，如果该交点的材质是反射性或折射性的，可以在该交点向反射方向或折射方向继续追踪，如此递归下去，直到设定的最大递归深度或者射线追踪到光源处（或者背景色），如此便计算处一个像素的着色值。 &emsp;&emsp;基本的光线追踪tracing()递归算法如下所示： &emsp;&emsp;Algorithm 1: 光线追踪递归算法 &emsp;&emsp;Input: 射线ray &emsp;&emsp;Output: 反向光颜色 &emsp;&emsp;Function tracing(): &emsp;&emsp;if no intersection with any object then&emsp;&emsp;&emsp;&emsp;return background color&emsp;&emsp;else&emsp;&emsp;&emsp;&emsp;obj $\\leftarrow$ find nearest object from the ray;&emsp;&emsp;&emsp;&emsp;reflect ray $\\leftarrow$getReflectRay(obj);&emsp;&emsp;&emsp;&emsp;refract ray $\\leftarrow$ getRefractRay(obj);&emsp;&emsp;&emsp;&emsp;main color $\\leftarrow$ the radiance of obj;&emsp;&emsp;&emsp;&emsp;reflect color $\\leftarrow$ tracing(reflect ray);&emsp;&emsp;&emsp;&emsp;refract color $\\leftarrow$ tracing(refract ray); &emsp;&emsp;&emsp;&emsp;return mix(main color, reflect color, refract color); 二、实现光线追踪渲染器&emsp;&emsp;采用C++语言不借助第三方图形渲染API实现一个简易的光线追踪器，为了将最后的结果显示出来，我采用stb_image将计算得到的像素矩阵保存为png图片。本篇实现的光线追踪只包含求交运行、计算光线反射和折射向量、反走样、景深等较为初级的方面，而实现的材质包含磨砂材质、玻璃材质和金属材质。 1、摄像机&emsp;&emsp;与光栅化的空间变换过程相反，光线追踪大部分操作都是在世界空间中进行，因而需要将屏幕空间的像素坐标变换到世界空间中，并相应地发射出一条射线。在这里我们不再构建矩阵，直接求解出摄像机的三个坐标轴，然后根据视锥体的视域fov和屏幕的宽高比aspect得到每个像素发射出来的射线。 &emsp;&emsp;首先我们创建一个射线类$Ray$，射线通常用一个射线原点$m_origin$和射线方向$m_direction$表示，射线上的每个点则表示为$p(t)=m_origin+t*m_direction$，射线上每一个独立的点都有一个自己唯一的$t$值。因而创建的$Ray$类如下所示，其中$pointAt$函数根据给定的$t$值返回相应的射线上的点： 12345678910111213141516171819202122class Ray&#123;private: Vector3D m_origin; Vector3D m_direction;public: // ctor/dtor. Ray() = default; ~Ray() = default; Ray(const Vector3D &amp;org, const Vector3D &amp;dir) :m_origin(org), m_direction(dir) &#123; m_direction.normalize(); &#125; // Getter. Vector3D getOrigin() const &#123; return m_origin; &#125; Vector3D getDirection() const &#123; return m_direction; &#125; // p(t) = origin + t*dir; Vector3D pointAt(const float &amp;t)const &#123; return m_origin + m_direction * t; &#125;&#125;; &emsp;&emsp;我们实现的基于cpu的光线追踪核心渲染流程是对给定分辨率的像素矩阵，逐行逐列地遍历每个像素坐标，如下所示： 1234567891011unsigned char *RayTracing::render()&#123; for(int row = 0;row &lt; m_height;++ row) &#123; for(int col = 0;col &lt; m_width;++ col) &#123; ...... &#125; &#125; return m_image;&#125; &emsp;&emsp;因而对于每个给定的像素坐标$(x,y)$，我们需要获取这个像素坐标对应的发射出去的射线，首先我们把值域为$[0,m_width]$和$[0,m_height]$的像素坐标映射到$[0,1]$，正如如下所示： 12float u = static_cast&lt;float&gt;(col) / static_cast&lt;float&gt;(m_config.m_width);float v = static_cast&lt;float&gt;(row) / static_cast&lt;float&gt;(m_config.m_height); &emsp;&emsp;接下来我们根据$u$和$v$获取射线方向向量，这涉及到两个方面，一个摄像机的坐标系统，另一个是关于视锥的大小设置。摄像机的坐标轴决定了当前的朝向，视锥的大小设定决定了当前视域的大小。为此，我把摄像机与视锥合并一起，坐标系类型依然是右手坐标系。创建的摄像机类如下所示： 12345678910111213141516171819202122232425262728293031class Camera&#123;public: Vector3D m_pos; Vector3D m_target; Vector3D m_lowerLeftCorner; Vector3D m_horizontal; Vector3D m_vertical; float m_fovy, m_aspect; Vector3D m_axisX, m_axisY, m_axisZ; Camera(const Vector3D &amp;cameraPos, const Vector3D &amp;target,float vfov, float aspect); // Getter. Ray getRay(const float &amp;s, const float &amp;t) const; Vector3D getPosition() const &#123; return m_pos; &#125; Vector3D getTarget() const &#123; return m_target; &#125; Vector3D getAxisX() const &#123; return m_axisX; &#125; Vector3D getAxisY() const &#123; return m_axisY; &#125; Vector3D getAxisZ() const &#123; return m_axisZ; &#125; // Setter. void setPosition(const Vector3D &amp;pos) &#123; m_pos = pos; update(); &#125; void setTarget(const Vector3D &amp;_tar) &#123; m_target = _tar; update(); &#125; void setFovy(const float &amp;fov) &#123; m_fovy = fov; update(); &#125; void setAspect(const float &amp;asp) &#123; m_aspect = asp; update(); &#125;private: void update();&#125;; &emsp;&emsp;其中$m_pos$即摄像机的世界坐标位置，$m_target$即目标位置，而$m_lowerLeftCorner$表示视锥近平面的左下角位置，$m_horizontal$表示近平面在摄像机坐标系下水平方向的跨度，$m_vertical$则是近平面在摄像机坐标系下垂直方向的跨度。$m_fovy$和$m_aspect$分别是视锥的垂直视域和屏幕的宽高比。初始时我们传入摄像机坐标、目标点以及垂直视域和视口宽高比，然后我们根据这些计算摄像机的三个坐标轴，以及近平面的位置： 123456789101112131415161718192021void Camera::update()&#123; const Vector3D worldUp(0.0f, 1.0f, 0.0f); // frustum. float theta = radians(m_fovy); float half_height = static_cast&lt;float&gt;(tan(theta * 0.5f)); float half_width = m_aspect * half_height; // camera coordinate system. m_axisZ = m_pos - m_target; m_axisZ.normalize(); m_axisX = worldUp.crossProduct(m_axisZ); m_axisX.normalize(); m_axisY = m_axisZ.crossProduct(m_axisX); m_axisY.normalize(); // view port. m_lowerLeftCorner = m_pos - m_axisX * half_width - m_axisY * half_height - m_axisZ; m_horizontal = m_axisX * 2.0f * half_width; m_vertical = m_axisY * 2.0f * half_height;&#125; &emsp;&emsp;然后我们对于给定在$[0,1]$的$u$和$v$，就可以计算出一条对应的射线向量了。 1234Ray Camera::getRay(const float &amp;s, const float &amp;t) const&#123; return Ray(m_pos , m_lowerLeftCorner + m_horizontal * s + m_vertical * t - m_pos );&#125; 12345678910for (int row = m_config.m_height - 1; row &gt;= 0; --row)&#123; for (int col = 0; col &lt; m_config.m_width; ++col) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = camera.getRay(u, v); ...... &#125;&#125; 2、物体求交&emsp;&emsp;射线发射出去之后要与物体进行求交运行，对于这类能够被射线碰撞到的物体我们把它抽象为$Hitable$，并用一个虚函数$Hit$作为所有的碰撞求交的接口，创建$Hitable$虚类如下： 12345678910111213141516class Material;struct HitRecord&#123; float m_t; Vector3D m_position; Vector3D m_normal; Material *m_material;&#125;;class Hitable&#123;public: Hitable() = default; virtual ~Hitable() &#123;&#125; virtual bool hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const = 0;&#125;; &emsp;&emsp;可以看到我们还创建了一个$HitRecord$结构体，它包含一次射线碰撞求交的结果记录，其中$m_t$是射线方程的参数$t$，$m_position$是交点的位置，$m_normal$是交点的法向量，而$m_material$则是交点所在物体的材质，求交之后我们需要根据这些记录来计算物体的折射、反射。 &emsp;&emsp;$Hitable$中的$hit$接口以一条射线$ray$作为输入参数，以一个$Hitable$的引用$ret$作为求交的结果记录，函数返回布尔值以表示是否发生了射线碰撞。此外，值得一提的是我们还输入了两个参数，分别是$t_min$和$t_max$，这个是我们自己对射线线段长度做的一个限制，可以分别去掉太近和太远的物体。 &emsp;&emsp;然后我们需要向场景中添加物体，光线追踪器的一个”Hello, world!”是球体。我们知道，一个球体的数学表达式为如下所示： (x-cx)^2+(y-cy)^2+(z-cz)^2=R^2 \\tag {1}&emsp;&emsp;其中$c=(cx,cy,cz)$是球体的球心，$R$为球体半径。我们现在要求的就是，对于射线上的一点$P(t)=S+tV$，使得$(x,y,z)=P(t)=S+tV$带入公式$(1)$成立，公式$(1)$可以写成点乘的形式如下： (P-c)\\cdot (P-c) = R^2 \\tag {2}&emsp;&emsp;将$P=P(t)=S+tV$带入公式$(2)$可得： (V\\cdot V)t^2+2(V\\cdot(S-c))t+(S-c)\\cdot(S-c)-R^2=0 \\tag {3}&emsp;&emsp;可以看到公式$(3)$中只有$t$未知，它是一个一元二次方程。对于任意的一元二次方程$at^2+bt+c=0$，其解有如下形式： t=\\frac{-b\\pm \\sqrt{b^2-4ac}}{2a} \\tag {4}&emsp;&emsp;其中根号内的$D=b^2-4ac$称为根的判别式，它可以反应多项式根的数量。若$D&gt;0$则有两个实数根，若$D=0$则只有一个实数根，若$D&lt;0$则没有实数根。我们首先可以根据判别式判断是否存在交点，然后再求出具体的交点坐标。下面的$Hit$函数，我们首先求出多项式方程的常数项$a$、$b$和$c$，然后求判别式，最后再有解的情况下求取交点。注意，在有两个交点的情况下，我们首先取较近的点，不符合再取较远的那个点。只有一个交点的情况下（如下图2所示），我们不当作射线发生了碰撞（擦边而过）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Sphere : public Hitable &#123; public: float m_radius; Vector3D m_center; Material *m_material; Sphere(const Vector3D &amp;cen, const float r, Material *mat) :m_center(cen), m_radius(r), m_material(mat) &#123;&#125; ~Sphere() &#123; if (m_material)delete m_material; m_material = nullptr; &#125;; virtual bool hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const; &#125;; bool Sphere::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const &#123; Vector3D oc = ray.getOrigin() - m_center; float a = ray.getDirection().dotProduct(ray.getDirection()); float b = oc.dotProduct(ray.getDirection()); float c = oc.dotProduct(oc) - m_radius * m_radius; // discriminant float discriminant = b * b - a * c; if (discriminant &gt; 0) &#123; float temp = (-b - sqrt(b * b - a * c)) / a; if (temp &gt; t_min &amp;&amp; temp &lt; t_max) &#123; ret.m_t = temp; ret.m_position = ray.pointAt(ret.m_t); ret.m_normal = (ret.m_position - m_center) / m_radius; ret.m_material = m_material; return true; &#125; temp = (-b + sqrt(b * b - a * c)) / a; if (temp &gt; t_min &amp;&amp; temp &lt; t_max) &#123; ret.m_t = temp; ret.m_position = ray.pointAt(ret.m_t); ret.m_normal = (ret.m_position - m_center) / m_radius; ret.m_material = m_material; return true; &#125; &#125; return false; &#125; 图2 射线与球体的相交情况 &emsp;&emsp;当场景中有多个物体时，当前的做法是在每次求交时遍历所有的物体，我们需要一个$HitableList$来存储这些物体。我们令$HitableList$继承自$Hitable$，这样$HitableList$就表现得好像只有一个很大的物体一样，并在实现$hit$函数中对场景得所有物体遍历调用他们的$Hit$方法： 1234567891011121314151617181920212223242526272829303132333435363738class HitableList : public Hitable&#123;public: std::vector&lt;Hitable*&gt; m_list; HitableList() = default; ~HitableList() = default; void addHitable(Hitable *target) &#123; m_list.push_back(target); &#125; void clearHitable() &#123; for (int x = 0; x &lt; m_list.size(); ++x) &#123; delete m_list[x]; m_list[x] = nullptr; &#125; &#125; virtual bool hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const;&#125;;bool HitableList::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const&#123; HitRecord tempRec; bool hitAny = false; double closestSoFar = t_max; for (unsigned int x = 0; x &lt; m_list.size(); ++x) &#123; if (m_list[x]-&gt;hit(ray, t_min, closestSoFar, tempRec)) &#123; hitAny = true; closestSoFar = tempRec.m_t; ret = tempRec; &#125; &#125; return hitAny;&#125; &emsp;&emsp;此外，值得一提的是，在$HitableList$的$hit$函数中我们需要做一个类似于深度测试的步骤，我们从摄像机发射的射线只能跟最靠近摄像机的那个交点做反射、折射，一条射线发射出去可能会与多个物体相交，我们必须取最近的交点。这个距离我们用射线方程中的$t$来描述，显然$t$越大则交点越远，因此用$closestSoFar$来记录当前获取的交点的最小$t$，以此作为$t$的上限，这样最终求出来的必然就是最近的交点。 3、物体材质&emsp;&emsp;现在我们的一个问题就是求出交点之后，光线在交点上做什么样的反射和折射？这取决于物体的材质。若物体的材质是透明的玻璃，那么光线一般做折射；而若物体是光滑的镜面，则光线做完美的反射。针对不同物体的材质，光线的散射情况各不相同，为此我们创建一个虚类$Material$，并把光线散射的这一过程抽象为$sactter$函数接口。 123456789class Material&#123;public: Material() = default; virtual ~Material() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const = 0;&#125;; &emsp;&emsp;可以看到，$scatter$函数接收入射射线$Ray$以及求交获得的$HitRecord$，计算散射光线的向量，返回的结果表示是否发生了散射。其中的$attenuation$本质上是物体自身的反射颜色，之所以叫$attenuation$是因为光线照射到物体上，物体一般会吸收光线中的大部分颜色，然后仅反射自身颜色的部分，这个过程使得光线在反射过程中不断衰减。 3.1 Lambertian反射材质&emsp;&emsp;首先我们要实现的是Lambertian反射的材质，Lambertian反射也叫理想散射。Lambertian表面是指在一个固定的照明分布下从所有的视场方向上观测都具有相同亮度的表面，Lambertian表面不吸收任何入射光。Lambertian反射也叫散光反射，不管照明分布如何，Lambertian表面在所有的表面方向上接收并发散所有的入射照明，结果是每一个方向上都能看到相同数量的能量。这是一种理想情况，现实中不存在完全漫反射，但Lambertian可以用来近似的模拟一些粗糙表面的效果，比如纸张。 图3 Lambertian反射 &emsp;&emsp;为了实现Lambertian表面的均匀反射现象，我们令射线碰撞到表面之后，在交点的半球方向上随机地反射，只要随机性够均匀，我们就能模拟出理想散射的情况。为此，我们取一个正切于交点$P$表面的单位球体，在这个球体内随机取一个点$S$，则反射的向量就为$S-P$。这个正切于交点$P$表面的单位球体不难求得，设交点$P$的单位法向量为$N$，那么该正切球体的球心为$P+N$。我们首先在球心为原点的单位球内随机求得一个方向向量，然后将这个方向向量加上正切球体的球心即可得出反射的方向向量。（$drand48$是生成$[0,1)$之间的均匀随机数函数，一般linux下才有这个内建函数，windows下没有，所以我们就自己写了。） 123456789101112131415161718192021222324252627282930313233343536373839404142434445#define rndm 0x100000000LL#define rndc 0xB16#define rnda 0x5DEECE66DLL static unsigned long long seed = 1; inline double drand48(void) &#123; seed = (rnda * seed + rndc) &amp; 0xFFFFFFFFFFFFLL; unsigned int x = seed &gt;&gt; 16; return ((double)x / (double)rndm); &#125;=============================================================== static Vector3D randomInUnitSphere() &#123; Vector3D pos; do &#123; pos = Vector3D(drand48(), drand48(), drand48()) * 2.0f - Vector3D(1.0, 1.0, 1.0); &#125; while (pos.getSquaredLength() &gt;= 1.0); return pos; &#125;=============================================================== class Lambertian : public Material &#123; private: Vector3D m_albedo; public: Lambertian(const Vector3D &amp;a) : m_albedo(a) &#123;&#125; virtual ~Lambertian() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const; &#125;; bool Lambertian::scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const &#123; Vector3D target = rec.m_position + rec.m_normal + Vector3D::randomInUnitSphere(); scattered = Ray(rec.m_position, target - rec.m_position); attenuation = m_albedo; return true; &#125; &emsp;&emsp;其中的$m_albedo$为物体自身的反射颜色。 3.2 金属镜面反射材质&emsp;&emsp;金属的表面比较光滑，因而不会呈现出光线随机散射的情况。对于一个完美镜面的材质来说，入射光线和反射光线遵循反射定律，即光射到镜面上时，反射线跟入射线和法线在同一平面内，反射线和入射线分居法线两侧，并且与界面法线的夹角（分别叫做入射角和反射角）相等。反射角等于入射角。 &emsp;&emsp;求反射向量如下图4所示，比较简单，不再赘述。 图4 反射向量 R = I-2(N\\cdot I)N \\tag {5}1234static Vector3D reflect(const Vector3D &amp;ray, const Vector3D &amp;normal)&#123; return ray - normal * (ray.dotProduct(normal)) * 2.0f;&#125; &emsp;&emsp;对于一个完美镜面的金属材质来说，我们只需求出反射向量，然后按照这个反射向量递归下去就行了。但是有些金属并没有那么光滑，它的高光反射并没有那么锐利，为此我们对求出的反射向量做一定的扰动，使反射向量在一定的波瓣内随机，这个波瓣有多大由用户决定（波瓣越大则金属越粗糙）。废话不多说直接上图就明白了。 &emsp;&emsp;我们在反射向量的终点上取一个给定半径的球体，在这个球体内随机选一个点作为新的反射向量的终点即可。这个半径的大小我们用$m_fuzz$变量存储，交给用户决定。 12345678910111213141516171819202122class Metal : public Material&#123;private: float m_fuzz; Vector3D m_albedo;public: Metal(const Vector3D &amp;a, const float &amp;f) : m_albedo(a), m_fuzz(f) &#123; if (f &gt; 1.0f)m_fuzz = 1.0f; &#125; virtual ~Metal() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const;&#125;;bool Metal::scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const&#123; Vector3D reflectedDir = Vector3D::reflect(in.getDirection(), rec.m_normal); scattered = Ray(rec.m_position, reflectedDir + Vector3D::randomInUnitSphere() * m_fuzz); attenuation = m_albedo; return (scattered.getDirection().dotProduct(rec.m_normal) &gt; 0.0f);&#125; 3.3 透明玻璃折射材质&emsp;&emsp;对于水、玻璃和钻石等等物体的材质，光线照射到它们的表面时，它会把光线分成折射（也叫透射）光线和反射光线两部分。我们实现的材质采用随机的策略， 就是在折射和反射两个部分中随机选取一种。首先我们要根据入射向量、法线以及入射介质系数和折射介质系数计算折射方向向量，相比反射向量，推导计算的过程稍微有点复杂。折射表面有折射系数属性，根据Snell定律，如图5所示，入射角$\\theta _L$和折射角$\\theta _T$之间的关系有： \\eta _Lsin\\theta _L=\\eta _rsin\\theta _r \\tag {6} 图5 折射向量的计算 &emsp;&emsp;其中，$\\eta _L$时光线离开的介质的折射系数，$\\eta _r$是光线进入的介质的折射系数。空气的折射系数通常定位$1.00$，折射系数越大，则在两种不同介质之间光线弯曲效果越明显。$N$和$L$都是单位方向向量。折射向量$T$可为与法向量平行的向量$-Ncos\\theta_T$和垂直的向量$-Gsin\\theta _T$，$G$是上图所示的单位向量。而$perp_NL$与$G$向量平行，且$||perp_NL=sin\\theta_L||$，故有： G=\\frac{perp_NL}{sin\\theta_L}=\\frac{L-(N\\cdot L)N}{sin\\theta_L} \\tag {7}&emsp;&emsp;折射向量$T$可以表示为： T=-Ncos\\theta_T-Gsin\\theta_T\\\\ =-Ncos\\theta_T-\\frac{sin\\theta_T}{sin\\theta_L}[L-(N\\cdot L)N] \\tag {8}&emsp;&emsp;利用公式$(6)$，我们可以将上式中的正弦商替换为$\\eta _L/\\eta _T$，可得： T=-Ncos\\theta_T-\\frac{\\eta _L}{\\eta _T}[L-(N\\cdot L)N] \\tag {9}&emsp;&emsp;注意到公式$(9)$中的$cos\\theta_T$未知，用$\\sqrt{1-sin^2\\theta_T}$替换$cos\\theta_T$，再用$(\\eta_L/\\eta_r)sin\\theta_L$代替$sin\\theta_T$，可得： T=-N\\sqrt{1-\\frac{\\eta^2_L}{\\eta^2_T}sin^2\\theta_L}-\\frac{\\eta_L}{\\eta_T}[L-(N\\cdot L)N] \\tag {10}&emsp;&emsp;最后再用$1-cos^2\\theta_L=1-(N\\cdot L)^2$代替$sin^2\\theta_L$，得到最终的表达式为： T=(\\frac{\\eta_L}{\\eta_T}N\\cdot L-\\sqrt{1-\\frac{\\eta^2_L}{\\eta^2_T}[1-(N\\cdot L)^2]}\\ )N-\\frac{\\eta_L}{\\eta_T}L \\tag {11}&emsp;&emsp;如果$\\eta_L&gt;\\eta_T$，则上式平方根里的数值可能为负，这种情况发生在当光线从一个大折射率的介质进入一个小折射率的介质时，此时光线与表面之间的入射角较大。特别的，若仅当$sin\\theta_L\\leq \\eta_r/\\eta_L$时，公式$(11)$有效，如果平方根里的数值为负，则会出现所谓的全内反射现象，也就是光线不被折射，仅在介质内部反射。此外，需要注意的是，我们在程序实现中的入射向量与图5中$L$是相反的，所以需要将公式中的$(11)$的入射向量取反，如下所示： T=\\frac{\\eta_L}{\\eta_T}(L-(N\\cdot L)N)-N\\sqrt{1-\\frac{\\eta^2_L}{\\eta^2_T}[1-(N\\cdot L)^2]}\\ \\tag {12}123456789101112131415static bool refract(const Vector3D &amp;ray, const Vector3D &amp;normal, float niOvernt, Vector3D &amp;refracted)&#123; Vector3D uv = ray; uv.normalize(); float dt = uv.dotProduct(normal); float discriminant = 1.0f - niOvernt * niOvernt * (1.0f - dt * dt); if (discriminant &gt; 0.0f) &#123; refracted = (uv - normal * dt) * niOvernt - normal * sqrt(discriminant); return true; &#125; else return false;&#125; &emsp;&emsp;然后创建一个$Dielectric$类，它有一个私有变量$refIdx$，它表面该物体的材质折射系数。在实现玻璃材质物体的散射函数$scatter$时，我们需要判断当前射线是从外部折射到内部还是从内部折射到外部，这可以通过计算入射向量与法向量的夹角余弦值来判断（通常法向量朝外），然后相应地将法向量的方向扭正。这里用$ni-over-nt$变量来记录$\\frac{\\eta_L}{\\eta_T}$，我们知道空气的折射系数为$1.00$，所以从外面折射入物体内部时其取值等于$1.0/refIdx$，从内部折射到外部时取值为$refIdx$。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354 class Dielectric : public Material &#123; private: float refIdx; public: Dielectric(float ri) : refIdx(ri) &#123;&#125; virtual ~Dielectric() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const; &#125;; bool Dielectric::scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const &#123; Vector3D outward_normal; Vector3D reflected = Vector3D::reflect(in.getDirection(), rec.m_normal); float ni_over_nt; attenuation = Vector3D(1.0f, 1.0f, 1.0f); Vector3D refracted; float reflect_prob; float cosine; // from inside to outside. if (in.getDirection().dotProduct(rec.m_normal) &gt; 0.0f) &#123; outward_normal = -rec.m_normal; ni_over_nt = refIdx; cosine = refIdx * in.getDirection().dotProduct(rec.m_normal) / in.getDirection().getLength(); &#125; // from outside to inside. else &#123; outward_normal = rec.m_normal; ni_over_nt = 1.0 / refIdx; cosine = -in.getDirection().dotProduct(rec.m_normal) / in.getDirection().getLength(); &#125; if (Vector3D::refract(in.getDirection(), outward_normal, ni_over_nt, refracted)) &#123; reflect_prob = schlick(cosine, refIdx); &#125; else &#123; scattered = Ray(rec.m_position, reflected); reflect_prob = 1.0f; &#125; if (drand48() &lt; reflect_prob) scattered = Ray(rec.m_position, reflected); else scattered = Ray(rec.m_position, refracted); return true; &#125;&#125; &emsp;&emsp;这里还要引入一个菲涅尔反射现象（仅对电介质和非金属表面有定义）。生活中，当我们以垂直的视角观察时，任何物体或者材质表面都有一个基础反射率(Base Reflectivity)，但是如果以一定的角度往平面上看的时候所有反光都会变得明显起来。你可以自己尝试一下，用垂直的视角观察你自己的木制桌面，此时一定只有最基本的反射性。但是如果你从近乎与法线成90度的角度观察的话反光就会变得明显的多。如果从理想的90度的视角观察，所有的平面理论上来说都能完全的反射光线。这种现象因菲涅尔而闻名，并体现在了菲涅尔方程之中。菲涅尔方程是一个相当复杂的方程式，不过幸运的是菲涅尔方程可以用Fresnel-Schlick近似法求得近似解： F_{schlick(h,v,F_0)}=F_0+(1-F_0)(1-(h\\cdot v))^5 \\tag {13}&emsp;&emsp;这里的$F_0$y由物体的折射系数得到，$h$是入射向量的负向量（因为我们定义的入射向量方向朝向交点），$v$则是交点处的法向量$v$，我们实现一个$schlick$函数如下： 123456float schlick(float cosine, float ref_idx) const&#123; float r0 = (1.0f - ref_idx) / (1.0f + ref_idx); r0 = r0 * r0; return r0 + (1.0f - r0) * pow((1.0f - cosine), 5.0f);&#125; &emsp;&emsp;我们还定义了一个reflect_prob变量，它介于0~1之间。我们根据reflect_prob与介于$[0,1)$的随机数做比较确定选择反射还是折射，这个还是很合理的，为什么呢？因为我们做了100次采样！那么我们可以理直气壮的说，我们的透明电介质真正做到了反射和折射的混合（除了全反射现象），而且这样符合光线照射透明电介质时，它会分裂为反射光线和折射光线的物理现象。（在程序中，教程作者在从内部折射到外部的时候将$cosine$值还乘上了个$refIdx$，这个操作没明白作者的意图，不乘上$refIdx$好像也没有发现渲染结果有明显的错误）。 &emsp;&emsp;最后，我们实现的玻璃球球内图像是颠倒的，这属于正常现象，原因如下图所示。光线经过两次折射最终导致了图像的颠倒。 4、抗锯齿&emsp;&emsp;为了减少光线追踪方法的噪声点和锯齿，我们需要做一些抗锯齿处理。方法就是在计算一个像素坐标的像素值时，发射很多条射线，射线的取值范围在一个像素之内，然后将所有光线获取的像素值累加起来，最后除以总的采样数。代码如下： 123456789101112131415161718192021int samples = 100;for (int row = m_config.m_height - 1; row &gt;= 0; --row)&#123; for (int col = 0; col &lt; m_config.m_width; ++col) &#123; Vector4D color; for (int sps = 0; sps &lt; samples; ++sps) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = camera.getRay(u, v); color += tracing(ray, world, 0); &#125; color /= static_cast&lt;float&gt;(samples); color.w = 1.0f; // gamma correction. color = Vector4D(sqrt(color.x), sqrt(color.y), sqrt(color.z), color.w); drawPixel(col, row, color); &#125;&#125; &emsp;&emsp;这里还提到了gamma矫正，关于gamma矫正请看这里)。我们对计算得到的像素做了一个简单的gamma矫正，gamma矫正系数取为$2.0$。不进行gamma矫正的话，渲染出来的图片明显偏暗。 5、景深&emsp;&emsp;关于现实生活中摄像机的景深原理，我不再详细说明。在光线追踪中实现景深并不复杂。实现的方法：首先是射线的出发点视点，我们的眼睛（或者相机）不再是一个点而是眼睛所在的周围圆盘上的随机点，因为实际的相机是有摄像镜头的，摄像镜头是一个大光圈（很大一个镜片），并不是针孔类的东东，所以，我们要模拟镜头，就要随机采针孔周围的光圈点。 &emsp;&emsp;此外还有一个焦距的问题，我们一开始假设成像平面在摄像机坐标系的$z=-1$上，为了实现摄像机的景深效果，现在我们要引入现实摄像机的焦距概念。简单的说焦距是焦点到面镜的中心点之间的距离。因此我们提供了一个焦距的参数给用户调整，以确定所需的景深效果。通常情况下焦距$focusDist$等于$length(target-cameraPos)$。这个时候我们将成像平面挪到了摄像机坐标系的$z=-focusDist$上，相应地需要调整计算成像平面的$halfHeight$（在前面的基础上再乘上个$focusDist$）。 12345678910111213141516171819202122232425262728293031323334Camera::Camera(const Vector3D &amp;cameraPos, const Vector3D &amp;target, float vfov, float aspect, float aperture, float focus_dist)&#123; m_pos = cameraPos; m_target = target; m_fovy = vfov; m_aspect = aspect; m_lensRadius = aperture * 0.5f; m_focusDist = focus_dist; update();&#125;void Camera::update()&#123; const Vector3D worldUp(0.0f, 1.0f, 0.0f); // frustum. float theta = radians(m_fovy); float half_height = static_cast&lt;float&gt;(tan(theta * 0.5f)) * m_focusDist; float half_width = m_aspect * half_height; // camera coordinate system. m_axisZ = m_pos - m_target; m_axisZ.normalize(); m_axisX = worldUp.crossProduct(m_axisZ); m_axisX.normalize(); m_axisY = m_axisZ.crossProduct(m_axisX); m_axisY.normalize(); // view port. m_lowerLeftCorner = m_pos - m_axisX * half_width - m_axisY * half_height - m_axisZ * m_focusDist; m_horizontal = m_axisX * 2.0f * half_width; m_vertical = m_axisY * 2.0f * half_height;&#125; 6、递归光线追踪&emsp;&emsp;最后，我们实现的光线追踪器$Tracer$如下，追踪器的核心实现主要在$tracing$函数和$render$函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168class Hitable;class Vector3D;class Vector4D;class Tracer&#123;private: class Setting &#123; public: int m_maxDepth; int m_width, m_height, m_channel; Setting():m_maxDepth(50), m_channel(4) &#123;&#125; &#125;; Setting m_config; unsigned char *m_image;public: Tracer(); ~Tracer(); void initialize(int w, int h, int c = 4); unsigned char *render(); int getWidth() const &#123; return m_config.m_width; &#125; int getHeight() const &#123; return m_config.m_height; &#125; int getChannel() const &#123; return m_config.m_channel; &#125; int getRecursionDepth() const &#123; return m_config.m_maxDepth; &#125; unsigned char *getImage() const &#123; return m_image; &#125; void setRecursionDepth(int depth); void setCamera(const Vector3D &amp;cameraPos, const Vector3D &amp;target, const Vector3D &amp;worldUp, float fovy, float aspect, float aperture, float focus_dist);private: Hitable *randomScene(); Vector4D tracing(const Ray &amp;r, Hitable *world, int depth); float hitSphere(const Vector3D &amp;center, const float &amp;radius, const Ray &amp;ray); void drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color);&#125;;void Tracer::initialize(int w, int h, int c)&#123; m_config.m_width = w; m_config.m_height = h; if (m_image != nullptr) delete m_image; m_image = new unsigned char[m_config.m_width * m_config.m_height * m_config.m_channel];&#125;unsigned char *Tracer::render()&#123; // viewport Vector3D lower_left_corner(-2.0, -1.0, -1.0); Vector3D horizontal(4.0, 0.0, 0.0); Vector3D vertical(0.0, 2.0, 0.0); Vector3D origin(0.0, 0.0, 0.0); // scene Hitable* world = randomScene(); // camera Vector3D lookfrom(3, 4, 10); Vector3D lookat(0, 0, 0); float dist_to_focus = 10.0f; float aperture = 0.0f; Camera camera(lookfrom, lookat, 45, static_cast&lt;float&gt;(m_config.m_width) / m_config.m_height, aperture, dist_to_focus); int samples = 100; for (int row = m_config.m_height - 1; row &gt;= 0; --row) &#123; for (int col = 0; col &lt; m_config.m_width; ++col) &#123; Vector4D color; for (int sps = 0; sps &lt; samples; ++sps) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = camera.getRay(u, v); color += tracing(ray, world, 0); &#125; color /= static_cast&lt;float&gt;(samples); color.w = 1.0f; // gamma correction. color = Vector4D(sqrt(color.x), sqrt(color.y), sqrt(color.z), color.w); drawPixel(col, row, color); &#125; &#125; reinterpret_cast&lt;HitableList*&gt;(world)-&gt;clearHitable(); delete world; return m_image;&#125;void Tracer::drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color)&#123; if (x &lt; 0 || x &gt;= m_config.m_width || y &lt; 0 || y &gt;= m_config.m_height) return; unsigned int index = (y * m_config.m_width + x) * m_config.m_channel; m_image[index + 0] = static_cast&lt;unsigned char&gt;(255 * color.x); m_image[index + 1] = static_cast&lt;unsigned char&gt;(255 * color.y); m_image[index + 2] = static_cast&lt;unsigned char&gt;(255 * color.z); m_image[index + 3] = static_cast&lt;unsigned char&gt;(255 * color.w);&#125;Hitable *Tracer::randomScene()&#123; int n = 500; HitableList *list = new HitableList(); list-&gt;addHitable(new Sphere(Vector3D(0, -1000.0, 0), 1000, new Lambertian(Vector3D(0.5, 0.5, 0.5)))); for (int a = -11; a &lt; 11; ++a) &#123; for (int b = -11; b &lt; 11; ++b) &#123; float choose_mat = drand48(); Vector3D center(a + 0.9*drand48(), 0.2, b + 0.9*drand48()); if ((center - Vector3D(4, 0.2, 0)).getLength() &gt; 0.9) &#123; // diffuse. if (choose_mat &lt; 0.4f) list-&gt;addHitable(new Sphere(center, 0.2, new Lambertian (Vector3D(drand48()*drand48(), drand48()*drand48(), drand48()*drand48())))); // metal else if (choose_mat &lt; 0.6f) list-&gt;addHitable(new Sphere(center, 0.2, new Metal (Vector3D(0.5f*(1.0f + drand48()), 0.5f*(1.0f + drand48()), 0.5f*(1.0f + drand48())), 0.5f*drand48()))); // glass else list-&gt;addHitable(new Sphere(center, 0.2, new Dielectric (1.5f))); &#125; &#125; &#125; list-&gt;addHitable(new Sphere(Vector3D(0, 1, 0), 1.0, new Dielectric(1.5f))); list-&gt;addHitable(new Sphere(Vector3D(-4, 1, 0), 1.0, new Lambertian(Vector3D(0.4, 0.2, 0.1)))); list-&gt;addHitable(new Sphere(Vector3D(4, 1, 0), 1.0, new Metal(Vector3D(0.7, 0.6, 0.5), 0.0f))); return list;&#125;Vector4D Tracer::tracing(const Ray &amp;r, Hitable *world, int depth)&#123; HitRecord rec; if (world-&gt;hit(r, 0.001f, FLT_MAX, rec)) &#123; Ray scattered; Vector3D attenuation; if (depth &lt; m_config.m_maxDepth &amp;&amp; rec.m_material-&gt;scatter(r, rec, attenuation, scattered)) return attenuation * tracing(scattered, world, depth + 1); else return Vector4D(0.0f, 0.0f, 0.0f, 1.0f); //return backgroundColor(Ray(rec.m_position, target - rec.m_position), world) * 0.5f; //return rec.normal * 0.5f + Vector3D(0.5f, 0.5f, 0.5f); &#125; else &#123; float t = 0.5f * (r.getDirection().y + 1.0f); Vector4D ret = Vector3D(1.0f, 1.0f, 1.0f) * (1.0f - t) + Vector3D(0.5f, 0.7f, 1.0f) * t; ret.w = 1.0f; return ret; &#125;&#125; 三、程序结果 参考资料$[1]$ https://www.cnblogs.com/jerrycg/p/4941359.html $[2]$ https://blog.csdn.net/baishuo8/article/details/81476422 $[3]$ https://blog.csdn.net/silangquan/article/details/8176855 $[4]$ Peter Shirley. Ray Tracing in One Weekend. Amazon Digital Services LLC, January 26, 2016. $[5]$ https://learnopengl-cn.github.io/07%20PBR/01%20Theory/ $[6]$ https://www.cnblogs.com/lv-anchoret/p/10223222.html","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"http://yoursite.com/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"http://yoursite.com/tags/Ray-Tracer/"}]},{"title":"软渲染器Soft Renderer：光照着色篇（完结）","slug":"SoftRenderer-Shading","date":"2019-05-05T12:39:50.871Z","updated":"2019-05-23T12:43:40.060Z","comments":true,"path":"2019/05/05/SoftRenderer-Shading/","link":"","permalink":"http://yoursite.com/2019/05/05/SoftRenderer-Shading/","excerpt":"在前面的博客我们已经实现了基本的三维渲染管线流程，这一章主要是在此基础上进行润色，不借助任何库实现obj模型导入、Bulin-Phong光照模型、摄像机漫游（第一人称摄像机、第三人称摄像机）。注意：初学者慎入","text":"在前面的博客我们已经实现了基本的三维渲染管线流程，这一章主要是在此基础上进行润色，不借助任何库实现obj模型导入、Bulin-Phong光照模型、摄像机漫游（第一人称摄像机、第三人称摄像机）。注意：初学者慎入 obj模型导入 Blinn-Phong光照着色 虚拟场景漫游 程序结果 结语 一、obj模型导入&emsp;&emsp;obj模型文件（这里不是指c++编译得到的.o中间文件）是一种格式简单、清晰的模型文件，这种模型的格式非常容易解析。目前有一个非常流行的开源的模型导入库Assimp，封装了各种各样模型文件的加载，省去很多麻烦。而我因为一方面为了尽量避免引入第三方库，另一方面obj模型的导入不难，所以自己实现了一个obj加载类$ObjModel$。实现obj模型加载并不难，只需简单了解一下obj文件的格式即可。 &emsp;&emsp;obj文件格式有类数据，一类一行，分别以v、vt、vn和f开头。用记事本打开一个简单的obj文件，如下所示： &emsp;&emsp;以v（即vertex的缩写）开头的一行分别为模型顶点的$x$、$y$、$z$坐标，以vt（即vertex texcoord的缩写）开头的一行分别为纹理坐标的$u$、$v$值，以vn（即vertex normal的缩写）开头的一行分别是法向量的$x$、$y$、$z$值。而f（即face的缩写）格式为v/vt/vn，其中对应的是各自的索引值，一个v/vt/vn描述了一个三角形顶点的顶点坐标、纹理坐标、法线向量，通常以f的一行有三列v/vt/vn，组成一个三角形面片。所以我们读取的时候按照这些开头标记读取即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394class ObjModel : public Mesh&#123;private: Vector3D minPoint, maxPoint; // Bounding box.public: // ctor/dtor. ObjModel(const std::string &amp;path); virtual ~ObjModel(); // Size setting. Vector3D setSizeToVector(float sx, float sy, float sz) const; Matrix4x4 setSizeToMatrix(float sx, float sy, float sz) const;private: // Obj file loader. void loadObjFile(const std::string &amp;path);&#125;;void ObjModel::loadObjFile(const std::string &amp;path)&#123; // obj loader. ifstream in; in.open(path, ifstream::in); if(in.fail()) &#123; std::cout &lt;&lt; \"Fail to load obj-&gt;\" &lt;&lt; path &lt;&lt; endl; &#125; string line; minPoint = Vector3D(+10000000000,+10000000000,+10000000000); maxPoint = Vector3D(-10000000000,-10000000000,-10000000000); vector&lt;Vector3D&gt; vertices; vector&lt;Vector3D&gt; normals; vector&lt;Vector2D&gt; texcoords; while(!in.eof()) &#123; getline(in, line); istringstream iss(line.c_str()); char trash; //vertex if(!line.compare(0, 2, \"v \")) &#123; iss &gt;&gt; trash; Vector3D vertex; iss &gt;&gt; vertex.x; iss &gt;&gt; vertex.y; iss &gt;&gt; vertex.z; vertices.push_back(vertex); if(minPoint.x &gt; vertex.x)minPoint.x = vertex.x; if(minPoint.y &gt; vertex.y)minPoint.y = vertex.y; if(minPoint.z &gt; vertex.z)minPoint.z = vertex.z; if(maxPoint.x &lt; vertex.x)maxPoint.x = vertex.x; if(maxPoint.y &lt; vertex.y)maxPoint.y = vertex.y; if(maxPoint.z &lt; vertex.z)maxPoint.z = vertex.z; &#125; // normal else if(!line.compare(0, 3, \"vn \")) &#123; iss &gt;&gt; trash &gt;&gt; trash; Vector3D normal; iss &gt;&gt; normal.x; iss &gt;&gt; normal.y; iss &gt;&gt; normal.z; normal.normalize(); normals.push_back(normal); &#125; // texcoord else if(!line.compare(0, 3, \"vt \")) &#123; iss &gt;&gt; trash &gt;&gt; trash; Vector2D texcoord; iss &gt;&gt; texcoord.x; iss &gt;&gt; texcoord.y; texcoords.push_back(texcoord); &#125; // face else if(!line.compare(0, 2, \"f \")) &#123; iss &gt;&gt; trash; int index[3]; while(iss &gt;&gt; index[0] &gt;&gt; trash &gt;&gt; index[1] &gt;&gt; trash &gt;&gt; index[2]) &#123; Vertex data; data.position = vertices[index[0] - 1]; data.texcoord = texcoords[index[1] - 1]; data.normal = normals[index[2] - 1]; data.color = Vector4D(1.0,1.0,1.0,1.0); m_indices.push_back(m_vertices.size()); m_vertices.push_back(data); &#125; &#125; &#125; in.close();&#125; &emsp;&emsp;可以看到这里继承了父类$Mesh$，这样读进来就作为一个网格类，能够传进渲染管线中渲染。测试读取了几个模型文件，效果如下： 二、Blin-Phong光照着色&emsp;&emsp;之前我们的着色器一直都是直接传输数据，没有做一些着色器计算，这里我们给渲染出来的模型加上光照着色。采用的光照模型是Blinn-Phong光照模型，并实现了两种着色器方法，分别是Gouraud着色、Phong着色。注意别混淆了光照模型和着色模型，光照模型是一种理论模型，着色模型则是具体的实现方式。Gouraud着色和Phong着色都是采用Blinn-Phong光照模型，差别在于两者在何处实现光照计算。 &emsp;&emsp;网上的LearnOpenGL教程很详细地介绍了Phong光照模型以及Blinn-Phong光照（Phong和Blinnn的差别只在于高光计算的一小部分），我就不再说太多这些方面的东西了，想具体了解的朋友请看这里)和这里)。概括起来，Phong光照模型包含环境光、漫反射光和镜面高光，其计算方式如下： I=K_aI_a+k_dI_ecos\\alpha+k_sI_scos^n\\lambda \\tag {1}&emsp;&emsp;其中的$k_a$、$k_d$和$k_s$分别为物体的环境光颜色、漫反射颜色和镜面高光颜色数，$n$是物体的高光读，而$I_a$、$I_e$和$I_s$是光源的环境光颜色、漫反射照亮的颜色和镜面反射的颜色。针对物体材质和光照的种类，我们创建一个$Material$和虚类$Light$，并把光照的计算过程抽象为一个函数$lighting$： 123456789101112131415161718192021222324252627282930313233class Material&#123;public: Material() = default; ~Material() = default; double m_shininess; Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_reflect; void setMaterial(Vector3D _amb, Vector3D _diff, Vector3D _spec, double _shin) &#123; m_shininess = _shin; m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; &#125;&#125;;class Light&#123;public: Light() = default; virtual ~Light() = default; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const = 0;&#125;; &emsp;&emsp;根据光源的种类不同，通常有平行光、点光源和聚束光三类（关于这类光，请看LearnOpenGL的这篇)）。平行光的特点就是光线束都是平行的，因而只需记录平行光的方向即可： 12345678910111213141516171819202122232425262728293031323334353637383940414243class DirectionalLight : public Light&#123;public: Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_direction; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const; void setDirectionalLight(Vector3D _amb, Vector3D _diff, Vector3D _spec, Vector3D _dir) &#123; m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; m_direction = _dir; m_direction.normalize(); &#125;&#125;;void DirectionalLight::lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D &amp;ambient, Vector3D &amp;diffuse, Vector3D &amp;specular) const&#123; float diff = max(normal.dotProduct(-this-&gt;m_direction), 0.0f); Vector3D halfwayDir = eyeDir + this-&gt;m_direction; halfwayDir.normalize(); float spec = pow(max(eyeDir.dotProduct(halfwayDir), 0.0f), material.m_shininess); ambient = m_ambient; diffuse = m_diffuse * diff; specular = m_specular * spec;&#125; &emsp;&emsp;点光源则需要记录光源的位置，用以计算光照的方向。与平行光还有一点不同的是，点光源通常有个照明区域范围，光照的强度随着距离的增加而削弱，且这类减弱不是线性的。因此我们还要衰减因子，把计算得到的光照颜色再乘上这个衰减因子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class PointLight : public Light&#123;public: Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_position; Vector3D m_attenuation; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const; void setPointLight(Vector3D _amb, Vector3D _diff, Vector3D _spec, Vector3D _pos, Vector3D _atte) &#123; m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; m_position = _pos; m_attenuation = _atte; &#125;&#125;;void PointLight::lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D &amp;ambient, Vector3D &amp;diffuse, Vector3D &amp;specular) const&#123; // ambient ambient = this-&gt;m_ambient; // diffuse Vector3D lightDir = (this-&gt;m_position - position); lightDir.normalize(); float diff = max(normal.dotProduct(lightDir), 0.0f); diffuse = this-&gt;m_diffuse * diff; // specular Vector3D halfwayDir = eyeDir + lightDir; halfwayDir.normalize(); float spec = pow(max(eyeDir.dotProduct(halfwayDir), 0.0f), material.m_shininess); specular = this-&gt;m_specular * spec; // attenuation float distance = (this-&gt;m_position - position).getLength(); float attenuation = 1.0 / (m_attenuation.x + m_attenuation.y * distance + m_attenuation.z * (distance * distance)); ambient *= attenuation; diffuse *= attenuation; specular *= attenuation;&#125; &emsp;&emsp;聚束光是一种比较特殊的光源（例如手电筒光、舞台灯光），它的特点就是只有在聚光方向的特定半径内的物体才会被照亮，其它的物体都会保持黑暗。我们采用一个光源位置、照明方向和切光角来描述一个聚光灯： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class SpotLight : public Light&#123;public: double m_cutoff, m_outcutoff; Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_position; Vector3D m_direction; Vector3D m_attenuation; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const; void setSpotLight(Vector3D _amb, Vector3D _diff, Vector3D _spec, double _cut, Vector3D _pos, Vector3D _dir, Vector3D _atte) &#123; m_cutoff = cos(_cut * M_PI/180.0); m_outcutoff = cos((_cut + 10.0) * M_PI/180.0); m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; m_position = _pos; m_direction = _dir; m_attenuation = _atte; m_direction.normalize(); &#125;&#125;;void SpotLight::lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D &amp;ambient, Vector3D &amp;diffuse, Vector3D &amp;specular) const&#123; // ambient ambient = this-&gt;m_ambient; // diffuse Vector3D lightDir = this-&gt;m_position - position; lightDir.normalize(); float diff = max(normal.dotProduct(lightDir), 0.0f); diffuse = this-&gt;m_diffuse * diff ; // specular Vector3D halfwayDir = eyeDir + lightDir; halfwayDir.normalize(); float spec = pow(max(eyeDir.dotProduct(halfwayDir), 0.0f), material.m_shininess); specular = this-&gt;m_specular * spec; // spotlight (soft edges) float theta = lightDir.dotProduct(-this-&gt;m_direction); float epsilon = (this-&gt;m_cutoff - this-&gt;m_outcutoff); float intensity = (theta - this-&gt;m_outcutoff) / epsilon; if(intensity &lt; 0.0f)intensity = 0.0f; if(intensity &gt; 1.0f)intensity = 1.0f; diffuse *= intensity; specular *= intensity; // attenuation float distance = (this-&gt;m_position - position).getLength(); float attenuation = 1.0 / (m_attenuation.x + m_attenuation.y * distance + m_attenuation.z * (distance * distance)); ambient *= attenuation; diffuse *= attenuation; specular *= attenuation;&#125; &emsp;&emsp;然后我们就需要把光照计算集成到着色器中，这里提供了两种方式：光照计算集成到顶点着色器，即Gouraud着色方法，逐顶点光照，然后靠线性插值得到每个像素的光照颜色；光照计算集成到片元着色器，即Phong着色法，逐像素光照，根据插值得到的法向量做相应的计算。显然前者计算量少了很多，但是后者更为真实。我们建立一个$Gouraud$着色类如下： 123456789101112131415161718192021222324252627282930313233class GouraudShader : public BaseShader&#123;private: // Those are not created by shader. const Light *m_light; // Light.(just only one) const Material *m_material; // Mesh material. const Texture2D *m_unit; // Texture unit. Vector3D m_eyePos; // Observer's position. Matrix4x4 m_modelMatrix; // Model matrix. Matrix4x4 m_viewMatrix; // View matrix. Matrix4x4 m_projectMatrix; // Projection matrix. Matrix4x4 m_invModelMatrix; // Inverse of model matrix for normal.public: // ctor/dtor. GouraudShader(); virtual ~GouraudShader() = default; // Shader stage. virtual VertexOut vertexShader(const Vertex &amp;in); virtual Vector4D fragmentShader(const VertexOut &amp;in); // Shader setting. virtual void bindShaderUnit(Texture2D *unit)&#123;m_unit = unit;&#125; virtual void setModelMatrix(const Matrix4x4 &amp;world) &#123;m_modelMatrix = world;m_invModelMatrix = m_modelMatrix.getInverseTranspose();&#125; virtual void setViewMatrix(const Matrix4x4 &amp;view)&#123;m_viewMatrix = view;&#125; virtual void setProjectMatrix(const Matrix4x4 &amp;project)&#123;m_projectMatrix = project;&#125; virtual void setMaterial(const Material *material)&#123;m_material = material;&#125; virtual void setLight(const Light *light)&#123;m_light = light;&#125; virtual void setEyePos(const Vector3D eye)&#123;m_eyePos = eye;&#125;&#125;; &emsp;&emsp;这里提一下关于顶点法向量的变换矩阵。我们目前已经有顶点的model矩阵，但是顶点做变换之后的法向量却不能直接乘上model矩阵获得。我们知道顶点的切线与法线相互垂直，因而它们的点乘为$0$，即有： N\\cdot T = N^T*T = 0 \\tag {2}&emsp;&emsp;顶点切线必然随着模型矩阵的变换而变换，即模型矩阵为$M$，因而变换后的切线$T’=M\\cdot T$。我们记变换后的法向量为$N’$，其正确的法线变换为$Q$，则$N’=Q\\cdot N$，那么变换后$N’$和$T’$应该依旧保持垂直关系，依旧有$N’\\cdot T’=(Q\\cdot N)\\cdot (M\\cdot T)=(Q\\cdot N)^T\\cdot (M\\cdot T)=N^T\\cdot (Q^T\\cdot M)\\cdot T$，与公式$(2)$对比，我们只要令$Q^T\\cdot M = I$结果为单位矩阵，则有$N’\\cdot T’=N\\cdot T = 0$。从而可得法线的变换矩阵为： Q= (N^{-1})^T \\tag {3}123456789101112131415161718192021222324252627282930313233343536373839404142434445VertexOut GouraudShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = m_modelMatrix * in.position; result.posH = m_projectMatrix * m_viewMatrix * result.posTrans; result.color = in.color; result.texcoord = in.texcoord; result.normal = m_invModelMatrix * Vector4D(in.normal); // Gouraud shading. if(m_unit) result.color = m_unit-&gt;sample(result.texcoord); Vector3D _amb, _diff, _spec; if(m_light) &#123; Vector3D eyeDir = m_eyePos - result.posTrans; eyeDir.normalize(); m_light-&gt;lighting(*m_material, result.posTrans, result.normal, eyeDir, _amb, _diff, _spec); result.color.x *= (_amb.x + _diff.x + _spec.x); result.color.y *= (_amb.y + _diff.y + _spec.y); result.color.z *= (_amb.z + _diff.z + _spec.z); result.color.w = 1.0f; &#125; // oneDivZ to correct lerp. result.oneDivZ = 1.0 / result.posH.w; result.posTrans *= result.oneDivZ; result.texcoord *= result.oneDivZ; result.color *= result.oneDivZ; return result;&#125;Vector4D GouraudShader::fragmentShader(const VertexOut &amp;in)&#123; Vector4D litColor = in.color; return litColor;&#125; &emsp;&emsp;Phong着色方式则在$fragmentShader$中实现光照计算，原理简单，不再赘述。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class PhongShader : public BaseShader&#123;private: // Those are not created by shader. const Light *m_light; // Light.(just only one) const Material *m_material; // Mesh material. const Texture2D *m_unit; // Texture unit. Vector3D m_eyePos; // Observer's position. Matrix4x4 m_modelMatrix; // Model matrix. Matrix4x4 m_viewMatrix; // View matrix. Matrix4x4 m_projectMatrix; // Projection matrix. Matrix4x4 m_invModelMatrix; // Inverse of model matrix for normal.public: // ctor/dtor PhongShader(); virtual ~PhongShader() = default; // Shader stage. virtual VertexOut vertexShader(const Vertex &amp;in); virtual Vector4D fragmentShader(const VertexOut &amp;in); // Shader setting. virtual void bindShaderUnit(Texture2D *unit)&#123;m_unit = unit;&#125; virtual void setModelMatrix(const Matrix4x4 &amp;world) &#123;m_modelMatrix = world;m_invModelMatrix = m_modelMatrix.getInverseTranspose();&#125; virtual void setViewMatrix(const Matrix4x4 &amp;view)&#123;m_viewMatrix = view;&#125; virtual void setProjectMatrix(const Matrix4x4 &amp;project)&#123;m_projectMatrix = project;&#125; virtual void setMaterial(const Material *material)&#123;m_material = material;&#125; virtual void setLight(const Light *light)&#123;m_light = light;&#125; virtual void setEyePos(const Vector3D eye)&#123;m_eyePos = eye;&#125;&#125;;VertexOut PhongShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = m_modelMatrix * in.position; result.posH = m_projectMatrix * m_viewMatrix * result.posTrans; result.color = in.color; result.texcoord = in.texcoord; result.normal = m_invModelMatrix * Vector4D(in.normal); // oneDivZ to correct lerp. result.oneDivZ = 1.0 / result.posH.w; result.posTrans *= result.oneDivZ; result.texcoord *= result.oneDivZ; result.color *= result.oneDivZ; return result;&#125;Vector4D PhongShader::fragmentShader(const VertexOut &amp;in)&#123; Vector4D litColor = in.color; // Gouraud shading. if(m_unit) litColor = m_unit-&gt;sample(in.texcoord); Vector3D _amb, _diff, _spec; if(m_light) &#123; Vector3D eyeDir = m_eyePos - in.posTrans; eyeDir.normalize(); m_light-&gt;lighting(*m_material, in.posTrans, in.normal, eyeDir, _amb, _diff, _spec); litColor.x *= (_amb.x + _diff.x + _spec.x); litColor.y *= (_amb.y + _diff.y + _spec.y); litColor.z *= (_amb.z + _diff.z + _spec.z); litColor.w = 1.0f; &#125; return litColor;&#125; &emsp;&emsp;下图分别为Phong着色方式的平行光、点光源、聚束光效果： 三、虚拟场景漫游&emsp;&emsp;虚拟场景漫游是一个三维程序必不可少的，我们比较常用的虚拟摄像机有两类：第一人称摄像机、第三人生摄像机。第三人称摄像机又称为半上帝视角，一般的rpg游戏都是采用的第三人称视角。摄像机一般都是相应键盘按键、鼠标移动、鼠标滚轮事件，为了方便描述，我们创建一个$Camera3D$虚类如下： 12345678910111213141516171819202122class Camera3D&#123;public: // Local axis. // Here LocalForward should (0,0,-1). static const Vector3D LocalForward; static const Vector3D LocalUp; static const Vector3D LocalRight; // ctor/dtor. Camera3D() = default; virtual ~Camera3D()&#123;&#125; // Getter. virtual Matrix4x4 getViewMatrix() = 0; virtual Vector3D getPosition() = 0; // Key/Mouse reaction. virtual void onKeyPress(char key) = 0; virtual void onWheelMove(double delta) = 0; virtual void onMouseMove(double deltaX, double deltaY, std::string button) = 0;&#125;; 1、第一人称相机&emsp;&emsp;LearnOpenGl的这篇)对第一人称相机的构建做了的很详细的描述。不同的是，我不再采用欧拉角来描述渲染，而是采用了四元数（关于四元数，请看知乎的这篇）。理解了四元数，采用欧拉角反而比较繁琐。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class FPSCamera : public Camera3D&#123;private: mutable bool m_dirty; // Should update or not. Vector3D m_translation; // Camera's translation. Quaternion m_rotation; // Camera's rotation. Matrix4x4 m_viewMatrix; // View Matrix.public: // ctor/dtor FPSCamera(Vector3D _pos); virtual ~FPSCamera() = default; // Getter. virtual Vector3D getPosition() &#123;return m_translation;&#125; virtual Matrix4x4 getViewMatrix(); // Key/Mouse reaction. virtual void onKeyPress(char key); virtual void onWheelMove(double delta); virtual void onMouseMove(double deltaX, double deltaY, std::string button); // Transform camera's axis. void translate(const Vector3D &amp;dt); void rotate(const Vector3D &amp;axis, float angle); void setTranslation(const Vector3D &amp;t); void setRotation(const Quaternion &amp;r); // Query for camera's axis. Vector3D forward() const; Vector3D up() const; Vector3D right() const;&#125;;void FPSCamera::onKeyPress(char key)&#123; switch(key) &#123; case 'W': this-&gt;translate(forward() * 0.2f); break; case 'S': this-&gt;translate(-forward() * 0.2f); break; case 'A': this-&gt;translate(-right() * 0.2f); break; case 'D': this-&gt;translate(+right() * 0.2f); break; case 'Q': this-&gt;translate(up() * 0.2f); break; case 'E': this-&gt;translate(-up() * 0.2f); break; default: break; &#125;&#125;void FPSCamera::onWheelMove(double delta)&#123; // nothing now.&#125;void FPSCamera::onMouseMove(double deltaX, double deltaY, std::string button)&#123; double speed = 0.1f; deltaX *= speed; deltaY *= speed; this-&gt;rotate(LocalUp, -deltaX); this-&gt;rotate(right(), -deltaY);&#125; 2、第三人称摄像机&emsp;&emsp;第三人称有一个固定的目标，这个目标通常就是玩家操控的物体。摄像机可以拉远拉近、围绕目标在$xz$平面旋转、绕$x$轴上下旋转，而且摄像机永远在玩家的上方（即俯视）。为此，我们用$distance$（摄像机到玩家的距离）、$pitch$（绕$x$轴的旋转角）、$yaw$（绕$y$轴的旋转角）来获取摄像机的位置，最后获取了摄像机的位置后我们就可以直接用$LookAt$矩阵获得视图矩阵。更多关于第三人称摄像机方面的细节请看youtube上的这个视频。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596class TPSCamera : public Camera3D&#123;private: mutable bool m_dirty; // Should update or not. Vector3D m_cameraPos; // Camera's position. Transform3D m_player; // Player's transformation. Matrix4x4 m_viewMatrix; // View matrix. double m_yaw, m_pitch, m_distance; // yaw, pitch and distance to player's space.public: // ctor/dtor. TPSCamera(Vector3D target); virtual ~TPSCamera() = default; // Getter. Matrix4x4 getPlayerMatrix(); virtual Matrix4x4 getViewMatrix(); virtual Vector3D getPosition() &#123;update();return m_cameraPos;&#125; // Key/Mouse reaction. virtual void onKeyPress(char key); virtual void onWheelMove(double delta); virtual void onMouseMove(double deltaX, double deltaY, std::string button);private: // Update view matrix. void update();&#125;;void TPSCamera::onKeyPress(char key)&#123; double speed = 2.0f; switch(key) &#123; case 'W': m_dirty = true; m_player.translate(-m_player.forward() * 0.1f); break; case 'S': m_dirty = true; m_player.translate(+m_player.forward() * 0.1f); break; case 'A': m_dirty = true; m_player.rotate(m_player.up(), +speed); break; case 'D': m_dirty = true; m_player.rotate(m_player.up(), -speed); break; &#125;&#125;void TPSCamera::onWheelMove(double delta)&#123; m_dirty = true; double speed = 0.01; m_distance += -speed * delta; if(m_distance &gt; 35.0)m_distance = 35.0; if(m_distance &lt; 5.00)m_distance = 5.0;&#125;void TPSCamera::onMouseMove(double deltaX, double deltaY, std::string button)&#123; double speed = 0.2; if(button == \"RIGHT\") &#123; m_dirty = true; m_pitch += speed * deltaY; if(m_pitch &lt; 0.0)m_pitch = 0.0; if(m_pitch &gt; 89.9)m_pitch = 89.9; &#125; else if(button == \"LEFT\") &#123; m_dirty = true; m_yaw += -speed * deltaX; fmod(m_yaw, 360.0); &#125;&#125;void TPSCamera::update()&#123; if(m_dirty) &#123; m_dirty = false; Vector3D target = m_player.translation(); float height = m_distance * sin(radians(m_pitch)); float horizon = m_distance * cos(radians(m_pitch)); Vector3D _playerRot = m_player.rotation().eulerAngle(); _playerRot.y = fmod(_playerRot.y, 360); m_cameraPos.y = target.y + height; m_cameraPos.x = target.x + horizon * sin(radians(m_yaw)); m_cameraPos.z = target.z + horizon * cos(radians(m_yaw)); m_viewMatrix.setLookAt(m_cameraPos, m_player.translation(), LocalUp); &#125;&#125; 四、程序结果 五、结语&emsp;&emsp;软渲染器的搭建就此告一段落，不借助任何图形库从零开始搭建这么一个渲染管线的初衷是为了更加深入地了解当前三维渲染的整个流程，很多理论东西需要实践才能彻底地理解。这么几天关于搭建软渲染器的折腾让我收获不少，这为以后的图形学道路打下了深厚的基础。目前我实现的软渲染管线已经包含了一个传统固定管线的基本功能，我借助一些工具统计得软渲染管线的核心代码（不包括空行、注释）共2838行。不再打算加入更多的功能特性如透明融合、阴影等等，因为没必要了。相关的全部源代码已经提交到github上，请点这里。 &emsp;&emsp;由于本人的知识水平有限，若网友发现任何bug或者博文叙述错误，欢迎指正，感谢！ 参考资料$[1]$ https://learnopengl-cn.github.io/02%20Lighting/02%20Basic%20Lighting/ $[2]$ https://learnopengl-cn.github.io/01%20Getting%20started/09%20Camera/ $[3]$ https://www.youtube.com/watch?v=PoxDDZmctnU&amp;list=PLRIWtICgwaX0u7Rf9zkZhLoLuZVfUksDP&amp;index=19 $[4]$ https://github.com/ssloy/tinyrenderer/wiki","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/tags/Soft-Renderer/"},{"name":"3D pipeline","slug":"3D-pipeline","permalink":"http://yoursite.com/tags/3D-pipeline/"}]},{"title":"软渲染器Soft Renderer：进击三维篇","slug":"SoftRenderer-3DPipeline","date":"2019-05-02T06:26:22.186Z","updated":"2019-05-23T12:43:15.290Z","comments":true,"path":"2019/05/02/SoftRenderer-3DPipeline/","link":"","permalink":"http://yoursite.com/2019/05/02/SoftRenderer-3DPipeline/","excerpt":"有了自己实现好的的3D数学库和一个基本的光栅化渲染框架，就可以开始向这个渲染框架填充内容了。本章内容主要关于3维渲染管线的实现、深度测试、背面剔除、几何裁剪、透视纹理映射，这些内容早已被渲染API集成。学习和实现这些算法，是为了彻底了解三维物体的整个渲染流程。注意：初学者慎入","text":"有了自己实现好的的3D数学库和一个基本的光栅化渲染框架，就可以开始向这个渲染框架填充内容了。本章内容主要关于3维渲染管线的实现、深度测试、背面剔除、几何裁剪、透视纹理映射，这些内容早已被渲染API集成。学习和实现这些算法，是为了彻底了解三维物体的整个渲染流程。注意：初学者慎入 进入三维世界 裁剪、剔除优化 透视纹理映射、采样 程序结果 一、进入三维世界&emsp;&emsp;尽管二维的屏幕只能显示二维的像素，但是我们可以通过将三维的物体变换到二维的屏幕上，从而渲染出三维空间的一个投影面。这与我们人类的视觉系统类似，视网膜上最终获取的也只是三维空间某个角度下的投影。为了让三维物体正确地显示到屏幕上，我们需要借助一系列的坐标空间变换。 1、坐标系统&emsp;&emsp;在渲染管线中，三维物体的顶点在最终转换为屏幕坐标之前会被变换到多个坐标系统，这其中有几个过渡性的坐标系，使得整个变换流程逻辑清晰、便于理解。此外在某些特定情况下在这些特定的坐标系中，一些操作更加容易、方便和灵活。通常，渲染管线有$5$个不同的坐标系统，分别是局部空间、世界空间、视觉空间、裁剪空间和屏幕空间，以下是LearnOpenGL CN)的原话： 局部坐标是对象相对于局部原点的坐标，也是物体起始的坐标。 下一步是将局部坐标变换为世界空间坐标，世界空间坐标是处于一个更大的空间范围的。这些坐标相对于世界的全局原点，它们会和其它物体一起相对于世界的原点进行摆放。 接下来我们将世界坐标变换为观察空间坐标，使得每个坐标都是从摄像机或者说观察者的角度进行观察的。 坐标到达观察空间之后，我们需要将其投影到裁剪坐标。裁剪坐标会被处理至-1.0到1.0的范围内，并判断哪些顶点将会出现在屏幕上。 最后，我们将裁剪坐标变换为屏幕坐标，我们将使用一个叫做视口变换(Viewport Transform)的过程。视口变换将位于-1.0到1.0范围的坐标变换到由glViewport函数所定义的坐标范围内。最后变换出来的坐标将会送到光栅器，将其转化为片段 &emsp;&emsp;通过以上的几个步骤，三维的物体坐标最终变换到了屏幕的坐标上，其中视图矩阵和投影矩阵的构建较为复杂一点，前面我的博文软渲染器Soft Renderer：3D数学篇已经推导过这两个矩阵，这里就不再赘述了。若想查看更多关于坐标系统的内容，请查看LearnOpenGL CN的这篇文章：坐标系统)。坐标变换是一般发生在顶点着色器以及顶点着色器输出到光栅化这一阶段，视口变换在顶点着色器输出之后，不在着色器中进行（视口变换已经在前面的光栅化篇提到过了）。所以为了实现坐标变换，我们的着色器要存储$model$、$view$、$project$这三个矩阵，在$SimpleShader$中添加相关的成员变量及方法： 1234567891011121314151617181920212223242526272829class SimpleShader : public BaseShader&#123;private: Matrix4x4 m_modelMatrix; Matrix4x4 m_viewMatrix; Matrix4x4 m_projectMatrix;public: ...... virtual void setModelMatrix(const Matrix4x4 &amp;world); virtual void setViewMatrix(const Matrix4x4 &amp;view); virtual void setProjectMatrix(const Matrix4x4 &amp;project);&#125;;void SimpleShader::setModelMatrix(const Matrix4x4 &amp;world)&#123; m_modelMatrix = world;&#125;void SimpleShader::setViewMatrix(const Matrix4x4 &amp;view)&#123; m_viewMatrix = view;&#125;void SimpleShader::setProjectMatrix(const Matrix4x4 &amp;project)&#123; m_projectMatrix = project;&#125; &emsp;&emsp;这样外部要渲染时，应该向着色器输入这三个矩阵。然后在我们的顶点着色器中填入相关的逻辑： 12345678910VertexOut SimpleShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = m_modelMatrix * in.position; result.posH = m_projectMatrix * m_viewMatrix * result.posTrans; result.color = in.color; result.normal = in.normal; result.texcoord = in.texcoord; return result;&#125; &emsp;&emsp;$VertexOut$是前面文章定义的顶点着色器输出的类，它存储投影后的顶点$posH$、世界空间中的顶点$posTrans$、物体的颜色、顶点法线以及纹理坐标。接着在视口变换并送入光栅化部件之前执行透视除法，即直接将裁剪空间的顶点坐标除以它的第四个分量$w$即可。然后我们在外部的渲染循环中设置模型矩阵、视图矩阵已经投影矩阵，就能显示出三维的立体感了，以我们前一章画的三角形为例（gif录制的好像有bug，出现绿色它就给我录制成这个模糊的鬼样，实际上是非常清晰，不是渲染的锅）。 &emsp;&emsp;进入3D世界，怎么能少了3D渲染的”hello world!”——立方体呢？在$Mesh.h$手动创建一个立方体的网格数据，然后用立方体替换掉上面丑陋的三角形： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154void Mesh::asBox(double width, double height, double depth)&#123; vertices.resize(24); indices.resize(36); float halfW = width * 0.5f; float halfH = height * 0.5f; float halfD = depth * 0.5f; //front vertices[0].position = Vector3D(halfW, halfH, halfD); vertices[0].normal = Vector3D(0.f, 0.f, 1.f); vertices[0].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[0].texcoord = Vector2D(1.f, 1.f); vertices[1].position = Vector3D(-halfW, halfH, halfD); vertices[1].normal = Vector3D(0.f, 0.f, 1.f); vertices[1].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[1].texcoord = Vector2D(0.f, 1.f); vertices[2].position = Vector3D(-halfW,-halfH, halfD); vertices[2].normal = Vector3D(0.f, 0.f, 1.f); vertices[2].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[2].texcoord = Vector2D(0.f, 0.f); vertices[3].position = Vector3D(halfW, -halfH, halfD); vertices[3].normal = Vector3D(0.f, 0.f, 1.f); vertices[3].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[3].texcoord = Vector2D(1.f, 0.f); //left vertices[4].position = Vector3D(-halfW, +halfH, halfD); vertices[4].normal = Vector3D(-1.f, 0.f, 0.f); vertices[4].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[4].texcoord = Vector2D(1.f, 1.f); vertices[5].position = Vector3D(-halfW, +halfH, -halfD); vertices[5].normal = Vector3D(-1.f, 0.f, 0.f); vertices[5].color = Vector4D(1.f, 1.f, 0.f, 1.f); vertices[5].texcoord = Vector2D(0.f, 1.f); vertices[6].position = Vector3D(-halfW, -halfH, -halfD); vertices[6].normal = Vector3D(-1.f, 0.f, 0.f); vertices[6].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[6].texcoord = Vector2D(0.f, 0.f); vertices[7].position = Vector3D(-halfW, -halfH, halfD); vertices[7].normal = Vector3D(-1.f, 0.f, 0.f); vertices[7].color = Vector4D(1.f, 1.f, 1.f, 1.f); vertices[7].texcoord = Vector2D(1.f, 0.f); //back vertices[8].position = Vector3D(-halfW, +halfH, -halfD); vertices[8].normal = Vector3D(0.f, 0.f, -1.f); vertices[8].color = Vector4D(1.f, 0.f, 1.f, 1.f); vertices[8].texcoord = Vector2D(0.f, 0.f); vertices[9].position = Vector3D(+halfW, +halfH, -halfD); vertices[9].normal = Vector3D(0.f, 0.f, -1.f); vertices[9].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[9].texcoord = Vector2D(1.f, 0.f); vertices[10].position = Vector3D(+halfW, -halfH, -halfD); vertices[10].normal = Vector3D(0.f, 0.f, -1.f); vertices[10].color = Vector4D(1.f, 1.f, 0.f, 1.f); vertices[10].texcoord = Vector2D(1.f, 1.f); vertices[11].position = Vector3D(-halfW, -halfH, -halfD); vertices[11].normal = Vector3D(0.f, 0.f, -1.f); vertices[11].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[11].texcoord = Vector2D(0.f, 1.f); //right vertices[12].position = Vector3D(halfW, +halfH, -halfD); vertices[12].normal = Vector3D(1.f, 0.f, 0.f); vertices[12].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[12].texcoord = Vector2D(0.f, 0.f); vertices[13].position = Vector3D(halfW, +halfH, +halfD); vertices[13].normal = Vector3D(1.f, 0.f, 0.f); vertices[13].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[13].texcoord = Vector2D(1.f, 0.f); vertices[14].position = Vector3D(halfW, -halfH, +halfD); vertices[14].normal = Vector3D(1.f, 0.f, 0.f); vertices[14].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[14].texcoord = Vector2D(1.f, 1.f); vertices[15].position = Vector3D(halfW, -halfH, -halfD); vertices[15].normal = Vector3D(1.f, 0.f, 0.f); vertices[15].color = Vector4D(1.f, 0.f, 1.f, 1.f); vertices[15].texcoord = Vector2D(0.f, 1.f); //top vertices[16].position = Vector3D(+halfW, halfH, -halfD); vertices[16].normal = Vector3D(0.f, 1.f, 0.f); vertices[16].color = Vector4D(0.f, 0.f, 0.f, 1.f); vertices[16].texcoord = Vector2D(0.f, 0.f); vertices[17].position = Vector3D(-halfW, halfH, -halfD); vertices[17].normal = Vector3D(0.f, 1.f, 0.f); vertices[17].color = Vector4D(1.f, 1.f, 0.f, 1.f); vertices[17].texcoord = Vector2D(1.f, 0.f); vertices[18].position = Vector3D(-halfW, halfH, halfD); vertices[18].normal = Vector3D(0.f, 1.f, 0.f); vertices[18].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[18].texcoord = Vector2D(1.f, 1.f); vertices[19].position = Vector3D(+halfW, halfH, halfD); vertices[19].normal = Vector3D(0.f, 1.f, 0.f); vertices[19].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[19].texcoord = Vector2D(0.f, 1.f); //down vertices[20].position = Vector3D(+halfW, -halfH, -halfD); vertices[20].normal = Vector3D(0.f, -1.f, 0.f); vertices[20].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[20].texcoord = Vector2D(0.f, 0.f); vertices[21].position = Vector3D(+halfW, -halfH, +halfD); vertices[21].normal = Vector3D(0.f, -1.f, 0.f); vertices[21].color = Vector4D(1.f, 1.f, 1.f, 1.f); vertices[21].texcoord = Vector2D(1.f, 0.f); vertices[22].position = Vector3D(-halfW, -halfH, +halfD); vertices[22].normal = Vector3D(0.f, -1.f, 0.f); vertices[22].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[22].texcoord = Vector2D(1.f, 1.f); vertices[23].position = Vector3D(-halfW, -halfH, -halfD); vertices[23].normal = Vector3D(0.f, -1.f, 0.f); vertices[23].color = Vector4D(1.f, 0.f, 1.f, 1.f); vertices[23].texcoord = Vector2D(0.f, 1.f); //front indices[0] = 0; indices[1] = 1; indices[2] = 2; indices[3] = 0; indices[4] = 2; indices[5] = 3; //left indices[6] = 4; indices[7] = 5; indices[8] = 6; indices[9] = 4; indices[10] = 6; indices[11] = 7; //back indices[12] = 8; indices[13] = 9; indices[14] = 10; indices[15] = 8; indices[16] = 10; indices[17] = 11; //right indices[18] = 12; indices[19] = 13; indices[20] = 14; indices[21] = 12; indices[22] = 14; indices[23] = 15; //top indices[24] = 16; indices[25] = 17; indices[26] = 18; indices[27] = 16; indices[28] = 18; indices[29] = 19; //down indices[30] = 20; indices[31] = 21; indices[32] = 22; indices[33] = 20; indices[34] = 22; indices[35] = 23;&#125; &emsp;&emsp;结果我们就得到一个如下面所示的奇怪的立方体： &emsp;&emsp;下面是动图gif（再重复一遍，模糊不是渲染的锅）： &emsp;&emsp;这的确有点像是一个立方体，但又有种说不出的奇怪。立方体的某些本应被遮挡住的面被绘制在了这个立方体其他面之上。出现这样结果的原因是因为我们的软渲染器是对一个一个三角形进行绘制的，而且计算像素时时直接覆盖而不管这个像素是否已经有其他值了，所以一个像素的值完全取决于最后赋予它的$RGBA$。除非渲染管线自动按照从远到近的顺序（这类算法有画家算法、空间分割BSP树算法）绘制三角形，否则直接覆盖的方法获取不了正确的像素值。正确渲染结果应该是像素的$RGBA$值为最靠近视点的片元值，一种常用的技术是借助第三维信息——深度来对每个相同位置的不同片元做深度的比较，并且取深度较低的那一个。 2、深度测试&emsp;&emsp;为了获取正确的三维渲染结果，我们采用一种深度缓冲的技术。深度缓冲存储深度信息，它的分辨率应该与颜色缓冲一致，深度值存储在每个片段里面（作为片段的z值），当片段想要输出它的颜色时，我们将它的深度值和z缓冲进行比较，如果当前的片段在其它片段之后，它将会被丢弃，否则将会覆盖。这个过程称为深度测试。在OpenGL和DirectX这些渲染API中，深度缓冲会自动执行而无需用户操作。在我们的软渲染器中，我们自己实现一个这样的深度测试，算法原理很简单，但是效果非常不错！ &emsp;&emsp;深度缓冲通常和颜色缓冲一起，作为帧缓冲的附件，我们在帧缓冲类中增加深度缓冲相关的变量、方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class FrameBuffer&#123;private: ...... std::vector&lt;double&gt; m_depthBuffer;public: ...... void clearColorAndDepthBuffer(const Vector4D &amp;color); double getDepth(const unsigned int &amp;x, const unsigned int &amp;y)const; void drawDepth(const unsigned int &amp;x, const unsigned int &amp;y, const double &amp;value);&#125;;void FrameBuffer::clearColorAndDepthBuffer(const Vector4D &amp;color)&#123; // fill the color buffer and depth buffer. unsigned char red = static_cast&lt;unsigned char&gt;(255*color.x); unsigned char green = static_cast&lt;unsigned char&gt;(255*color.y); unsigned char blue = static_cast&lt;unsigned char&gt;(255*color.z); unsigned char alpha = static_cast&lt;unsigned char&gt;(255*color.w); for(unsigned int row = 0;row &lt; m_height;++ row) &#123; for(unsigned int col = 0;col &lt; m_width;++ col) &#123; m_depthBuffer[row*m_width+col] = 1.0f; ...... &#125; &#125;&#125;double FrameBuffer::getDepth(const unsigned int &amp;x, const unsigned int &amp;y) const&#123; if(x &lt; 0 || x &gt;= m_width || y &lt; 0 || y &gt;= m_height) return 0.0f; return m_depthBuffer[y*m_width+x];&#125;void FrameBuffer::drawDepth(const unsigned int &amp;x, const unsigned int &amp;y, const double &amp;value)&#123; if(x &lt; 0 || x &gt;= m_width || y &lt; 0 || y &gt;= m_height) return; unsigned int index = y*m_width + x; m_depthBuffer[index] = value;&#125; &emsp;&emsp;然后我们对于每一个片元，我们获取深度缓冲中相应的数值并进行比较。在这之前，我们还要简单回顾一下在透视投影矩阵中深度值的非线性映射，在前面的数学篇中我们知道透视投影矩阵有如下形式： M_{projection}= \\left( \\begin{matrix} \\frac{1}{aspect*tan(fovy/2)}&0&0&0\\\\ 0&\\frac{1}{tan(fovy/2)}&0&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right)&emsp;&emsp;因而视图空间中的深度信息$z_e$和标准化设备空间中的深度信息$z_n$关系为： z_n=(-\\frac{f+n}{f-n}z_e-\\frac{2fn}{f-n})/{-z_e} =\\frac{2fn}{z_e(f-n)}+\\frac{f+n}{f-n} \\tag {1}&emsp;&emsp;可以看到$z_e$d到$z_n$是一种从$[-f, -n]$到$[-1,1]$的非线性映射。当$z_e$比较小的时候，公式$(1)$有很高的精度；当$z_e$比较大的时候，公式$(1)$应为取值精度降低。这个关系可以直观地从下图的函数曲线看出来： &emsp;&emsp;可以看到，深度值很大一部分是由很小的z值所决定的，这给了近处的物体很大的深度精度。$z_n$取值为$[-1,1]$，我们最后将其简单地映射到$[0,1]$，这一步我放在透视除法后。 123456789void Pipeline::perspectiveDivision(VertexOut &amp;target)&#123; target.posH.x /= target.posH.w; target.posH.y /= target.posH.w; target.posH.z /= target.posH.w; target.posH.w = 1.0f; // map from [-1,1] to [0,1] target.posH.z = (target.posH.z+1.0f) * 0.5f;&#125; &emsp;&emsp;在写入深度缓冲之前应该要清除上一帧的深度缓冲，全部置$1.0f$即可，我把这一步和清除颜色缓冲放一起了，即前面的帧缓冲类的$clearColorAndDepthBuffer$方法。在光栅化步骤，获取每个片元的屏幕位置，查找深度缓并比较，若小于当前深度缓冲中获取的值，则通过深度测试并写入深度缓冲。 12345678910111213141516171819202122232425262728void Pipeline::scanLinePerRow(const VertexOut &amp;left, const VertexOut &amp;right)&#123; // scan the line from left to right. VertexOut current; int length = right.posH.x - left.posH.x + 1; for(int i = 0;i &lt;= length;++i) &#123; // linear interpolation double weight = static_cast&lt;double&gt;(i)/length; current = lerp(left, right, weight); current.posH.x = left.posH.x + i; current.posH.y = left.posH.y; // depth testing. double depth = m_backBuffer-&gt;getDepth(current.posH.x, current.posH.y); if(current.posH.z &gt; depth) continue;// fail to pass the depth testing. m_backBuffer-&gt;drawDepth(current.posH.x,current.posH.y,current.posH.z); double w = 1.0/current.oneDivZ; current.posTrans *= w; current.color *= w; current.texcoord *= w; // fragment shader m_backBuffer-&gt;drawPixel(current.posH.x, current.posH.y, m_shader-&gt;fragmentShader(current)); &#125;&#125; &emsp;&emsp;然后就可以根据深度信息正确地渲染出三维的立体感了。 3、裁剪、剔除优化&emsp;&emsp;目前目前我们已经构建出三维的渲染管线，但是这还不够，因为图形渲染计算量很大，通常我们需要做一些优化。常见的嵌入在渲染管线中的优化算法有几何裁剪、背面剔除。 几何裁剪&emsp;&emsp;注意在坐标系统的变换过程中，位于视锥体内的顶点坐标各分量都会被映射到$[-1,1]$的范围内，超出视锥体的顶点则被映射到超出$[-1,1]$的范围。我们在这个基础上的做相关的裁剪，注意在透视除法之前各分量实际上是处于$[-w,w]$的范围内的，这里的$w$就是该顶点坐标的第四个分量$w$。针对线框模式渲染和填充模式渲染，我们有两种不同的裁剪算法。 Cohen-Sutherland线条裁剪算法&emsp;&emsp;一条线段在视口内的情况有如下所示的四种。其中端点完全在视口内和一端在视口内而另一端是在视口外的情况很好判断，但是线段完全在视口外就没那么简单了。可以看到线段$GH$的端点都在视口外部，但是线段的一部分却在视口的内部，这是如果直接根据两个端点是否在视口外做剔除的话会导致在边缘部分的线段直接消失，得到错误的结果。一种暴力的解法就是计算线段与视口窗口的交点，但是这并不高效。 &emsp;&emsp;Cohen-Sutherland提出了一种基于编码的判断算法，通过简单的移位、与或逻辑运算就可以判断一条线段处于哪种情况。对于每一个端点$(x,y)$，我们定义一个outcode——$b_0b_1b_2b_3$，视口所处的范围用$x_{min}$、$x_{max}$、$y_{min}$、$y_{max}$表示。每个端点$(x,y)$的outcode的计算方法如下： &emsp;&emsp;$b_0 = 1\\ if \\ y &gt; y_{max},\\ 0\\ otherwiose$ &emsp;&emsp;$b_1 = 1\\ if \\ y &lt; y_{min},\\ 0\\ otherwiose$ &emsp;&emsp;$b_2 = 1\\ if \\ x &gt; x_{min},\\ 0\\ otherwiose$ &emsp;&emsp;$b_3 = 1\\ if \\ x &lt; x_{max},\\ 0\\ otherwiose$ &emsp;&emsp;可以看出outcode将屏幕空间分成了$9$个部分： &emsp;&emsp;观察上面的$9$个区域，对于两个端点outcode1和outcode2，做如下的判断策略，其中的$OR$和$AND$是逻辑按位运算： &emsp;&emsp;若$(outcode1\\ OR\\ outcode2)==0$，那么线段就完全在视口内部； &emsp;&emsp;若$(outcode1\\ AND\\ outcode2)!=0$，那么线段就完全在视口外部； &emsp;&emsp;若$(outcode1\\ AND\\ outcode2)==0$，那么线段就可能部分在视口外部，部分在内部，还需要做进一步的判断（这里我进一步判断用了包围盒，因为比较常见和简单，就不过多描述了）。 &emsp;&emsp;这里我的实现就是只裁剪掉肯定完全在视口外部的线段，若还想裁剪掉部分外视口外部的线段则需要进一步的求交运算。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950bool Pipeline::lineCliping(const VertexOut &amp;from, const VertexOut &amp;to)&#123; // return whether the line is totally outside or not. float vMin = -from.posH.w, vMax = from.posH.w; float x1 = from.posH.x, y1 = from.posH.y; float x2 = to.posH.x, y2 = to.posH.y; int tmp = 0; int outcode1 = 0, outcode2 = 0; // outcode1 calculation. tmp = (y1&gt;vMax)?1:0; tmp &lt;&lt;= 3; outcode1 |= tmp; tmp = (y1&lt;vMin)?1:0; tmp &lt;&lt;= 2; outcode1 |= tmp; tmp = (x1&gt;vMax)?1:0; tmp &lt;&lt;= 1; outcode1 |= tmp; tmp = (x1&lt;vMin)?1:0; outcode1 |= tmp; // outcode2 calculation. tmp = (y2&gt;vMax)?1:0; tmp &lt;&lt;= 3; outcode2 |= tmp; tmp = (y2&lt;vMin)?1:0; tmp &lt;&lt;= 2; outcode2 |= tmp; tmp = (x2&gt;vMax)?1:0; tmp &lt;&lt;= 1; outcode2 |= tmp; tmp = (x2&lt;vMin)?1:0; outcode2 |= tmp; if((outcode1 &amp; outcode2) != 0) return true; // bounding box judge. Vector2D minPoint,maxPoint; minPoint.x = min(from.posH.x, to.posH.x); minPoint.y = min(from.posH.y, to.posH.y); maxPoint.x = max(from.posH.x, to.posH.x); maxPoint.y = max(from.posH.y, to.posH.y); if(minPoint.x &gt; vMax || maxPoint.x &lt; vMin || minPoint.y &gt; vMax || maxPoint.y &lt; vMin) return true; return false;&#125; 三角形裁剪&emsp;&emsp;判断三角形是否完全在外面也不能直接根据三个端点是否完全在视口外部来判断（我看有些软渲染的博主就用了这个错误的策略），因为还要考略以下的特殊情况。 &emsp;&emsp;为此，我直接计算三角形的轴向包围盒，然后这个包围盒判断三角形是否完全是视口外部。更进一步的裁剪是将部分在视口内部的三角形做求交，然后重新分割成完全在视口内部的三角形，这里我没有做进一步的裁剪。 1234567891011121314151617181920212223242526bool Pipeline::triangleCliping(const VertexOut &amp;v1, const VertexOut &amp;v2, const VertexOut &amp;v3)&#123; // true:not clip; // false: clip. float vMin = -v1.posH.w; float vMax = +v1.posH.w; // if the triangle is too far to see it, just return false. if(v1.posH.z &gt; vMax &amp;&amp; v2.posH.z &gt; vMax &amp;&amp; v3.posH.z &gt; vMax) return false; // if the triangle is behind the camera, just return false. if(v1.posH.z &lt; vMin &amp;&amp; v2.posH.z &lt; vMin &amp;&amp; v3.posH.z &lt; vMin) return false; // calculate the bounding box and check if clip or not. Vector2D minPoint,maxPoint; minPoint.x = min(v1.posH.x, min(v2.posH.x, v3.posH.x)); minPoint.y = min(v1.posH.y, min(v2.posH.y, v3.posH.y)); maxPoint.x = max(v1.posH.x, max(v2.posH.x, v3.posH.x)); maxPoint.y = max(v1.posH.y, max(v2.posH.y, v3.posH.y)); if(minPoint.x &gt; vMax || maxPoint.x &lt; vMin || minPoint.y &gt; vMax || maxPoint.y &lt; vMin) return false; return true;&#125; &emsp;&emsp;然后我们把几何裁剪放到渲染管线中，几何裁剪一般是在顶点着色器之后、光栅化之前。这里我把它放到了透视除法和视口变换之前。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364void Pipeline::drawIndex(RenderMode mode)&#123; // renderer pipeline. bool line1 = false, line2 = false, line3 = false; m_mode = mode; if(m_indices.empty()) return; for(unsigned int i = 0;i &lt; m_indices.size();i += 3) &#123; //! assembly to triangle primitive. Vertex p1,p2,p3; &#123; ...... &#125; //! vertex shader stage. VertexOut v1,v2,v3; &#123; ...... &#125; //! geometry cliping. &#123; if(m_mode == RenderMode::wire) &#123; line1 = lineCliping(v1,v2); line2 = lineCliping(v2,v3); line3 = lineCliping(v3,v1); &#125; else if(m_mode == RenderMode::fill &amp;&amp; !triangleCliping(v1,v2,v3)) continue; &#125; //! perspective division. &#123; ...... &#125; //! view port transformation. &#123; ...... &#125; //! rasterization and fragment shader stage. &#123; if(mode == RenderMode::wire) &#123; if(!line1) bresenhamLineRasterization(v1,v2); if(!line2) bresenhamLineRasterization(v2,v3); if(!line3) bresenhamLineRasterization(v3,v1); &#125; else if(mode == RenderMode::fill) &#123; edgeWalkingFillRasterization(v1,v2,v3); &#125; &#125; ...... &#125;&#125; 背面剔除&emsp;&emsp;背面剔除网上的这篇博客已经讲得非常详细了，原理也很简单，我就不过多描述。我们定义顶点逆时针的环绕顺序正面，然后通过三角形的三个顶点计算出法线，将顶点与视线做点乘并判断其符号即可。 123456789101112131415bool Pipeline::backFaceCulling(const Vector4D &amp;v1, const Vector4D &amp;v2, const Vector4D &amp;v3)&#123; // back face culling. if(m_mode == RenderMode::wire) return true; Vector4D tmp1 = v2 - v1; Vector4D tmp2 = v3 - v1; Vector3D edge1(tmp1.x, tmp1.y, tmp1.z); Vector3D edge2(tmp2.x, tmp2.y, tmp2.z); Vector3D viewRay(m_eyePos.x - v1.x, m_eyePos.y - v1.y, m_eyePos.z - v1.z); Vector3D normal = edge1.crossProduct(edge2); return normal.dotProduct(viewRay) &gt; 0;&#125; &emsp;&emsp;然后背面剔除应该放在渲染管线的顶点着色器输出之后，如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849void Pipeline::drawIndex(RenderMode mode)&#123; // renderer pipeline. bool line1 = false, line2 = false, line3 = false; m_mode = mode; if(m_indices.empty())return; for(unsigned int i = 0;i &lt; m_indices.size();i += 3) &#123; //! assembly to triangle primitive. Vertex p1,p2,p3; &#123; ...... &#125; //! vertex shader stage. VertexOut v1,v2,v3; &#123; ...... &#125; //! back face culling. &#123; if(!backFaceCulling(v1.posTrans, v2.posTrans, v3.posTrans)) continue; &#125; //! geometry cliping. &#123; ...... &#125; //! perspective division. &#123; ...... &#125; //! view port transformation. &#123; ...... &#125; //! rasterization and fragment shader stage. &#123; ...... &#125; &#125;&#125; 二、透视纹理映射、采样&emsp;&emsp;纹理映射是丰富三维物体细节的一个非常重要的方法，简单、廉价、快速，只需计算好的纹理坐标、纹理图片即可实现物体的多姿多彩。通常纹理图片的制作（除了过程式纹理的生成）由设计师完成，无需我们关心。而纹理坐标的计算则需要非常注意，送入渲染管线的纹理坐标只是逐顶点的纹理坐标，在光栅化阶段我们还要将纹理坐标做插值操作，最后根据插值后得到的纹理坐标对纹理图片采样获取片元的像素值。 1、透视纹理映射&emsp;&emsp;在光栅化阶段，我们是根据屏幕空间的$x$值和$y$值做线性插值操作获取片元的位置，而片元的纹理坐标如果也这么获得的话（这种方法叫做仿射纹理映射），将会导致严重的纹理扭曲。这是因为仿射纹理映射是基于这样的一个假设：物体空间的纹理坐标与屏幕空间的顶点坐标呈线性管线。 &emsp;&emsp;我们知道纹理坐标是定义在物体的顶点上面的，当我们根据屏幕空间的顶点坐标插值时，就默认了纹理坐标的变化与屏幕空间顶点坐标的变化是呈线性、均匀的关系的。但是问题在于：默认的屏幕空间上的线性关系，还原到世界空间中，就不是那么回事了，如下图所示。这张图是相机空间的一张俯视图。我们把一个多边形通过透视投影的方式变换到了投影平面上，图中红色的是世界空间中的多边形，蓝色的是变换到投影平面之后的多边形。可以看到，在投影平面上的蓝色线段被表示成若干个相等的单位步长线段。与此同时，投影面上单位步长的线段所对应的投影之前的红色线段的长度却不是相等的，从左到右所对应的长度依次递增。我们的纹理坐标是定义在红色的多边形上的，因此纹理坐标的增量应该是和红色线段的步长对应的。我们的线性插值却把纹理坐标增量根据蓝色线段的步长平均分配了。 &emsp;&emsp;这就导致了仿射纹理映射的错误的结果，如下图所示，仿射纹理映射产生了严重的扭曲。 &emsp;&emsp;而如果你不信，大可以试一试，然后你就会得到和我下面这张图一样奇怪的结果。 &emsp;&emsp;那么如何进行矫正了？网上的这篇博客已经非常详细地说明了相关的矫正方法，核心思想就是想办法让纹理坐标变得与屏幕空间的坐标线性相关，这一点可以看成纹理坐标的透视投影（与世界空间的顶点坐标投影到屏幕空间，从而通过插值获得其他的屏幕空间坐标进行光栅化有异曲同工之妙）。 &emsp;&emsp;纹理透视投影的详细过程请看这篇博客，其中借助的关系就是纹理坐标与世界空间顶点坐标是相关的（我们定义纹理坐标就是逐个顶点定义的），然后世界空间顶点坐标（为了便于讨论，这里世界空间就是视图空间）通过投影矩阵变成屏幕空间顶点坐标。在世界空间中，顶点的$x$和$y$值与$z$值呈线性关系（因为我们定义基本图元是三角形，在三角形平面上，必然是线性的，否则就是非线性的曲面了），即存在$A$和$B$有： x_e = Az_e+B\\\\ y_e = Az_e+B \\tag {2}&emsp;&emsp;$(x_e,y_e,z_e)$是视图空间的顶点坐标，即$(x’,y’)$是投影到近平面的顶点坐标。根据透视投影矩阵可知（其实就是相似三角形），$(x’,y’)$与视图空间的顶点坐标关系如下： \\begin{cases} x'=-N\\frac {x_e}{z_e}\\ \\to x_e= -\\frac{x'z_e}{N} \\\\ y'=-N\\frac {y_e}{z_e}\\ \\to y_e= -\\frac{y'z_e}{N} \\end{cases} \\tag {3}&emsp;&emsp;将公式$(3)$带入公式$(2)$，则有： \\begin{cases} x'=-N\\frac{B}{z_e}-AN\\\\ y'=-N\\frac{B}{z_e}-AN \\end{cases} \\tag {4}&emsp;&emsp;其中的$A$、$B$、$N$都是常量，把$\\frac 1{z_e}$看成一个整体，则通过透视投影矩阵的变换之后$x’$、$y’$均与$\\frac{1}{z_e}$成线性关系，这也就是透视投影的效果是近大远小的根本原因。然后注意到在三维空间中，纹理坐标$(s,t)$和$(x_e,y_e)$成线性关系。即有（这里只是定性分析，$A$和$B$具体多少我们不用关心）： \\begin{cases} x_e=As+B\\\\ x_e=At+B\\\\ y_e=As+B\\\\ y_e=At+B \\end{cases} \\tag {5}&emsp;&emsp;把公式$(5)$带入$(3)$则有（以公式$(5)$的第一个为例，其他类似）： As+B=-\\frac{x'z_e}{N}\\ \\to\\ A\\frac{s}{z_e}+B\\frac{1}{z_e}=-\\frac{x'}{N} \\tag {6}&emsp;&emsp;公式$(6)$彻底说明了纹理坐标与屏幕空间的顶点坐标的关系！$s$和$x’$并不是简单的线性关系，因为还出现了$\\frac{1}{z_e}$这个项，如果$\\frac{1}{z_e}$具体值已知，那么$\\frac{s}{z_e}$就与 $x’$成线性关系！那么我们在线性插值之前给纹理坐标$s$乘上一个$\\frac{1}{z_e}$，就可以根据屏幕空间的顶点坐标做线性插值了，然后对插值得到的纹理坐标$s’$乘上$z_e$就能还原出正确的纹理坐标！！！！ &emsp;&emsp;说了这么多都是在捋清函数关系，实现其实很简单的，上面已经说的很清楚了。我们在$VertexOut$中定义的变量$oneDivZ$就用于的透射投影映射的。除开纹理坐标，其他的世界空间坐标、顶点颜色、法线都是定义在世界空间的坐标顶点上的，为了得到正确的插值，都需要做与纹理坐标一样的处理。乘上$\\frac{1}{z_e}$这一步我放在了顶点着色器的最后一步，只要放在插值之前都行。 1234567891011VertexOut SimpleShader::vertexShader(const Vertex &amp;in)&#123; ..... // oneDivZ to correct mapping. result.oneDivZ = 1.0 / result.posH.w; result.posTrans *= result.oneDivZ; result.texcoord *= result.oneDivZ; result.color *= result.oneDivZ; return result;&#125; &emsp;&emsp;然后再光栅化插值之后各自乘上相应的倒数即可恢复出正确的插值结果。 123456789101112131415161718192021222324252627void Pipeline::scanLinePerRow(const VertexOut &amp;left, const VertexOut &amp;right)&#123; // scan the line from left to right. VertexOut current; int length = right.posH.x - left.posH.x + 1; for(int i = 0;i &lt;= length;++i) &#123; // linear interpolation double weight = static_cast&lt;double&gt;(i)/length; current = lerp(left, right, weight); current.posH.x = left.posH.x + i; current.posH.y = left.posH.y; // depth testing. ...... // restore. double w = 1.0/current.oneDivZ; current.posTrans *= w; current.color *= w; current.texcoord *= w; // fragment shader m_backBuffer-&gt;drawPixel(current.posH.x, current.posH.y, m_shader-&gt;fragmentShader(current)); &#125;&#125; 2、双线性纹理采样&emsp;&emsp;定义的纹理坐标都是$[0.0f,1.0f]$的浮点数，为了采样纹理我们需要把它乘上纹理的宽高转成整数的下标取访问纹理的像素矩阵。乘上纹理的宽高之后我们得到的依然应该是一个浮点数，为了获取像素下标，一个简单的方法就是向下取整（这种采样方法对应于OpenGL的GL_NEAREST纹理过滤方法）。如下所示： 12345678910double trueU = texcoord.x * (m_width - 1);double trueV = texcoord.y * (m_height - 1);x = static_cast&lt;unsigned int&gt;(trueU);y = static_cast&lt;unsigned int&gt;(trueV);int index[0] = (x * m_width + y) * m_channel;Vector3D texels;// INV_SCALE is 1.0/255texels.x = static_cast&lt;float&gt;(m_pixelBuffer[index + 0]) * INV_SCALE;texels.y = static_cast&lt;float&gt;(m_pixelBuffer[index + 1]) * INV_SCALE;texels.z = static_cast&lt;float&gt;(m_pixelBuffer[index + 2]) * INV_SCALE; &emsp;&emsp;问题就出在这里，这样直接抛弃小数点以后的值导致采样出的相邻纹理并不连续，那么用float采样行吗？答案是：不行！这边实现的采样函数是从数组取值，纹理坐标转为数组下标，数组下标不能用float只能用int，那么就没办法了吗？并不是，可以对周围纹理进行采样然后按照各自比例进行混合，这样能够提高显示效果。混合的方法就是双线性插值。所谓双线性插值，就是先后线性插值一次，共两次。即横向线性插值一次，然后根据前面一次的插值结果竖向插值一次，二维纹理是有两个维度，所以做双线性插值。 &emsp;&emsp;除了采样之外，还有一个纹理坐标溢出的问题。纹理坐标超过的$[0,1]$通常由两种处理方式，一种是$clamp$，超过$[0,1]$的地方的像素都获取边上的像素，这样效果就是拉伸。一种是$repeat$，故名思议，即重复平铺。这里我实现的是重复平铺，在计算真正的纹理下标之前做相应的判断和处理即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108class Texture2D&#123;private: int m_width; int m_height; int m_channel; unsigned char *m_pixelBuffer;public: Texture2D():m_width(0), m_height(0), m_channel(0), m_pixelBuffer(nullptr)&#123;&#125; ~Texture2D(); bool loadImage(const std::string &amp;path); Vector4D sample(const Vector2D &amp;texcoord) const;&#125;;bool Texture2D::loadImage(const std::string &amp;path)&#123; if(m_pixelBuffer)delete m_pixelBuffer; m_pixelBuffer = nullptr; m_pixelBuffer = stbi_load(path.c_str(), &amp;m_width, &amp;m_height, &amp;m_channel, 0); if(m_pixelBuffer == nullptr) &#123; qDebug() &lt;&lt; \"Failed to load image-&gt;\" &lt;&lt; QString::fromStdString(path); &#125; return m_pixelBuffer != nullptr;&#125;Vector4D Texture2D::sample(const Vector2D &amp;texcoord) const&#123; // just for rgb and rgba format. Vector4D result(0.0,0.0,0.0,1.0); if(m_pixelBuffer == nullptr) return result; unsigned int x = 0, y = 0; // for bilinear interpolation. double factorU = 0, factorV = 0; // calculate the corresponding coordinate. if(texcoord.x &gt;= 0.0f &amp;&amp; texcoord.x &lt;= 1.0f &amp;&amp; texcoord.y &gt;= 0.0f &amp;&amp; texcoord.y &lt;= 1.0f) &#123; double trueU = texcoord.x * (m_width - 1); double trueV = texcoord.y * (m_height - 1); x = static_cast&lt;unsigned int&gt;(trueU); y = static_cast&lt;unsigned int&gt;(trueV); factorU = trueU - x; factorV = trueV - y; &#125; else &#123; // repeating way. float u = texcoord.x,v = texcoord.y; if(texcoord.x &gt; 1.0f) u = texcoord.x - static_cast&lt;int&gt;(texcoord.x); else if(texcoord.x &lt; 0.0f) u = 1.0f - (static_cast&lt;int&gt;(texcoord.x) - texcoord.x); if(texcoord.y &gt; 1.0f) v = texcoord.y - static_cast&lt;int&gt;(texcoord.y); else if(texcoord.y &lt; 0.0f) v = 1.0f - (static_cast&lt;int&gt;(texcoord.y) - texcoord.y); double trueU = u * (m_width - 1); double trueV = v * (m_height - 1); x = static_cast&lt;unsigned int&gt;(trueU); y = static_cast&lt;unsigned int&gt;(trueV); factorU = trueU - x; factorV = trueV - y; &#125; // texel fetching. Vector3D texels[4]; int index[4]; index[0] = (x * m_width + y) * m_channel; index[1] = (x * m_width + y + 1) * m_channel; index[2] = ((x + 1) * m_width + y + 1) * m_channel; index[3] = ((x + 1) * m_width + y) * m_channel; // left bottom texels[0].x = static_cast&lt;float&gt;(m_pixelBuffer[index[0] + 0]) * INV_SCALE; texels[0].y = static_cast&lt;float&gt;(m_pixelBuffer[index[0] + 1]) * INV_SCALE; texels[0].z = static_cast&lt;float&gt;(m_pixelBuffer[index[0] + 2]) * INV_SCALE; //return texels[0]; // left top texels[1].x = static_cast&lt;float&gt;(m_pixelBuffer[index[1] + 0]) * INV_SCALE; texels[1].y = static_cast&lt;float&gt;(m_pixelBuffer[index[1] + 1]) * INV_SCALE; texels[1].z = static_cast&lt;float&gt;(m_pixelBuffer[index[1] + 2]) * INV_SCALE; // right top texels[2].x = static_cast&lt;float&gt;(m_pixelBuffer[index[2] + 0]) * INV_SCALE; texels[2].y = static_cast&lt;float&gt;(m_pixelBuffer[index[2] + 1]) * INV_SCALE; texels[2].z = static_cast&lt;float&gt;(m_pixelBuffer[index[2] + 2]) * INV_SCALE; // right bottom texels[3].x = static_cast&lt;float&gt;(m_pixelBuffer[index[3] + 0]) * INV_SCALE; texels[3].y = static_cast&lt;float&gt;(m_pixelBuffer[index[3] + 1]) * INV_SCALE; texels[3].z = static_cast&lt;float&gt;(m_pixelBuffer[index[3] + 2]) * INV_SCALE; // bilinear interpolation. // horizational texels[0] = texels[0] * (1.0 - factorU) + texels[3] * factorU; texels[1] = texels[1] * (1.0 - factorU) + texels[2] * factorU; //vertical result = texels[0] * (1.0 - factorV) + texels[1] *factorV; return result;&#125; &emsp;&emsp;加载图片我的用的stb_image，一个简单使用的头文件，因为加载图片不是我们的重点，所以就不造这方面的轮子了。 三、程序结果&emsp;&emsp;目前的帧率还不错hhh。 参考资料$[1]$ https://learnopengl.com/Advanced-OpenGL/Depth-testing $[2]$ https://www.cnblogs.com/pbblog/p/3484193.html $[3]$ https://learnopengl.com/Getting-started/Coordinate-Systems $[4]$ http://www.songho.ca/opengl/gl_projectionmatrix.html $[5]$ https://blog.csdn.net/popy007/article/details/5570803 $[6]$ https://learnopengl-cn.github.io/01%20Getting%20started/06%20Textures/","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/tags/Soft-Renderer/"},{"name":"3D pipeline","slug":"3D-pipeline","permalink":"http://yoursite.com/tags/3D-pipeline/"}]},{"title":"软渲染器Soft Renderer：光栅化篇","slug":"SoftRenderer-Rasterization","date":"2019-05-01T02:37:54.047Z","updated":"2019-05-23T12:43:31.730Z","comments":true,"path":"2019/05/01/SoftRenderer-Rasterization/","link":"","permalink":"http://yoursite.com/2019/05/01/SoftRenderer-Rasterization/","excerpt":"本章开始构建基于Qt平台软渲染器的初步框架，当然Qt相关的内容并不是软渲染器的重点，我只是借助Qt平台将渲染出来的像素矩阵用Qt的控件显示出来。光栅化是当今图形学渲染的一种方式，与之对应的是光线追踪渲染方式，本章我根据自己的理解着重讲述线框光栅化的Bresenham画线算法以及三角形填充光栅化的Edge-Walking算法。注意：初学者慎入。本篇相关的完整代码请看这里。","text":"本章开始构建基于Qt平台软渲染器的初步框架，当然Qt相关的内容并不是软渲染器的重点，我只是借助Qt平台将渲染出来的像素矩阵用Qt的控件显示出来。光栅化是当今图形学渲染的一种方式，与之对应的是光线追踪渲染方式，本章我根据自己的理解着重讲述线框光栅化的Bresenham画线算法以及三角形填充光栅化的Edge-Walking算法。注意：初学者慎入。本篇相关的完整代码请看这里。 渲染管线框架 光栅化算法 一、渲染管线框架&emsp;&emsp;渲染管线的搭建主要包含像素显示、网格数据封装、渲染循环、帧率fps计算、帧缓冲、着色器、渲染逻辑、光栅化等等，其中光栅化作为重点对象抽出来放在后面。当然我们不会一下子就完成渲染管线的基本功能，我们现在是要搭建一个框架，大部分的内容不用写入或者仅仅是做简单的处理，这样后面完善软渲染器的时候只需在相应的位置填写相应的代码逻辑即可。本章目标就是搭建一个渲染管线，用光栅化算法画三角形。当然，如果仅仅是画一个三角形，当然不用这么麻烦，但是我的目标是实现三维的软渲染器，深入理解三维渲染的整个流程，得从基础一步一步慢慢来。 1、像素显示的画布&emsp;&emsp;渲染器最终渲染出来的是一个像素矩阵，我们要把这个像素矩阵显示出来。显示的方法有很多，因人而异，这里我采用自己最熟悉的$Qt$来实现。显示的窗口继承一个普通的$QWidget$父类，然后我们通过重写它的$paintEvent$函数，将渲染出来的像素画到$QWidget$上。但是采用$QPainter$直接画上去的方式效率非常低，我通过查询资料得知，若想要快速地绘制给定的像素矩阵，可以利用$QImage$来实现。话不多说，上代码： 123456789101112131415class Window : public QWidget&#123; Q_OBJECTpublic: explicit Window(QWidget *parent = nullptr); ~Window();private: void paintEvent(QPaintEvent *) override;private: Ui::Window *ui; QImage *canvas;&#125;; &emsp;&emsp;接收到一帧的像素之后，在重绘事件里面利用$QImage$绘制给定的像素数组（记得调用$update$触发重绘事件）。由于篇幅原因，我不会讲太多细节方面的东西，代码也不会全部放出来，那样没意义。想看完整源代码的朋友直接去本人的github上看。 12345678910111213141516void Window::receiveFrame(unsigned char *image)&#123; if(canvas) delete canvas; canvas = new QImage(image, width(), height(), QImage::Format_RGBA8888); update();&#125;void Window::paintEvent(QPaintEvent *event)&#123; if(canvas) &#123; QPainter painter(this); painter.drawImage(0, 0, *canvas); &#125; QWidget::paintEvent(event);&#125; 2、帧缓冲类&emsp;&emsp;帧缓冲通常包含基本的颜色缓冲附件、深度缓冲附件等，这里我们暂且只实现颜色缓冲附件（四通道，格式为$RGBA$，各占一个字节），深度缓冲附件后面再加上。渲染管线最终的渲染结果是写入帧缓冲的，我们采用一个一维的单字节数组作为帧缓冲的颜色缓冲。帧缓冲的最基本的功能就是清楚缓冲区、写入像素： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162class FrameBuffer&#123;private: int m_width, m_height, m_channel; std::vector&lt;unsigned char&gt; m_colorBuffer;public: FrameBuffer(int width, int height); ~FrameBuffer() = default; int getWidth()&#123;return m_width;&#125; int getHeight()&#123;return m_height;&#125; unsigned char *getColorBuffer() &#123;return m_colorBuffer.data();&#125; void clearColorBuffer(const Vector4D &amp;color); void drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color);&#125;;FrameBuffer::FrameBuffer(int width, int height) :m_channel(4), m_width(width), m_height(height)&#123; m_colorBuffer.resize(m_width*m_height*m_channel, 255);&#125;void FrameBuffer::clearColorBuffer(const Vector4D &amp;color)&#123; // fill the color buffer. unsigned char red = static_cast&lt;unsigned char&gt;(255*color.x); unsigned char green = static_cast&lt;unsigned char&gt;(255*color.y); unsigned char blue = static_cast&lt;unsigned char&gt;(255*color.z); unsigned char alpha = static_cast&lt;unsigned char&gt;(255*color.w); for(int row = 0;row &lt; m_height;++ row) &#123; for(int col = 0;col &lt; m_width;++ col) &#123; m_colorBuffer[row*m_width*m_channel+col*m_channel + 0] = red; m_colorBuffer[row*m_width*m_channel+col*m_channel + 1] = green; m_colorBuffer[row*m_width*m_channel+col*m_channel + 2] = blue; m_colorBuffer[row*m_width*m_channel+col*m_channel + 3] = alpha; &#125; &#125;&#125;void FrameBuffer::drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color)&#123; if(x &lt; 0 || x &gt;= m_width || y &lt; 0 || y &gt;= m_height) return; unsigned char red = static_cast&lt;unsigned char&gt;(255*color.x); unsigned char green = static_cast&lt;unsigned char&gt;(255*color.y); unsigned char blue = static_cast&lt;unsigned char&gt;(255*color.z); unsigned char alpha = static_cast&lt;unsigned char&gt;(255*color.w); unsigned int index = y*m_width*m_channel + x*m_channel; m_colorBuffer[index + 0] = red; m_colorBuffer[index + 1] = green; m_colorBuffer[index + 2] = blue; m_colorBuffer[index + 3] = alpha;&#125; 3、网格顶点数据&emsp;&emsp;三维的渲染程序中的顶点数据通常包含顶点位置、顶点颜色、纹理坐标、顶点法线，然后在此基础上利用一组给定顺序的顶点数据表示一个网格，渲染时网格的数据将被送入管线进行处理。为此，有必要对顶点数据做一定的封装。 1234567891011121314class Vertex&#123;public: Vector4D position; Vector4D color; Vector2D texcoord; Vector3D normal; Vertex() = default; Vertex(Vector4D _pos, Vector4D _color, Vector2D _tex, Vector3D _normal) :position(_pos),color(_color),texcoord(_tex),normal(_normal) &#123;&#125; Vertex(const Vertex &amp;rhs) :position(rhs.position),color(rhs.color),texcoord(rhs.texcoord),normal(rhs.normal)&#123;&#125;&#125;; &emsp;&emsp;顶点数据经过顶点着色器的处理之后，会被送到下一个渲染管线的阶段处理。顶点着色器的顶点数据输出与输入有些差异，为此我们也定义一个类表示为顶点着色器的输出，这对于构建渲染管线尤为重要。 12345678910111213141516171819class VertexOut&#123;public: Vector4D posTrans; //世界变换后的坐标 Vector4D posH; //投影变换后的坐标 Vector2D texcoord; //纹理坐标 Vector3D normal; //法线 Vector4D color; //颜色 double oneDivZ; //1/z用于深度测试 VertexOut() = default; VertexOut(Vector4D _posT, Vector4D _posH, Vector2D _tex, Vector3D _normal, Vector4D _color, double _oneDivZ) :posTrans(_posT),posH(_posH),texcoord(_tex), normal(_normal),color(_color),oneDivZ(_oneDivZ) &#123;&#125; VertexOut(const VertexOut&amp; rhs) :posTrans(rhs.posTrans), posH(rhs.posH), texcoord(rhs.texcoord), normal(rhs.normal), color(rhs.color), oneDivZ(rhs.oneDivZ) &#123;&#125;&#125;; &emsp;&emsp;然后就是关于网格的表示，为了节省空间（特别是对于很大的模型），我们直接采用索引来组织网格。若想详细了解OpenGL的顶点索引概念请看这里。一个网格有两个数组，分别是$Vertex$数组和$Index$数组。下面的代码中，有一个$asTriangle$方法，这是一个三角形网格，调用这个方法之后网格存储的就是一个三角形，用于后面的光栅化调试，光栅化的基本单元就是三角形。通常情况，所有的网格模型都可以用一定数量的三角形构成，因而我们实现的软渲染器的基本图元就是三角形。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class Mesh&#123;public: std::vector&lt;Vertex&gt; vertices; std::vector&lt;unsigned int&gt; indices; Mesh() = default; ~Mesh() = default; Mesh(const Mesh&amp; mesh) :vertices(mesh.vertices), indices(mesh.indices)&#123;&#125; Mesh&amp; operator=(const Mesh&amp; mesh) &#123; if (&amp;mesh == this) return *this; vertices = mesh.vertices; indices = mesh.indices; return *this; &#125; void setVertices(Vertex* _vs, int count) &#123; vertices.resize(count); new(&amp;vertices[0])std::vector&lt;Vertex&gt;(_vs, _vs + count); &#125; void setIndices(int* _es, int count) &#123; indices.resize(count); new(&amp;indices)std::vector&lt;unsigned int&gt;(_es, _es + count); &#125; void asBox(double width, double height, double depth); void asTriangle(const Vector3D p1, const Vector3D p2, const Vector3D p3);&#125;;void Mesh::asTriangle(Vector3D p1, Vector3D p2, Vector3D p3)&#123; vertices.resize(3); indices.resize(3); vertices[0].position = p1; vertices[0].normal = Vector3D(0.f, 0.f, 1.f); vertices[0].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[0].texcoord = Vector2D(0.f, 0.f); vertices[1].position = p2; vertices[1].normal = Vector3D(0.f, 0.f, 1.f); vertices[1].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[1].texcoord = Vector2D(1.f, 0.f); vertices[2].position = p3; vertices[2].normal = Vector3D(0.f, 0.f, 1.f); vertices[2].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[2].texcoord = Vector2D(0.5f, 1.f); indices[0] = 0; indices[1] = 1; indices[2] = 2;&#125; 4、简单的着色器&emsp;&emsp;着色器方面时软渲染中较为高级的内容，目前我们只是搭建一个框架，因而着色器不需要什么复杂的操作，只需简单地传递数据就行了。博主实现的软渲染器只包含必不可少的顶点着色器和片元着色器，目前的顶点着色器将顶点原封不动地输出，片元着色器也是如此，这样我们后面要实现光照效果的时候直接在着色器里写上就行了。为了更加有条理，我们设计一个着色器的虚类，这样实现不同效果的着色器时我们直接继承这个虚类即可。 123456789101112class BaseShader&#123;public: BaseShader() = default; virtual ~BaseShader() = default; virtual VertexOut vertexShader(const Vertex &amp;in) = 0; virtual Vector4D fragmentShader(const VertexOut &amp;in) = 0; virtual void setModelMatrix(const Matrix4x4 &amp;world) = 0; virtual void setViewMatrix(const Matrix4x4 &amp;view) = 0; virtual void setProjectMatrix(const Matrix4x4 &amp;project) = 0;&#125;; 12345678910111213141516171819202122232425262728293031323334353637383940414243class SimpleShader : public BaseShader&#123;public: SimpleShader() = default; virtual ~SimpleShader() = default; virtual VertexOut vertexShader(const Vertex &amp;in); virtual Vector4D fragmentShader(const VertexOut &amp;in); virtual void setModelMatrix(const Matrix4x4 &amp;world); virtual void setViewMatrix(const Matrix4x4 &amp;view); virtual void setProjectMatrix(const Matrix4x4 &amp;project);&#125;;VertexOut SimpleShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = in.position; result.posH = in.position; result.color = in.color; result.normal = in.normal; result.oneDivZ = 1.0; result.texcoord = in.texcoord; return result;&#125;Vector4D SimpleShader::fragmentShader(const VertexOut &amp;in)&#123; Vector4D litColor; litColor = in.color; return litColor;&#125;void SimpleShader::setModelMatrix(const Matrix4x4 &amp;world)&#123;&#125;void SimpleShader::setViewMatrix(const Matrix4x4 &amp;view)&#123;&#125;void SimpleShader::setProjectMatrix(const Matrix4x4 &amp;project)&#123;&#125; &emsp;&emsp;可以看到$SimpleShader$仅仅是将顶点数据直接输出，不进行任何处理。 5、搭建基本的渲染管线&emsp;&emsp;目前我们已经有了一些渲染管线的基本组件，现在就需要把这些组件串起来。首先是渲染循环的问题，$Qt$有它自己的事件循环，而且主线程的事件循环要尽量避免大量的运算（否则UI控件会陷入未响应），因此将渲染循环放到子线程里是一个不错的渲染，这样也可以避免我们的软渲染逻辑与$Qt$的接口耦合得太高。 渲染线程&emsp;&emsp;$Qt$提供了$QThread$类构建线程，我采用的方式为：渲染循环类继承$QObject$，然后调用$moveToThread$番方法挂到子线程上运行，最后将线程的启动信号与$loop$渲染循环关联即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class RenderLoop : public QObject&#123; Q_OBJECTpublic: explicit RenderLoop(int w, int h, QObject *parent = nullptr); ~RenderLoop(); void stopIt() &#123;stoped = true;&#125; void setFpsZero()&#123;fps = 0;&#125; int getFps()&#123;return fps;&#125;signals: void frameOut(unsigned char *image);public slots: void loop();private: bool stoped; int fps; int width, height, channel;&#125;;RenderLoop::RenderLoop(int w, int h, QObject *parent) : QObject(parent), width(w), height(h), channel(4)&#123; fps = 0; stoped = false;&#125;RenderLoop::~RenderLoop()&#123;&#125;void RenderLoop::loop()&#123; // pipeline initialization ...... // fps counting. fps = 0; while(!stoped) &#123; // render logic ...... ++ fps; &#125;&#125; &emsp;&emsp;然后在主窗口中创建$RenderLoop$对象，挂到$QThread$上启动。此外还有一点要注意的是在子线程中最好不用使用$QTimer$类，因此我在主窗口中创建$QTimer$类，设定为每秒触发，触发时主线程读取子线程的$fps$，这样就达到了显示帧率的目的了。 12345678910111213141516171819202122232425262728293031在Window类声明处：private: QTimer *timer; QThread *loopThread; RenderLoop *loop;在Window类构造函数处： loop = new RenderLoop(width(), height(), nullptr); loopThread = new QThread(this); // fps counting. timer = new QTimer(); connect(timer,&amp;QTimer::timeout,this,&amp;Window::fpsTimeOut); // render thread. loop-&gt;moveToThread(loopThread); connect(loopThread,&amp;QThread::finished,loop, &amp;RenderLoop::deleteLater); connect(loopThread,&amp;QThread::started,loop,&amp;RenderLoop::loop); connect(loop,&amp;RenderLoop::frameOut,this,&amp;Window::receiveFrame); // begin the thread. loopThread-&gt;start(); timer-&gt;start(1000);Window的其他函数：void Window::fpsTimeOut()&#123; int fps = loop-&gt;getFps(); loop-&gt;setFpsZero(); this-&gt;setWindowTitle(QString(\" fps: %1\").arg(fps));&#125; 渲染流程&emsp;&emsp;回顾一下$OpenGL$的渲染流程（这里只考虑一般的情况，即不包含几何着色器、细分着色器等），首先外部处理网格，将网格顶点数据和网格顶点索引送入渲染管线，设置基本图元（如三角形）、渲染方式（如线框模式）。渲染管线的第一阶段为顶点着色器阶段（在这之前还有个缓冲清理阶段），顶点着色器对网格数据逐顶点处理（包含坐标空间变换、投影变换等等），随之输出。然后渲染管线对输出的顶点数据进行裁剪，送入光栅化部件，计算几何图元覆盖的像素点，其中进行了大量的线性插值操作。接着片元着色器获取光栅化后的像素，对每个像素做颜色计算等，然后输出颜色数据、深度数据，最后根据这些缓冲数据做深度测试。 &emsp;&emsp;所以一个最基本的渲染管线应该有如下几个步骤： &emsp;&emsp;初始化（如缓冲区创建）$\\to$输入顶点缓冲、索引缓冲$\\to$清除缓冲区$\\to$设置着色器、渲染方式$\\to$绘制$\\to$交换双缓冲$\\to$输出。根据这些步骤，创建$Pipeline$类如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192class Pipeline&#123;private: int m_width, m_height; // width and height of viewport. BaseShader *m_shader; // shaders including vertex shader and fragment shader. FrameBuffer *m_frontBuffer; FrameBuffer *m_backBuffer; Matrix4x4 viewPortMatrix; // viewport transformation matrix. std::vector&lt;Vertex&gt; m_vertices; // vertex buffer. std::vector&lt;unsigned int&gt; m_indices;// index buffer.public: Pipeline(int width, int height); ~Pipeline(); void initialize(); void clearBuffer(const Vector4D &amp;color, bool depth = false); void setVertexBuffer(const std::vector&lt;Vertex&gt; &amp;vertices)&#123;m_vertices = vertices;&#125; void setIndexBuffer(const std::vector&lt;unsigned int&gt; &amp;indices)&#123;m_indices = indices;&#125; void setShaderMode(ShadingMode mode); void drawIndex(RenderMode mode); void swapBuffer(); unsigned char *output()&#123;return m_frontBuffer-&gt;getColorBuffer();&#125;&#125;;Pipeline::Pipeline(int width, int height) :m_width(width),m_height(height) ,m_shader(nullptr),m_frontBuffer(nullptr) ,m_backBuffer(nullptr)&#123;&#125;Pipeline::~Pipeline()&#123; if(m_shader)delete m_shader; if(m_frontBuffer)delete m_frontBuffer; if(m_backBuffer)delete m_backBuffer; m_shader = nullptr; m_frontBuffer = nullptr; m_backBuffer = nullptr;&#125;void Pipeline::initialize()&#123; if(m_frontBuffer) delete m_frontBuffer; if(m_backBuffer) delete m_backBuffer; if(m_shader) delete m_shader; viewPortMatrix.setViewPort(0,0,m_width,m_height); m_frontBuffer = new FrameBuffer(m_width, m_height); m_backBuffer = new FrameBuffer(m_width, m_height); m_shader = new SimpleShader();&#125;void Pipeline::drawIndex(RenderMode mode)&#123; 输入顶点着色器; 光栅化; 输入片元着色器; 写入缓冲区;&#125;void Pipeline::clearBuffer(const Vector4D &amp;color, bool depth)&#123; (void)depth; m_backBuffer-&gt;clearColorBuffer(color);&#125;void Pipeline::setShaderMode(ShadingMode mode)&#123; if(m_shader)delete m_shader; if(mode == ShadingMode::simple) m_shader = new SimpleShader(); else if(mode == ShadingMode::phong) ;&#125;void Pipeline::swapBuffer()&#123; FrameBuffer *tmp = m_frontBuffer; m_frontBuffer = m_backBuffer; m_backBuffer = tmp;&#125; &emsp;&emsp;注意到我创建了帧缓冲，分别是$m_frontBuffer$和$m_backBuffer$，前者存储着当前显示的像素，后者缓冲区用于写入像素。这就是著名的双缓冲原理，可以避免画面的闪烁、撕裂等现象。除此之外，还有一个值得特别说明的就是视口变换矩阵$viewPortMatrix$，这个一般很少见到，因为被内嵌在了渲染管线里面了。经过投影变换、透视除法操作之后，顶点数据都在标准化设备空间中，即$x$轴、$y$轴、$z$轴取值范围为$[-1,1]$。但是屏幕的像素坐标范围并非如此，通常屏幕的$x$轴坐标范围为$[0,width]$，$y$轴坐标范围为$[0,height]$，屏幕像素坐标原点在左上角，$x$轴正向朝右，$y$轴正向朝下，所以我们还要把标准化设备坐标顶点数据变换到屏幕的坐标范围中，这就是视口变换（$z$轴一般保持不变）。视口变换矩阵的构造并没有难度，因为这仅仅是简单的线性映射，因此不再赘述。视口变换矩阵如下所示： viewPortMatrix= \\left[ \\begin{matrix} \\frac{w}{2}&0&0&s_x+\\frac{w}{2}\\\\ 0&-\\frac{h}{2}&0&s_y+\\frac{h}{2}\\\\ 0&0&1&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {1}&emsp;&emsp;其中$(s_x,s_y)$是视口左上角的坐标，$(w,h)$为屏幕的宽度和高度。 12345678void Matrix4x4::setViewPort(int left, int top, int width, int height)&#123; loadIdentity(); entries[0] = static_cast&lt;float&gt;(width)/2.0f; entries[5] = -static_cast&lt;float&gt;(height)/2.0f; entries[12] = static_cast&lt;float&gt;(left)+static_cast&lt;float&gt;(width)/2.0f; entries[13] = static_cast&lt;float&gt;(top)+static_cast&lt;float&gt;(height)/2.0f;&#125; &emsp;&emsp;$Pipeline$还有个非常重要的函数$drawIndex$，它是渲染管线的核心部分，涉及到了图元装配、顶点着色器调度、光栅化、片元着色器调度、写入帧缓冲这几个重要的步骤。我们实现的软渲染器几何图元默认为三角形，所以图元装配就是每三个顶点装成一个图元。 123456789101112131415161718192021222324252627282930313233343536373839void Pipeline::drawIndex(RenderMode mode)&#123; if(m_indices.empty())return; for(unsigned int i = 0;i &lt; m_indices.size()/3;++ i) &#123; //! vertices assembly to triangle primitive Vertex p1,p2,p3; &#123; p1 = m_vertices[3*i+0]; p2 = m_vertices[3*i+1]; p3 = m_vertices[3*i+2]; &#125; //! vertex shader stage. VertexOut v1,v2,v3; &#123; v1 = m_shader-&gt;vertexShader(p1); v2 = m_shader-&gt;vertexShader(p2); v3 = m_shader-&gt;vertexShader(p3); &#125; //! rasterization and fragment shader stage. &#123; v1.posH = viewPortMatrix * v1.posH; v2.posH = viewPortMatrix * v2.posH; v3.posH = viewPortMatrix * v3.posH; if(mode == RenderMode::wire) &#123; // bresenham rasterization &#125; else if(mode == RenderMode::fill) &#123; // edge walking rasterization &#125; &#125; &#125;&#125; &emsp;&emsp;有了以上的$Pipeline$函数，我们的渲染循环逻辑的一般形式如下： 1234567891011while(!stoped)&#123; pipeline-&gt;clearBuffer(Vector4D(0.502f,0.698f,0.800f,1.0f)); pipeline-&gt;drawIndex(RenderMode::fill); pipeline-&gt;swapBuffer(); emit frameOut(pipeline-&gt;output()); ++ fps;&#125; 二、光栅化算法&emsp;&emsp;顶点着色器处理的还是一个个离散的几何顶点，在顶点着色器之后我们还需要进行光栅化操作，将几何覆盖的屏幕像素计算出来，送入片元着色器计算每个点的像素数据。光栅化一般有两种模式：一种是线框模式，即只描绘几何的边；二是填充模式，即将几何的面片全部填充完。Bresenham算法是经典的描线算法，它采用迭代的形式将所需的算术操作降低到最少。除此之外还有DDA描线算法，效率上不如Bresenham算法，所以我没有实现。 1、Bresenham描线算法&emsp;&emsp;我们要描绘的是从$(x_0,y_0)$到$(x_1,y_1)$的一条直线线段。一些数学符号标记如下： \\Delta x= x_1-x_0>0,\\ \\Delta y=y_1-y_0>0,\\ m=\\frac{\\Delta y}{\\Delta x}&emsp;&emsp;其中$m$即直线线段的斜率，为了便于讨论，我们假设$|m|\\leq 1$，其他情况很容易推广。 &emsp;&emsp;在如上的情况下，Bresenham算法从$x=x_0$开始，每次将$x$坐标值加一，然后推算相应的$y$坐标值。记第$i$次迭代获得的点为$(x_i,y_i)$。那么第$i+1$次迭代时获取的点就在$(\\overline x_i+1,\\overline y_i)$和$(\\overline x_i+1,\\overline y_i+1)$这两个中选取。那如何判断应该选哪个呢？即选择这两个点之一的判断标准是什么？直观上，我们应该选取距离的直线线段在该$y$轴上的交点最近的点，如下图1所示。 图1 判别标准 &emsp;&emsp;直线的一般表达式为$y=mx+B$，$m$为直线的斜率，那么$(x_{i+1},y_{i+1})$表示为如下（注意$y_{i+1}$表示的是直线在$x_{i+1}$上真正的$y$值）： x_{i+1}=x_i+1\\\\ y_{i+1}=mx_{i+1}+B=m(x_i+1)+B \\tag {2} 图2 交点到右边的点、右上的点的距离 &emsp;&emsp;故$d_{upper}$和$d_{lower}$的取值如下： d_{upper}=\\overline y_i+1-\\overline y_{i+1}=\\overline y_i+1-m\\overline x_{i+1}-B\\\\ d_{lower}=y_{i+1}-\\overline y_i=mx_{i+1}+B-\\overline y_i \\tag {3}&emsp;&emsp;显然，如果$d_{lower}-d_{upper}&gt;0$，则应该取右上方的点；如果$d_{lower}-d_{upper}0$的符号。 d_{lower}-d_{upper}=m(x_i+1)+B-\\overline y_i-(\\overline y_i+1-m(x_i+1)-B)\\\\ =2m(x_i+1)-2\\overline y_i+2B-1 \\tag {4}&emsp;&emsp;式$(4)$中的$m$是直线的斜率，因此将式$(4)$作为判断标准需要做非常昂贵的浮点数除法运算。为了消去除法，注意到$m=\\frac{\\Delta y}{\\Delta x}$，两边同时乘上$\\Delta x&gt;0$，正负符号不变。 p_i=\\Delta x\\cdot (d_{lower}-d_{upper}) =2\\Delta y\\cdot(x_i+1)-2\\Delta x\\cdot \\overline y_i+(2B-1)\\Delta x\\\\ =2\\Delta y\\cdot x_i-2\\Delta x\\cdot\\overline y_i+c\\\\ where \\ \\ c=(2B-1)\\Delta x+2\\Delta y \\tag {5}&emsp;&emsp;所以可以用$p_i$的符号作为选取的标准。但是，式$(5)$的计算能够进一步简化，考虑$p_i$和$p_{i+1}$（注意我们根据$p_i$的符号来选取$\\overline y_{i+1}$）： p_{i+1}-p_{i} = (2\\Delta y\\cdot x_{i+1}-2\\Delta x\\cdot\\overline y_{i+1}+c) - (2\\Delta y\\cdot x_i-2\\Delta x\\cdot\\overline y_i+c) \\\\= 2\\Delta y-2\\Delta x(\\overline y_{i+1}-\\overline y_i) \\tag {6}&emsp;&emsp;若$p_i\\leq 0$，那么选择右边的点，此时$\\overline y_{i+1}=\\overline y_i$，那么有： p_{i+1}=p_i+2\\Delta y \\tag {7}&emsp;&emsp;若$p_i&gt;0$，那么选择右上角的点，此时$\\overline y_{i+1}=\\overline y_i+1$，那么有： p_{i+1}=p_i+2\\Delta y-2\\Delta x \\tag {8}&emsp;&emsp;所以我们可以根据$p_i$的符号快速计算出$p_{i+1}$的符号，如此迭代下去： Bresenham Algorithm: $draw (x_0, y_0);$ Calculate $\\Delta x$,$\\Delta y$,$2\\Delta y$,$2\\Delta y-2\\Delta x$,$p_0=2\\Delta y-\\Delta x$; for $x$ from $x_0$ to $x_1$: &emsp;&emsp;if $p_i\\leq 0$ &emsp;&emsp;&emsp;&emsp;draw $(x_{i+1},\\overline y_{i+1})=(x_i+1,\\overline y_i)$ ; &emsp;&emsp;&emsp;&emsp;compute $p_{i+1}=p_i+2\\Delta y$; &emsp;&emsp;if $p_i &gt; 0$ &emsp;&emsp;&emsp;&emsp;draw $(x_{i+1},\\overline y_{i+1})=(x_i+1,\\overline y_i+1)$ ; &emsp;&emsp;&emsp;&emsp;compute $p_{i+1}=p_i+2\\Delta y-2\\Delta x$; &emsp;&emsp;$x += 1;$ &emsp;&emsp;上面我们讨论的都是$|m|1$的情况呢？其实这是对称的，这时把$x$看成$y$，把$y$看成$x$即可。另外，当$\\Delta x &lt;0$时，我们的$x$不是递增$1$，而是递减$1$，具体实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465void Pipeline::bresenhamLineRasterization(const VertexOut &amp;from, const VertexOut &amp;to)&#123; int dx = to.posH.x - from.posH.x; int dy = to.posH.y - from.posH.y; int stepX = 1, stepY = 1; // judge the sign if(dx &lt; 0) &#123; stepX = -1; dx = -dx; &#125; if(dy &lt; 0) &#123; stepY = -1; dy = -dy; &#125; int d2x = 2*dx, d2y = 2*dy; int d2y_minus_d2x = d2y - d2x; int sx = from.posH.x; int sy = from.posH.y; VertexOut tmp; // slope &lt; 1. if(dy &lt;= dx) &#123; int flag = d2y - dx; for(int i = 0;i &lt;= dx;++ i) &#123; // linear interpolation tmp = lerp(from, to, static_cast&lt;double&gt;(i)/dx); // fragment shader m_backBuffer-&gt;drawPixel(sx,sy,m_shader-&gt;fragmentShader(tmp)); sx += stepX; if(flag &lt;= 0) flag += d2y; else &#123; sy += stepY; flag += d2y_minus_d2x; &#125; &#125; &#125; // slope &gt; 1. else &#123; int flag = d2x - dy; for(int i = 0;i &lt;= dy;++ i) &#123; // linear interpolation tmp = lerp(from, to, static_cast&lt;double&gt;(i)/dy); // fragment shader m_backBuffer-&gt;drawPixel(sx,sy,m_shader-&gt;fragmentShader(tmp)); sy += stepY; if(flag &lt;= 0) flag += d2x; else &#123; sx += stepX; flag -= d2y_minus_d2x; &#125; &#125; &#125;&#125; 2、Edge-Walking三角形填充算法&emsp;&emsp;三角形光栅化填充对输入给定的三个三角形顶点，计算这个三角区域覆盖的所有像素。三角形填充的光栅化算法有很多种，这里仅实现了Edge-Walking算法，此外还有Edge-Equation算法。关于Edge-Walking算法的前世今生我不再赘述了，这个算法的思路比较简单，但是实现起来比较麻烦一点。 &emsp;&emsp;话不多少，直接上伪代码（懒得自己写了伪代码了）： &emsp;&emsp;大致的思想就是从上往下（或从下往上）扫描，获取每对$X_L$、$X_R$，然后在$[X_L,X_R]$范围内从左到右扫描。显然就是双重循环。一般，我们的三角形光栅化对象有如下四种情况： 图3 四类三角形 &emsp;&emsp;先来看平底三角形的情况，如下图4所示。显然，平底三角形很容易地实现从下往上扫面，竖直方向上仅需考虑左右两条边。当然这里有个问题，就是如何确定$X_L$和$X_R$？如果直接采用算法伪代码中的利用$dx/dy$迭代获取$X$值，因为$X$值是整数，而$dx/dy$是浮点数，当$dx/dy&lt;1$时，把$dx/dy$加到$X$上面计算机对整数类型坐标自动向下取整，结果相当于没加。（即便是浮点数类型，最终也要取整，因为屏幕空间的像素坐标必须是整数） 图4 平底三角形 &emsp;&emsp;一种解决方案就是线性插值，算法从下往上扫描时，$y-=1$，我们根据当前的$y$值来获取$x$值： X_L = (1.0f-\\frac{y1-y}{y1-y0})*x1+\\frac{y1-y}{y1-y0}*x0 \\\\ X_y = (1.0f-\\frac{y2-y}{y2-y0})*x2+\\frac{y2-y}{y2-y0}*x0&emsp;&emsp;平顶的三角形光栅化亦类似，不再赘述。那么除了平底和平顶的情况之外，我们该如何处理其余的情况？一个技巧就是将其他情况的三角形分割乘一个平底三角形、一个平顶三角形，如下图所示： 图5 三角形分割 &emsp;&emsp;这样我们通过调用平底三角形光栅化方法、平顶三角形光栅化方法即可实现一般情况的三角形光栅化： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114void Pipeline::scanLinePerRow(const VertexOut &amp;left, const VertexOut &amp;right)&#123; VertexOut current; int length = right.posH.x - left.posH.x + 1; for(int i = 0;i &lt;= length;++i) &#123; // linear interpolation double weight = static_cast&lt;double&gt;(i)/length; current = lerp(left, right, weight); current.posH.x = left.posH.x + i; current.posH.y = left.posH.y; // fragment shader m_backBuffer-&gt;drawPixel(current.posH.x, current.posH.y, m_shader-&gt;fragmentShader(current)); &#125;&#125;void Pipeline::rasterTopTriangle(VertexOut &amp;v1, VertexOut &amp;v2, VertexOut &amp;v3)&#123; VertexOut left = v2; VertexOut right = v3; VertexOut dest = v1; VertexOut tmp, newleft, newright; if(left.posH.x &gt; right.posH.x) &#123; tmp = left; left = right; right = tmp; &#125; int dy = left.posH.y - dest.posH.y + 1; for(int i = 0;i &lt; dy;++i) &#123; double weight = 0; if(dy != 0) weight = static_cast&lt;double&gt;(i)/dy; newleft = lerp(left, dest, weight); newright = lerp(right, dest, weight); newleft.posH.y = newright.posH.y = left.posH.y - i; scanLinePerRow(newleft, newright); &#125;&#125;void Pipeline::rasterBottomTriangle(VertexOut &amp;v1, VertexOut &amp;v2, VertexOut &amp;v3)&#123; VertexOut left = v1; VertexOut right = v2; VertexOut dest = v3; VertexOut tmp, newleft, newright; if(left.posH.x &gt; right.posH.x) &#123; tmp = left; left = right; right = tmp; &#125; int dy = dest.posH.y - left.posH.y + 1; for(int i = 0;i &lt; dy;++i) &#123; double weight = 0; if(dy != 0) weight = static_cast&lt;double&gt;(i)/dy; newleft = lerp(left, dest, weight); newright = lerp(right, dest, weight); newleft.posH.y = newright.posH.y = left.posH.y + i; scanLinePerRow(newleft, newright); &#125;&#125;void Pipeline::edgeWalkingFillRasterization(const VertexOut &amp;v1, const VertexOut &amp;v2, const VertexOut &amp;v3)&#123; // split the triangle into two part VertexOut tmp; VertexOut target[3] = &#123;v1, v2,v3&#125;; if(target[0].posH.y &gt; target[1].posH.y) &#123; tmp = target[0]; target[0] = target[1]; target[1] = tmp; &#125; if(target[0].posH.y &gt; target[2].posH.y) &#123; tmp = target[0]; target[0] = target[2]; target[2] = tmp; &#125; if(target[1].posH.y &gt; target[2].posH.y) &#123; tmp = target[1]; target[1] = target[2]; target[2] = tmp; &#125; // bottom triangle if(equal(target[0].posH.y,target[1].posH.y)) &#123; rasterBottomTriangle(target[0],target[1],target[2]); &#125; // top triangle else if(equal(target[1].posH.y,target[2].posH.y)) &#123; rasterTopTriangle(target[0], target[1], target[2]); &#125; // split it. else &#123; double weight = static_cast&lt;double&gt;(target[1].posH.y-target[0].posH.y)/(target[2].posH.y-target[0].posH.y); VertexOut newPoint = lerp(target[0],target[2],weight); newPoint.posH.y = target[1].posH.y; rasterTopTriangle(target[0], newPoint, target[1]); rasterBottomTriangle(newPoint,target[1],target[2]); &#125;&#125; 三、程序结果&emsp;&emsp;最终，不借用任何图形接口通过自己实现的光栅化算法画出了三角形： 参考资料$[1]$ https://blog.csdn.net/cppyin/article/details/6232453 $[2]$ https://blog.csdn.net/y1196645376/article/details/78937614 $[3]$ https://blog.csdn.net/y1196645376/article/details/78907914","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/tags/Soft-Renderer/"},{"name":"Rasterization","slug":"Rasterization","permalink":"http://yoursite.com/tags/Rasterization/"}]},{"title":"软渲染器Soft Renderer：3D数学篇","slug":"SoftRenderer-Math","date":"2019-05-01T02:37:49.425Z","updated":"2019-05-05T13:13:44.571Z","comments":true,"path":"2019/05/01/SoftRenderer-Math/","link":"","permalink":"http://yoursite.com/2019/05/01/SoftRenderer-Math/","excerpt":"本章开始博主将手动搭建一个渲染管线，深入理解3D渲染的整个流程。线性代数中的向量和矩阵是计算机图形学的常客，深入理解和掌握对于图形渲染有着非常重要的意义，本节主要是关于3D数学库的内容。","text":"本章开始博主将手动搭建一个渲染管线，深入理解3D渲染的整个流程。线性代数中的向量和矩阵是计算机图形学的常客，深入理解和掌握对于图形渲染有着非常重要的意义，本节主要是关于3D数学库的内容。 向量 矩阵 一、向量&emsp;&emsp;$n$维向量本质就是一个$n$元组，从几何意义上来说，向量是有大小和方向的有向线段。向量的大小就是向量的长度（模）向量有非负的长度，而向量的方向描述了空间中向量的指向。向量的相关内容高中就已涉及，因此不再赘述。若想要重新深入了解相关内容，可以查看这个地址。 &emsp;&emsp;图形渲染中通常使用的向量为$2$到$4$维，如下分别是$2$维、$3$维、$4$维向量类的常用方法，主要是运算操作符重载以及点乘、叉乘、模、标准化、线性插值等基本操作。向量的内容简单，没什么要特别说明的。 1、2D向量类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Vector2D&#123;public: float x,y; // constructors Vector2D():x(0.0f), y(0.0f) &#123;&#125; Vector2D(float newX, float newY):x(newX), y(newY)&#123;&#125; Vector2D(const float * rhs):x(*rhs), y((*rhs)+1) &#123;&#125; Vector2D(const Vector2D &amp; rhs):x(rhs.x), y(rhs.y)&#123;&#125; ~Vector2D() = default; // setter,getter void set(float newX, float newY)&#123;x=newX;y=newY; &#125; void setX(float newX) &#123;x = newX;&#125; void setY(float newY) &#123;y = newY;&#125; float getX() const &#123;return x;&#125; float getY() const &#123;return y;&#125; // normalization void normalize(); Vector2D getNormalize()const; // length float getLength() const &#123; return static_cast&lt;float&gt;(sqrt(x*x + y*y));&#125; float getSquaredLength()const&#123;return static_cast&lt;float&gt;(x*x + y*y);&#125; // overloaded operators Vector2D operator+(const Vector2D &amp;rhs) const &#123;return Vector2D(x + rhs.x, y + rhs.y);&#125; Vector2D operator-(const Vector2D &amp;rhs) const &#123;return Vector2D(x - rhs.x, y - rhs.y);&#125; Vector2D operator*(const float rhs) const &#123;return Vector2D(x*rhs, y*rhs);&#125; Vector2D operator/(const float rhs) const &#123;return (rhs==0) ? Vector2D(0.0f, 0.0f) : Vector2D(x / rhs, y / rhs);&#125; bool operator==(const Vector2D &amp;rhs) const &#123;return (equal(x,rhs.x) &amp;&amp; equal(y,rhs.y));&#125; bool operator!=(const Vector2D &amp;rhs) const &#123;return !((*this)==rhs);&#125; void operator+=(const Vector2D &amp;rhs)&#123;x+=rhs.x; y+=rhs.y;&#125; void operator-=(const Vector2D &amp;rhs)&#123;x-=rhs.x; y-=rhs.y;&#125; void operator*=(const float rhs)&#123;x*=rhs;y*=rhs;&#125; void operator/=(const float rhs)&#123;if(!equal(rhs, 0.0))&#123;x/=rhs;y/=rhs;&#125;&#125; Vector2D operator-() const &#123;return Vector2D(-x, -y);&#125; Vector2D operator+() const &#123;return *this;&#125; // interpolation Vector2D lerp(const Vector2D &amp;v2,const float factor)const &#123;return (*this)*(1.0f - factor) + v2*factor;&#125; Vector2D quadraticInterpolate(const Vector2D &amp; v2, const Vector2D &amp; v3, const float factor) const &#123;return (*this)*(1.0f-factor)*(1.0f-factor) + v2*2.0f*factor*(1.0f-factor) + v3*factor*factor;&#125;&#125;; 2、3D向量类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Vector3D&#123;public: float x,y,z; // constructors Vector3D():x(0.0f), y(0.0f), z(0.0f)&#123;&#125; Vector3D(float newX, float newY, float newZ):x(newX), y(newY), z(newZ)&#123;&#125; Vector3D(const float * rhs):x(*rhs), y(*(rhs+1)), z(*(rhs+2))&#123;&#125; Vector3D(const Vector3D &amp;rhs):x(rhs.x), y(rhs.y), z(rhs.z)&#123;&#125; ~Vector3D() = default; // setter,getter void set(float newX, float newY, float newZ)&#123;x=newX;y=newY;z=newZ;&#125; void setX(float newX) &#123;x = newX;&#125; void setY(float newY) &#123;y = newY;&#125; void setZ(float newZ) &#123;z = newZ;&#125; float getX() const &#123;return x;&#125; float getY() const &#123;return y;&#125; float getZ() const &#123;return z;&#125; // normalization void normalize(); Vector3D getNormalized() const; // length caculation float getLength() const &#123;return static_cast&lt;float&gt;(sqrt(x*x+y*y+z*z));&#125; float getSquaredLength() const &#123;return x*x+y*y+z*z;&#125; // product float dotProduct(const Vector3D &amp;rhs) const &#123;return x*rhs.x + y*rhs.y + z*rhs.z;&#125; Vector3D crossProduct(const Vector3D &amp;rhs) const &#123;return Vector3D(y*rhs.z - z*rhs.y, z*rhs.x - x*rhs.z, x*rhs.y - y*rhs.x);&#125; // linear interpolation Vector3D lerp(const Vector3D &amp;v2, float factor) const &#123;return (*this)*(1.0f-factor) + v2*factor;&#125; Vector3D QuadraticInterpolate(const Vector3D &amp;v2, const Vector3D &amp;v3, float factor) const &#123;return (*this)*(1.0f-factor)*(1.0f-factor) + v2*2.0f*factor*(1.0f-factor) + v3*factor*factor;&#125; // overloaded operators Vector3D operator+(const Vector3D &amp;rhs) const &#123;return Vector3D(x + rhs.x, y + rhs.y, z + rhs.z);&#125; Vector3D operator-(const Vector3D &amp;rhs) const &#123;return Vector3D(x - rhs.x, y - rhs.y, z - rhs.z);&#125; Vector3D operator*(const float rhs) const &#123;return Vector3D(x*rhs, y*rhs, z*rhs);&#125; Vector3D operator/(const float rhs) const &#123;return (equal(rhs,0.0f))?Vector3D(0.0f, 0.0f, 0.0f):Vector3D(x/rhs, y/rhs, z/rhs);&#125; bool operator==(const Vector3D &amp;rhs) const &#123;return (equal(x,rhs.x) &amp;&amp; equal(y,rhs.y) &amp;&amp; equal(z,rhs.z));&#125; bool operator!=(const Vector3D &amp;rhs) const &#123;return !((*this)==rhs);&#125; void operator+=(const Vector3D &amp;rhs) &#123;x+=rhs.x;y+=rhs.y;z+=rhs.z;&#125; void operator-=(const Vector3D &amp; rhs) &#123;x-=rhs.x;y-=rhs.y;z-=rhs.z;&#125; void operator*=(const float rhs)&#123;x*=rhs;y*=rhs;z*=rhs;&#125; void operator/=(const float rhs)&#123;if(!equal(rhs,0.0f))&#123;x/=rhs; y/=rhs; z/=rhs;&#125;&#125; Vector3D operator-() const &#123;return Vector3D(-x, -y, -z);&#125; Vector3D operator+() const &#123;return *this;&#125;&#125;; 3、4D向量类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Vector4D&#123;public: float x,y,z,w; // constructors Vector4D():x(0.0f), y(0.0f), z(0.0f), w(0.0f)&#123;&#125; Vector4D(float newX, float newY, float newZ, float newW):x(newX), y(newY), z(newZ), w(newW)&#123;&#125; Vector4D(const float * rhs):x(*rhs), y(*(rhs+1)), z(*(rhs+2)), w(*(rhs+3))&#123;&#125; Vector4D(const Vector4D &amp;rhs):x(rhs.x), y(rhs.y), z(rhs.z), w(rhs.w)&#123;&#125; Vector4D(const Vector3D &amp; rhs): x(rhs.x), y(rhs.y), z(rhs.z), w(1.0f)&#123;&#125; ~Vector4D() = default; // setter,getter void set(float newX, float newY, float newZ, float newW)&#123;x=newX;y=newY;z=newZ;w=newW;&#125; void setX(float newX) &#123;x = newX;&#125; void setY(float newY) &#123;y = newY;&#125; void setZ(float newZ) &#123;z = newZ;&#125; void setW(float newW) &#123;w = newW;&#125; float getX() const &#123;return x;&#125; float getY() const &#123;return y;&#125; float getZ() const &#123;return z;&#125; float getW() const &#123;return w;&#125; // product float dotProduct(const Vector4D &amp;rhs) const &#123;return x*rhs.x + y*rhs.y + z*rhs.z + w*rhs.w;&#125; // linear interpolation Vector4D lerp(const Vector4D &amp;v2, float factor) const &#123;return (*this)*(1.0f-factor) + v2*factor;&#125; Vector4D QuadraticInterpolate(const Vector4D &amp;v2, const Vector4D &amp;v3, float factor) const &#123;return (*this)*(1.0f-factor)*(1.0f-factor)+v2*2.0f*factor*(1.0f-factor)+v3*factor*factor;&#125; // overloaded operators Vector4D operator+(const Vector4D &amp;rhs) const &#123;return Vector4D(x+rhs.x, y+rhs.y, z+rhs.z, w+rhs.w);&#125; Vector4D operator-(const Vector4D &amp;rhs) const &#123;return Vector4D(x-rhs.x, y-rhs.y, z-rhs.z, w-rhs.w);&#125; Vector4D operator*(const float rhs) const &#123;return Vector4D(x*rhs, y*rhs, z*rhs, w*rhs);&#125; Vector4D operator/(const float rhs) const &#123;return (equal(rhs,0.0f))?Vector4D(0.0f, 0.0f, 0.0f, 0.0f):Vector4D(x/rhs, y/rhs, z/rhs, w/rhs);&#125; bool operator==(const Vector4D &amp;rhs) const &#123;return (equal(x,rhs.x)&amp;&amp;equal(y,rhs.y)&amp;&amp;equal(z,rhs.z)&amp;&amp;equal(w,rhs.w));&#125; bool operator!=(const Vector4D &amp;rhs) const &#123;return !((*this)==rhs);&#125; void operator+=(const Vector4D &amp;rhs) &#123;x+=rhs.x;y+=rhs.y;z+=rhs.z;w+=rhs.w;&#125; void operator-=(const Vector4D &amp; rhs) &#123;x-=rhs.x;y-=rhs.y;z-=rhs.z;w-=rhs.w;&#125; void operator*=(const float rhs)&#123;x*=rhs;y*=rhs;z*=rhs;w*=rhs;&#125; void operator/=(const float rhs)&#123;if(!equal(rhs,0.0f))&#123;x/=rhs; y/=rhs; z/=rhs; w/=rhs;&#125;&#125; Vector4D operator-() const &#123;return Vector4D(-x, -y, -z, -w);&#125; Vector4D operator+() const &#123;return *this;&#125;&#125;; 二、矩阵&emsp;&emsp;矩阵本质就是向量的进一步扩展的，一个$n\\times m$的矩阵可看成$n$个$m$维行向量组成或者$m$个$n$维列向量组成，关于矩阵的基本概念、操作请看这里。通常我们采用方阵来描述线性变换。所谓线性变换，即变换之后保留了直线而不被弯曲，平行线依然平行，原点没有变化，但其他的几何性质如长度、角度、面积和体积可能被变换改变了。直观来说，线性变换可能“拉伸”坐标系，但不会“弯曲”或“卷折”坐标系。 &emsp;&emsp;矩阵在计算机中有行主序存储、列主序存储两种方式，行主序存储即按照顺序逐行存储，列主序存储则按照顺序逐列存储。图形学渲染中我们通常采用的是列主序的方式，以下的讨论都是列主序的矩阵存储方式。那么矩阵是如何变换向量的？ &emsp;&emsp;向量在几何上能被解释成一系列与轴平行的位移，一般来说，任意向量$\\vec v$都能写成如下的形式： \\vec v=\\left[\\begin{matrix}x\\\\y\\\\z\\end{matrix}\\right]=\\left[\\begin{matrix}x\\\\0\\\\0\\end{matrix}\\right]+\\left[\\begin{matrix}0\\\\y\\\\0\\end{matrix}\\right]+\\left[\\begin{matrix}0\\\\0\\\\z\\end{matrix}\\right]=x\\left[\\begin{matrix}1\\\\0\\\\0\\end{matrix}\\right]+y\\left[\\begin{matrix}0\\\\1\\\\0\\end{matrix}\\right]+z\\left[\\begin{matrix}0\\\\0\\\\1\\end{matrix}\\right] \\tag {1}&emsp;&emsp;公式$(1)$右边的单位向量就是$x$、$y$、$z$轴方向的向量，向量的每个坐标都表明了平行于相应坐标轴的有向位移。我们记$\\vec p$、$\\vec q$、$\\vec r$分别为公式$(1)$中右边的$x$、$y$、$z$轴的单位列向量，则有： \\vec v=x\\vec p+y\\vec q+z\\vec r=\\left[\\begin{matrix}\\vec p &\\vec q&\\vec r\\end{matrix}\\right]\\left[\\begin{matrix}x \\\\y\\\\z\\end{matrix}\\right] \\tag {2}&emsp;&emsp;向量$\\vec v$就变成了向量$\\vec p$、$\\vec q$、$\\vec r$的线性表示，向量$\\vec p$、$\\vec q$、$\\vec r$称作基向量。以上仅仅讨论的是笛卡尔坐标系，但更通用的情况是，一个$3$维坐标系能用任意$3$个线性无关的基向量表示，以列向量$\\vec p$、$\\vec q$、$\\vec r$构建$3\\times 3$的矩阵$M$： M=\\left[\\begin{matrix}\\vec p &\\vec q&\\vec r\\end{matrix}\\right]=\\left[\\begin{matrix}p_x &q_x&r_x\\\\p_y &q_y&r_y\\\\p_z &q_z&r_z\\end{matrix}\\right] \\tag {3}&emsp;&emsp;结合公式$(2)$和公式$(3)$，即有： \\vec v=M\\left[\\begin{matrix}x \\\\y\\\\z\\end{matrix}\\right] \\tag{4}&emsp;&emsp;坐标系变换矩阵的每一列（如果是行主序，就是每一行）都是该坐标系的基向量，一个点$v$右乘该矩阵就相当于执行了一次坐标系转换。求解线性变换矩阵的关键就是根据当前的坐标系求解变换之后的坐标系的基向量，然后将基向量填入向量位置！ &emsp;&emsp;一个矩阵类通常有如下方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class Matrix4x4&#123;public: float entries[16]; // constructors Matrix4x4()&#123;loadIdentity();&#125; Matrix4x4(float e0, float e1, float e2, float e3, float e4, float e5, float e6, float e7, float e8, float e9, float e10,float e11, float e12,float e13,float e14,float e15); Matrix4x4(const float *rhs); Matrix4x4(const Matrix4x4 &amp;rhs); ~Matrix4x4() = default; // setter,getter void setEntry(int position, float value); float getEntry(int position) const; Vector4D getRow(int position) const; Vector4D getColumn(int position) const; void loadIdentity(); void loadZero(); // overloaded operators Matrix4x4 operator+(const Matrix4x4 &amp; rhs) const; Matrix4x4 operator-(const Matrix4x4 &amp; rhs) const; Matrix4x4 operator*(const Matrix4x4 &amp; rhs) const; Matrix4x4 operator*(const float rhs) const; Matrix4x4 operator/(const float rhs) const; bool operator==(const Matrix4x4 &amp; rhs) const; bool operator!=(const Matrix4x4 &amp; rhs) const; void operator+=(const Matrix4x4 &amp; rhs); void operator-=(const Matrix4x4 &amp; rhs); void operator*=(const Matrix4x4 &amp; rhs); void operator*=(const float rhs); void operator/=(const float rhs); Matrix4x4 operator-() const; Matrix4x4 operator+() const &#123;return (*this);&#125; Vector4D operator*(const Vector4D rhs) const; // inverse, transpose void inverted(); Matrix4x4 getInverse() const; void transpose(); Matrix4x4 getTranspose() const; void invertTranspose(); Matrix4x4 getInverseTranspose() const; // operation on space void setTranslation(const Vector3D &amp; translation); void setScale(const Vector3D &amp; scaleFactor); void setRotationAxis(const double angle, const Vector3D &amp; axis); void setRotationX(const double angle); void setRotationY(const double angle); void setRotationZ(const double angle); void setRotationEuler(const double angleX, const double angleY, const double angleZ); void setPerspective(float fovy, float aspect, float near, float far); void setPerspective(float left, float right, float bottom, float top, float near, float far); void setOrtho(float left, float right, float bottom, float top, float near, float far);&#125;; 1、线性变换、仿射变换&emsp;&emsp;满足$F(a+b)=F(a)+F(b)$和$F(ka)=kF(a)$的映射$F(a)$就是线性的。对于映射$F(a)=Ma$，当$M$为任意方阵时，也可以说明$F$映射是一个线性变换。在计算机图形学中，缩放、旋转的变换操作都是线性的，但是平移不是线性变换。 &emsp;&emsp;具有$v’=Mv’+b$形式的变换都是仿射变换。平移作为最常用的变换之一，然而却不是线性变换；所以为了包括平移变换提出了仿射变换。仿射变换是指线性变换后接着平移。因此，仿射变换的集合是线性变换的超集，任何线性变换都是仿射变换，但不是所有的仿射变换都是线性变换。为了统一用矩阵表示低维度的仿射变换，我们可以通过高维度的线性变换来完成，为此引入了$4$维齐次坐标。（当然引入第$4$维$w$还有其他的用途，如当$w=0$时，可解释为无穷远的“点”，其意义是描述方向），关于齐次坐标的更多内容请查看这里。 &emsp;&emsp;从而，对于高维度来说只是经历了一次切变+投影变换就可以实现低维度的平移（更多内容查看这里），在$3D$渲染中，我们采用$4\\times 4$的矩阵做相应的变换。关于平移和缩放不再赘述： 123456789101112131415void Matrix4x4::setTranslation(const Vector3D &amp;translation)&#123; loadIdentity(); entries[12] = translation.x; entries[13] = translation.y; entries[14] = translation.z;&#125;void Matrix4x4::setScale(const Vector3D &amp;scaleFactor)&#123; loadIdentity(); entries[0] = scaleFactor.x; entries[5] = scaleFactor.y; entries[10] = scaleFactor.z;&#125; 2、绕任意轴旋转&emsp;&emsp;在3D中，绕坐标轴旋转，而不是绕点旋转，此时首先需要定义的是何为旋转正方向： 左手坐标系中定义此方向的规则为左手法则。首先，要明确旋转轴指向哪个方向。当然，旋转轴在理论上是无限延伸的，但我们还是要认为它有正端点和负端点。与笛卡尔坐标轴定义坐标系相同，左手法则是这样的:伸出左手，大拇指向上，其余手指弯曲。大拇指指向旋转轴的正方向，此时，四指弯曲的方向就是旋转的正方向。右手坐标系则根据右手法则利用右手判断旋转正方向，本文讨论的是常见的右手坐标系。 &emsp;&emsp;在旋转变换中，一个常见的特殊情况就是绕$x$轴、绕$y$轴、绕$z$轴旋转，这类的旋转矩阵求解比较简单，只需牢牢记住列主序矩阵的列向量就是变换后的坐标系的基向量即可快速推导出相应的旋转矩阵： R_x(\\theta)=\\left[ \\begin{matrix} 1&0&0\\\\ 0&cos\\theta&-sin\\theta\\\\ 0&sin\\theta&cos\\theta \\end{matrix}\\right] \\\\ R_y(\\theta)=\\left[\\begin{matrix}cos\\theta&0&sin\\theta\\\\0&1&0\\\\-sin\\theta&0&cos\\theta \\end{matrix}\\right]\\\\ R_z(\\theta)=\\left[\\begin{matrix}cos\\theta&-sin\\theta&0\\\\ sin\\theta&cos\\theta&0\\\\0&0&1\\end{matrix}\\right] \\tag {5}1234567891011121314151617181920212223242526void Matrix4x4::setRotationX(const double angle)&#123; loadIdentity(); entries[5] = static_cast&lt;float&gt;(cos(M_PI*angle/180)); entries[6] = static_cast&lt;float&gt;(sin(M_PI*angle/180)); entries[9] = -entries[6]; entries[10] = entries[5];&#125;void Matrix4x4::setRotationY(const double angle)&#123; loadIdentity(); entries[0] = static_cast&lt;float&gt;(cos(M_PI*angle/180)); entries[2] = -static_cast&lt;float&gt;(sin(M_PI*angle/180)); entries[8] = -entries[2]; entries[10] = entries[0];&#125;void Matrix4x4::setRotationZ(const double angle)&#123; loadIdentity(); entries[0] = static_cast&lt;float&gt;(cos(M_PI*angle/180)); entries[1] = static_cast&lt;float&gt;(sin(M_PI*angle/180)); entries[4] = -entries[1]; entries[5] = entries[0];&#125; &emsp;&emsp;但是更一般的情况是绕任意轴进行旋转，构建这样的矩阵稍微有点麻烦，我们接下来就做一些绕任意轴旋转的矩阵构建推到。在这里我们不考虑平移，因而围绕旋转的轴一定是通过原点的。如下图1所示，将$\\vec v$旋转到$\\vec v ‘$，任意轴用单位向量$\\vec n$表示，绕$\\vec n$旋转$\\theta$角度的矩阵记为$R(\\vec n, \\theta)$，$\\vec v’$是向量绕轴$\\vec n$旋转后的向量，即$\\vec v’=R(\\vec n,\\theta)\\vec v$。 图1 绕任意轴旋转 &emsp;&emsp;我们的目标就是用$\\vec v$、$\\vec n$和$\\theta$来表示$\\vec v’$，从而构造出$R(\\vec n, \\theta)$。首先将$\\vec v$分解成平行于$\\vec n$的向量$\\vec v_{||}$和垂直于$\\vec n$的分量$\\vec v_{⊥}$，而$\\vec v’_{⊥}$是垂直于$\\vec n$的分向量。注意，$\\vec n$是单位向量，但$\\vec v$不是单位向量，可得$\\vec v$在$\\vec n$方向的投影向量$\\vec v_{||}$为： \\vec v_{||}=(\\vec v\\cdot\\vec n)\\vec n \\tag {6}&emsp;&emsp;从而根据$\\vec v_{||}$和$\\vec v$可知$\\vec v_{⊥}$和$w$，$w$是垂直于$\\vec n$和$\\vec v_{⊥}$的向量： \\vec v_{⊥}=\\vec v-\\vec v_{||} \\tag {7} w=\\vec n \\times \\vec v_{⊥} = \\vec n\\times (\\vec v-\\vec v_{||})\\\\ =\\vec n\\times\\vec v-\\vec n\\times\\vec v_{||}=\\vec n\\times\\vec v-0=\\vec n\\times \\vec v \\tag{8}&emsp;&emsp;$\\vec w$和$\\vec v_{⊥}$相互垂直，$\\vec w$、$\\vec v_{⊥}$和$\\vec v’_{⊥}$在同一个平面上，$\\vec v’_{⊥}$和$\\vec v_{⊥}$的夹角为$\\theta$，从而$\\vec v’_{⊥}$可由$\\vec w$和$\\vec v_{⊥}$线性表示为： \\vec v'_{⊥}=cos\\theta\\vec v_{⊥}+sin\\theta\\vec w\\\\ =cos\\theta(\\vec v-(\\vec v\\cdot\\vec n)\\vec n)+sin\\theta(\\vec n\\times\\vec v)\\tag {9}&emsp;&emsp;最后，根据公式$(6)$和公式$(9)$我们已知$\\vec v_{||}$和$\\vec v’_{⊥}$，从而可以得出$\\vec v’$： \\vec v'=\\vec v_{||}+\\vec v'_{⊥}\\\\ =cos\\theta(\\vec v-(\\vec v\\cdot\\vec n)\\vec n)+sin\\theta(\\vec n\\times\\vec v)+(\\vec v\\cdot\\vec n)\\vec n \\tag {10}&emsp;&emsp;由公式$(10)$可知，我们已经用$\\vec v$、$\\vec n$和$\\theta$表示$\\vec v’$，那如何根据上述的公式$(10)$构建旋转矩阵$R(\\vec n, \\theta)$？还是那个思路：列主序变换矩阵的列向量就是变换后的坐标系的基向量。我们只需求出笛卡尔坐标系的$\\vec x$、$\\vec y$、$\\vec z$三个轴方向上的基向量按照公式$(10)$旋转之后的基向量$\\vec x’$、$\\vec y’$、$\\vec z’$，然后填入矩阵$R(\\vec n, \\theta)$即可，以$\\vec x=[1\\ \\ 0 \\ \\ 0]^T$为例： \\vec x'=cos\\theta(\\vec x-(\\vec x\\cdot\\vec n)\\vec n)+sin\\theta(\\vec n\\times\\vec x)+(\\vec x\\cdot\\vec n)\\vec n =\\left[\\begin{matrix} n^2_x(1-cos\\theta)+cos\\theta \\\\n_xn_y(1-cos\\theta)+n_zsin\\theta \\\\n_xn_z(1-cos\\theta)-n_ysin\\theta) \\end{matrix}\\right] \\tag {11}&emsp;&emsp;$\\vec y=[0\\ \\ 1\\ \\ 0]^T$和$\\vec z=[0\\ \\ 0\\ \\ 1]^T$同理： \\vec y' =\\left[\\begin{matrix} n_xn_y(1-cos\\theta)-n_zsin\\theta \\\\n^2_y(1-cos\\theta)+cos\\theta \\\\n_yn_z(1-cos\\theta)+n_xsin\\theta \\end{matrix}\\right] \\tag {12} \\vec z' =\\left[\\begin{matrix} n_xn_z(1-cos\\theta)+n_ysin\\theta \\\\n_yn_z(1-cos\\theta)-n_xsin\\theta \\\\n^2_z(1-cos\\theta)+cos\\theta \\end{matrix}\\right] \\tag {13}&emsp;&emsp;将$\\vec x’$、$\\vec y’$、$\\vec z’$合并到$R(\\vec n, \\theta)$中： R(\\vec n, \\theta) =\\left[\\begin{matrix} \\vec x'&\\vec y'&\\vec z' \\end{matrix}\\right] \\\\=\\begin{bmatrix} {n_x}^2(1-cos\\theta)+cos\\theta&n_xn_y(1-cos\\theta)-n_zsin\\theta&n_xn_z(1-cos\\theta)+n_ysin\\theta \\\\n_xn_y(1-cos\\theta)+n_zsin\\theta&n^2_y(1-cos\\theta)+cos\\theta&n_yn_z(1-cos\\theta)-n_xsin\\theta \\\\n_xn_z(1-cos\\theta)-n_ysin\\theta)&n_yn_z(1-cos\\theta)+n_xsin\\theta&n^2_z(1-cos\\theta)+cos\\theta \\end{bmatrix} \\tag {14}12345678910111213141516171819202122void Matrix4x4::setRotationAxis(const double angle, const Vector3D &amp;axis)&#123; Vector3D u = axis.getNormalized(); float sinAngle = static_cast&lt;float&gt;(sin(M_PI*angle/180)); float cosAngle = static_cast&lt;float&gt;(cos(M_PI*angle/180)); float oneMinusCosAngle = 1.0f - cosAngle; loadIdentity(); entries[0] = (u.x)*(u.x) + cosAngle*(1-(u.x)*(u.x)); entries[4] = (u.x)*(u.y)*(oneMinusCosAngle) - sinAngle*u.z; entries[8] = (u.x)*(u.z)*(oneMinusCosAngle) + sinAngle*u.y; entries[1] = (u.x)*(u.y)*(oneMinusCosAngle) + sinAngle*u.z; entries[5] = (u.y)*(u.y) + cosAngle*(1-(u.y)*(u.y)); entries[9] = (u.y)*(u.z)*(oneMinusCosAngle) - sinAngle*u.x; entries[2] = (u.x)*(u.z)*(oneMinusCosAngle) - sinAngle*u.y; entries[6] = (u.y)*(u.z)*(oneMinusCosAngle) + sinAngle*u.x; entries[10] = (u.z)*(u.z) + cosAngle*(1-(u.z)*(u.z));&#125; 3、透视投影、正交投影&emsp;&emsp;$3D$空间中的物体最终都要通过投影显示到$2D$的屏幕上，这一过程就是投影变换。投影变换矩阵将视图空间中的顶点数据变换到裁剪空间，裁剪空间中的顶点最后通过透视除法被变换到标准化设备坐标（$NDC$）。通常由两类投影：透视投影、正交投影。 透视投影矩阵&emsp;&emsp;关于透视投影矩阵的前世今生我不过多说，直接上透视投影矩阵的推导过程。一个视锥体我们目前用六个参数表示：$left$，$right$，$bottom$，$top$，$near$，$far$，简写为$l$、$r$、$b$、$t$、$n$和$f$，即视锥体的六个面。我们的目标就是将视图空间中在视锥体内的点变换到标准化设备坐标中的立方体内。即$x$轴方向从$[l,r]$映射到$[-1,1]$，$y$轴方向从$[b,t]$映射到$[-1,1]$，$z$轴方向从$[-n,-f]$映射到$[-1,1]$。 &emsp;&emsp;可能你会觉得奇怪，$z$轴方向为什么是从$[-n,-f]$映射到$[-1,1]$？这是因为摄像机空间的坐标系是右手坐标系，在视图空间中摄像机是朝向视图坐标系的$z$轴的负方向，如下图左边所示，$+Y$、$+Z$、$+X$标准摄像机坐标系的三个轴，而摄像机的观察视锥体是朝向$-Z$方向的。而$NDC$又是左手坐标系，朝向$+Z$方向，所以我们要取负。 图2 透视投影视锥和标准化设备坐标 图3 从-Y方向看去的视锥横截面 图4 从-X方向看去的视锥横截面 &emsp;&emsp;在视锥体中的顶点$(x_e,y_e,z_e)$被投影到视锥体的近平面，近平面上的点我们记为$(x_p,y_p,-n)$。如图3和图4所示，根据三角形相似的原理，我们有： \\frac{x_p}{x_e}=\\frac{-n}{z_e}\\ \\rightarrow\\ x_p=\\frac{-n\\cdot x_e}{z_e}=\\frac{n\\cdot x_e}{-z_e} \\tag {15} \\frac{y_p}{y_e}=\\frac{-n}{y_e}\\ \\rightarrow\\ y_p=\\frac{-n\\cdot y_e}{z_e}=\\frac{n\\cdot y_e}{-z_e} \\tag {16}&emsp;&emsp;注意到公式$(15)$和$(16)$中分母都是一个$-z_e$，这与我们将裁剪空间中的顶点做透视除法相对应，透视投影然后做透视除法如下公式$(17)$、$(18)$所示： \\left( \\begin{matrix} x_{clip}\\\\ y_{clip}\\\\ z_{clip}\\\\ w_{clip} \\end{matrix} \\right) =M_{projection}\\cdot \\left( \\begin{matrix} x_{eye}\\\\ y_{eye}\\\\ z_{eye}\\\\ w_{eye} \\end{matrix} \\right) \\tag {17} \\left( \\begin{matrix} x_{ndc}\\\\ y_{ndc}\\\\ z_{ndc} \\end{matrix} \\right) = \\left( \\begin{matrix} x_{clip}/w_{clip}\\\\ y_{clip}/w_{clip}\\\\ z_{clip}/w_{clip} \\end{matrix} \\right) \\tag {18}&emsp;&emsp;为了便于构建矩阵（$x_e$和$y_e$均与$-z_e$相除，不好构建矩阵），我们令裁剪空间中的$w_{clip}$为$-z_e$，将除以$-z_e$的这一步挪到了透视除法去做。故目前的透视矩阵就变为： \\left( \\begin{matrix} x_{c}\\\\ y_{c}\\\\ z_{c}\\\\ w_{c} \\end{matrix} \\right) = \\left( \\begin{matrix} .&.&.&.\\\\ .&.&.&.\\\\ .&.&.&.\\\\ 0&0&-1&0 \\end{matrix} \\right) \\left( \\begin{matrix} x_{e}\\\\ y_{e}\\\\ z_{e}\\\\ w_{e} \\end{matrix} \\right) \\tag {19}&emsp;&emsp;其中”$.$”均表示未知。得到在近平面的$x_p$和$y_p$之后，我们还要将$x_p$映射到$[-1,1]$范围，同理$y_p$也是。以$x_p$为例，我们知道其值域为$[l,r]$。为了将$x_p$其映射到$[-1,1]$，我们首先将其映射到$[0,1]$，不难得到如下式子： \\frac{x_p-l}{r-l}\\in[0,1] \\tag {20}&emsp;&emsp;式$(20)$乘上一个$2$再减去$1$就映射到了$[-1,1]$，映射之后记为$x_n$： x_n=2\\frac{x_p-l}{r-l}-1=\\frac{2x_p}{r-l}-\\frac{r+l}{r-l}\\in[-1,1] \\tag {21}&emsp;&emsp;同理$y_p$到$y_n$的映射： y_n=\\frac{2y_p}{r-l}-\\frac{t+b}{t-b}\\in[-1,1] \\tag {22}&emsp;&emsp;然后将公式$(15)$中的$x_p$带入公式$(21)$，将公式$(16)$中的$y_p$带入公式$(22)$，以$x_p$为例： x_n=\\frac{2x_p}{r-l}-\\frac{r+l}{r-l} =\\frac{2\\frac{n\\cdot x_e}{-z_e}}{r-l}-\\frac{r+l}{r-l}\\\\ =\\frac{2n\\cdot x_e}{(r-l)(-z_e)}-\\frac{r+l}{r-l} =\\frac{\\frac{2n}{r-l}\\cdot x_e}{-z_e}-\\frac{r+l}{r-l}\\\\ =\\frac{\\frac{2n}{r-l}\\cdot x_e}{-z_e}+\\frac{\\frac{r+l}{r-l}\\cdot z_e}{-z_e} =\\underbrace{(\\frac{2n}{r-l}\\cdot x_e+\\frac{r+l}{r-l}\\cdot z_e)}_{x_c}/-z_e \\tag {23}&emsp;&emsp;其中$x_c$即公式$(19)$中的裁剪空间中的$x$轴坐标值。$y_p$同理可得$y_c$: y_n =\\underbrace{(\\frac{2n}{t-b}\\cdot y_e+\\frac{t+b}{t-b}\\cdot z_e)}_{y_c}/-z_e \\tag {24}&emsp;&emsp;现在我们已经知道了$x_c$和$y_c$分辨关于$x_e$、$y_e$以及$z_e$的表达形式，我们可以填充式$(19)$中的投影矩阵第一行与第二行： \\left( \\begin{matrix} x_{c}\\\\ y_{c}\\\\ z_{c}\\\\ w_{c} \\end{matrix} \\right) = \\left( \\begin{matrix} \\frac{2n}{r-l}&0&\\frac{r+l}{r-l}&0\\\\ 0&\\frac{2n}{t-b}&\\frac{t+b}{t-b}&0\\\\ 0&0&A&B\\\\ 0&0&-1&0 \\end{matrix} \\right) \\left( \\begin{matrix} x_{e}\\\\ y_{e}\\\\ z_{e}\\\\ w_{e} \\end{matrix} \\right) \\tag {25}&emsp;&emsp;现在我们还剩下投影矩阵的第三行还不知道。因为我们知道$z$的投影与$x_e$和$y_e$无关，只与$z_e$、$w_e$有关，故可以假设投影矩阵的第三行如上式$(25)$所示，$A$和$B$就是我们假设的要求解的未知表达式。此外，在视图空间中的$w_e$是等于$1$的，$w_c$即前面提到的$-z_e$，从而有： z_n=z_c/w_c=\\frac{Az_e+Bw_e}{-z_e}=\\frac{Az_e+B}{-z_e} \\tag {26}&emsp;&emsp;为了求出公式$(26)$中的$A$和$B$，我们取两个极端的例子：在$-n$处的$z$值被映射到$-1$，在$-f$处的$z$值被映射到$1$，将$(z_n,z_e)=(-1,-n)$和$(z_n,z_e)=(1,-f)$带入式$(26)$中，可得方程组： \\begin{cases} \\frac{-An+B}{n}=-1\\\\ \\frac{-Af+B}{f}=1\\\\ \\end{cases}\\ \\rightarrow\\ \\begin{cases} {-An+B}=-n\\\\ {-Af+B}=f\\\\ \\end{cases} \\tag {27}&emsp;&emsp;求解方程$(27)$，可得$A$与$B$如下所示： A=-\\frac{f+n}{f-n}\\\\ B=-\\frac{2fn}{f-n} \\tag {28}&emsp;&emsp;将公式$(28)$带入公式$(26)$中： z_n=\\underbrace{(-\\frac{f+n}{f-n}z_e-\\frac{2fn}{f-n})}_{z_c}/{-z_e} \\tag {29}&emsp;&emsp;我们最终得到了$z_c$关于$z_e$的表达式，将$A$与$B$填入式$(25)$的投影矩阵即可，$M_{projection}$就是我们一直在寻求的透视投影矩阵： M_{projection}= \\left( \\begin{matrix} \\frac{2n}{r-l}&0&\\frac{r+l}{r-l}&0\\\\ 0&\\frac{2n}{t-b}&\\frac{t+b}{t-b}&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right) \\tag {30}&emsp;&emsp;公式$(30)$中的透视投影矩阵只是一个通用的形式，在视图空间中的视锥体通常都是关于$x$轴和$y$轴对称的，从而有$r=-l$、$t=-b$，将式$(30)$简化成如下形式： M_{projection}= \\left( \\begin{matrix} \\frac{2n}{r-l}&0&0&0\\\\ 0&\\frac{2n}{t-b}&0&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right) \\tag {31}&emsp;&emsp;&emsp;但是通常我们传入构建透视矩阵函数的参数是$fovy$（$y$轴方向的视域角）、$aspect$（屏幕的宽高比）、$near$（近平面）以及$far$（远平面），如何根据这些参数构造式$(31)$的透视投影矩阵呢？注意到$r-l=width$即近平面宽度，$t-b=height$即近平面的高度，我们可以根据$fovy$和$aspect$得出$width$和$height$，具体细节不再赘述： r-l=width=2*near*aspect*tan(fovy/2)\\\\ t-b=height=2*near*tan(fovy/2) M_{projection}= \\left( \\begin{matrix} \\frac{1}{aspect*tan(fovy/2)}&0&0&0\\\\ 0&\\frac{1}{tan(fovy/2)}&0&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right) \\tag {32}123456789101112void Matrix4x4::setPerspective(float fovy, float aspect, float near, float far)&#123; loadZero(); // convert fov from degrees to radians float rFovy = fovy*M_PI/180; const float tanHalfFovy = tanf(static_cast&lt;float&gt;(rFovy*0.5f)); entries[0] = 1.0f/(aspect*tanHalfFovy); entries[5] = 1.0f/(tanHalfFovy); entries[10] = -(far+near)/(far-near); entries[11] = -1.0f; entries[14] = (-2.0f*near*far)/(far-near);&#125; 正交投影矩阵&emsp;&emsp;理解了透视投影矩阵的构造之后，正交投影就简单太多了，正交投影只需做简单的线性映射就行了。只需将$x$轴方向从$[l,r]$映射到$[-1,1]$，$y$轴方向从$[b,t]$映射到$[-1,1]$，$z$轴方向从$[-n,-f]$映射到$[-1,1]$，而这个映射的过程很简单，正如前面公式$(20)$和$(21)$那样，先映射到$[0,1]$，再映射到$[0,2]$，最后映射到$[-1,1]$，这个过程我也不细说了，直接上结果： M_{projection}= \\left( \\begin{matrix} \\frac{2}{r-l}&0&0&-\\frac{r+l}{r-l}\\\\ 0&\\frac{2}{t-b}&0&-\\frac{t+b}{t-b}\\\\ 0&0&\\frac{-2}{f-n}&-\\frac{f+n}{f-n}\\\\ 0&0&0&1 \\end{matrix} \\right) \\tag {33}&emsp;&emsp;然后又因为视锥体关于$x$轴、$y$轴对称，简化的正交投影矩阵就为： M_{projection}= \\left( \\begin{matrix} \\frac{2}{r-l}&0&0&0\\\\ 0&\\frac{2}{t-b}&0&0\\\\ 0&0&\\frac{-2}{f-n}&-\\frac{f+n}{f-n}\\\\ 0&0&0&1 \\end{matrix} \\right) \\tag {33}12345678910void Matrix4x4::setOrtho(float left, float right, float bottom, float top, float near, float far)&#123; loadIdentity(); entries[0] = 2.0f/(right-left); entries[5] = 2.0f/(top-bottom); entries[10] = -2.0f/(far-near); entries[12] = -(right+left)/(right-left); entries[13] = -(top+bottom)/(top-bottom); entries[14] = -(far+near)/(far-near);&#125; 4、lookAt函数构造视图矩阵&emsp;&emsp;视图矩阵的工作目标是将世界坐标系中的所有物体的顶点的坐标从世界坐标系转换到摄像机坐标系。这是因为摄像机坐标系的原点不一定与世界坐标系重合，同时由于自身的旋转，坐标轴也一定不与世界坐标系的坐标轴平行。为完成工作任务，需要分为两步走：首先整体平移，将摄像机平移至世界坐标系原点，然后将顶点从世界坐标系变换至摄像机坐标系。 &emsp;&emsp;lookAt函数的输入参数分别为：$eye$摄像机的位置，$target$摄像机目标点，$up$世界空间的上向量,。首先我们要根据这些参数确定摄像机坐标系的三个轴向量，其中需要非常注意的就是变换到视图空间中时摄像机是朝向视图空间的$-Z$方向的，所以求视图空间中的$Z$轴时是摄像机的位置减去目标点的位置： Z = normalize(eye - target)\\\\ X = normalize(cross(up, Z))\\\\ Y = normalize(cross(Z,X))&emsp;&emsp;通过以上的方式我们就求出了视图空间的三条轴向量，再加上摄像机的位置我们就可以求出将世界坐标变换到与视图坐标重合的矩阵了，记为$M=T\\cdot R$，其中$T$是平移到摄像机位置$eye$的变换矩阵，$R$是旋转到摄像机坐标轴方向的旋转矩阵： M=T\\cdot R= \\left[ \\begin{matrix} 1&0&0&eye_x\\\\ 0&1&0&eye_x\\\\ 0&0&1&eye_x\\\\ 0&0&0&1 \\end{matrix} \\right] \\cdot \\left[ \\begin{matrix} X_x&Y_x&Z_x&0\\\\ X_y&Y_y&Z_y&0\\\\ X_z&Y_z&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {34}&emsp;&emsp;然而公式$(34)$并不是我们要求的视图矩阵，上式中的矩阵$M$仅仅是将世界坐标轴变换到摄像机坐标轴。摄像机只是一个虚拟的物品，我们不能将上述的矩阵$M$作用于摄像机，因为摄像机根本不存在！我们视图矩阵最终作用的世界空间中的物体，这就涉及到了一个相对运动的概念！ &emsp;&emsp;当我们向前移动摄像机的时候，可以看成是摄像机不动，而物体朝着与摄像机朝向相反的方向移动。当我们向右旋转摄像机时，相当于摄像机不动而物体朝着摄像机的左边移动。摄像机的构造得益于相对于运动的理论，计算机图形学中的虚拟$3D$摄像机实际上是通过物体的移动来实现的，所以我们要构造的视图矩阵是公式$(34)$中的逆矩阵。 viewMatrix = M^{-1}=(T\\cdot R)^{-1}=R^{-1}\\cdot T^{-1} = \\left[ \\begin{matrix} X_x&Y_x&Z_x&0\\\\ X_y&Y_y&Z_y&0\\\\ X_z&Y_z&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} \\cdot \\left[ \\begin{matrix} 1&0&0&eye_x\\\\ 0&1&0&eye_x\\\\ 0&0&1&eye_x\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} \\tag {35}&emsp;&emsp;由上式可知，构造视图矩阵涉及到$R$和$T$的求逆，其中的平移矩阵$T$的求逆则是直接取平移量的相反数即可： T^{-1}= \\left[ \\begin{matrix} 1&0&0&eye_x\\\\ 0&1&0&eye_x\\\\ 0&0&1&eye_x\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} = \\left[ \\begin{matrix} 1&0&0&-eye_x\\\\ 0&1&0&-eye_x\\\\ 0&0&1&-eye_x\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {36}&emsp;&emsp;至于旋转矩阵$R$，我们知道旋转矩阵都是正交矩阵，正交矩阵的一个特点就是它的逆等于它的转置： R^{-1}= \\left[ \\begin{matrix} X_x&Y_x&Z_x&0\\\\ X_y&Y_y&Z_y&0\\\\ X_z&Y_z&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} = \\left[ \\begin{matrix} X_x&X_y&X_z&0\\\\ Y_x&Y_y&Y_z&0\\\\ Z_x&Z_y&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {37}&emsp;&emsp;最后，我们得到视图矩阵： viewMatrix=R^{-1}\\cdot T^{-1}= \\left[ \\begin{matrix} X_x&X_y&X_z&0\\\\ Y_x&Y_y&Y_z&0\\\\ Z_x&Z_y&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\cdot \\left[ \\begin{matrix} 1&0&0&-eye_x\\\\ 0&1&0&-eye_x\\\\ 0&0&1&-eye_x\\\\ 0&0&0&1 \\end{matrix} \\right] \\\\= \\left[ \\begin{matrix} X_x&X_y&X_z&-(\\vec X\\cdot \\vec {eye})\\\\ Y_x&Y_y&Y_z&-(\\vec Y\\cdot \\vec {eye})\\\\ Z_x&Z_y&Z_z&-(\\vec Z\\cdot \\vec {eye})\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {38}1234567891011121314151617181920212223242526void Matrix4x4::setLookAt(Vector3D cameraPos, Vector3D target, Vector3D worldUp)&#123; Vector3D zAxis = cameraPos - target; zAxis.normalize(); Vector3D xAxis = worldUp.crossProduct(zAxis); xAxis.normalize(); Vector3D yAxis = zAxis.crossProduct(xAxis); yAxis.normalize(); loadIdentity(); entries[0] = xAxis.x; entries[4] = xAxis.y; entries[8] = xAxis.z; entries[1] = yAxis.x; entries[5] = yAxis.y; entries[9] = yAxis.z; entries[2] = zAxis.x; entries[6] = zAxis.y; entries[10] = zAxis.z; entries[12] = -(xAxis.dotProduct(cameraPos)); entries[13] = -(yAxis.dotProduct(cameraPos)); entries[14] = -(zAxis.dotProduct(cameraPos));&#125; 参考资料$[1]$ http://www.songho.ca/opengl/gl_projectionmatrix.html $[2]$ https://blog.csdn.net/zsq306650083/article/details/8773996 $[3]$ https://blog.csdn.net/y1196645376/article/details/78463248 $[4]$ https://www.cnblogs.com/J1ac/p/9340622.html $[5]$ https://learnopengl-cn.github.io/01%20Getting%20started/08%20Coordinate%20Systems/","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"http://yoursite.com/tags/Soft-Renderer/"},{"name":"3D Math","slug":"3D-Math","permalink":"http://yoursite.com/tags/3D-Math/"}]},{"title":"流体模拟基础","slug":"fluidSimulation","date":"2019-05-01T02:37:38.127Z","updated":"2019-05-01T02:14:11.978Z","comments":true,"path":"2019/05/01/fluidSimulation/","link":"","permalink":"http://yoursite.com/2019/05/01/fluidSimulation/","excerpt":"本文主要参考文献《FLUID SIMULATION SIGGRAPH 2007 Course Notes》，结合我的理解单纯地讲述一下流体渲染的一些基础知识，本人水平有限，如有错误，欢迎指出。本文只是单纯针对流体模拟领域，可能一些地方不太严谨，但是对于虚拟模拟来说是可行的。即便如此，本文涉及到大量的数学方法。","text":"本文主要参考文献《FLUID SIMULATION SIGGRAPH 2007 Course Notes》，结合我的理解单纯地讲述一下流体渲染的一些基础知识，本人水平有限，如有错误，欢迎指出。本文只是单纯针对流体模拟领域，可能一些地方不太严谨，但是对于虚拟模拟来说是可行的。即便如此，本文涉及到大量的数学方法。 矢量微积分 Naiver-Stokes偏微分方程组 N-S方程的分步求解 对流算法 一、矢量微积分&emsp;&emsp;高等数学中太多数讨论的是一维的微积分，而矢量微积分则是一维微积分的高维扩展。矢量微积分的三个基础算子：梯度（符号为$∇$），散度（符号为$∇\\cdot$)，旋度（符号为$∇\\times$），在此基础上流体力学中经常用到的还有拉普拉斯算子。 1、梯度（Gradient）&emsp;&emsp;梯度实际上就是矢量的空间偏导数，且结果依然是一个矢量，$2$维的梯度如下： ∇f(x,y)=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}) \\tag {1.1}&emsp;&emsp;依此类推，$3$维的梯度有如下形式： ∇f(x,y,z)=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y},\\frac{\\partial f}{\\partial z}) \\tag {1.2}&emsp;&emsp;有时也会采用如下形式来表示梯度： ∇f=\\frac{\\partial f}{\\partial \\vec x} \\tag {1.3}&emsp;&emsp;梯度通常用来近似计算函数值（实际上就是一维形式的推广)： f(\\vec x+\\Delta \\vec x)\\approx f(\\vec x)+∇f(\\vec x)\\cdot \\Delta \\vec x \\tag {1.4}&emsp;&emsp;同样的，多个函数的梯度就构成了一个矩阵： ∇\\vec F=∇(f,g,h)=\\left( \\begin{matrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} & \\frac{\\partial f}{\\partial z} \\\\ \\frac{\\partial g}{\\partial x} & \\frac{\\partial g}{\\partial y} & \\frac{\\partial g}{\\partial z} \\\\ \\frac{\\partial h}{\\partial x} & \\frac{\\partial h}{\\partial y} & \\frac{\\partial h}{\\partial z} \\\\ \\end{matrix} \\right) =\\left( \\begin{matrix}∇f\\\\ ∇g\\\\ ∇h\\\\ \\end{matrix} \\right) \\tag {1.5}2、散度（Divergence）&emsp;&emsp;散度算子仅仅应用于向量场，它衡量在某一点出相应的矢量聚集或者发散程度，测量方向为径向，结果为标量。$2$维、$3$维形式的散度算子如下所示： ∇\\cdot \\vec u=∇\\cdot (u,v)=\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y} ∇\\cdot \\vec u=∇\\cdot (u,v,w)=\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}+\\frac{\\partial w}{\\partial z} \\tag {1.6}&emsp;&emsp;输入是矢量，而输出为标量。类比梯度，散度符号$∇\\cdot \\vec u$可以理解为梯度$∇$与矢量$\\vec u$的点乘： ∇\\cdot \\vec u=(\\frac{\\partial}{\\partial x},\\frac{\\partial}{\\partial y},\\frac{\\partial}{\\partial z})\\cdot (u,v,w)=\\frac{\\partial}{\\partial x}u+\\frac{\\partial}{\\partial y}v+\\frac{\\partial}{\\partial z}w \\tag {1.7}&emsp;&emsp;若矢量场散度为$0$，则称该矢量场无散度。 3、旋度（Curl）&emsp;&emsp;旋度衡量围绕某一点的旋转速度，测量方向为切向，三维形式的旋度是一个向量： ∇\\times \\vec u=∇\\times (u,v,w) =(\\frac{\\partial w}{\\partial y}-\\frac{\\partial v}{\\partial z}, \\frac{\\partial u}{\\partial z}-\\frac{\\partial w}{\\partial x}, \\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y}) \\tag {1.8}&emsp;&emsp;倒推到$2$维，我们取上式中的$w=0$，即矢量场为$(u,v,0)$，$2$维向量场的旋度是一个标量： ∇\\times \\vec u=∇\\times (u,v)=\\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y} \\tag {1.9}&emsp;&emsp;同样地，旋度符号$∇\\times \\vec u$我们可以理解为梯度$∇$与矢量场$\\vec u$的叉乘： ∇\\times \\vec u= (\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z})\\times(u,v,w) \\tag {1.10}&emsp;&emsp;若矢量场旋度为$0$，则称该矢量场无旋度。 4、拉普拉斯算子（Laplacian）&emsp;&emsp;拉普拉斯算子定义为梯度的散度，符号表示为$∇\\cdot∇$，显然$∇\\cdot$是散度，而后面的$∇$则为梯度，故拉普拉斯算子即梯度的散度，是一个二阶微分算子。$2$维、$3$维形式分别如下： ∇\\cdot∇f=\\frac{\\partial^2f}{\\partial x^2}+\\frac{\\partial^2f}{\\partial y^2} ∇\\cdot∇f=\\frac{\\partial^2f}{\\partial x^2}+\\frac{\\partial^2f}{\\partial y^2}+\\frac{\\partial^2f}{\\partial z^2} \\tag {1.11}&emsp;&emsp;简言之，拉普拉斯算子定义如下： ∇\\cdot∇f=\\Sigma_{i=1}^n\\frac{\\partial^2f}{\\partial x_i^2} \\tag {1.12}&emsp;&emsp;偏微分方程$∇\\cdot ∇f=0$被称为拉普拉斯方程；而如果右边为某个非$0$常数，即$∇\\cdot ∇f=q$，我们称之为泊松方程。更一般地，如果梯度再乘上一个标量$a$（如$1/\\rho$)，即$∇\\cdot (a∇f)=q$，我们依旧称之为泊松问题。 二、$Naiver-Stokes$偏微分方程组&emsp;&emsp;流体模拟器的构建主要围绕著名的不可压缩$Navier-Stokes$方程展开，它是一个流体力学领域的偏微分方程，方程形式如下： \\frac{\\partial \\vec u}{\\partial t}+\\vec u\\cdot ∇\\vec u+\\frac1\\rho∇p=\\vec g+\\nu∇\\cdot∇\\vec u \\tag {2.1} ∇\\cdot\\vec u=0 \\tag {2.2}&emsp;&emsp;这个方程组看起非常地复杂，接下来我们就把它剖析成一个个比较简单的部分来理解。 1、符号标记&emsp;&emsp;我们有必要定义一些物理量的符号用以标记： &emsp;&emsp;符号$\\vec u$在流体力学中通常表示为流体的速度矢量，记$3$维的速度矢量$\\vec u=(u,v,w)$； &emsp;&emsp;希腊字符$\\rho$是流体的密度，对于水，该值大约为$1000kg/m^3$，而空气则大约为$1.3kg/m^3$； &emsp;&emsp;字符$p$代表压力，流体对任何物体施加的单位面积力； &emsp;&emsp;字符$\\vec g$则是我们熟悉的重力加速度，通常取$(0,-9.81,0)m/s^2$。我们约定$y$轴向上，而$x$轴和$z$轴在水平面上。另外补充一点，我们把其他的一些类似的力都累加到$\\vec g$上，也就是我们统一用$\\vec g$表示所有类似力之和，这类力我们称之为体积力（称之为体积力是因为它们的力是作用到整个流体而不只是流体的表面）； &emsp;&emsp;希腊字符$\\nu$是流体的运动粘度，它衡量流体的黏滞性。糖蜜之类的流体有非常高的粘度，而像酒精之类的流体有很低的粘度； &emsp;&emsp;其它一些矢量微积分的符号算子前面已经提到过，不再赘述。 2、动量方程&emsp;&emsp;偏微分方程$(2.1)$我们称之为动量方程，它本质上就是我们熟悉的牛顿定律$\\vec F=m\\vec a$的形式，描述了施加在流体上的力是如何影响流体的运动。 &emsp;&emsp;假设我们用粒子系统来模拟流体，每个粒子代表流体的一小滴，每个粒子有各自的质量$m$、体积$V$和速度$\\vec u$。为了让整个粒子系统运作起来，我们必须弄清楚每个粒子所承受的力的作用。牛顿第二定律告诉我们：$\\vec F=m\\vec a$，而根据加速度定义，我们有： \\vec a=\\frac{D\\vec u}{Dt} \\tag {2.3}&emsp;&emsp;符号$D$是指物质导数，所谓物质导数，就是对流体质点求导数，而且是针对流体质点（在这里就是流体粒子）而不是空间的固定点。因而牛顿第二定律就变成： m\\frac{D\\vec u}{Dt}=\\vec F \\tag {2.4}&emsp;&emsp;那么流体粒子承受的力有哪些呢？一个最简单的力就是重力：$m\\vec g$。而其他的流体质点（或其他流体粒子）也会对当前的流体粒子产生作用力。流体内部的相互作用力之一便是压力，较大压力的区域会向较低压力区域产生作用力。值得注意的是，我们只关注施加在粒子上的压力的净合力。例如，若施加在粒子上压力在每个方向上都相等，那么它的压力的合力便为0。我们用压力的负梯度（取负是因为方向是由压力大的区域指向压力小的区域）来衡量在当前流体粒子处压力的不平衡性，即取$-∇p$。那么流体粒子所承受的压力就是对$-∇p$在整个流体粒子的体积上进行积分，为了简化，我们简单地将$V$与$-∇p$相乘，故粒子压力部分为$-V∇p$。 &emsp;&emsp;其他的流体相互作用力则是由流体的黏性产生的，我们直观地把这种力理解为尽可能使得粒子以周围区域的平均速度移动的力，也就是使得粒子的速度与周围区域粒子速度的差距最小化。拉普拉斯算子是衡量一个量与之周围区域该量平均值之差的算符，因而$∇\\cdot∇\\vec u$是当前粒子速度矢量与周围区域平均速度矢量之差。为了计算粘滞力，我们同样对$∇\\cdot∇\\vec u$在整个粒子体积$V$上进行积分，与前面类似，我们简单取$V∇\\cdot∇\\vec u$。除此之外，我们还引进一个称为动力粘度系数的物理量，符号为$\\mu$。因而粘滞力为$V\\mu∇\\cdot∇\\vec u$。 &emsp;&emsp;把重力、压力和粘滞力综合一起，我们可得： m\\frac{D\\vec u}{Dt}=\\vec F=m\\vec g-V∇p+V\\mu∇\\cdot∇\\vec u \\tag {2.5}&emsp;&emsp;当粒子系统中的粒子数量趋于无穷大，而每个粒子大小趋于$0$时，会产生一个问题：此时每个粒子的质量$m$和体积$V$变为$0$，此时上式变得没有意义。为此，我们把$(2.5)$式调整一下，两边同除以体积$V$，又因$\\rho=m/V$，故有： \\rho\\frac{D\\vec u}{Dt}=\\rho\\vec g-∇p+\\mu∇\\cdot∇\\vec u \\tag {2.6}&emsp;&emsp;两边同除以$\\rho$，移项调整： \\frac{D\\vec u}{Dt}+\\frac1\\rho∇p=\\vec g+\\frac\\mu\\rho∇\\cdot∇\\vec u \\tag {2.7}&emsp;&emsp;为了进一步简化，定义运动粘度为$\\nu=\\mu/\\rho$，式$(2.7)$变为： \\frac{D\\vec u}{Dt}+\\frac1\\rho∇p=\\vec g+\\nu∇\\cdot∇\\vec u \\tag {2.8}&emsp;&emsp;我们已经快把动量方程推导出来，现在我们要把物质导数$\\frac{D\\vec u}{Dt}$弄清楚，为此，我们需要了解两种描述方法：拉格朗日描述和欧拉描述。 3、拉格朗日描述与欧拉描述&emsp;&emsp;当我们尝试研究流体或可变形固体的运动的时候，通常有两种方法来描述：拉格朗日描述（ Lagrangian viewpoint）、欧拉描述（Eulerian viewpoint）。 &emsp;&emsp;拉格朗日描述方法是我们比较熟悉的方法，这种描述方法把物体看成是由类似于粒子系统的形式组成，固体或流体的每个点看作一个独立的粒子，粒子有各自相应的位置$\\vec x$和速度$\\vec u$。我们可以把粒子理解为组成物体的分子。对于我们通常采用拉格朗日描述法进行建模模拟，即用一系列离散的粒子集来构建，粒子之间通过网格相联系。 &emsp;&emsp;欧拉描述方法则采用了完全不同的角度，它常被用于流体力学中。与拉格朗日描述追踪每个物体粒子的方法不同，欧拉描述关注点是空间中的一个固定点，并考察在这个固定点上流体性质（如密度、速度、温度等）是如何随着时间变化的。流体流动经过这个固定点可能会导致这个固定点的物理性质发生一些变化（如一个温度较高的流体粒子流经这个固定点，后面紧跟着一个温度较低的流体粒子流过固定点，那么这个固定点的温度会降低，但是并没有任何一个流体粒子的温度发生了变化）。 &emsp;&emsp;用天气测量举个简单的例子：拉格朗日描述方法就是你乘坐在一个随风而飘的热气球上，测量周围空气的压力、密度和浑浊度等天气指标；而欧拉描述方法就是你固定在地面上，测量流过的空气的天气指标。 &emsp;&emsp;欧拉描述法似乎看起来带来了一些不必要的复杂度，但是目前大多数的流体模拟器都是基于欧拉描述法，这是因为欧拉描述法相比于拉格朗日描述法有一些不可比拟的优点：欧拉描述法能够更加方便地计算一些物理量的空间导数（例如压力梯度和粘度）；而如果用粒子方法的话（即拉格朗日描述法），那么计算物理量相对于空间位置的变化是比较难的。 &emsp;&emsp;把拉格朗日描述法和欧拉描述法联系起来的关键点就是物质导数。首先从拉格朗日描述法出发，假设有一群粒子，每个粒子都有各自的位置$\\vec x$和速度$\\vec u$。记$q$为通用的物理量（如密度、速度和温度等），每个粒子有其对应的$q$值。方程$q(t,\\vec x)$描述在时间点$t$而位置为$\\vec x$的粒子对应的物理量值$q$。则一个粒子的物理量$q$随时间$t$的变化率是多少？这是一个拉格朗日描述角度下的问题，我们取对时间$t$的导数（注意用到了求导链式法则，以及$\\frac{\\partial q}{\\partial \\vec x}=∇q$和$\\vec u=\\frac{d\\vec x}{dt}）$： \\frac d{dt}q(t,\\vec x)=\\frac{\\partial q}{\\partial t}+∇q\\cdot\\frac{d\\vec x}{dt}=\\frac{\\partial q}{\\partial t}+∇q\\cdot\\vec u\\equiv\\frac{Dq}{Dt} \\tag {2.9}&emsp;&emsp;这就是物质导数。把式$(2.9)$代入式$(2.8)$我们就得到了流体动量方程$(2.1)$。物质导数针对的是流体质点（在这里就是流体粒子）而不是空间的固定点。式$(2.9)$写完整一点就是： \\frac{Dq}{Dt}=\\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}+v\\frac{\\partial q}{\\partial y}+w\\frac{\\partial q}{\\partial z} \\tag {2.10}&emsp;&emsp;对于给定的速度场$\\vec u$， 流体的物理性质如何在这个速度场$\\vec u$下变化的计算我们称之为对流（advection）。一个最简单的对流方程，就是其物理量的物质导数为$0$，如下所示： \\frac{Dq}{Dt}=0\\implies\\frac{\\partial q}{\\partial t}+\\vec u\\cdot ∇q = 0 \\tag {2.11}&emsp;&emsp;公式$(2.11)$的意义即在拉格朗日视角观察下，每个流体粒子的物理量保持不变。 4、不可压缩性&emsp;&emsp;关于流体的压缩性在此不做过多的物理细节描述，只需知道一点：通常情况下流体的体积变化非常小（除开一些极端的情况，而且这些极端情况我们日常生活中较少出现）。可压缩流体的模拟涉及到非常复杂的情况，往往需要昂贵的计算资源开销，为此在计算机流体模拟中我们通常把所有的流体当作是不可压缩的，即它们的体积不会发生变化。 &emsp;&emsp;任取流体的一部分，设其体积为$\\Omega$而其边界闭合曲面为$\\partial\\Omega$，我们可以通过围绕边界曲面$\\partial\\Omega$对流体速度$\\vec u$在曲面法线方向上的分量进行积分来衡量这块部分流体的体积变化速率： \\frac d{dt}Volume(\\Omega)=\\int\\int_{\\partial\\Omega}\\vec u\\cdot n \\tag{2.12}&emsp;&emsp;对于不可压缩的流体，其体积保持为某个常量，故其体积变化速率为$0$： \\int\\int_{\\partial\\Omega}\\vec u\\cdot n=0 \\tag {2.13}&emsp;&emsp;由高斯散度定理，我们可以把式$(2.13)$转换为体积分： \\int\\int_{\\partial\\Omega}\\vec u\\cdot n=\\int\\int\\int_\\Omega∇\\cdot \\vec u=0 \\tag{2.14}&emsp;&emsp;式$(13)$应该对任意的$\\Omega$成立，意即无论$\\Omega$取何值，积分值均为$0$。这种情况下只有令积分函数值取$0$方可成立，即对$0$积分无论$\\Omega$取何值结果均为$0$。所以有： ∇\\cdot \\vec u=0 \\tag{2.15}&emsp;&emsp;这就是$Navier-Stokes$方程中的不可压缩条件$(2.2)$。满足不可压缩条件的速度场被称为是无散度的，即在该速度场下流体体积既不膨胀也不坍缩，而是保持在一个常量。模拟不可压缩流体的关键部分就是使得流体的速度场保持无散度的状态，这也是流体内部压力的来源。 &emsp;&emsp;为了把压力与速度场的散度联系起来，我们在动量方程$(2.1)$两边同时取散度： ∇\\cdot\\frac{\\partial \\vec u}{\\partial t}+∇\\cdot(\\vec u\\cdot ∇\\vec u)+∇\\cdot\\frac1\\rho∇p=∇\\cdot(\\vec g+\\nu∇\\cdot∇\\vec u) \\tag {2.16}&emsp;&emsp;对于上式$(2.16)$第一项，我们转变一下求导次序： \\frac {\\partial}{\\partial t}∇\\cdot\\vec u \\tag {2.17}&emsp;&emsp;如果满足流体不可压缩条件，那么式$(2.17)$取值$0$（因为无散度），然后我们调整一下式$(2.16)$可得关于压力的方程： ∇\\cdot\\frac1\\rho∇p=∇\\cdot(-\\vec u\\cdot ∇\\vec u+\\vec g+\\nu∇\\cdot∇\\vec u) \\tag{2.18}5、丢弃粘度项&emsp;&emsp;在某些流体如蜂蜜、小水珠等的模拟中，粘滞力起着非常重要的作用。但是在大多数流体动画模拟中，粘滞力的影响微乎其微，为此秉持着方程组越简单越好的原则，我们常常丢弃粘度项。当然这也不可避免地带来一些误差，事实上，在计算流体力学中尽可能地减少丢弃粘度项带来的误差是一个非常大的挑战。下面的叙述都是基于丢弃粘度项的前提。 &emsp;&emsp;丢弃了粘度项的$Navier-Stokes$方程被称为欧拉方程，而这种理想的流体则是无粘度的。丢弃了粘度项的欧拉方程如下： \\frac{D\\vec u}{Dt}+\\frac1\\rho∇p=\\vec g \\tag {2.19} ∇\\cdot\\vec u=0 \\tag{2.20}&emsp;&emsp;大多数的流体模拟的计算方程都是欧拉方程。 6、边界条件&emsp;&emsp;目前为止我们讨论的都是流体内部的情况，然而边界部分也是流体模拟非常关键的部分。在流体模拟中我们仅仅关注两种边界条件：固体墙（solid walls）、自由面（free surfaces）。 &emsp;&emsp;固体墙顾名思义就是流体与固体接触的边界，用速度来描述很简单：流体既不会流进固体内部也不会从固体内部流出，因此流体在固体墙法线方向上的分量为$0$： \\vec u\\cdot n=0 \\tag {2.21}&emsp;&emsp;当然，上述是固体自身不移动的情况下。通常来说，流体速度在法线方向上的分量与固体的移动速度在法线方向上的分量应该保持一致： \\vec u\\cdot n=\\vec u_{solid}\\cdot n \\tag{2.22}&emsp;&emsp;上述的两个公式都是仅对流体速度在法线方向上的分量做了限制，对于无粘度的流体，切线方向上的流体速度与固体的移动速度无必然的联系。 &emsp;&emsp;自由面是另外一个非常重要的边界条件，它通常就是与另外一种流体相接壤的边界部分。例如在模拟水花四溅时，水流表面不与固体接触的都是自由面（如与空气这种流体接触）。因空气密度远小于水导致空气对水体的仿真影响非常小，为了简化模拟，我们将空气所占的空间设为某个固定大气压的区域，设为$0$是最方便的方案，此时自由面就是压强$p=0$的水体表面。 &emsp;&emsp;在小规模的流体仿真中，自由面的表面张力占据着非常重要的地位。在微观分子层面下，表面张力的存在是因为不同的分子相互吸引产生的力。从几何的角度来解释就是，表面张力就是促使流体的表面积尽可能小的一种力。物理学上，两种不同的流体之间实际上存在着与表面平均曲率成正比的压力骤变： [p]=\\lambda k. \\tag {2.23}&emsp;&emsp;公式$(2.23)$中的$[p]$记为压力之差。$\\lambda$是表面张力系数，可以根据模拟的流体类型查找对应的张力系数（例如空气与水在室温下张力系数为$\\lambda \\approx 0.073N/m$）。而$k$就是平均曲率，单位为$m^{-1}$。又因为我们常常设空气的压力为$0$，因此水与空气交界的自由面的压力为： p=\\lambda k \\tag {2.24}​ 三、N-S方程的分步求解&emsp;&emsp;有了对以上对$Navier-Stokes$方程的理论支撑，接下来我们就要如何用计算机来对该组偏微分方程进行离散化求解。为了程序的松耦合性以及使计算尽可能地高效、简单，在流体模型领域，我们将流体方程分成几个独立的步骤，然后按顺序先后推进。对于不可压缩的无粘度流体方程（即前面的欧拉方程$(2.19)$和$(2.20)$，我们将其离散化成对流项（advection）如公式$(3.1)$、体积力项（body force）如公式$(3.2)$、压力/不可压缩项如公式$(3.3)$： \\frac{Dq}{Dt}=0 \\tag {3.1} \\frac{\\partial \\vec u}{\\partial t}=\\vec g \\tag {3.2} \\begin{cases} \\frac{\\partial \\vec u}{\\partial t}+\\frac{1}{\\rho}∇p=0\\\\ ∇\\cdot\\vec u=0 \\end{cases} \\tag {3.3}&emsp;&emsp;需要注意的是，在对流项公式$(3.1)$中我们用了一个通用量的符号$q$是因为我们不仅仅要对流体的速度进行对流，还需要对其他物理量进行对流。我们记对流项公式$(3.1)$的对流计算算法为$advect(\\vec u, \\Delta t, q)$，即对于给定的时间步长$\\Delta t$和速度场$\\vec u$，对物理量q进行对流。 &emsp;&emsp;对于体积力项$(3.2)$，我们采用简单的前向欧拉法即可：$\\vec u \\leftarrow \\vec u + g\\Delta t$。 &emsp;&emsp;对于压力/不可压缩项$(3.3)$，我们用一个称为$project(\\Delta t, \\vec u)$的算法，通过$project(\\Delta t, \\vec u)$计算出正确的压力以确保速度场$\\vec u$的无散度性质。欧拉方案不会着重研究具体粒子间的作用力，因而不会正向去求解$\\frac{1}{\\rho}∇p$，它是利用流体不可压缩的特性，将速度场$\\vec u$投影到散度为$0$的空间上，间接地解算了压力项。这种思想相当于，已知一个中间量$\\vec u_{temp}$，对这个中间量的唯一一个操作（如正向求解压力$\\frac{1}{\\rho}∇p$）不可行，但是直到最终量$\\vec u_{fianl}$符号的一个性质（散度为$0$），于是只要将$\\vec u_{temp}$投影到符合散度为$0$的特性平面上，即可间接地还原正向求解压力的操作，得到最终的速度场$\\vec u_{temp}$。 &emsp;&emsp;对流项$advect(\\vec u, \\Delta t, q)$的输入速度场$\\vec u$要确保为无散度的状态，投影项$project(\\Delta t, \\vec u)$确保了流体体积保持不变，因而投影项输出的速度场必然是无散度的。所以我们只要确保投影项$project(\\Delta t, \\vec u)$输出的速度场$\\vec u$作为对流项$advect(\\vec u, \\Delta t, q)$的输入即可，这时我们的分步求解流体方程的优势就体现出来了，其伪代码如下所示。 算法1 Fluid Simulation($\\vec u_n$, $\\Delta t$): 1: 初始化速度场$\\vec u_n$,使得$\\vec u_n$无散度 2: 对于每个时间步$n = 0,1,2,…$ 3: &emsp;&emsp;决定一个合理的时间步长$\\Delta t = t_{n+1}-t_n$ 4: &emsp;&emsp;对流项计算$\\vec u_A=advect(\\vec u_n,\\Delta t,\\vec q)$ 5: &emsp;&emsp;体积力项计算$\\vec u_B=\\vec u_A+\\Delta t\\vec g$ 6: &emsp;&emsp;无散度投影$\\vec u_{n+1}=project(\\Delta t,\\vec u_B)$ 1、时间步长&emsp;&emsp;在流体模拟算法中，确定适当的时间步长是算法的第一步。因为计算流体模拟的最后结果是呈现在屏幕上的，所以$\\Delta t$的选取与屏幕的刷新率有重要的关系。若选取的$\\Delta t$有$t_n+\\Delta t &gt; t_{frame}$，那么必须做一个截断使$\\Delta t=t_{frame}-t_n$。此外，流体模拟的三个步骤即对流项、体积力项、无散度投影项对时间步长$\\Delta t$的要求不尽相同，要选择一个满足所有要求的最小时间步长能确保计算的收敛性。此外，一方面为了流体模拟的真实性，我们可能需要选取一个足够小的时间步长来复现流体的高质量细节。另一方面，有时高性能的需求又使得我们不能选取太小的时间步长去渲染一帧。假设一帧至少要进行三个时间步的模拟，那么$\\Delta t$应该至少设成帧间隔时间的三分之一。 2、网格结构&emsp;&emsp;欧拉法的整个流程都是基于网格的，所以合理的网格结构是算法高效的关键点。$Harlow$和$Welch$提出了一种经典的$MAC$（marker and cell）网格结构，许多不可压缩流体模拟的算法都在这个网格结构上呈现出了良好的效率。$MAC$网格是一种交叉排列的网格，不同类型的物理量被存储于网格的不同位置。以二维的网格为例，如图3-1左图所示，流体粒子的压力数据存储于网格的中心点$P_{i,j}$，而速度则沿着笛卡尔坐标被分成了两部分。水平方向的$u$成分被存储在了网格单元竖直边的中心处，例如网格单元$(i,j)$和$(i+1,j)$之间的水平速度记为$u_{i+1/2,j}$。垂直方向的$v$成分则被存储在了网格单元水平面的中心上。这样的存储方案十分有利于估算流体流进/流出某个网格单元的量。 图3-1 MAC网格,左图二维,右图三维 &emsp;&emsp;扩展到三维的情况，$MAC$网格同样是交错排列的结构网格，如图3-1右图所示。压力数值存储在立方体网格单元的中心，三个速度分量分别被记录在立方体网格单元的三个表面的中心点上。在数值计算时，这样的分配方式使得我们可以准确地采用中心差分法计算压力梯度和速度的散度，同时克服了中心差分法的一个普遍的缺点。一维的情况为例，在网格顶点位置$…,q_{i-1},q_i,q_{i+1}…$上估算量场$q$的导数，为了无偏（所谓无偏，就是不偏向左边或者右边）估计网格顶点$i$处的$\\frac{\\partial q}{\\partial x}$，一种比较自然的方式就是采用一阶中心差分法： (\\frac{\\partial q}{\\partial x})_i\\approx \\frac{q_{i+1}-q_{i-1}}{2\\Delta x} \\tag {3.4}&emsp;&emsp;公式$(3.4)$是无偏的，且精确度为$O(\\Delta x^2)$。而前向欧拉差分法偏向右边且精确度只有$O(\\Delta x)$： (\\frac{\\partial q}{\\partial x})_i\\approx \\frac{q_{i+1}-q_i}{\\Delta x} \\tag {3.5}&emsp;&emsp;然而，公式$(3.4)$存在着一个非常严重的问题：网格点$i$的估算导数完全忽略了$q_i$的值。数学上，只有常数函数的一阶导数为零。但是公式$(3.4)$遇到了锯齿函数如$q_i=(-1)^i$时，它错误地将该类函数的导数估算为$0$，这种问题被称为零空间问题（null-space problem）。 &emsp;&emsp;交叉错排的$MAC$网格完美地克服了中心差分法的零空间问题，同时也保持了它的无偏二阶精度。在$MAC$网格上运用中心差分法，网格点$i$处的估算导数公式如下所示： (\\frac{\\partial q}{\\partial x})_i\\approx\\frac{q_{i+1/2}-q_{i-1/2}}{\\Delta x} \\tag {3.6}&emsp;&emsp;$MAC$网格确实给流体的压力计算和不可压缩性的处理带来了很大的便利，但与此同时也带来了一些其他方面的麻烦。如果我们要估算某个地方的速度向量，即便采样点恰好在网格点上我们也要做一些插值才能获取相应的速度向量。在网格点处，我们通常采用平均法，以二维为例： \\vec u_{i,j}=(\\frac{u_{i-1/2,j}+u_{i+1/2,j}}{2},\\frac{v_{i,j-1/2}+v_{i,j+1/2}}{2}),\\\\ \\vec u_{i+1/2,j}=(u_{i+1/2,j},\\frac{v_{i,j-1/2}+v_{i,j+1/2}+v_{i+1,j-1/2}+v_{i+1,j+1/2}}{4}),\\\\ \\vec u_{i,j+1/2}=(\\frac{u_{i-1/2,j}+u_{i+1/2,j}+u_{i-1/2,j+1}+u_{i+1/2,j+1}}{4},v_{i,j+1/2}).\\tag {3.7}&emsp;&emsp;最后，在实现中下标索引一般没有浮点数之说，前面直接采用$i+1/2$的记法是为了便于叙述。一般约定如下： p(i,j,k)=p_{i,j,k},\\\\ u(i,j,k)=u_{i-1/2,j,k},\\\\ v(i,j,k)=v_{i,j-1/2,k},\\\\ w(i,j,k)=w_{i,j,k-1/2}. \\tag{3.8}&emsp;&emsp;因而对于$nx\\times ny\\times nz$分辨率的网格，压力数值存储在$nx\\times ny\\times nz$的数组中，速度的$u$成分存储在$(nx+1)\\times ny\\times nz$数组中，速度的$v$成分存储在$nx\\times (ny+1)\\times nz$数组中，速度的$w$成分存储在$nx\\times ny\\times (nz+1)$数组中。 四、对流算法&emsp;&emsp;求解如下所示的对流方程是流体模拟的关键一步： \\frac{Dq}{Dt}=0 \\tag {4.1}&emsp;&emsp;我们把这个对流数值计算的算法记为： q^{n+1}=advect(\\vec u,\\Delta t,q^n) \\tag {4.2}&emsp;&emsp;公式$(4.2)$中的各个符号含义： &emsp;&emsp;$\\vec u$：在$MAC$网格上的离散化的速度场； &emsp;&emsp;$\\Delta t$：时间步长； &emsp;&emsp;$q^n$：当前的物理量场$q$（如流体密度、速度、燃烧物浓度等）； &emsp;&emsp;$q^{n+1}$：经过对流后得到的新的量场。 &emsp;&emsp;在这里要特别注意，输入对流算法的速度场$\\vec u$必须是无散度的，否则模拟结果会出现一些奇怪的失真现象。 1、半拉格朗日对流算法（Semi-Lagrangian Advection）&emsp;&emsp;一维情况下，对流方程$(4.1)$写成偏微分的形式如下： \\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}=0 \\tag {4.3}&emsp;&emsp;分别采用前向欧拉差分法计算对时间的偏导和中心差分法计算对空间的偏导，我们有： \\frac{q^{n+1}_{i}-q^n_i}{\\Delta t}+u^n_i\\frac{q^n_{i+1}-q^n_{i-1}}{2\\Delta x}=0 \\tag {4.4}&emsp;&emsp;转成以$q^{n+1}_i$为计算目标的显式公式，得： q^{n+1}_i=q^n_i-\\Delta t u^n_i\\frac{q^n_{i+1}-q^n_{i-1}}{2\\Delta x} \\tag {4.5}&emsp;&emsp;公式$(4.5)$看起来没什么问题，但是却存在非常严重的漏洞。首先，前向欧拉法被证明是无条件不稳定的空间离散方法：无论取多么小Δ𝑡，随着时间步的推进，累积误差终将发散。即使使用更稳定的时间积分方法来取代前向欧拉方法，解决了时间上的PDE（Partial Differential Equation，偏微分方程）计算，空间上的PDE计算还是会带来重大的麻烦。标准中心差分方法不可避免地会出现的零空间问题，具有高频震荡性质的速度场对空间的导数被错误地计算为$0$或几乎为$0$，低离速度分量被分离出来，从而导致模拟效果中出现许多奇怪的高频摆动和震荡。 &emsp;&emsp;针对这些问题，研究者们提出了一个解然不同的、更加简单和更具物理直观意义的半拉格朗日法。之所以叫半拉格朗日法，是因为这种方法是以拉格朗日视角去解决欧拉视角的对流方程（“半”字的由来）。假设我们的目标是求解网格点$\\vec x_G$的在第$n+1$个时间步时关于物理量$q$的新值，记为$q^{n+1}_G$。在拉格朗日的视角下，我们可以寻找在第$n+1$时间步之前，是空间中的哪一个点上的流体粒子在速度场$\\vec u$的作用下“流向”了$\\vec x_G$，我们记这个粒子在第$n$个时间步时的网格位置为$\\vec x_P$，则第$n+1$个时间步时$\\vec x_G$的$q^{n+1}_G$即为第$n$个时间步时$\\vec x_P$的$q^{n}_P$。如下图4-1为半拉格朗日对流法的示意图。 图4-1 半拉格朗日对流法 &emsp;&emsp;半拉格朗日对流法的第一步就是要找出$\\vec x_P$，为此我们根据$\\vec x_G$做反向的追踪。粒子位置对时间的导数就是速度场： \\frac{d\\vec x}{dt}=\\vec u(\\vec x) \\tag {4.6}&emsp;&emsp;经过一个时间步长$\\Delta t$之后，粒子由$\\vec x_P$移动到$\\vec x_G$。为了得到$\\vec x_P$，最简单的方法就是采用前向欧拉法进行倒推： \\vec x_P=\\vec x_G-\\Delta t\\vec u(\\vec x_G) \\tag {4.7}&emsp;&emsp;然而前向欧拉法只有一阶的精度，若在不改变$\\Delta t$的情况下提高精度，我们可以采用高阶的龙格库塔法（Runge-Kutta method）。采用二阶的龙格库塔法如下所示： \\vec x_{mid}=\\vec x_G-\\frac12\\Delta t\\vec u(\\vec x_G),\\\\ \\vec x_P=\\vec x_G-\\Delta t\\vec u(\\vec x_{mid}). \\tag {4.7}&emsp;&emsp;倒推得到$\\Delta t$之前的网格位置$\\vec x_P$一般不会恰好在网格顶点上，为此我们需要做些插值。三维模拟通常采用三线性插值，而二维的则采用双线性插值。 q^{n+1}_G=interpolate(q_n,\\vec x_P) \\tag {4.8}2、边界情况&emsp;&emsp;若我们倒推得到的$\\vec x_P$仍然在流体的内部，那么做插值是完全没问题的。但若$\\vec x_P$在流体的边界之外呢？这种情况的出现的原因通常有两个：一个是$\\vec x_P$确确实实在流体的外部且即将流入流体内部，另一个是由前向欧拉法或龙格库塔法的数值计算方法带来的误差导致。 &emsp;&emsp;在一种情况下，我们应该知道当流体流入时其携带的物理量，此时我们将这个外部流入的物理量作为返回值即可。例如，第$n$个时间步时的外部流体以速度$\\vec U$和温度$T$在第$n+1$个时间步时注入流体内部$\\vec x_G$的位置，那么$\\vec T^{n+1}_G$的值就为$T$。 &emsp;&emsp;在第二种由误差导致的情况下，一个适当的策略就是根据边界上的最近点外推出所求得物理量。在模拟某些流体时，外推变得很简单。例如，在模拟烟雾时我们简单地假设烟雾流体外部即空气的速度风场为某个常数$\\vec U$（可能为$0$），这样边界上的速度场都取$\\vec U$。但还有一些必须根据流体内部的已知量外推出未知量，这时情况就变得比较复杂了。具体如何外推将在后面介绍，目前我们只需要知道大概的步骤：首先寻找边界上的最近点，然后在最近点的领域内插值获取相应的物理量场。 3、时间步长大小&emsp;&emsp;对任何一种数值计算方法的主要的考虑点就是它是否稳定。幸运的是，半拉格朗日对流法已经被证明是一种无条件稳定的算法：无论$\\Delta t$取多大，它永远不会出现数值爆炸的现象。因为每一个新值$q$的确定，都是通过对旧值得插值，无论是线性插值、双线性插值还是三线性插值，$q$的大小都是处于插值点之间，不会得到比原来插值点更大或者更小的值，因而$q$是有上下界的。这使得我们可以尽情地根据所需的模拟质量和模拟效率去调整时间步长。 &emsp;&emsp;但是在实践中，时间步长的大小也不能选得太过极端，否则会产生一些奇观的现象。Foster和Fekiw提出了一个对$\\Delta t$的限制：流体粒子在$\\Delta t$内的倒推轨迹最多经过某个常数个网格单元为宜，例如5个： \\Delta t \\leq \\frac{5\\Delta x}{u_{max}} \\tag {4.9}&emsp;&emsp;公式$(4.9)$中，$u_{max}$是速度场的最大值，我们可以简单地取 存储在网格中的最大速度值。一个更鲁棒的方法考虑了体积力（如重力、浮力等）对最大速度的影响： u_{max}=max(|u^n|)+\\Delta t|g| \\tag {4.10}&emsp;&emsp;将不等式$(4.9)$的最大值带入公式$(4.10)$，我们有： u_{max}=max(|u^n|)+\\frac{5\\Delta x}{u_{max}}|g| \\tag {4.11}&emsp;&emsp;取一个简单的速度上界（简化了公式$(4.11)$），$u_{max}$： u_{max}=max(|u^n|)+\\sqrt{5\\Delta xg} \\tag {4.12}&emsp;&emsp;这样确保了$u_{max}$始终为正，且避免公式$(4.9)$的除$0$错误。 &emsp;&emsp;关于时间步长的讨论离不开$CFL$（以Courant、Friedrichs、Lewy三人的名字命名）条件。$CFL$条件是一个简单而直观的判断计算是否收敛的必要条件。它的直观物理解释就是时间推进求解的速度必须大于物理扰动传播的速度，只有这样才能将物理上所有的扰动俘获到。满足$CFL$条件意味着当$\\Delta x$和$\\Delta t$趋于取极限$0$时，数值计算所求的解就会收敛到原微分方程的解。 &emsp;&emsp;对于半拉格朗日对流法，其满足$CFL$条件当且仅当在极限情况下，追踪得到的粒子轨迹足够逼近真实的轨迹。足够逼近的意思是经过正确的网格插值能够得到正确的依赖域（即差分格式的依赖域包含了原微分方程的依赖域），追踪的轨迹就会收敛到正确真实的轨迹。 &emsp;&emsp;因而，对于采用标准的显式有限差分法的对流方程求解，为了保证收敛，我们要求$q^{n+1}$的新值是由以当前网格点为中心、以$C\\Delta x$（$C$是一个小的整数常量）为半径的邻域范围内插值得到： \\Delta t \\leq C\\frac{\\Delta x}{|\\vec u|} \\tag {4.13}&emsp;&emsp;公式$(4.13)$中的$C$被称为$CFL$数，因而不等式$(4.9)$可以看成是公式$(4.13)$取$CFL$数为$5$得到。 4、数值耗散&emsp;&emsp;对流算法在对流获取新的物理量场$q^{n+1}_i$时会进行一些插值操作，插值不可避免地会平滑物理量场，这带来了一些数值耗散。一次两次的数值耗散不会由太大的影响，但是在流体模拟中我们会在每个时间步都进行对流运算，反反复复的平滑操作将数值耗散不断扩大，损失大量的流体细节。 &emsp;&emsp;以一维的对流项计算为例，流体速度为常量$u&gt;0$： \\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}=0 \\tag {4.14}&emsp;&emsp;假设$\\Delta t &lt; \\frac{\\Delta x}{u}$，即单个时间步长内粒子追踪轨迹长度小于单个网格单元的大小。我们的目标点是$x_i$，则倒推得到的粒子位置就落在了$[x_{i-1},x_i]$上的$x_i-\\Delta tu$，然后进行线性插值得到$q^{n+1}_i$： q^{n+1}=\\frac{\\Delta tu}{\\Delta x}q^n_{i-1}+(1-\\frac{\\Delta tu}{\\Delta x})q^n_i \\tag {4.15}&emsp;&emsp;将公式$(4.15)$整理一下，有： q^{n+1}_i=q^n_i-\\Delta tu\\frac{q^n_i-q^n_{i-1}}{\\Delta x} \\tag {4.16}&emsp;&emsp;公式$(4.16)$实际上正好就是采用时间上的前向欧拉差分法和空间上的单向有限差分法的欧拉方案，把$q^n_i$看成是$q^n$关于$x_i$的函数，对$q^n_{i-1}$进行泰勒级数展开： q^n_{i-1}=q^n_i-(\\frac{\\partial q}{\\partial x})^n_i\\Delta x+(\\frac{\\partial^2q}{\\partial x^2})^n_i\\frac{\\Delta x^2}{2}+O(\\Delta x^3) \\tag {4.17}&emsp;&emsp;将公式$(4.17)$代入公式$(4.16)$，并做一些变量消去，可得： q^{n+1}_i=q^n_i-\\Delta tu(\\frac{\\partial q}{\\partial x})^n_i+\\Delta tu\\Delta x(\\frac{\\partial^2q}{\\partial x^2})^n_i+O(\\Delta x^2) \\tag {4.18}&emsp;&emsp;在二阶截断误差的情况下，结合公式$(4.18)$和公式$(4.14)$，有： \\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}=u\\Delta x(\\frac{\\partial^2q}{\\partial x^2}) \\tag {4.19}&emsp;&emsp;右边就是对流方程计算时引入的额外类似粘度乘上系数$u\\Delta x$的项。这也就是说，当我们采用简单的半拉格朗日法去求解无粘度的对流方程时，模拟的结果却看起来我们像时在模拟有粘度的流体。这就是数值耗散！当然，当$\\Delta x\\to 0$时，这个数值耗散系数也会趋于$0$，所以取时间步无穷小时能够得到正确的模拟结果，但这需要耗费巨额的计算资源开销。我们通常模拟的流体大多数都是无粘度的，所以如何减少这个数值耗散是个至关重要的难题。 &emsp;&emsp;一个简单有效的修复数值耗散的方法就是采用更加锐利的插值方法，从而尽可能地减少由插值带来的数值耗散。在一维的情况时，我们采用三次插值（cubic interpolant）如下公式$(4.21)$，而不是简单的一次线性插值$(4.20)$： q\\approx(1-s)x_i+sx_{i+1} \\tag {4.20} q\\approx[-\\frac13s+\\frac12s^2-\\frac16s^3]q_{i-1}+[1-s^2+\\frac12(s^3-s)]q_i\\\\ +[s+\\frac12(s^2-s^3)]q_{i+1}+[\\frac16(s^3-s)]q_{i+2} \\tag {4.21}&emsp;&emsp;扩展到二维或者三维就是双三次插值（bicubic interpolation）或三三次插值（tricubic interpolation）。以二维情况为例，我们可以先沿着$x$轴做第一遍的三次插值如公式$(4.22)$，然后再沿着$y$轴做第二遍插值如公式$(4.23)$： q_{j-1}=w_{-1}(s)q_{i-1,j-1}+w_0(s)+q_{i,j-1}+w_1(s)q_{i+1,j-1}+w_2(s)q_{i+2,j-1},\\\\ q_{j}=w_{-1}(s)q_{i-1,j}+w_0(s)+q_{i,j}+w_1(s)q_{i+1,j}+w_2(s)q_{i+2,j},\\\\ q_{j+1}=w_{-1}(s)q_{i-1,j+1}+w_0(s)+q_{i,j+1}+w_1(s)q_{i+1,j+1}+w_2(s)q_{i+2,j+1},\\\\ q_{j+2}=w_{-1}(s)q_{i-1,j+2}+w_0(s)+q_{i,j+2}+w_1(s)q_{i+1,j+2}+w_2(s)q_{i+2,j+2}. \\tag {4.22} q=w_{-1}(t)q_{j-1}+w_0(t)q_j+w_1(t)q_{j+1}+w_2(t)q_{j+2} \\tag {4.23}&emsp;&emsp;当然也可以先沿着$y$轴，然后再沿着$x$轴做插值操作。","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"http://yoursite.com/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"http://yoursite.com/tags/Computer-Graphics/"},{"name":"Naiver-Stokes Equations","slug":"Naiver-Stokes-Equations","permalink":"http://yoursite.com/tags/Naiver-Stokes-Equations/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"http://yoursite.com/tags/Fluid-Simulation/"},{"name":"Advection","slug":"Advection","permalink":"http://yoursite.com/tags/Advection/"}]}]}