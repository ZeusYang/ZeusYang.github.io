{"meta":{"title":"YangWC's Blog","subtitle":null,"description":"Personal blog website.","author":"WC Yang","url":"https://yangwc.com/about","root":"/"},"pages":[{"title":"404 Not Found","date":"2019-04-27T07:26:21.624Z","updated":"2019-04-27T07:26:21.624Z","comments":true,"path":"404.html","permalink":"https://yangwc.com/about/404.html","excerpt":"","text":"**404 Not Found** **�ܱ�Ǹ�������ʵ�ҳ�治����** �����������ַ�����õ�ַ�ѱ�ɾ��"},{"title":"关于","date":"2019-09-28T03:20:39.601Z","updated":"2019-09-28T03:20:39.601Z","comments":true,"path":"about/1.html","permalink":"https://yangwc.com/about/about/1.html","excerpt":"","text":"个人本科：中山大学计算机科学与技术，已毕业硕士：中山大学软件工程，一年级在读研究意向：计算机图形学，流体模拟地址：广州市番禺区大学城外环东路中山大学数据科学与计算机学院A207邮箱：1579148717@qq.comGithub：https://github.com/ZeusYang实验室主页：https://sysu-imsl.github.io 一些收集的网站论文收录网站： Siggraph论文收录网址 皮克斯在线论文库 Physics-Based Animation 国内图形学论坛： 计算机图形学与混合现实研讨会 免费3D模型下载： Free 3D Pbr Models 免费 3D 模型 学习资源： GPU Gems 鸠摩搜索（各种电子pdf搜索引擎） Eurographics Tutorial 2019 Fluid Simulation for Video Games Learn Computer Graphics From Scratch! Hybrid Lagrangian-Eulerian Simulation Methods 图形学大佬： Robert Bridson Doyub Kim Christopher Batty 闫令琪 张心欣 蒋陈凡夫 Tamar Shinar 胡渊鸣 北航高阳 北航翟骁 Shader： GLSL Sandbox Shader toy 关于本站欢迎来到 YangWC 的博客！本站会记录自己的一些学习内容，如若有错，欢迎指正，感谢！ 关于主题本站的主题风格是：Material X有任何问题请留言。"},{"title":"所有分类","date":"2019-04-27T08:54:04.778Z","updated":"2019-04-27T08:54:04.778Z","comments":true,"path":"categories/index.html","permalink":"https://yangwc.com/about/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2019-04-27T08:02:52.306Z","updated":"2019-04-27T08:02:52.306Z","comments":true,"path":"tags/index.html","permalink":"https://yangwc.com/about/tags/index.html","excerpt":"","text":""},{"title":"大佬的博客","date":"2019-09-10T02:18:51.844Z","updated":"2019-04-27T10:52:08.692Z","comments":true,"path":"friends/index.html","permalink":"https://yangwc.com/about/friends/index.html","excerpt":"","text":"名称： YangWC’s Blog头像： https://cdn.jsdelivr.net/gh/ZeusYang/CDN-for-yangwc.com@1.1.4//globalImage/avator.jpg网址： https://yangwc.com"},{"title":"about","date":"2019-09-28T07:59:42.818Z","updated":"2019-09-28T07:59:42.818Z","comments":true,"path":"about/index.html","permalink":"https://yangwc.com/about/about/index.html","excerpt":"","text":"Wencong Yang(杨文聪)I am a first-year master student in Intelligent and Multimedia Science Laboratory at Sun Yat-sen University (SYSU), advised by Prof. Chengying GAO. I do research in Computer Graphics, with primary focus on fluid simulation. Currently I am investigating methods for high-performance fluid surface reconstruction for particle-based fluid simulation. LocationGuangzhou, ChinaSun Yat-sen University (SYSU)（中山大学）School Of Data and Computer Science（数据科学与计算机学院）Intelligent and Multimedia Science Laboratory（智能与多媒体科学实验室） Education 2015.8 ~ 2019.7: Bachelor of Engineering in Computer Science in Sun Yat-sen University 2019.9 ~ 2021.7: Master of Engineering in Software Engineering in Sun Yat-sen University Projects FluidEngineAn offline fluid simulation engine for computer graphics applications. Features: Basic math and geometry operations and data structures; Jacobi, Gauss-Seidel, SOR, MG, CG, ICCG, and MGPCG linear system solvers; Spherical, SPH, Zhu & Bridson, and Anisotropic kernel for points-to-surface converter; Intel TBB multi-threading backends; SPH and PCISPH fluid simulators; Converters between signed distance function and triangular mesh; Stable fluids-based smoke simulator (Pure Euler fluid solver); Level set-based liquid simulator. Soft-RendererA soft renderer using C++ from scratch without any graphics library. Features: A simple 3D rendering pipeline built from scratch; Bresenham algorithom for triangle rasterization; Back face culling and Invisible elimination; Texture mapping and bilinear texture filtering; Obj loader and Blin-Phong lighting. Ray-TracerAn offline renderer : path tracer. Just a personal toy for learning and playing. Features: BVH tree for fast intersection detection; Intel TBB multi-threading for rendering acceleration; Monte Carlo method for important sampling; Lambertian, dielectric and metal material. This is a tiny path tracer and I would like to polish it. PhysicallyBasedRendererPhysically based renderer using OpenGL library. Features: High Dynamic Range; Bloom Effect for glowing; Cook-Torrance(BRDF lighting model) Physically Based Rendering pipeline; Screen Space Ambient Occlusion for dark details; Defered Shading for large amount of light sources; Image Based Lighting for enviroment map. Nowadays , physically based rendering is popular for its realistic and awesome visual effect. ContactPlease feel free to let me know if you catch any error in my blog. Academic discussions are welcome. Leaving a comment is ok I guess. Github: yangwc Wechat: ywc1579148717 E-mail: 1579148717@qq.com ResourcesSome pretty useful and interseting resources and links collected. Conferences/Periodicals Papers: SIGGRAPH、SIGGRAPH Asia、EG、PG、I3D; Physics-Based Animation; Pixar Technical Memos 3D Models Downloading: Free 3D Pbr Models; Free 3D models for CG digital design and artwork; Humster3D: the biggest online collection of cars, electronics, weapons and other hot things.; Yobi3D: Free 3D Models Search Engine; Rendering Resources Tutorials/Courses on Computer Graphics: Learn OpenGL; Learn Vulkan; GPU Gems; Fluid Simulation for Video Games; Learn Computer Graphics From Scratch!; 3d-game-shaders-for-beginners; TU Wien Rendering/Ray Tracing Course; CMU 15-462/662: Computer Graphics Course; Stanford CS148: Introduction to Computer Graphics and Imaging; UCSB CS291A: Real-Time High Quality Rendering Course; DTU 02941: Physically Based Rendering and Material Appearance Modelling; Advances in Real-Time Rendering in 3D Graphics and Games; Rasterization in One Weekend Books on Computer Graphics: 《Real-Time Rendering》; 《Physically Based Rendering: From Theory To Implementation》; 《全局光照技术》; 《Fluid Engine Development》; 《Fluid Simulation For Computer Graphics》; 《Ray Tracing in One Weekend》; 《Ray Tracing: the Next Week》; 《Ray Tracing: the Rest of Your Life》; 《Ray Tracing Gems》; 《Fundamentals of Computer Graphics, Fourth Edition》; 《Real-Time Rendering 3rd》提炼总结; 《Real-Time Collision Detection, Christer Ericson》; 《The art of fluid animation》 Researchers and Scholars: Robert Bridson; Doyub Kim; Christopher Batty; Xinxin Zhang（张心欣）; Lingqi Yan（闫令琪）; Chenfanfu Jiang（蒋陈凡夫）; Prof. Dr. Ligang Liu（刘利刚）; Tamar Shinar; Yuanming Hu（胡渊鸣）; Yang Gao（高阳）; Xiao Zhai（翟骁） Something Interesting: GLSL Sandbox; Shader toy; Jiumo Search; McGuire Computer Graphics Archive; Awesome Creative Coding; Coding Labs; HUMUS; TYLER HOBBS; casual-effects"}],"posts":[{"title":"流体模拟Fluid Simulation：Free-Surface Flow","slug":"FreeSurfaceFlow","date":"2019-09-22T13:58:59.573Z","updated":"2019-09-27T12:46:37.763Z","comments":true,"path":"2019/09/22/FreeSurfaceFlow/","link":"","permalink":"https://yangwc.com/about/2019/09/22/FreeSurfaceFlow/","excerpt":"在基于欧拉网格的烟雾模拟中，流体与自由面的接触面是一个模糊的边界，因此无需对烟雾的边界进行对流。但在液体模拟中，流体与自由面之间是有一个明确的边界，因此我们需要显式地跟踪这个流体边界来模拟液体的物理行为，基于水平集的自由表面流（Free-Surface Flow）是目前的主流方法。","text":"在基于欧拉网格的烟雾模拟中，流体与自由面的接触面是一个模糊的边界，因此无需对烟雾的边界进行对流。但在液体模拟中，流体与自由面之间是有一个明确的边界，因此我们需要显式地跟踪这个流体边界来模拟液体的物理行为，基于水平集的自由表面流（Free-Surface Flow）是目前的主流方法。 流体表面的定义 流体的水平集对流 流体水平集的扭曲矫正 参考资料 Free-Surface Flow &emsp;&emsp;在基于欧拉网格的烟雾模拟中，流体与自由面的接触面是一个模糊的边界，因此无需对烟雾的边界进行对流。但在液体模拟中，流体与自由面之间是有一个明确的边界，因此我们需要显式地跟踪这个流体边界来模拟液体的物理行为，基于水平集（Level set）的方法是目前的主流方法。 ## 一、流体表面的定义 &emsp;&emsp;在基于粒子的流体模拟中，我们无需显式地定义液体与自由面的边界，流体粒子所占据的区域就是液体所在的区域。通过周围的流体粒子，我们能够确定给定的区域是否处于流体内部还是外部。但是在基于欧拉网格的流体模拟中，我们很难直接确定区域是处于流体内部还是外部，这是因为流体表面是连续可微的。如果直接给欧拉网格一个二值标量场，$0$表示流体外部，$1$表示流体内部，那么在流体的边界处该标量场不是连续的，因而也不是可微的。保证流体边界处连续可微对于模拟高质量的流体来说非常重要，这是因为流体表面张力或者计算法线向量的计算都需要在流体表面做一些微分算子。 &emsp;&emsp;目前最为主流的方法就是采用隐式表面也就是符号距离场来表示流体的边界。隐式表面具有许多非常良好的性质。给定一个隐式表面的符号距离场函数$\\phi(x)$，那么我们可以通过符号来判断一个区域是处于流体内部还是外部，隐式表面被定义在$\\phi(x)=0$的零等值面上（如图1中的虚线）。$\\phi(x)$的梯度满足如下方程（程函方程）： $$ |\\nabla \\phi(x)|= 1\\tag {1} $$ 图1 符号距离场 &emsp;&emsp;符号距离场在表面附近的函数值是连续可微的，因而它能够表示连续平滑的流体表面。采用符号距离场来对物体表面进行建模的方法通常被称为水平集法（Level set method），目前水平集在各个领域都有涉及，流体模拟只是其中之一。通过将一个表面转换成符号距离场，我们可以非常容易地计算表面的一些几何性质。 &emsp;&emsp;水平法一个非常有用的点就是它处理拓扑结构变化的能力。如下图2所示，这张图分别展示了显式表面和隐式表面在处理圆形表面扩张时的差别。圆形表面向外扩张时，显式表面需要将每一个网格点沿着法线向量向外挪动得到扩张后的几何体，而隐式表面只需将其相应的符号距离场值减去扩张距离即可。当几何体大到一定的程度时，两个圆形会产生相交的区域（在流体中表现为液滴相融），显式表面法需要逐个计算相交点，非常复杂且很容易出错；而隐式表面无需额外的特殊处理。这就是隐式表面的优势所在。 图2 (a)显式以及(b)隐式的圆形表面扩张 &emsp;&emsp;水平集法尽管在这里有众多优势，但是实现起来也没那么容易。采用水平集法跟踪流体表面需要解决两个主要问题。一个就是如何正确地追踪流体表面，这个其实就是一个流体对流问题；另外一个就是如何解决符号距离场对流带来的扭曲问题，这是因为在对流时我们不能够很好地保持符号距离场的几何性质。 二、流体的水平集对流&emsp;&emsp;在对流步骤，流体的水平集与流体其他的标量场属性别无二致，因此我们只需将流体的水平集传入半拉格朗日对流解算器即可。但是因为流体的符号距离场并不是流体本身的物理属性，因此采用半拉格朗日法对符号距离场对流会在距离流体表面较远的地方带来一些扭曲的问题，这个我们稍后再看。 &emsp;&emsp;在常见的液体模拟中，实际上还有一个流体被我们忽略了，这个流体就是空气。普遍情况下，空气对液体的影响比较小，几乎可以忽略，因此一种简化的液体模拟模型——自由表面流（Free-Surface Flow）被提出。这种流体模拟假设空气对液体的压强、粘性作用很小，通常空气的压强被设置为某一个给定的常量值。这种液体模型是最简单的，但是也能够模拟出非常真实的液体。此外，还有一些模拟例如泡状流（Bubby Flow）能够模拟空气与液体的交互现象，这类流体模拟属于多相流模型（Multiphase Fluid Flow）。多相流在同一个系统中采用不同的密度、粘度系数等来模拟流体模拟多种不同的流体，因而较为复杂。 &emsp;&emsp;这里目前讨论的还是属于自由表面流，我们通过构建额外的流体水平集标量场来追踪流体的表面，流体的水平集和其他可对流的标量场一样在对流阶段采用半拉格朗日对流法进行对流。自由表面流直接忽略空气的流体量场，因此在对流时我们需要处理空气区域的速度场，因为自由表面流对空气区域的速度场是没有定义的，如下图3(a)所示，圆形区域是流体区域，在圆形区域外速度场无定义。但在半拉格朗日对流法中，由于其后向追踪的特性，我们有可能会追踪流体区域外部的空气区域去获取流体的速度场，因此我们需要做一个速度场外推（extrapolation）操作。 图3 (a)流体无外推的速度场,(b)外推后的速度场 &emsp;&emsp;将速度场从流体区域外推到空气区域与从流体区域外推到固体墙区域的过程类似，我们简单地采用诺伊曼边界条件，使速度场在法线方向上的变化率为零： \\frac{\\partial v}{\\partial n}=0 \\tag {2}&emsp;&emsp;我们通过在法线方向外推流体的速度场来求解上述的边界条件。最大的外推距离取决于流体的CFL距离，超出CFL距离的外推没有意义，因为即便在边界处逆向追踪，其追踪的距离也不会超过CFL距离。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960void LevelSetLiquidSolver3::extrapolateVelocityToAir(double currentCfl)&#123; auto sdf = signedDistanceField(); auto vel = gridSystemData()-&gt;velocity(); auto u = vel-&gt;uAccessor(); auto v = vel-&gt;vAccessor(); auto w = vel-&gt;wAccessor(); auto uPos = vel-&gt;uPosition(); auto vPos = vel-&gt;vPosition(); auto wPos = vel-&gt;wPosition(); Array3&lt;char&gt; uMarker(u.size()); Array3&lt;char&gt; vMarker(v.size()); Array3&lt;char&gt; wMarker(w.size()); uMarker.parallelForEachIndex([&amp;](size_t i, size_t j, size_t k) &#123; if (isInsideSdf(sdf-&gt;sample(uPos(i, j, k)))) uMarker(i, j, k) = 1; else &#123; uMarker(i, j, k) = 0; u(i, j, k) = 0; &#125; &#125;); vMarker.parallelForEachIndex([&amp;](size_t i, size_t j, size_t k) &#123; if (isInsideSdf(sdf-&gt;sample(vPos(i, j, k)))) vMarker(i, j, k) = 1; else &#123; vMarker(i, j, k) = 0; v(i, j, k) = 0; &#125; &#125;); wMarker.parallelForEachIndex([&amp;](size_t i, size_t j, size_t k) &#123; if (isInsideSdf(sdf-&gt;sample(wPos(i, j, k)))) wMarker(i, j, k) = 1; else &#123; wMarker(i, j, k) = 0; w(i, j, k) = 0; &#125; &#125;); const Vector3D gridSpacing = sdf-&gt;gridSpacing(); const double h = max3(gridSpacing.x, gridSpacing.y, gridSpacing.z); const double maxDist = std::max(2.0 * currentCfl, _minReinitializeDistance) * h; std::cout &lt;&lt; \"Max velocity extrapolation distance: \" &lt;&lt; maxDist &lt;&lt; std::endl; FmmLevelSetSolver3 fmmSolver; fmmSolver.extrapolate(*vel, *sdf, maxDist, vel.get()); applyBoundaryCondition();&#125; &emsp;&emsp;有了外推值，我们就可以应用半拉格朗日对流解算器了。 12345678910void LevelSetLiquidSolver3::computeAdvection(double timeIntervalInSeconds)&#123; double currentCfl = cfl(timeIntervalInSeconds); Timer timer; extrapolateVelocityToAir(currentCfl); std::cout &lt;&lt; \"velocity extrapolation took \" &lt;&lt; timer.durationInSeconds() &lt;&lt; \" seconds\\n\"; GridFluidSolver3::computeAdvection(timeIntervalInSeconds);&#125; 三、流体水平集的扭曲矫正&emsp;&emsp;前面我们提到，由于流体的水平集并不是流体自身的物理属性，因而在进行半拉格朗日对流时我们没有办法很好地保证水平集的优良几何属性在对流过程中不被破坏。事实上，在经过对流之后，水平集在流体表面附近的几何属性依旧保持得很好，只是在距离表面比较远的地方它的符号距离场值不再代表当前的点到表面的最近距离（如下图4(c)所示），当然符号没有被破坏。注意到在流体表面处的符号距离场值被保持得很好，因此我们可以通过表面附近的值去修正远离表面的被扭曲的值（如下图4(d)所示）。 图4 (a)初始距离场,涡流(b)旋转导致距离场扭曲(c),复原后的距离场(d) &emsp;&emsp;在经过对流之后，流体的符号距离场被扭曲的现象在数学上被解释为不再满足程函方程$|\\nabla \\phi(x)|=1$，即前面的公式$(1)$。但我们知道$\\phi(x)$的符号依然是正确的，而且在流体表面附近$\\phi(x)$依然是一个正确的值。因此为了恢复流体的符号距离场，一种暴力的方法就是遍历所有的欧拉网格点并找寻找其到表面的最近点，显然这种方法太过复杂暴力且非常耗时。我们可以采用一个相反的策略。 图5 (a)复原后的距离场,(b)距离场修复过程 &emsp;&emsp;以图5(b)的一维情况为例（图中的水平线是零等值线），因为在零等值线上的距离场是没有被扭曲的（图5(b)中的虚线与实线在零等高线处的梯度相等，虚线是修复的距离场），因此我们可以从零等值线附近的欧拉网格点出发，逐步扩散到其他网格点，在扩散的过程中累加距离场。在一维的情况中，扩散的方向就是左和右两个方向，通过这个扩散过程逐步矫正经过的网格点的符号距离场，最终得到正确的符号距离场（如图5(a)所示）。 &emsp;&emsp;这个矫正的过程实际上跟波在空间中的传播很类似，因此我们可以借鉴流体的对流方程并做一些修改： \\frac{\\partial \\phi}{\\partial t}+u\\cdot \\nabla \\phi=1 \\tag {3}&emsp;&emsp;其中$u$是速度场（注意这里的速度场不再是流体的速度场），$t$是一个伪时间轴（因为这并不是物理模拟，它属于几何处理）。与原对流方程不同，方程右边是$1$而不是$0$。当方程右边是$0$时，它表明$\\phi$仅在速度场$u$的作用下变化；当方程右边是某个不为零的常量$c$时，它表明$\\phi$在速度场$u$的作用下经过一个单位距离时$\\phi$的增量为$c$。因此令方程右边的$c=1$就是使得$\\phi$在速度场$u$的作用下经过一个单位距离时$\\phi$的增量就是一个单位的距离。 &emsp;&emsp;公式$(3)$中的速度场就是传播速度，我们令其沿着表面的法线方向传播。我们根据隐式表面的符号距离场计算表面的单位法线向量： n=\\frac{\\nabla \\phi(x)}{|\\nabla \\phi(x)|} \\tag {4}&emsp;&emsp;在远离表面被扭曲的地方采用公式$(4)$计算法线向量显然会带来一些误差，这些暂且不管。将公式$(3)$中的$u$替换为公式$(4)$的法线场： \\frac{\\partial \\phi}{\\partial t}+\\frac{\\nabla \\phi}{|\\nabla \\phi|}\\cdot \\nabla \\phi=1 \\tag {5}&emsp;&emsp;然后公式$(5)$可以进一步简化为： \\frac{\\partial \\phi}{\\partial t}+(|\\nabla \\phi|-1)=0 \\tag {6}&emsp;&emsp;需要注意公式$(6)$仅适用于正距离场区域（也就是流体外部区域），对于负距离场区域，我们将采用的是下面的公式： \\frac{\\partial \\phi}{\\partial t}-(|\\nabla \\phi|-1)=0 \\tag {7}&emsp;&emsp;把公式$(6)$和公式$(7)$合并一起，可以写成如下的形式： \\frac{\\partial \\phi}{\\partial t}+sign(\\phi)(|\\nabla \\phi|-1)=0 \\tag {8}&emsp;&emsp;公式$(8)$其实不难理解。如果我们有一个正确的符号距离场，那么其满足方程$|\\nabla \\phi|=1$即$|\\nabla \\phi|-1=0$，此时由公式$(8)$可知$\\frac{\\partial \\phi}{\\partial t}=0$，这意味着$\\phi$不会发生变化，这也正是我们想要的（因为不需要矫正了）。而若$\\phi$产生了一下扭曲，导致一些区域的$|\\nabla \\phi|\\neq 1$，这就是说$|\\nabla\\phi|-1\\neq 0$，这个相当于一个误差项使得我们的距离场朝着误差项减小的方向变化，最终达到$|\\nabla \\phi|=1$。 &emsp;&emsp;最后就是关于公式$(8)$的实现了，我们通过迭代的方式来实现公式$(8)$，设置一个时间步长和迭代次数，在每一次的迭代中计算当前的$\\frac{\\partial \\phi}{\\partial t}$，然后采用一个显式欧拉积分更新流体的水平集。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758void IterativeLevelSetSolver3::reinitialize( const ScalarGrid3&amp; inputSdf, double maxDistance, ScalarGrid3* outputSdf) &#123; const Size3 size = inputSdf.dataSize(); const Vector3D gridSpacing = inputSdf.gridSpacing(); ArrayAccessor3&lt;double&gt; outputAcc = outputSdf-&gt;dataAccessor(); const double dtau = pseudoTimeStep(inputSdf.constDataAccessor(), gridSpacing); const unsigned int numberOfIterations = distanceToNumberOfIterations(maxDistance, dtau); copyRange3(inputSdf.constDataAccessor(), size.x, size.y, size.z, &amp;outputAcc); Array3&lt;double&gt; temp(size); ArrayAccessor3&lt;double&gt; tempAcc = temp.accessor(); std::cout &lt;&lt; \"Reinitializing with pseudoTimeStep: \" &lt;&lt; dtau &lt;&lt; \" numberOfIterations: \" &lt;&lt; numberOfIterations &lt;&lt; std::endl; for (unsigned int n = 0; n &lt; numberOfIterations; ++n) &#123; inputSdf.parallelForEachDataPointIndex( [&amp;](size_t i, size_t j, size_t k) &#123; double s = sign(outputAcc, gridSpacing, i, j, k); std::array&lt;double, 2&gt; dx, dy, dz; getDerivatives(outputAcc, gridSpacing, i, j, k, &amp;dx, &amp;dy, &amp;dz); // Explicit Euler step double val = outputAcc(i, j, k) - dtau * std::max(s, 0.0) * (std::sqrt( square(std::max(dx[0], 0.0)) + square(std::min(dx[1], 0.0)) + square(std::max(dy[0], 0.0)) + square(std::min(dy[1], 0.0)) + square(std::max(dz[0], 0.0)) + square(std::min(dz[1], 0.0))) - 1.0) - dtau * std::min(s, 0.0) * (std::sqrt(square(std::min(dx[0], 0.0)) + square(std::max(dx[1], 0.0)) + square(std::min(dy[0], 0.0)) + square(std::max(dy[1], 0.0)) + square(std::min(dz[0], 0.0)) + square(std::max(dz[1], 0.0))) - 1.0); tempAcc(i, j, k) = val; &#125;); std::swap(tempAcc, outputAcc); &#125; auto outputSdfAcc = outputSdf-&gt;dataAccessor(); copyRange3(outputAcc, size.x, size.y, size.z, &amp;outputSdfAcc);&#125; &emsp;&emsp;在计算符号的函数中，为了获取更加平滑的效果，我们采用一个更加平滑的符号计算方法，如下所示，其中$h$是网格单元的边长： sign=\\frac{\\phi}{\\sqrt{\\phi^2+h^2}} \\tag {9}1234567891011double IterativeLevelSetSolver3::sign( const ConstArrayAccessor3&lt;double&gt;&amp; sdf, const Vector3D&amp; gridSpacing, size_t i, size_t j, size_t k) &#123; double d = sdf(i, j, k); double e = min3(gridSpacing.x, gridSpacing.y, gridSpacing.z); return d / std::sqrt(d * d + e * e);&#125; &emsp;&emsp;然后紧接着我们用getDerivatives计算$\\phi$的梯度$\\nabla \\phi$，一种逆风法就是计算两个单边的差分，然后根据符号选择采用两个差分结果中的一个。最后根据符号以及梯度，在给定的时间步长下，我们更新每一个网格点的符号距离场值： \\phi=\\phi_{old}-\\Delta t\\cdot sign(\\phi)(|\\nabla \\phi|-1) \\tag {10}&emsp;&emsp;最后还有个时间步长的选取，如下所示： 1234567891011121314151617181920212223242526272829303132333435double IterativeLevelSetSolver3::pseudoTimeStep( ConstArrayAccessor3&lt;double&gt; sdf, const Vector3D&amp; gridSpacing) &#123; const Size3 size = sdf.size(); const double h = max3(gridSpacing.x, gridSpacing.y, gridSpacing.z); double maxS = -std::numeric_limits&lt;double&gt;::max(); double dtau = _maxCfl * h; for (size_t k = 0; k &lt; size.z; ++k) &#123; for (size_t j = 0; j &lt; size.y; ++j) &#123; for (size_t i = 0; i &lt; size.x; ++i) &#123; double s = sign(sdf, gridSpacing, i, j, k); maxS = std::max(s, maxS); &#125; &#125; &#125; while (dtau * maxS / h &gt; _maxCfl) dtau *= 0.5; return dtau;&#125;unsigned int IterativeLevelSetSolver3::distanceToNumberOfIterations( double distance, double dtau) &#123; return static_cast&lt;unsigned int&gt;(std::ceil(distance / dtau));&#125; &emsp;&emsp;这种迭代的方法非常有意思，在一层一层的迭代中，距离场的函数逐渐地被恢复原样、不再扭曲。符号距离场的这个恢复过程被称为重初始化（reinitialization）。在每一帧的对流之后，我们都要调用这个过程来复原真正的符号距离场，重初始化的范围我们取当前的CFL距离，因为超出CFL距离的重初始化没有意义。 12345678910void LevelSetLiquidSolver3::onEndAdvanceTimeStep(double timeIntervalInSeconds)&#123; double currentCfl = cfl(timeIntervalInSeconds); Timer timer; reinitialize(currentCfl); std::cout &lt;&lt; \"reinitializing level set field took \" &lt;&lt; timer.durationInSeconds() &lt;&lt; \" seconds\\n\"; ......&#125; 参考资料：$[1]$ Kim, D. (2017). Fluid engine development. Boca Raton: Taylor &amp; Francis, a CRC Press, Taylor &amp; Francis Group.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/tags/Fluid-Simulation/"}]},{"title":"流体模拟Fluid Simulation：基于欧拉网格的流体模拟","slug":"Smoke","date":"2019-09-12T08:52:08.033Z","updated":"2019-09-23T11:14:07.801Z","comments":true,"path":"2019/09/12/Smoke/","link":"","permalink":"https://yangwc.com/about/2019/09/12/Smoke/","excerpt":"基于粒子与基于网格的流体模拟算法是计算机图形学中的流体模拟界的两大算法框架，这两种算法分别对应了物理模拟中的两个视角：拉格朗日视角和欧拉视角。与基于粒子的拉格朗日流体模拟解然不同，基于欧拉网格的流体模拟采用了完全不同的算法策略，它不再关注随着物理规律运动的流体粒子，而将目光转向空间中的一组固定的网格点。","text":"基于粒子与基于网格的流体模拟算法是计算机图形学中的流体模拟界的两大算法框架，这两种算法分别对应了物理模拟中的两个视角：拉格朗日视角和欧拉视角。与基于粒子的拉格朗日流体模拟解然不同，基于欧拉网格的流体模拟采用了完全不同的算法策略，它不再关注随着物理规律运动的流体粒子，而将目光转向空间中的一组固定的网格点。 基于欧拉网格的流体模拟 MAC网格数据结构 边界条件处理 体积力项 半拉格朗日对流 流体粘度项 流体压强梯度项 烟雾流体模拟 参考资料 基于欧拉网格的流体模拟 &emsp;&emsp;基于粒子与基于网格的流体模拟算法是计算机图形学中的流体模拟界的两大算法框架，这两种算法分别对应了物理模拟中的两个视角：拉格朗日视角和欧拉视角。与基于粒子的拉格朗日流体模拟解然不同，基于欧拉网格的流体模拟采用了完全不同的算法策略，它不再关注随着物理规律运动的流体粒子，而将目光转向空间中的一组固定的网格点。流体模拟的这两种算法各有利弊，其中基于欧拉网格的模拟方法可以实现无条件稳定，这是基于粒子的模拟方法所不能及的。目前已经有人提出了两种方法结合的模拟策略，这里暂且不说。**相比于基于粒子的模拟方法，无论是从理论上还是从实现上，基于欧拉网格的模拟方法要复杂得多**。 ## 一、基于欧拉网格的流体模拟 &emsp;&emsp;首先来看基于欧拉网格的流体模拟算法的整体框架，主要参考了Jos Stam$^{[1]}$的极为经典的无条件稳定的流体模拟论文。基于欧拉网格的流体模拟算法同样是围绕Navier-Stokes流体力学方程展开，只不过采用了不同的偏微分方程求解方式： $$ \\nabla \\cdot u=0 \\tag {1} $$ $$ \\frac{D u}{Dt}=\\frac{\\partial u}{\\partial t}+u\\cdot \\nabla u=-\\frac1\\rho\\nabla p+\\mu \\nabla ^2u+f \\tag {2} $$ &emsp;&emsp;还是熟悉的流体力学方程，其中$u$是流体的速度场，$t$为时间，$\\rho$是流体密度，$\\mu$是流体的粘度系数，$f$是体积力项（如重力）。$\\frac{Du}{Dt}$是流体关于速度场的物质导数，在基于粒子的流体模拟中，每个粒子的加速度就是关于速度的物质导数$\\frac{Du}{Dt}$，这是因为物质导数的定义就是从拉格朗日视角展开的。拉格朗日视角下的物质导数可以转换成欧拉视角下的一个形式，就是公式$(2)$中的左边$\\frac{Du}{Dt}=\\frac{\\partial u}{\\partial t}+u\\cdot \\nabla u$，这是因为欧拉视角下我们关注的是空间中的一个固定点，因而物质导数就变成了给定点上的速度随时间的变化率$\\frac{\\partial u}{\\partial t}$与在流体先前的速度场作用下的变化率$u\\cdot \\nabla u$之和。因此，在基于欧拉网格的流体模拟方法中，我们需要计算固定点上的加速度$\\frac{\\partial u}{\\partial t}$，将$u\\cdot \\nabla u$移至右边： $$ \\frac{\\partial u}{\\partial t}=-u\\cdot \\nabla u-\\frac1\\rho\\nabla p+\\mu \\nabla^2u+f \\tag {3} $$ &emsp;&emsp;所以，相比于基于粒子的模拟方法，基于欧拉网格的模拟方法多了一项需要计算，即$-u\\cdot \\nabla u$，后面将会提到这一项就是**对流项**，这是粒子方法与网格方法最大的不同之处。 &emsp;&emsp;公式$(3)$中的压强梯度项$-\\frac1\\rho \\nabla p$与保持流体的不可压缩性（即散度为零，公式$(1)$）密切相关，计算压强项这一步我们通常称之为投影，即将速度场投影到散度为零的空间中，从而使得流体表现出不可压缩的特性。根据Helmholtz-Hodge分解定理，给定的一个速度场$w$，可将其分解成一个散度为零的速度矢量$u$与压强标量场的梯度之和$^{[2]}$： $$ w=u+\\nabla p \\tag {4} $$ &emsp;&emsp;其中的$u$满足$\\nabla \\cdot u=0$，即散度为零，如下图1所示。为此，我们引入一个正交投影操作算子$P$，通过算子$P$将速度场$w$投影到散度为零的部分$u$，即$u=Pw$。在线性代数中，投影算子$P$有这样的一种性质，即无论投影多少次，最终都等价于只投影一次，例如$P^2=P$。故同样有$Pu=u$，因为$Pu=P(Pw)=P^2w=Pw=u$。 图1 Helmholtz-Hodge分解 &emsp;&emsp;将算子$P$作用到公式$(4)$两边，有： Pw=Pu+P(\\nabla p)\\to u=u+P(\\nabla p) \\to P(\\nabla p)=0 \\tag {5}&emsp;&emsp;公式$(5)$得出了$P(\\nabla p)=0$，直观的意义就是经过算子$P$的作用，流体的压强梯度$\\nabla p$变为零，流体处于一个不可压缩的平衡状态。现在我们将算子$P$作用到流体力学公式$(3)$的两边，消去其中的$P(\\nabla p)$： \\frac{\\partial u}{\\partial t}=P(-(u\\cdot \\nabla u)+\\mu \\nabla^2u+f) \\tag {6}&emsp;&emsp;通过这个投影算子$P$，我们消去了原流体方程中的压强梯度项$-\\frac1\\rho \\nabla p$，其实是实质上转化成了投影算子$P$，从而解释了为什么求压强梯度项的算法被称为投影。公式$(6)$就是我们将要求解的核心方程，当然公式$(6)$其实是一个易于理解的形式，在最后做压强投影时实际上还是通过求解压强值，并将其作用到流体的速度场上来保证流体的不可压缩性。 &emsp;&emsp;在基于粒子的流体模拟算法中，我们将粒子的所有受力计算出来并进行叠加获取其合力，最后做一个时间积分。但是在基于欧拉网格的流体模拟中，我们是将公式$(6)$中的项拆分开来然后分步进行的，每一步的输出作为下一步的输入，依次进行，而且顺序不能乱。总的来说，需要计算的项分别是体积力项$f$，对流项$-(u\\cdot \\nabla u)$，粘度项$\\mu \\nabla^2 u$以及最后的压力投影项$-\\frac1\\rho \\nabla p$，算法的总体流程如下图2所示： 图2 算法流程 &emsp;&emsp;接下来就按照图2的顺序依次展开相应的计算算法。 二、MAC网格数据结构&emsp;&emsp;在展开各项的计算之前我们先要看看需要用到的一些数据结构。基于欧拉网格的流体模拟都采用了空间中固定划分的网格来承载流体的物理属性，包括一些标量场、矢量场，对于普通的标量场和矢量场，我们通常采用紧密的网格即可，每个网格点记载了当前这个点上的物理属性值，任意给定的物理属性值可以通过三线性插值得到，而且相应的梯度、散度和旋度以及拉普拉斯算子等的计算可以通过有限差分法计算得到。 &emsp;&emsp;但有个特殊的矢量需要做一些不同的处理，这个矢量就是流体的速度场。对于流体的速度场矢量，我们将采用交错的MAC网格：将流体的每个分量都分离出来，每个分量存储方式如下图3所示（以二维为例）： 图3 MAC网格 &emsp;&emsp;上图3中的二维流体分量分别为$u$和$v$，它们不是一起存储在网格的中心处，而是分别单独存储在网格的边上，$u$分量存储在每个网格格子的垂直于$x$轴的边的中心点，$v$分量存储在每个网格格子的垂直于$y$轴的边的中心点。而普通的一些标量场（例如上图中的压强$p$、温度）和矢量场被存储在网格的中心点处。这个就是基于欧拉网格的流体模拟算法的数据分布结构。MAC网格增加了我们模拟算法实现的复杂度，但也带来了一些便利，通过MAC网格我们可以放心地采用中心差分法来做一些微分操作而不用担心普通的致密网格出现的零域（null space）问题。 &emsp;&emsp;采用了MAC网格存储流体的速度场后，速度场的$u$分量、$v$分量和$w$分量分别单独储存，后续求解流体方程时我们需要分别更新流体的速度场的各个分量（三个分量的更新都是类似的）。 三、边界条件处理&emsp;&emsp;紧接着我们还要考虑处理流体的边界问题，这也是整个流体模拟过程中非常重要的一部分。首先是固体边界的表示，这一部分设计到如何处理流体与边界的碰撞。在欧拉网格的流体模拟方法中，最为自然的方法就是采用水平集，同样构建一个空间网格，每个网格点存储采样碰撞体的符号距离场值，下面只是整体的算法代码，关于如何计算给定物体的符号距离场函数之前已经深入了解过了，这里不再赘述。 123456789101112131415161718192021222324252627282930313233343536void GridFractionalBoundaryConditionSolver3::onColliderUpdated( const Size3&amp; gridSize, const Vector3D&amp; gridSpacing, const Vector3D&amp; gridOrigin)&#123; if (_colliderSdf == nullptr) _colliderSdf = std::make_shared&lt;CellCenteredScalarGrid3&gt;(); _colliderSdf-&gt;resize(gridSize, gridSpacing, gridOrigin); if (collider() != nullptr) &#123; Surface3Ptr surface = collider()-&gt;surface(); ImplicitSurface3Ptr implicitSurface = std::dynamic_pointer_cast&lt;ImplicitSurface3&gt;(surface); if (implicitSurface == nullptr) implicitSurface = std::make_shared&lt;SurfaceToImplicit3&gt;(surface); _colliderSdf-&gt;fill([&amp;](const Vector3D&amp; pt) &#123; return implicitSurface-&gt;signedDistance(pt); &#125;); _colliderVel = CustomVectorField3::builder() .withFunction([&amp;](const Vector3D&amp; x) &#123; return collider()-&gt;velocityAt(x); &#125;) .withDerivativeResolution(gridSpacing.x) .makeShared(); &#125; else &#123; _colliderSdf-&gt;fill(kMaxD); _colliderVel = CustomVectorField3::builder() .withFunction([](const Vector3D&amp;) &#123; return Vector3D(); &#125;) .withDerivativeResolution(gridSpacing.x) .makeShared(); &#125;&#125; &emsp;&emsp;有了水平集网格，我们就通过采样流体网格点的符号距离场来判断是否在流体的内部，若值为负，则在物体的内部发生了穿透，需要做一些矫正。当然在这里并不存在显式的流体粒子，因而矫正的不是流体粒子的位置，而是每个空间网格点上的流体属性值，似乎有点抽象，其实质上还是等价于纠正了流体微团的位置。 &emsp;&emsp;我们将要实现的就是无通量边界条件。无通量边界条件就是碰撞不穿透约束，即在碰撞体的表面处，流体的速度场投影到表面法线方向上的值为零，这种边界条件数学上的定义为： u\\cdot n=0 \\tag {7}&emsp;&emsp;其中，$u$是边界处流体的速度场矢量，$n$是边界出碰撞体的单位表面法线。当然上面说的仅仅是碰撞体静止时的情况，我们还需要考虑碰撞体的移动，因此实际上要求的是流体的速度场与碰撞体的速度场的差（即相对速度）在法线方向上的投影为零： (u-u_c)\\cdot n=u_{rel}\\cdot n=0 \\tag {8}&emsp;&emsp;上式中，$u_c$是表面处碰撞体的速度场矢量，$u_{rel}$记为流体与碰撞体之间的相对速度。为了使得相对速度在法线方向上的投影为零，我们可以将相对速度分解成法线方向上的向量与切面向上的向量之和，然后直接取切面上的向量，得到满足碰撞约束的相对速度$u^t_{rel}$，最后得到满足约束的流体速度场$u^n$： u^t_{rel}=u_{rel}-(u_{rel}\\cdot n)n \\\\u^n=u^n_{rel}+u_c \\tag {9}&emsp;&emsp;碰撞体边界统一用水平集构成的符号距离场表示，因而其表面法线$n$就是其符号距离场函数的梯度向量。我们实际上处理的不单单是碰撞体表面上的流体网格点，还需要处理在物体内部的点，因而对于符号距离值为负的流体网格点均需要做以上的边界处理，其余部分保持不变，下面是以流体速度场的$u$分量为例（后面均以$u$分量为例，其余的$v$分量和$w$分量类似）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 velocity-&gt;parallelForEachUIndex([&amp;](size_t i, size_t j, size_t k) &#123; Vector3D pt = uPos(i, j, k); if (isInsideSdf(_colliderSdf-&gt;sample(pt))) &#123; Vector3D colliderVel = collider()-&gt;velocityAt(pt); Vector3D vel = velocity-&gt;sample(pt); Vector3D g = _colliderSdf-&gt;gradient(pt); if (g.lengthSquared() &gt; 0.0) &#123; Vector3D n = g.normalized(); Vector3D velr = vel - colliderVel; Vector3D velt = projectAndApplyFriction(velr, n, collider()-&gt;frictionCoefficient()); Vector3D velp = velt + colliderVel; uTemp(i, j, k) = velp.x; &#125; else uTemp(i, j, k) = colliderVel.x; &#125; else uTemp(i, j, k) = u(i, j, k); &#125;); ...... u.parallelForEachIndex([&amp;](size_t i, size_t j, size_t k) &#123; u(i, j, k) = uTemp(i, j, k); &#125;); //------------------------------------------------------------------- template &lt;size_t N&gt; inline Vector&lt;double, N&gt; projectAndApplyFriction( const Vector&lt;double, N&gt;&amp; vel, const Vector&lt;double, N&gt;&amp; normal, double frictionCoefficient) &#123; Vector&lt;double, N&gt; velt = vel.projected(normal); if (velt.lengthSquared() &gt; 0) &#123; double veln = std::max(-vel.dot(normal), 0.0); velt *= std::max(1.0 - frictionCoefficient * veln / velt.length(), 0.0); &#125; return velt; &#125; &emsp;&emsp;上面的projectAndApplyFriction就是实现了公式$(9)$的函数代码，它负责计算投影到切面上的向量，最后还乘上了一个缩放系数，这个系数来源于我们如何处理无通量边界条件得到的切面上的速度场。通常分为两种情况：自由滑移、无滑移$^{[3]}$。自由滑移就是流体在边界处的速度场直接取值为表面切向上的速度场，无滑移就是流体在边界处的速度场直接取零而不是切向速度，这个不难理解。在这里我们考虑这两者的综合，我们考虑碰撞体的摩擦力： u_t=max(1-\\mu \\frac{max(-u\\cdot n, 0)}{|u_t|},0)u_t \\tag {10}&emsp;&emsp;公式中的$\\mu$就是碰撞体表面的摩擦系数，$u_t$是前面计算得到的切向上的速度场。然后在展开整个流体的边界处理之前，我们需要做一些预处理。通常在整个欧拉网格中，只有属于流体区域的网格点的速度场才有定义，但是在边边界处理时，我们要处理的是那些碰撞体内部的速度场，因而需要根据当前流体的速度场来外推（extrapolation）非流体区域的速度场，用于后续的边界处理，这些物体内部的流体速度场是一个虚拟的速度场。外推算法采用简单的广度优先，但是我们需要指定一个外推深度，因此采用迭代的方法，每一次的迭代表示向外推进一层。一开始我们需要创建一个标记数组marker，这个marker规模与存储速度场的大小一致，对于每一个marker，若当前的网格点在碰撞体内部，则标记为”Solid”，否则标记为”Fluid”，然后采用外推算法填充速度场数组中被标记为”Solid”的那些网格点，对于每一个”Solid“的网格点，其速度场值等于周围相邻的八个网格点中被标记为”Fluid”的速度场的平均值。以速度场的$u$分量为例，其他两个分量类似： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108 auto u = velocity-&gt;uAccessor(); auto uPos = velocity-&gt;uPosition(); Array3&lt;double&gt; uTemp(u.size()); Array3&lt;char&gt; uMarker(u.size(), 1); Vector3D h = velocity-&gt;gridSpacing(); // Assign collider's velocity first and initialize markers velocity-&gt;parallelForEachUIndex([&amp;](size_t i, size_t j, size_t k) &#123; Vector3D pt = uPos(i, j, k); double phi0 = _colliderSdf-&gt;sample(pt - Vector3D(0.5 * h.x, 0.0, 0.0)); double phi1 = _colliderSdf-&gt;sample(pt + Vector3D(0.5 * h.x, 0.0, 0.0)); double frac = fractionInsideSdf(phi0, phi1); frac = 1.0 - clamp(frac, 0.0, 1.0); if (frac &gt; 0.0) uMarker(i, j, k) = 1; else &#123; Vector3D colliderVel = collider()-&gt;velocityAt(pt); u(i, j, k) = colliderVel.x; uMarker(i, j, k) = 0; &#125; &#125;); ...... // Extrapolate fluid velocity into the collider extrapolateToRegion(velocity-&gt;uConstAccessor(), uMarker.constAccessor(), extrapolationDepth, u);//------------------------------------------------------------------------- template &lt;typename T&gt; void extrapolateToRegion( const ConstArrayAccessor3&lt;T&gt;&amp; input, const ConstArrayAccessor3&lt;char&gt;&amp; valid, unsigned int numberOfIterations, ArrayAccessor3&lt;T&gt; output) &#123; const Size3 size = input.size(); assert(size == valid.size()); assert(size == output.size()); Array3&lt;char&gt; valid0(size); Array3&lt;char&gt; valid1(size); valid0.parallelForEachIndex([&amp;](size_t i, size_t j, size_t k) &#123; valid0(i, j, k) = valid(i, j, k); output(i, j, k) = input(i, j, k); &#125;); for (unsigned int iter = 0; iter &lt; numberOfIterations; ++iter) &#123; valid0.forEachIndex([&amp;](size_t i, size_t j, size_t k) &#123; T sum = zero&lt;T&gt;(); unsigned int count = 0; if (!valid0(i, j, k)) &#123; if (i + 1 &lt; size.x &amp;&amp; valid0(i + 1, j, k)) &#123; sum += output(i + 1, j, k); ++count; &#125; if (i &gt; 0 &amp;&amp; valid0(i - 1, j, k)) &#123; sum += output(i - 1, j, k); ++count; &#125; if (j + 1 &lt; size.y &amp;&amp; valid0(i, j + 1, k)) &#123; sum += output(i, j + 1, k); ++count; &#125; if (j &gt; 0 &amp;&amp; valid0(i, j - 1, k)) &#123; sum += output(i, j - 1, k); ++count; &#125; if (k + 1 &lt; size.z &amp;&amp; valid0(i, j, k + 1)) &#123; sum += output(i, j, k + 1); ++count; &#125; if (k &gt; 0 &amp;&amp; valid0(i, j, k - 1)) &#123; sum += output(i, j, k - 1); ++count; &#125; if (count &gt; 0) &#123; output(i, j, k) = sum / static_cast&lt;typename ScalarType&lt;T&gt;::value&gt;(count); valid1(i, j, k) = 1; &#125; &#125; else valid1(i, j, k) = 1; &#125;); valid0.swap(valid1); &#125; &#125; &emsp;&emsp;通过上面的外推处理，我们可以放心地处理边界情况了。在处理完边界情况的最后，我们还需要处理欧拉网格的边界，因为我们模拟的是空间中的固定区域，因而这些固定区域的边界需要处理，默认不处理的情况下就是允许流体流出我们的模拟区域，这个时候流出去的流体不会再被考虑。当然我们也可以让流体不流出这些区域，或者指定一些开口允许流出，这个时候将边上的流体速度场设为零即可，以速度场的$u$分量为例： 123456789101112if (closedDomainBoundaryFlag() &amp; kDirectionLeft) &#123; for (size_t k = 0; k &lt; u.size().z; ++k) for (size_t j = 0; j &lt; u.size().y; ++j) u(0, j, k) = 0;&#125;if (closedDomainBoundaryFlag() &amp; kDirectionRight) &#123; for (size_t k = 0; k &lt; u.size().z; ++k) for (size_t j = 0; j &lt; u.size().y; ++j) u(u.size().x - 1, j, k) = 0;&#125; &emsp;&emsp;总的来说流体的边界处理流程就是：构建碰撞体的水平集$\\to$根据水平集构建marker并外推速度场$\\to$处理碰撞体内部的速度场$\\to$模拟区域的边界处理。上面针对流体的速度场展开了具体的边界处理，但从宏观角度来说，这里的边界条件分为两种，分别是诺伊曼（Neumann）边界条件、狄拉克（Dirichlet）边界条件。 &emsp;&emsp;给定一个场$f$（可以是标量场，也可以是矢量场），诺伊曼边界条件是给出了量场$f$在边界处法线方向$n$上的导数等于某个具体给定的值$c$，即： \\frac{\\partial f}{\\partial n}=c \\tag {11}&emsp;&emsp;例如$c=0$，那么表示量场$f$在边界处沿着法线方向上不变。因此我们前面提到的无通量边界条件（公式$(7)$）就是诺伊曼边界条件的子集。而狄拉克边界条件非常简单，它是直接给出边界处量场$f$的取值$c$： f=c \\tag {12}&emsp;&emsp;当公式$(12)$中的$c=0$时，就等价于前面我们提到的无滑移速度场。每一次更新速度场我们都要调用一次固体边界处理。 四、体积力项&emsp;&emsp;首先来看最简单的部分——体积力项，或者说外力项。这一项主要是考虑外部的作用力，这类力不是通过接触直接作用到流体上，而是类似于隔山打牛的凌空作用，在流体模拟中主要有重力、浮力。体积力项的计算非常简单，根据牛顿定律简单地做一个前向欧拉积分即可： w_1(x)=w_0(x)+\\Delta t f(x,t) \\tag {13}&emsp;&emsp;外部的体积力有几种，其中最常见的就是重力，下面以重力为例： 1234567891011121314151617181920212223242526272829303132333435363738394041void GridFluidSolver3::computeExternalForces(double timeIntervalInSeconds) &#123; computeGravity(timeIntervalInSeconds);&#125;void GridFluidSolver3::computeGravity(double timeIntervalInSeconds)&#123; if (_gravity.lengthSquared() &gt; kEpsilonD) &#123; auto vel = _grids-&gt;velocity(); auto u = vel-&gt;uAccessor(); auto v = vel-&gt;vAccessor(); auto w = vel-&gt;wAccessor(); if (std::abs(_gravity.x) &gt; kEpsilonD) &#123; vel-&gt;forEachUIndex([&amp;](size_t i, size_t j, size_t k) &#123; u(i, j, k) += timeIntervalInSeconds * _gravity.x; &#125;); &#125; if (std::abs(_gravity.y) &gt; kEpsilonD) &#123; vel-&gt;forEachVIndex([&amp;](size_t i, size_t j, size_t k) &#123; v(i, j, k) += timeIntervalInSeconds * _gravity.y; &#125;); &#125; if (std::abs(_gravity.z) &gt; kEpsilonD) &#123; vel-&gt;forEachWIndex([&amp;](size_t i, size_t j, size_t k) &#123; w(i, j, k) += timeIntervalInSeconds * _gravity.z; &#125;); &#125; applyBoundaryCondition(); &#125;&#125; &emsp;&emsp;计算重力对流体的速度场的作用之后，需要调用边界处理，修正固体边界处的流体速度值。事实上，在每一次更新了流体的速度场之后，都需要进行边界处理。后续的对流、粘性以及压力等等处理完后都需要处理边界情况。 五、半拉格朗日对流&emsp;&emsp;接下来就是求解流体的对流项，在基于粒子的流体模拟中，对流项被自动地执行，即不需要对粒子进行对流。对流本质上就是在流体的速度场作用下，不同网格点之间传递流体微团。从拉格朗日视角来看，就是流体微团在速度场的作用下，携带流体微团在空间中移动，流体微团的一些性质（如流体微团的密度）在移动的过程中保持不变。因而，求解关于任意一个物理量场$q$的对流项实际上求解的是下面的对流方程： \\frac{Dq}{Dt}=\\frac{\\partial q}{\\partial t}+u\\cdot \\nabla q = 0 \\tag {14}&emsp;&emsp;其中$u$是流体的速度场向量。在基于粒子的流体模拟中，每个流体粒子已经自动实现了$\\frac{Dq}{Dt}=0$，因而省去了这一步骤。在基于欧拉网格的流体模拟中，我们关注的是空间中的固定点，因而空间中的固定点的关于物理量场$q$随时间的变化率为$\\frac{\\partial q}{\\partial t}$，故需要求解这个变化率，并在这个变化率的作用下更新相应网格点上保存的物理量场值$q$，这一个过程就是对流（Advection）。将公式$(14)$挪项： \\frac{\\partial q}{\\partial t}=-u\\cdot \\nabla q \\tag {15}&emsp;&emsp;一种已经过时的方法就是采用有限差分法来估算$\\frac{\\partial q}{\\partial t}$，虽然这种方法效果很差且局限性太大，但是了解它能够加深我们对流体模拟中的”稳定性“的理解。我们现在来看看基于有限差分法的逆风对流法，以一维为例： \\frac{\\partial q}{\\partial t}=-u\\frac{\\partial q}{\\partial x} \\tag {16}&emsp;&emsp;采用有限差分法估算公式$(16)$： \\frac{\\partial q}{\\partial t}=\\begin{cases}-u\\frac{f_i-f_{i-1}}{\\Delta x},\\ \\ \\ \\ if\\ \\ u>0\\\\-u\\frac{f_{i+1}-f_i}{\\Delta x},\\ \\ \\ \\ otherwise\\end{cases} \\tag {17}&emsp;&emsp;当$u&gt;0$时我们在左边取差分，否则在右边取差分，这就是这种方法被称为”逆风”的原因。现在假设我们有一个速度$u&gt;0$，那么根据公式$(17)$估算了$\\frac{\\partial q}{\\partial t}$之后，再采用前向欧拉法来更新物理量场$q$，即有： q_{i}^{t+\\Delta t}=q_{i}^t-\\Delta t u\\frac{f_i^t-f_{i-1}^t}{\\Delta x} \\tag {18}&emsp;&emsp;下图4很好的描述了这个对流的过程，灰色的点的$f$值就是我们更新后的量场$q$的值。 图4 1D的逆风对流法 &emsp;&emsp;可以看到，当我们的$u\\Delta t$小于网格单元格长度$\\Delta x$时，对流算法没什么问题。但是当$u\\Delta t$大于$\\Delta x$时，将会导致追踪了错误的灰色点，如下图5所示，正确的点应该是灰色的点下面的那个点。所以基于有限差分法的对流方法会出现不稳定的问题，归根揭底还是因为差分方法仅仅考虑了一个$\\Delta x$范围内的网格点，而$u\\Delta t$有可能过大导致追踪的点超出$\\Delta x$的范围，基于有限差分法的对流方法仅在$u\\Delta t/\\Delta x&lt;1$时保持稳定，这里的$u\\Delta t/\\Delta x$就是流体模拟中常说的Courant数，这种限制条件被称为Courant–Friedrichs–Lewy条件（简称为CFL条件）。在这里CFL条件的限制为1，也就是不能超过一个$\\Delta x$的范围，因而使得模拟的时间步长$\\Delta t$不能太大，极大地降低模拟效率。 图5 逆风对流的不稳定 &emsp;&emsp;目前，基于有限差分法的对流方法已经几乎不用，更多的还是采用了Jos Stam$^{[1]}$提出的无条件稳定的半拉格朗日对流算法，这种对流算法已经被证明了无条件稳定的对流算法，因而可以使用大的时间步长去模拟。这种对流算法被称为半拉格朗日对流是有原因的，它从拉格朗日视角来展开对流过程。如下图6所示，当前的每一个网格点，都可以看作是上一个时间点模拟区域中的某个流体粒子经过移动刚好落在当前的网格点，因而当前网格点（图6中的黑色点）上的物理场$q$取值应该是这个流体粒子（图6中的灰色点）的物理量场值。这个就是它的核心思想，可以看到它采用了拉格朗日的视角去解决欧拉网格对流的问题，因而被称为半拉格朗日对流。 图6 半拉格朗日对流 &emsp;&emsp;给定一个网格点位置为$x$，其相应的速度场为$u$，现在要计算第$n+1$个时间步长时其物理量场的取值，可以后向追踪到灰色的粒子位置为$x-\\Delta tu$，然后这个粒子的物理量场的值通过周围网格点做线性插值得到，最后网格点$x$的物理量场值$q(x)^{n+1}$为： q(x)^{n+1}=\\overline q(x-\\Delta tu)^n \\tag {19}1234567891011121314151617181920212223void SemiLagrangian3::advect( const ScalarGrid3&amp; input, const VectorField3&amp; flow, double dt, ScalarGrid3* output, const ScalarField3&amp; boundarySdf) &#123; auto outputDataPos = output-&gt;dataPosition(); auto outputDataAcc = output-&gt;dataAccessor(); auto inputSamplerFunc = getScalarSamplerFunc(input); auto inputDataPos = input.dataPosition(); double h = min3(output-&gt;gridSpacing().x, output-&gt;gridSpacing().y, output-&gt;gridSpacing().z); output-&gt;parallelForEachDataPointIndex([&amp;](size_t i, size_t j, size_t k) &#123; if (boundarySdf.sample(inputDataPos(i, j, k)) &gt; 0.0) &#123; Vector3D pt = backTrace(flow, dt, h, outputDataPos(i, j, k), boundarySdf); outputDataAcc(i, j, k) = inputSamplerFunc(pt); &#125; &#125;);&#125; &emsp;&emsp;上面的代码以标量场的对流为例，其他矢量场以及MAC网格结构的类似。backTrace函数根据当前的网格点以及速度场进行后向追踪，返回一个追踪的点（其实就是$x-\\Delta tu$），然后根据这个点做一个线性插值得到新的物理量场的值。 &emsp;&emsp;可以看到半拉格朗日对流算法通过当前的速度场进行后向追踪，最终的物理量场值是通过其所在网格点做插值得到，因而不会超出原来量场中的最大值和最小值，而且也不会受限于$\\Delta tu$的大小，因而是无条件稳定的。半拉格朗日对流的实现非常简单，对于流体区域的每一个网格点的每一个物理量场，做一个公式$(19)$中的逆向追踪即可。但是存在一些准确率不高的问题，如下图7(a)所示，这是一个环形涡流，$p_0$后向追踪的点$p_1$实际上并不是上一个时间点流到$p_0$的点，因而计算的结果不准确。 图7 中点法 &emsp;&emsp;目前已经有许多方法来提高对流的准确性，其中一个遍是重点法。如图7(b)所示，我们后向步进半个时间步长而不是一个时间步长得到中点$p_{mid}=p_0-(\\Delta t/2)v_0$，然后在重点$p_{mid}$处采样其速度场$v_{mid}$，最后采用$v_{mid}$去后向追踪$p_1=p_0-(\\Delta t) v_{mid}$。通过这个简单的改动，对流算法的准确率得到了很大的提升。 &emsp;&emsp;然而半拉格朗日对流还存在另外一个比较严重的问题，就是数值耗散（numerical diffusion）。我们通过后向追踪得到的点$p_1$很少会刚刚好网格点上，大多数情况都是落在网格的格子内部，这个时候我们通过线性插值来计算$p_1$对应的物理量场$q$。问题就出现在线性插值这里，当我们采用线性插值的时候我们就默认了该量场$q$在这个网格内呈线性分布，这在欧拉网格的分辨率非常高时不会有什么大的问题，但当分辨率没那么高时，就会产生可观的估算误差。网格越粗糙，误差越大。这种误差带来的影响就是流体细节（例如涡旋细节）的消失，从而影响模拟的视觉效果。 &emsp;&emsp;因为这一步对流体的模拟效果非常关键，近年来，已经有不少学者针对这个数值耗散问题展开相关研究，提出了不少改进算法，有的是结合了粒子模拟方法的隐式粒子法（因为粒子法不需要对流），有的采用了一些技巧避免了插值，有的在插值方法上做一些改进，还有的采用特殊的方法还原消失在插值中的细节。这里我们先来看针对插值方法的改进——Catmull–Rom样条插值。 &emsp;&emsp;简单的线性插值只获取了周围最近点的函数值信息，然后根据给定点的位置去计算相应的值。Catmull–Rom样条插值借助了更多的邻域点信息进行插值，它的函数曲线不再是一条直线，而是样条曲线。以1D为例，Catmull–Rom样条插值函数为一个三阶多项式： f(t)=a_3t^3+a_2t^2+a_1t+a_0 \\tag {20} 图8 Catmull–Rom样条插值 &emsp;&emsp;构建Catmull–Rom样条插值函数除了要知道最近点的函数值（这里就是$f_0$和$f_1$），还需要再获取另外两个点的函数值$f_{-1}$和$f_2$。令$f_{-1}、f_0、f_1、f_2$（就是图8中从左到右的四个黑色点）分别对应$t=-1、0、1、2$时的样条函数值，现在需要求解插值函数（即公式$(20)$）中的系数$a_3、a_2、a_1、a_0$，通过一些差分法以及方程联立，可得下面的方程组（$f_{-1}、f_0、f_1、f_2$均已知）： \\begin{align}&a_0=f(0)=f_0\\\\&a_1=f'(0)=(f_1-f_{-1})/2 \\\\&f(1)=a_3+a_2+a_1+a_0\\\\&f'(1)=3a_3+2a_2+a_1=(f_2-f_1)/2\\end{align}\\tag {21}&emsp;&emsp;通过公式$(21)$的方程组我可以求得全部系数，我们将传入的介于$[0,1]$之间的权重值传入插值函数并返回插值的结果。 1234567891011121314template &lt;typename S, typename T&gt;inline S catmullRom(const S&amp; f0, const S&amp; f1, const S&amp; f2, const S&amp; f3, T f) &#123; S d1 = (f2 - f0) / 2; S d2 = (f3 - f1) / 2; S D1 = f2 - f1; S a3 = d1 + d2 - 2 * D1; S a2 = 3 * D1 - 2 * d1 - d2; S a1 = d1; S a0 = f1; return a3 * cubic(f) + a2 * square(f) + a1 * f + a0;&#125; &emsp;&emsp;上面我们讨论了一维的情况，二维和三维的情况只是一维的推广，这一方面与单线性插值与双线性插值、三线性插值之间的关系类似，不再赘述。但在半拉格朗日对流中直接用Catmull–Rom插值替换线性插值会带来一些问题。线性插值函数是单调的，通常是从$f_0$到$f_1$单调增或者单调减，这保证了通过插值得到的值不会超过原来的最大值也不会低于原来的最小值，但是Catmull–Rom样条插值函数不能保证它是单调的，有可能导致插值得到的值超出了原来的范围，存在不稳定的问题。为此，需要做一些修改，使之在$[0,1]$范围内变成单调函数。如下图9所示，虚线部分是原Catmull–Rom样条函数，而实线部分就是修改后的函数曲线。记$D1$为$f_1-f_0$，$d_0=(f_1-f_{-1})/2$，$d_1=(f_2-f_1)/2$，这里$d_1$、$d_2$是$f’(0)$、$f’(1)$的差分近似，使之： \\begin{cases}sign(d_0)=sign(d_1)=sign(D1),\\ \\ \\ \\ D1\\neq 0\\\\d_1=d_2=0,\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ D1=0\\end{cases} \\tag {22} 图9 单调的Catmull–Rom样条插值 1234567891011121314151617181920212223template &lt;typename T&gt;inline T monotonicCatmullRom(const T&amp; f0, const T&amp; f1, const T&amp; f2, const T&amp; f3, T f) &#123; T d1 = (f2 - f0) / 2; T d2 = (f3 - f1) / 2; T D1 = f2 - f1; if (std::fabs(D1) &lt; kEpsilonD) d1 = d2 = 0; if (sign(D1) != sign(d1)) d1 = 0; if (sign(D1) != sign(d2)) d2 = 0; T a3 = d1 + d2 - 2 * D1; T a2 = 3 * D1 - 2 * d1 - d2; T a1 = d1; T a0 = f1; return a3 * cubic(f) + a2 * square(f) + a1 * f + a0;&#125; &emsp;&emsp;最后，我们还需要考虑后向追踪时的边界处理，我们后向追踪得到的点可能位于碰撞体内部，这个时候就需要做一个截断，使后向追踪的点落到碰撞体的表面上而不是内部，这个过程如下图10所示。我们通过获取网格点的符号距离场与追踪点的符号距离场的符号是否相同来判断是否发生了碰撞体穿透，若发生了穿透也通过它们的符号距离场来获取表面上的点。 图10 截断处理 123456789101112131415161718192021222324252627282930313233343536373839Vector3D SemiLagrangian3::backTrace( const VectorField3&amp; flow, double dt, double h, const Vector3D&amp; startPt, const ScalarField3&amp; boundarySdf) &#123; double remainingT = dt; Vector3D pt0 = startPt; Vector3D pt1 = startPt; while (remainingT &gt; kEpsilonD) &#123; // Adaptive time-stepping Vector3D vel0 = flow.sample(pt0); double numSubSteps = std::max(std::ceil(vel0.length() * remainingT / h), 1.0); dt = remainingT / numSubSteps; // Mid-point rule Vector3D midPt = pt0 - 0.5 * dt * vel0; Vector3D midVel = flow.sample(midPt); pt1 = pt0 - dt * midVel; // Boundary handling double phi0 = boundarySdf.sample(pt0); double phi1 = boundarySdf.sample(pt1); if (phi0 * phi1 &lt; 0.0) &#123; double w = std::fabs(phi1) / (std::fabs(phi0) + std::fabs(phi1)); pt1 = w * pt0 + (1.0 - w) * pt1; break; &#125; remainingT -= dt; pt0 = pt1; &#125; return pt1;&#125; &emsp;&emsp;上面的代码实现实际上是一个循环，这也是为了提升追踪的精度，每一次后向追踪步长最好不要超过一个$\\Delta x$的大小，超过了$\\Delta x$我们就把这个追踪过程分成几步，每一步最多走$\\Delta x$。半拉格朗日对流的算法复杂度$O(n)$，可以很容易地实现并行版本。 六、流体粘度项&emsp;&emsp;在经过拉格朗日对流之后，我们就需要求解流体的粘度项，流体的粘度项也是流体模拟一个重要的内容。要实现好的流体粘性对于一些高粘度的流体来说至关重要，这是一个研究热点。根据流体力学方程，流体的粘度项为： a_v=\\mu \\nabla^2 u \\tag {23}&emsp;&emsp;其中$a_v$就是在流体的粘性力作用下的加速度，$\\mu$粘度系数，$u$流体速度场矢量。求解这一项最简单的方法就是采用一个显式的欧拉前向时间积分： u^{n+1}=u^n+\\Delta t\\mu \\nabla^2 u^n \\tag {24}&emsp;&emsp;其中，$\\Delta t$为时间步长，$u^n$是当前的速度场，$u^{n+1}$是加入了粘度作用力的速度场，简单的欧拉前向时间积分实现很简单。对于除了流体速度场中的其他物理量场，只需简单地替换公式$(24)$中的速度场矢量即可，下面的代码以标量场为例，我们仅对在流体区域内的网格点做粘度项计算。 1234567891011121314151617181920212223242526void GridForwardEulerDiffusionSolver3::solve( const ScalarGrid3&amp; source, double diffusionCoefficient, double timeIntervalInSeconds, ScalarGrid3* dest, const ScalarField3&amp; boundarySdf, const ScalarField3&amp; fluidSdf)&#123; auto src = source.constDataAccessor(); Vector3D h = source.gridSpacing(); auto pos = source.dataPosition(); buildMarkers(source.resolution(), pos, boundarySdf, fluidSdf); source.parallelForEachDataPointIndex( [&amp;](size_t i, size_t j, size_t k) &#123; if (_markers(i, j, k) == kFluid) &#123; (*dest)(i, j, k) = source(i, j, k) + diffusionCoefficient * timeIntervalInSeconds * laplacian(src, _markers, h, i, j, k); &#125; else (*dest)(i, j, k) = source(i, j, k); &#125;);&#125; &emsp;&emsp;似乎看起来直接采用前向欧拉法计算流体粘度项非常简单、快速，但与采用基于有限差分法的对流类似，当模拟的时间步长超过一定的阈值时，也会出现不稳定的问题。以1D的情况为例，假设我们要求解物理量场$q$的粘度项，根据中心差分法来估算拉普拉斯算子，有： q[i]=q[i]+\\mu \\Delta t\\frac{q[i+1]-2q[i]+q[i-1]}{(\\Delta x)^2} \\tag {25}&emsp;&emsp;在整个求解过程中，$\\mu$、$\\Delta t$和$\\Delta x$时保持不变的，我们把它提取出来并记为$c$，公式$(25)$转变为： \\begin{align}c&=\\frac{\\mu \\Delta t}{(\\Delta x)^2}\\\\q[i]&=c\\cdot q[i+1]+(1-2c)\\cdot q[i]+c\\cdot q[i-1]\\end{align} \\tag {26}&emsp;&emsp;可以看到，把前向欧拉法求解流体粘度项写成公式$(26)$的形式，它就看起来像是一个滤波过程（如下图11(a)所示），对周围的领域做一个加权和，这个权重由$c$决定，因而$c$就是滤波核宽度。$c$越大，则滤波范围越广，从图像处理来说就会导致图像越模糊，在流体模拟这里就会导致流体粘性越大。但是这里的$c$是有范围限制的，$c$的下界为$0$（如下图11(b)所示），这个时候表现为不做任何改变；$c$的上界为$\\frac14$（如下图11(c)所示）。超出上界的$c$将会引入中心差分法涉及到的邻域网格点之外的其他网格点，这一点跟前面讨论的逆风对流的缺陷类似。 图11 diffusion filters &emsp;&emsp;因为$c=\\frac{\\mu \\Delta t}{(\\Delta x)^2}$，所以$c$的范围限制作用到了流体的粘度系数$\\mu$上，假设保持$\\Delta t$和$\\Delta x$不变，根据$c\\leq\\frac14$，那么$\\mu\\leq\\frac{(\\Delta x)^2}{4\\Delta t}$，这意味着我们不能取超过这个范围的流体粘度系数，否则将导致出现不稳定的问题，因此不能实现高粘性的流体（如蜂蜜、油类液体）。事实上，这类不稳定的因素是前向欧拉法固有存在的问题，因此为了实现高粘性的流体，前向欧拉法不能采用。 &emsp;&emsp;所以我们将采用的是后向欧拉法，前向欧拉法根据当前的状态以时间推进的顺序计算下一个时间点的状态，而后向欧拉法相反，它假设下一时间点的状态已知，那么可以推算前一个时间点的状态，然后建立一个方程组。后向欧拉法的方程如下（以1D为例）： q_i^n=q_i^{n+1}-\\Delta t \\mu \\nabla^2 q_i^{n+1} \\tag {27}&emsp;&emsp;公式$(27)$中，我们假设已知下一个时间点的$q_i^{n+1}$，然后推算前一个时间点的$q_i^n$，这个过程是前向欧拉法的逆过程，因而被称为后向欧拉法。这种方法从前往后推算，是无条件稳定的。同样采用中心差分法计算拉普拉斯算子，有： q_i^{n+1}-\\Delta t\\mu \\frac{q_{i+1}^{n+1}-2q_i^{n+1}+q_{i-1}^{n+1}}{\\Delta x^2}=q_i^n \\tag {28}&emsp;&emsp;做一些调整可得方程： \\begin{align}&c=\\frac{\\Delta t\\mu}{(\\Delta x)^2}\\\\&-cq_{i+1}^{n+1}+(2c+1)q_i^{n+1}-cq_{i-1}^{n+1} = q_i^n\\end{align} \\tag {29}&emsp;&emsp;对于每一个$q_i^{n+1}$都可以构建一个上述方程，从而构建了一个大规模的线性方程组。其中$q_i^{n+1}$的系数中的$c$的系数等于其周围4-邻居的数量，例如1D时最左边和最右边其邻居数只有一个，因而其系数为$(c+1)$。二维网格大多数邻居为4，因而其系数为$(4c+1)$，三维依次类推。将线性方程组写成矩阵的形式如下： \\left[\\begin{matrix}c+1&-c&0&...&0&0\\\\-c&2c+1&-c&...&0&0\\\\...&...&...&...&...&...\\\\0&0&...&-c&c+1&...\\end{matrix}\\right]\\cdot q^{n+1} = q^{n} \\tag {30}&emsp;&emsp;公式$(30)$中的系数矩阵是一个大规模的对称稀疏矩阵，问题转成了求解线性方程组$Ax=b$。求解大规模正定稀疏矩阵我们将采用预处理的共轭梯度法，涉及到的数学内容较多，这就不再细细展开。在求解之前需要构建一个稀疏矩阵，这里我们可以不需要耗费大量的空间去存贮大多数的零，其实在这个特定问题下矩阵的每一行元素数量是固定的，1D有3个，2D有5个，3D有7个，又因矩阵是对称的，所以我们每一行只需存储原来的一半。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192void GridBackwardEulerDiffusionSolver3::buildMatrix(const Size3&amp; size, const Vector3D&amp; c) &#123; _system.A.resize(size); bool isDirichlet = (_boundaryType == Dirichlet); // Build linear system _system.A.parallelForEachIndex( [&amp;](size_t i, size_t j, size_t k) &#123; auto&amp; row = _system.A(i, j, k); // Initialize row.center = 1.0; row.right = row.up = row.front = 0.0; if (_markers(i, j, k) == kFluid) &#123; if (i + 1 &lt; size.x) &#123; if ((isDirichlet &amp;&amp; _markers(i + 1, j, k) != kAir) || _markers(i + 1, j, k) == kFluid) row.center += c.x; if (_markers(i + 1, j, k) == kFluid) row.right -= c.x; &#125; if (i &gt; 0 &amp;&amp; ((isDirichlet &amp;&amp; _markers(i - 1, j, k) != kAir) || _markers(i - 1, j, k) == kFluid)) row.center += c.x; if (j + 1 &lt; size.y) &#123; if ((isDirichlet &amp;&amp; _markers(i, j + 1, k) != kAir) || _markers(i, j + 1, k) == kFluid) row.center += c.y; if (_markers(i, j + 1, k) == kFluid) row.up -= c.y; &#125; if (j &gt; 0 &amp;&amp; ((isDirichlet &amp;&amp; _markers(i, j - 1, k) != kAir) || _markers(i, j - 1, k) == kFluid)) row.center += c.y; if (k + 1 &lt; size.z) &#123; if ((isDirichlet &amp;&amp; _markers(i, j, k + 1) != kAir) || _markers(i, j, k + 1) == kFluid) row.center += c.z; if (_markers(i, j, k + 1) == kFluid) row.front -= c.z; &#125; if (k &gt; 0 &amp;&amp; ((isDirichlet &amp;&amp; _markers(i, j, k - 1) != kAir) || _markers(i, j, k - 1) == kFluid)) row.center += c.z; &#125; &#125;);&#125;void GridBackwardEulerDiffusionSolver3::buildVectors(const ConstArrayAccessor3&lt;double&gt;&amp; f, const Vector3D&amp; c) &#123; Size3 size = f.size(); _system.x.resize(size, 0.0); _system.b.resize(size, 0.0); // Build linear system _system.x.parallelForEachIndex( [&amp;](size_t i, size_t j, size_t k) &#123; _system.b(i, j, k) = _system.x(i, j, k) = f(i, j, k); if (_boundaryType == Dirichlet &amp;&amp; _markers(i, j, k) == kFluid) &#123; if (i + 1 &lt; size.x &amp;&amp; _markers(i + 1, j, k) == kBoundary) _system.b(i, j, k) += c.x * f(i + 1, j, k); if (i &gt; 0 &amp;&amp; _markers(i - 1, j, k) == kBoundary) _system.b(i, j, k) += c.x * f(i - 1, j, k); if (j + 1 &lt; size.y &amp;&amp; _markers(i, j + 1, k) == kBoundary) _system.b(i, j, k) += c.y * f(i, j + 1, k); if (j &gt; 0 &amp;&amp; _markers(i, j - 1, k) == kBoundary) _system.b(i, j, k) += c.y * f(i, j - 1, k); if (k + 1 &lt; size.z &amp;&amp; _markers(i, j, k + 1) == kBoundary) _system.b(i, j, k) += c.z * f(i, j, k + 1); if (k &gt; 0 &amp;&amp; _markers(i, j, k - 1) == kBoundary) _system.b(i, j, k) += c.z * f(i, j, k - 1); &#125; &#125;);&#125; &emsp;&emsp;构建完了矩阵就可以调用共轭梯度算法求解方程组： 1234567891011121314151617181920212223242526272829void GridBackwardEulerDiffusionSolver3::solve( const ScalarGrid3&amp; source, double diffusionCoefficient, double timeIntervalInSeconds, ScalarGrid3* dest, const ScalarField3&amp; boundarySdf, const ScalarField3&amp; fluidSdf)&#123; auto pos = source.dataPosition(); Vector3D h = source.gridSpacing(); Vector3D c = timeIntervalInSeconds * diffusionCoefficient / (h * h); buildMarkers(source.dataSize(), pos, boundarySdf, fluidSdf); buildMatrix(source.dataSize(), c); buildVectors(source.constDataAccessor(), c); if (_systemSolver != nullptr) &#123; // Solve the system _systemSolver-&gt;solve(&amp;_system); // Assign the solution source.parallelForEachDataPointIndex( [&amp;](size_t i, size_t j, size_t k) &#123; (*dest)(i, j, k) = _system.x(i, j, k); &#125;); &#125;&#125; 七、流体压强梯度项&emsp;&emsp;流体的压强项同样是采用了后向欧拉法，这样才能够保证流体模拟的无条件稳定。根据Navier-Stokes方程，流体的压强梯度项为： a_p=-\\frac{\\nabla p}{\\rho} \\tag {31}&emsp;&emsp;其中的$a_p$是流体内部压强梯度力作用下的加速度。采用后向欧拉的思想，我们已知下一个时间点的速度场$u^{n+1}$，求解压强梯度项就是为了保证流体的不可压缩性，即散度为零： u^{n+1}=u^n-\\Delta t\\frac{\\nabla p}{\\rho}\\\\\\nabla \\cdot u^{n+1} = 0 \\tag {32}&emsp;&emsp;设流体的密度保持不变，从而可得： \\nabla \\cdot u^{n+1}=\\nabla \\cdot u^n-\\frac{\\Delta t}{\\rho}\\nabla^2p\\\\\\to \\frac{\\Delta t}{\\rho}\\nabla^2p=\\nabla \\cdot u^n \\tag {33}&emsp;&emsp;上面推导处的方程与流体的粘度方程类似，左边是关于压强的拉普拉斯项，为未知项目，右边是关于当前速度场的散度，为已知项。这个就是关于压强的泊松方程（Pressure Poisson equation，简称为PPE），同样采用中心差分法计算拉普拉斯算子和散度： \\frac{\\Delta t}{\\rho}\\frac{p_{i+1}-2p_i+p_{i-1}}{(\\Delta x)^2}=\\frac{u^n_{i+1/2}-u^n_{i-1/2}}{\\Delta x} \\tag {34}&emsp;&emsp;与求解粘度方程类似，将其写成大规模稀疏的对称正定矩阵： \\frac{\\Delta t}{\\rho \\Delta x^2}\\left[\\begin{matrix}1&-1&0&0&...\\\\-1&2&-1&0&...\\\\0&-1&2&-1&...\\\\...&...&...&...&...\\end{matrix}\\right]\\left[\\begin{matrix}p_1\\\\p_2\\\\p_3\\\\...\\\\\\end{matrix}\\right]=-\\frac{1}{\\Delta x}\\left[\\begin{matrix}u^n_{3/2}-u^n_{1/2}\\\\u^n_{5/2}-u^n_{3/2}\\\\u^n_{7/2}-u^n_{5/2}\\\\...\\end{matrix}\\right] \\tag {35}&emsp;&emsp;同样地采用共轭梯度法求解上述的大规模的稀疏对称正定线性方程组。 八、烟雾流体模拟&emsp;&emsp;我们要模拟的烟雾是燃烧或者爆炸产生的，通常在烟雾周围的温度高于其他区域，温度较高的空气密度较低，从而产生一个上升力。与此同时，烟雾聚集的区域其质量增大，因而也会受到重力的因素产生下降力。这些垂直方向上的力就是浮力。一种计算浮力的方式就是考虑流体密度的变化，原来的压强泊松方程中的密度场$\\rho$不再是常量，而是会发生变化的，故求散度时不能直接提出公式外面，前面的公式$(33)$就变成了$^{[3]}$： \\nabla \\cdot \\frac{\\nabla p}{\\rho}=c\\frac{\\nabla \\cdot u}{\\Delta t} \\tag {36}&emsp;&emsp;公式$(36)$求解的是精确的浮力，需要耗费大量的计算资源，在一些密度差异非常关键的场合中这是非常重要的。但是在计算机图形学的烟雾模拟中不需要这么高精度地去计算流体密度产生的浮力作用力，我们采用下面的近似计算公式$^{[5]}$： f_{buoy}=-\\alpha \\rho \\vec y+\\beta (T-T_{amb})\\vec y \\tag {37}&emsp;&emsp;公式$(37)$中的$\\alpha$和$\\beta$是缩放系数，用于控制流体密度和温度对浮力的影响，$f_{buoy}$就是流体浮力，$T$是流体的温度场，$T_{amb}$为周围环境的温度，$\\rho$密度场，$\\vec y$时垂直方向上的单位向量。$T_{amb}$可以通过计算温度场的平均温度得到。公式$(37)$综合考虑了流体密度和流体温度的影响。流体密度的影响是产生一个垂直向下的作用力，因而$\\alpha \\rho \\vec y$前面还有个负号；流体温度的影响取决于当前温度与环境温度的差，高于环境温度会产生一个向上的作用力。 &emsp;&emsp;烟雾模拟需要存储的流体属性有速度场、密度场以及温度场，后两个都是标量场。这些属性在流体模拟中，都需要求解整个流体方程，即均需求解体积力项、对流项、粘度项以及压力项。在这里体积力项仅仅有浮力项（即公式$(37)$），重力因素已经被考虑进公式$(37)$了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263void GridSmokeSolver3::computeBuoyancyForce(double timeIntervalInSeconds)&#123; auto grids = gridSystemData(); auto vel = grids-&gt;velocity(); Vector3D up(0, 1, 0); if (gravity().lengthSquared() &gt; kEpsilonD) up = -gravity().normalized(); if (std::abs(_buoyancySmokeDensityFactor) &gt; kEpsilonD || std::abs(_buoyancyTemperatureFactor) &gt; kEpsilonD) &#123; auto den = smokeDensity(); auto temp = temperature(); // Ambient temperature double tAmb = 0.0; temp-&gt;forEachCellIndex([&amp;](size_t i, size_t j, size_t k) &#123; tAmb += (*temp)(i, j, k); &#125;); tAmb /= static_cast&lt;double&gt;(temp-&gt;resolution().x * temp-&gt;resolution().y * temp-&gt;resolution().z); auto u = vel-&gt;uAccessor(); auto v = vel-&gt;vAccessor(); auto w = vel-&gt;wAccessor(); auto uPos = vel-&gt;uPosition(); auto vPos = vel-&gt;vPosition(); auto wPos = vel-&gt;wPosition(); if (std::abs(up.x) &gt; kEpsilonD) &#123; vel-&gt;parallelForEachUIndex([&amp;](size_t i, size_t j, size_t k) &#123; Vector3D pt = uPos(i, j, k); double fBuoy = _buoyancySmokeDensityFactor * den-&gt;sample(pt) + _buoyancyTemperatureFactor * (temp-&gt;sample(pt) - tAmb); u(i, j, k) += timeIntervalInSeconds * fBuoy * up.x; &#125;); &#125; if (std::abs(up.y) &gt; kEpsilonD) &#123; vel-&gt;parallelForEachVIndex([&amp;](size_t i, size_t j, size_t k) &#123; Vector3D pt = vPos(i, j, k); double fBuoy = _buoyancySmokeDensityFactor * den-&gt;sample(pt) + _buoyancyTemperatureFactor * (temp-&gt;sample(pt) - tAmb); v(i, j, k) += timeIntervalInSeconds * fBuoy * up.y; &#125;); &#125; if (std::abs(up.z) &gt; kEpsilonD) &#123; vel-&gt;parallelForEachWIndex([&amp;](size_t i, size_t j, size_t k) &#123; Vector3D pt = wPos(i, j, k); double fBuoy = _buoyancySmokeDensityFactor * den-&gt;sample(pt) + _buoyancyTemperatureFactor * (temp-&gt;sample(pt) - tAmb); w(i, j, k) += timeIntervalInSeconds * fBuoy * up.z; &#125;); &#125; applyBoundaryCondition(); &#125;&#125; &emsp;&emsp;然后就是流体密度和流体温度的对流以及粘性计算，对流没什么需要特殊处理，这里不再赘述。粘性扩散计算其实也没什么特殊的地方，我们直接调用前面的流体粘性解算器。在求解完流体密度和温度的粘性项之后，我们给流体的密度和温度做一个衰减： 12345678910111213141516171819202122232425262728293031323334353637void GridSmokeSolver3::computeDiffusion(double timeIntervalInSeconds) &#123; if (diffusionSolver() != nullptr) &#123; if (_smokeDiffusionCoefficient &gt; kEpsilonD) &#123; auto den = smokeDensity(); auto den0 = std::dynamic_pointer_cast&lt;CellCenteredScalarGrid3&gt;(den-&gt;clone()); diffusionSolver()-&gt;solve(*den0, _smokeDiffusionCoefficient, timeIntervalInSeconds, den.get(), *colliderSdf()); extrapolateIntoCollider(den.get()); &#125; if (_temperatureDiffusionCoefficient &gt; kEpsilonD) &#123; auto temp = smokeDensity(); auto temp0 = std::dynamic_pointer_cast&lt;CellCenteredScalarGrid3&gt;(temp-&gt;clone()); diffusionSolver()-&gt;solve(*temp0, _temperatureDiffusionCoefficient, timeIntervalInSeconds, temp.get(), *colliderSdf()); extrapolateIntoCollider(temp.get()); &#125; &#125; // Decay auto den = smokeDensity(); den-&gt;parallelForEachDataPointIndex([&amp;](size_t i, size_t j, size_t k) &#123; (*den)(i, j, k) *= 1.0 - _smokeDecayFactor; &#125;); auto temp = temperature(); temp-&gt;parallelForEachDataPointIndex([&amp;](size_t i, size_t j, size_t k) &#123; (*temp)(i, j, k) *= 1.0 - _temperatureDecayFactor; &#125;);&#125; &emsp;&emsp;整个工程的代码具体见这里。 参考资料：$[1]$ Stam J. Stable fluids[J]. Acm Transactions on Graphics, 1999, 1999:121—128. $[2]$ A. J. Chorin and J. E. Marsden. A Mathematical Introduction to Fluid Mechanics. Springer-Verlag. Texts in Applied Mathematics 4. Second Edition., New York, 1990. $[3]$ Kim, D. (2017). Fluid engine development. Boca Raton: Taylor &amp; Francis, a CRC Press, Taylor &amp; Francis Group. $[4]$ R. Bridson and M. Müller-Fischer. Fluid simulation: Siggraph 2007 course notes. In ACM SIGGRAPH 2007 Courses, pages 1–81, ACM, 2007. $[5]$ Fedkiw R, Stam J, Jensen H W. Visual simulation of smoke[C]// Conference on Computer Graphics &amp; Interactive Techniques. 2001.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/tags/Fluid-Simulation/"}]},{"title":"流体模拟Fluid Simulation：基于SPH的拉格朗日流体模拟","slug":"SPH","date":"2019-08-29T07:28:55.738Z","updated":"2019-09-12T08:43:16.748Z","comments":true,"path":"2019/08/29/SPH/","link":"","permalink":"https://yangwc.com/about/2019/08/29/SPH/","excerpt":"本文主要是关于拉格朗日粒子的流体模拟经典算法——SPH。","text":"本文主要是关于拉格朗日粒子的流体模拟经典算法——SPH。 光滑粒子动力学 SPH流体模拟算法 PCISPH流体模拟算法 参考资料 基于SPH的拉格朗日流体模拟 &emsp;&emsp;SPH，全称为Smoothed Particle Hydrodynamics，最初提出于天体物理学领域，然后被广泛的应用到计算流体力学领域，成为基于拉格朗日粒子模拟方法的典型代表。实际上，目前除了流体，还有刚体、软体等的物理模拟也有不少采用了SPH的方法。SPH是一种基于光滑粒子核的物理模型，它将模拟的对象离散成一个个粒子，然后以光滑核将粒子之间联系起来，显然这是一种基于拉格朗日视角的模拟方法，相对于欧拉视角的模拟方法，它比较简单、速度较快。 图1 基于粒子的流体模拟 一、光滑粒子动力学&emsp;&emsp;SPH的核心在于它的光滑核函数，首先我们来看看光滑核函数的推导。对于一个标量函数$f$，给定一个积分区域，有： f(x)=\\int f(y)\\delta (x-y)dy \\tag {1}&emsp;&emsp;且$\\delta$是一个狄拉克函数，故上式$(1)$成立： \\delta (x-y)= \\{ \\begin{matrix} \\infty\\ \\ ,x=y\\\\ 0\\ \\ \\ \\ ,x\\neq y \\end{matrix} \\tag {2}&emsp;&emsp;将公式$(1)$中的狄拉克函数替换成一个光滑的核函数$\\omega$： f(x)\\approx \\int f(y)\\omega (||x-y||/h)dy \\tag {3}&emsp;&emsp;其中$h$是光滑核的半径长度，光滑核函数$\\omega$具有以下的性质： 当$||w-y||/h &gt;1$时，$w=0$，即超出核半径之外的定义域，函数值均取零。 $lim_{h\\to 0}w(||x-y||/h)=\\delta (x-y)$，即当核半径$h$趋于零时，核函数收敛到公式$(2)$的狄拉克函数。 归一化条件，$\\int w(||x-y||/h)dy=1$。 函数$\\omega$是光滑连续的，因而是可微的。 &emsp;&emsp;通过公式$(3)$，我们可以根据周围邻域的函数值近似获取给定的点的函数值，这些函数值可以是密度、压强等流体的属性值。公式$(3)$给出的是一个积分形式，在二维空间中的积分变量是面积，在三维空间中的积分变量是体积，我们采用黎曼和来离散地计算公式$(3)$： \\begin{align} f(x)\\approx &\\int f(y)\\omega (||x-y||/h)dy\\\\ \\approx &\\Sigma_{i=1}^N f_i w(||x-x_i||/h_i)V_i\\\\ \\approx &\\Sigma_{i=1}^N \\frac{m_i}{\\rho_i}f_i w(||x-x_i||/h_i) \\end{align} \\tag {4}&emsp;&emsp;其中$V_i$是我们取的体积微元，又因$\\rho=\\frac mv$，所以最终可以写成如上所示的计算公式。$f_i$是第$i$个邻域粒子的函数取值。公式$(4)$并不需要我们遍历在光滑核半径$h$之内的所有空间微元，在没有粒子的空间微元中，其函数取值$f$为零，因此我们只需遍历在光滑核半径$h$之内的所有粒子，因而上述的黎曼和是是邻居粒子的叠加形式。 &emsp;&emsp;除此之外，我们还需要根据邻居粒子的函数值计算给定粒子的函数梯度向量以及拉普拉斯算子。首先推导出理论上的公式，然后再做进一步的离散化。计算公式$(3)$的梯度向量如下所示，微分变量为$x$，即对$x$求矢量微分： \\begin{align} \\nabla_xf(x)\\approx &\\nabla_x \\int f(y)\\omega (||x-y||/h)dy\\\\ \\approx & \\int [\\nabla_xf(y)] \\omega (||x-y||/h)dy +\\int f(y)[\\nabla_x \\omega(||x-y||/h)]dy\\\\ \\approx &\\int f(y)[\\nabla_x \\omega(||x-y||/h)]dy \\end{align} \\tag {5}&emsp;&emsp;上述过程的推导用到了求导的乘法法则以及$\\nabla_xf(y)=0$。然后将公式$(5)$离散化为： \\nabla f(x)\\approx \\Sigma_{i=1}^N\\frac{m_i}{\\rho_i}f_i \\nabla w(||x-x_i||/h_i) \\tag {6}&emsp;&emsp;根据公式$(6)$计算梯度存在两个问题：一个是对于常量函数，公式$(6)$计算出来的梯度向量不为零；另一个就是不对称，我们通常需要计算梯度向量来获取粒子之间相互作用的力，力是相互作用的，因而是对称的，但是上述公式计算得到的向量并不是对称的。我们首先来看第一个问题，考虑将求导的函数再乘上一个密度函数$\\rho$： \\nabla (\\rho f)=f\\nabla \\rho +\\rho \\nabla f \\Rightarrow \\nabla f=\\frac1\\rho \\nabla(\\rho f)-\\frac1\\rho f\\nabla \\rho \\tag {7}&emsp;&emsp;详细展开，有： \\begin{align} \\nabla f(x_i)\\approx & \\frac 1{\\rho_i} \\Sigma_{j=1}^N\\frac{m_j}{\\rho_j}\\rho_j f_j\\nabla w(||x_i-x_j||/h_j)-\\frac 1{\\rho_i}f_i\\Sigma_{j=1}^N\\frac{m_j}{\\rho_j}\\rho_j\\nabla w(||x_i-x_j||/h_j)\\\\ \\approx & \\frac{1}{\\rho_i}\\Sigma_{j=1}^N m_j(f_j-f_i)\\nabla w(||x_i-x_j||/h_j) \\end{align} \\tag {8}&emsp;&emsp;公式$(8)$得到的梯度计算公式解决了第一个问题，即对于常量函数，其导数为零。对于第二个对称性的问题，我们采用同样的技巧，将求导函数除以一个密度函数$\\rho$： \\nabla (\\frac f\\rho)=-\\frac{f}{\\rho^2}\\nabla \\rho+\\frac{1}{\\rho}\\nabla f \\Rightarrow \\nabla f=\\rho(\\nabla (\\frac f\\rho) + \\frac{f}{\\rho^2}\\nabla \\rho) \\tag {9}&emsp;&emsp;详细展开后，得： \\nabla f(x_i)\\approx \\rho_i\\Sigma_{j=1}^N m_j(\\frac{f_j}{\\rho_j^2}+\\frac{f_i}{\\rho_i^2})\\nabla w(||x_i-xj||/h_j) \\tag {10}&emsp;&emsp;公式$(10)$计算得到得梯度向量是对称的。然后，还有拉普拉斯算子的计算非常常见，其计算公式如下： \\nabla^2 f(x_i)\\approx m\\Sigma_{j=1}^N \\frac{m_j}{\\rho_j}(f_j-f_i)\\nabla^2 w(||x_i-x_j||/h) \\tag {11}&emsp;&emsp;接下来我们就来看看核函数$w$的选取。在流体模拟中，一个标准的核函数如下： W_{poly6}(r,h)=\\frac{315}{64\\pi h^3} \\begin{cases} (1-\\frac{r^2}{h^2})^3\\ \\ 0\\leq r\\leq h\\\\ 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ otherwise \\end{cases} \\tag {12}&emsp;&emsp;因而实现的代码如下： 12345678910inline double SphStdKernel3::operator()(double distance) const &#123; if (distance * distance &gt;= h2) return 0.0; else &#123; double x = 1.0 - distance * distance / h2; return 315.0 / (64.0 * kPiD * h3) * x * x * x; &#125;&#125; &emsp;&emsp;其梯度计算公式为： \\nabla W_{poly6}(r,h)=\\frac{-945r}{32\\pi h^5}(1-\\frac{r^2}{h^2})^2, 0\\leq r\\leq h \\tag {13}123456789101112131415161718192021222324inline double SphStdKernel3::firstDerivative(double distance) const &#123; if (distance &gt;= h) return 0.0; else &#123; double x = 1.0 - distance * distance / h2; return -945.0 / (32.0 * kPiD * h5) * distance * x * x; &#125;&#125;inline Vector3D SphStdKernel3::gradient(const Vector3D&amp; point) const &#123; double dist = point.length(); if (dist &gt; 0.0) return gradient(dist, point / dist); else return Vector3D(0, 0, 0);&#125;inline Vector3D SphStdKernel3::gradient(double distance, const Vector3D&amp; directionToCenter) const&#123; return -firstDerivative(distance) * directionToCenter;&#125; &emsp;&emsp;其二阶导数为： \\nabla ^2W_{poly6}(r,h)=\\frac{945}{32\\pi h^5}(1-\\frac{r^2}{h^2})(\\frac{5r^2}{h^2}-1) \\tag {14}12345678910inline double SphStdKernel3::secondDerivative(double distance) const &#123; if (distance * distance &gt;= h2) return 0.0; else &#123; double x = distance * distance / h2; return 945.0 / (32.0 * kPiD * h5) * (1 - x) * (5 * x - 1); &#125;&#125; &emsp;&emsp;但是，当我们需要求取核函数的梯度向量核拉普拉斯算子的时候，我们并不会采用poly6核函数，这是因为虽然其原函数很光滑，但其一阶导数和二阶导数的性质却不是很好，因而不会使用poly6的导数。对于梯度向量和拉普拉斯的求取，我们用spiky核函数取而代之： W_{poly6}(r,h)=\\frac{15}{\\pi h^3} \\begin{cases} (1-\\frac{r}{h})^3\\ \\ 0\\leq r\\leq h\\\\ 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ otherwise \\end{cases} \\tag {15}&emsp;&emsp;下面的图2是关于poly6核函数与spiky核函数的函数曲线、一阶导数曲线和二阶导数曲线，左图是poly6函数的，右图是spiky函数的，实心曲线为函数自身的曲线，点线是函数的二阶导数曲线，而虚线则是函数的一阶导数曲线。可以看到poly6的一阶导数和二阶导数呈现出一个波动的形态，在中心处一阶导数甚至降为零。我们需要计算流体的压力梯度来确保流体的不可压缩性，如果直接采用poly6的一阶导数来计算流体的压力梯度力，那么当两个粒子重合时，其压力梯度力为零，不存在一个力使它们分开，从而违背了流体的不可压缩性。二阶导数同理，其甚至在中心处取值为负，二阶导数将用于拉普拉斯算子的计算，而流体的粘性力计算将用到拉普拉斯算子。 图2 poly6和spiky的原函数、一阶导数和二阶导数比较 &emsp;&emsp;spiky函数的一阶导数为： \\nabla W_{spiky}(r,h)=-\\frac{45}{\\pi h^4}(1-\\frac{r}{h})^2 \\tag {16}123456789101112131415161718192021222324inline double SphSpikyKernel3::firstDerivative(double distance) const &#123; if (distance &gt;= h) return 0.0; else &#123; double x = 1.0 - distance / h; return -45.0 / (kPiD * h4) * x * x; &#125;&#125;inline Vector3D SphSpikyKernel3::gradient(const Vector3D&amp; point) const &#123; double dist = point.length(); if (dist &gt; 0.0) return gradient(dist, point / dist); else return Vector3D(0, 0, 0);&#125;inline Vector3D SphSpikyKernel3::gradient(double distance, const Vector3D&amp; directionToCenter) const &#123; return -firstDerivative(distance) * directionToCenter;&#125; &emsp;&emsp;spiky函数的二阶导数为： \\nabla^2 W_{spiky}(r,h)=\\frac{90}{\\pi h^5}(1-\\frac{r}{h}) \\tag {17}12345678910inline double SphSpikyKernel3::secondDerivative(double distance) const &#123; if (distance &gt;= h) return 0.0; else &#123; double x = 1.0 - distance / h; return 90.0 / (kPiD * h5) * x; &#125;&#125; 二、SPH流体模拟算法&emsp;&emsp;首先是简化的Naiver-Stokes方程，分为两部分，分别是动量方程$(18)$和连续方程$(19)$： \\frac{D \\vec v}{D t}=-\\frac{1}{\\rho}\\nabla p+\\mu \\nabla^2 \\vec v+\\vec g \\tag {18} \\frac{\\partial \\rho}{\\partial t}+\\nabla \\cdot (\\rho \\vec v) = 0 \\tag {19}&emsp;&emsp;连续方程描述了流体的密度随着时间的变化速率，我们的模拟的是不可压缩流体，因而密度守恒，故$\\frac{\\partial \\rho}{\\partial t}=0$，即$\\nabla \\cdot \\vec v=0$，流体的速度场满足无散度的条件。动量方程左边的是速度关于时间的物质导数，$\\frac{D\\vec v}{D t}=\\frac{\\partial \\vec v}{\\partial t}+\\vec v\\cdot \\nabla \\vec v$。在我们的基于粒子的流体模拟算法中，质量守恒自动满足，$\\vec v\\cdot \\nabla \\vec v=0$，粒子速度场关于时间的物质导数就是$\\frac{\\partial \\vec v}{\\partial t}$。上述公式中的$\\vec g$是重力加速度，$\\mu$是流体的黏度系数，$\\rho$是流体密度，$p$是流体压强。 &emsp;&emsp;每一个粒子的加速度为： a=-\\frac{1}{\\rho}\\nabla p+\\mu \\nabla^2\\vec v+\\vec g \\tag {20}&emsp;&emsp;在公式$(20)$的右边分别是流体的压力项$-\\frac{1}{\\rho}\\nabla p$、粘性力项$\\mu \\nabla^2\\vec v$以及体积力项$\\vec g$（在这里目前只有重力）。在模拟的过程中，我们需要分别计算这几项。一个完整的SPH流体求解器计算流程如下所示： 1234561.Measure the density with particles’ current locations2.Compute the gravity and other extra forces3.Compute the viscosity force4.Compute the pressure based on the density5.Compute the pressure gradient force6.Perform time integration &emsp;&emsp;接下来按照这个步骤一一展开。 1、计算粒子的密度&emsp;&emsp;首先我们要计算流体粒子的密度值，因为后面粘性力和压强梯度力的计算将会用到粒子的密度。粒子的密度同样是采用光滑核对周围粒子进行加权计算，将密度函数代入公式$(4)$, \\begin{align} f(x)\\approx & \\Sigma_{i=1}^N \\frac{m_i}{\\rho_i}\\rho_i w(||x-x_i||/h_i)\\\\ \\approx & \\Sigma_{i=1}^Nm_i w(||x-x_i||/h_i) \\end{align} \\tag {21}&emsp;&emsp;原公式中的密度项被消去了，所以我们直接根据粒子的位置和质量计算每个粒子的密度，在这里我们的每个粒子质量都相同： 1234567891011121314151617181920212223void SphSystemData3::updateDensities() &#123; auto p = positions(); auto d = densities(); const double m = mass(); parallelFor(kZeroSize, numberOfParticles(), [&amp;](size_t i) &#123; double sum = sumOfKernelNearby(p[i]); d[i] = m * sum; &#125;);&#125;double SphSystemData3::sumOfKernelNearby(const Vector3D&amp; origin) const &#123; double sum = 0.0; SphStdKernel3 kernel(_kernelRadius); neighborSearcher()-&gt;forEachNearbyPoint( origin, _kernelRadius, [&amp;](size_t, const Vector3D&amp; neighborPosition) &#123; double dist = origin.distanceTo(neighborPosition); sum += kernel(dist); &#125;); return sum;&#125; 2、计算外部作用力&emsp;&emsp;这里指的外部的作用力通常是体积力，即间接地作用在流体上而非通过直接接触产生的作用力。常见的体积力有重力和风力，这些体积力我们直接根据需要指定其加速度的值，然后采用牛顿定律叠加到粒子的力场上： 123456789101112131415161718void ParticleSystemSolver3::accumulateExternalForces() &#123; size_t n = _particleSystemData-&gt;numberOfParticles(); auto forces = _particleSystemData-&gt;forces(); auto velocities = _particleSystemData-&gt;velocities(); auto positions = _particleSystemData-&gt;positions(); const double mass = _particleSystemData-&gt;mass(); parallelFor(kZeroSize, n, [&amp;](size_t i) &#123; // Gravity Vector3D force = mass * _gravity; // Wind Vector3D relativeVel = velocities[i] - _wind-&gt;sample(positions[i]); force += -_dragCoefficient * relativeVel; forces[i] += force; &#125;);&#125; 3、计算流体粘性力&emsp;&emsp;流体的粘性力是流体内部的一种阻力，粘性力的计算需要计算流体速度场的拉普拉斯算子，这是因为拉普拉斯算子衡量了给定位置的物理量与周围邻域物理量的差距值： f_{v}=m\\mu \\nabla ^2\\vec v \\tag {22}&emsp;&emsp;在SPH的算法中，其同样是通过光滑核函数计算粘性力，注意这里为了避免常量函数的拉普拉斯算子为非零，采用了前面的公式$(8)$： f_v(x)=m^2\\mu\\Sigma_j(\\frac{\\vec v_j-\\vec v_i}{\\rho_j})\\nabla^2W(x-x_j) \\tag {23}123456789101112131415161718192021222324void SphSolver3::accumulateViscosityForce() &#123; auto particles = sphSystemData(); size_t numberOfParticles = particles-&gt;numberOfParticles(); auto x = particles-&gt;positions(); auto v = particles-&gt;velocities(); auto d = particles-&gt;densities(); auto f = particles-&gt;forces(); const double massSquared = square(particles-&gt;mass()); const SphSpikyKernel3 kernel(particles-&gt;kernelRadius()); parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; const auto&amp; neighbors = particles-&gt;neighborLists()[i]; for (size_t j : neighbors) &#123; double dist = x[i].distanceTo(x[j]); f[i] += viscosityCoefficient() * massSquared * (v[j] - v[i]) / d[j] * kernel.secondDerivative(dist); &#125; &#125;);&#125; 4、计算流体的压强&emsp;&emsp;流体的压强是流体内部的一种力，它是维持流体不可压缩的关键。目前我们只知道流体的密度值，流体的密度与压强存在着某种联系，密度大的趋于压强必然大，因此需要根据流体的密度计算其对应的压强值。当然我们可以直接求解关于流体压强的泊松方程$\\nabla ^2P=\\rho \\frac{\\nabla \\vec v}{\\Delta t}$，这样求解得到的压强是非常精确的，直接求解泊松方程在基于欧拉网格的流体模拟中比较常见，但是求解大规模稀疏矩阵的泊松方程非常耗时，因此在基于拉格朗日粒子的流体模拟中比较少（但也有），一种非常廉价且效果非常不错的方法就是采用泰特的状态方程： P=B((\\frac{\\rho}{\\rho_0})^\\gamma -1) \\tag {24}&emsp;&emsp;公式$(24)$中，$\\rho_0$流体静止时的密度，$\\rho$是流体粒子当前的密度，$\\gamma$是状态方程的指数，取值为$7$，而$B$则是一个缩放系数因子，与声波在流体中的传播速度有关，其计算公式如下： B=\\frac{\\rho_0 c_s^2}{\\gamma} \\tag {25}&emsp;&emsp;通过这个状态方程，我们可以计算出流体的压强值。但是存在一个问题，对于的粒子密度，我们是通过邻域粒子来计算的，这意味着流体表面的粒子因为邻域粒子较少使得其计算出来的密度值$\\rho$低于静止时的密度$\\rho_0$，导致通过状态方程计算出来的压强为负数，如下图3所示，从而导致流体在表面上的不正常聚集现象，这种现象类似于表面张力，但它并不是物理意义上准确的，所以我们需要消除这种现象。当计算得到的压强为负时，我们赋予为零。 图3 表面的低密度导致的负压强使粒子不正常地聚集 12345678910111213141516171819202122232425262728293031323334353637383940double SphSolver3::computePressureFromEos( double density, double targetDensity, double eosScale, double eosExponent, double negativePressureScale)&#123; double p = eosScale * (std::pow((density / targetDensity), eosExponent) - 1.0); // Negative pressure Scaling. if (p &lt; 0) p *= negativePressureScale; return p;&#125;void SphSolver3::computePressure()&#123; auto particles = sphSystemData(); size_t numberOfParticles = particles-&gt;numberOfParticles(); auto d = particles-&gt;densities(); auto p = particles-&gt;pressures(); // See Equation 9 from // http://cg.informatik.uni-freiburg.de/publications/2007_SCA_SPH.pdf const double targetDensity = particles-&gt;targetDensity(); const double eosScale = targetDensity * square(_speedOfSound) / _eosExponent; parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; p[i] = computePressureFromEos( d[i], targetDensity, eosScale, eosExponent(), negativePressureScale()); &#125;);&#125; 5、计算压强梯度力&emsp;&emsp;前面一步我们得到了流体粒子的压强，在压强的作用下，流体粒子从高压强区域流向低压强区域，是流体保持不可压缩的性质。因此，我们需要计算作用在流体粒子上的压强梯度力项，压力梯度力计算公式为： f_p=-m\\frac{\\nabla p}{\\rho} \\tag {26}&emsp;&emsp;利用前面的核函数拉普拉斯算子计算公式$(11)$，压强梯度力离散计算公式为： f_p=-m^2\\Sigma_j(\\frac{p_i}{\\rho_i^2}+\\frac{p_j}{\\rho_j^2})\\nabla W(x-x_j) \\tag {27}1234567891011121314151617181920212223242526272829void SphSolver3::accumulatePressureForce( const ConstArrayAccessor1&lt;Vector3D&gt;&amp; positions, const ConstArrayAccessor1&lt;double&gt;&amp; densities, const ConstArrayAccessor1&lt;double&gt;&amp; pressures, ArrayAccessor1&lt;Vector3D&gt; pressureForces)&#123; auto particles = sphSystemData(); size_t numberOfParticles = particles-&gt;numberOfParticles(); const double massSquared = square(particles-&gt;mass()); const SphSpikyKernel3 kernel(particles-&gt;kernelRadius()); parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; const auto&amp; neighbors = particles-&gt;neighborLists()[i]; for (size_t j : neighbors) &#123; double dist = positions[i].distanceTo(positions[j]); if (dist &gt; 0.0) &#123; Vector3D dir = (positions[j] - positions[i]) / dist; pressureForces[i] -= massSquared * (pressures[i] / (densities[i] * densities[i]) + pressures[j] / (densities[j] * densities[j])) * kernel.gradient(dist, dir); &#125; &#125; &#125;);&#125; 6、时间步进积分&emsp;&emsp;前面我们计算得到每个流体粒子的作用力合力，接着需要根据这个力计算粒子的加速度，然后在加速度的作用下更新粒子的速度值，最后在速度的作用下计算粒子的位置向量。这个步骤只需简单地利用牛顿定律即可。 123456789101112131415161718void ParticleSystemSolver3::timeIntegration(double timeStepInSeconds) &#123; size_t n = _particleSystemData-&gt;numberOfParticles(); auto forces = _particleSystemData-&gt;forces(); auto velocities = _particleSystemData-&gt;velocities(); auto positions = _particleSystemData-&gt;positions(); const double mass = _particleSystemData-&gt;mass(); parallelFor(kZeroSize, n, [&amp;](size_t i) &#123; // Integrate velocity first Vector3D&amp; newVelocity = _newVelocities[i]; newVelocity = velocities[i] + timeStepInSeconds * forces[i] / mass; // Integrate position. Vector3D&amp; newPosition = _newPositions[i]; newPosition = positions[i] + timeStepInSeconds * newVelocity; &#125;);&#125; 7、碰撞检测处理&emsp;&emsp;在获取了新得速度场和位置向量之后，我们需要做碰撞检测处理，使之不发生穿透。这里的碰撞检测采用了隐式表面的符号距离场，为每个碰撞体构建一个水平集，这方面的内容比较多，但不是这里的核心点，故不赘述。目前实现的是刚体碰撞。下面只是一部分代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778void Collider3::resolveCollision( double radius, double restitutionCoefficient, Vector3D* newPosition, Vector3D* newVelocity) &#123; assert(_surface); if (!_surface-&gt;isValidGeometry()) return; ColliderQueryResult colliderPoint; getClosestPoint(_surface, *newPosition, &amp;colliderPoint); // Check if the new position is penetrating the surface if (isPenetrating(colliderPoint, *newPosition, radius)) &#123; // Target point is the closest non-penetrating position from the // new position. Vector3D targetNormal = colliderPoint.normal; Vector3D targetPoint = colliderPoint.point + radius * targetNormal; Vector3D colliderVelAtTargetPoint = colliderPoint.velocity; // Get new candidate relative velocity from the target point. Vector3D relativeVel = *newVelocity - colliderVelAtTargetPoint; double normalDotRelativeVel = targetNormal.dot(relativeVel); Vector3D relativeVelN = normalDotRelativeVel * targetNormal; Vector3D relativeVelT = relativeVel - relativeVelN; // Check if the velocity is facing opposite direction of the surface // normal if (normalDotRelativeVel &lt; 0.0) &#123; // Apply restitution coefficient to the surface normal component of // the velocity Vector3D deltaRelativeVelN = (-restitutionCoefficient - 1.0) * relativeVelN; relativeVelN *= -restitutionCoefficient; // Apply friction to the tangential component of the velocity // From Bridson et al., Robust Treatment of Collisions, Contact and // Friction for Cloth Animation, 2002 // http://graphics.stanford.edu/papers/cloth-sig02/cloth.pdf if (relativeVelT.lengthSquared() &gt; 0.0) &#123; double frictionScale = std::max(1.0 - _frictionCoeffient * deltaRelativeVelN.length() / relativeVelT.length(), 0.0); relativeVelT *= frictionScale; &#125; // Reassemble the components *newVelocity = relativeVelN + relativeVelT + colliderVelAtTargetPoint; &#125; // Geometric fix *newPosition = targetPoint; &#125;&#125;void ParticleSystemSolver3::resolveCollision( ArrayAccessor1&lt;Vector3D&gt; newPositions, ArrayAccessor1&lt;Vector3D&gt; newVelocities) &#123; if (_collider != nullptr) &#123; size_t numberOfParticles = _particleSystemData-&gt;numberOfParticles(); const double radius = _particleSystemData-&gt;radius(); parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; _collider-&gt;resolveCollision( radius, _restitutionCoefficient, &amp;newPositions[i], &amp;newVelocities[i]); &#125;); &#125;&#125; 8、自适应时间步长&emsp;&emsp;最后还有个非常关键的点，就是模拟的时间步长选取。为了使得流体的密度守恒（即保持不可压缩性），我们采用了泰特的状态方程，该方程引入了两个参数，分别是指数$\\gamma$和声波在流体中的传播速度，这带来了一些时间步长的限制问题。如下图4所示，假设一个流体粒子从空中落到一滩流体中，这个过程会产生一些震荡波，白色的粒子是落下的粒子以及获取到了震荡波动信息的粒子。 图4 流体粒子的信息传播 &emsp;&emsp;受限于有限的光滑核半径，在每一个时间步长内，信息传播的范围最大为光滑核半径的长度。设信息的传播速度为$c$，那么我们能取得最大时间步长就为$h/c$。在我们得物理模拟中，这个信息传播速度实际上就是声波在流体中的传播速度。为此，研究者们提出自适应的时间步长，根据当前的流体状态计算最大的时间步长，如果取超过这个最大时间步长限制，那么将会导致流体崩溃，产生不稳定的问题。时间步长的上界计算公式为： \\begin{align} &\\Delta t_v=\\frac{\\lambda_v h}{c_s}\\\\ &\\Delta t_f=\\lambda_f \\sqrt{\\frac{mh}{F_{max}}}\\\\ &\\Delta t\\leq min(\\Delta t_v, \\Delta t_f) \\end{align} \\tag {28}&emsp;&emsp;其中，$h$是光滑核半径，$m$粒子质量，$c_s$是声波在流体中的传播速度，$F_{max}$是流体粒子当中受到的最大合力的长度值，$\\lambda_v$和$\\lambda_f$是一个缩放系数，分别取$0.4$和$0.25$。可以看到，时间步长的上界不仅仅取决于声波的传播速度，还取决于每一个不同的时刻流体所受的最大合力，因而每一次模拟都要重新计算下一个时间步长。 123456789101112131415161718192021unsigned int SphSolver3::numberOfSubTimeSteps(double timeIntervalInSeconds) const &#123; auto particles = sphSystemData(); size_t numberOfParticles = particles-&gt;numberOfParticles(); auto f = particles-&gt;forces(); const double kernelRadius = particles-&gt;kernelRadius(); const double mass = particles-&gt;mass(); double maxForceMagnitude = 0.0; for (size_t i = 0; i &lt; numberOfParticles; ++i) maxForceMagnitude = std::max(maxForceMagnitude, f[i].length()); double timeStepLimitBySpeed = kTimeStepLimitBySpeedFactor * kernelRadius / _speedOfSound; double timeStepLimitByForce = kTimeStepLimitByForceFactor * std::sqrt(kernelRadius * mass / maxForceMagnitude); double desiredTimeStep = _timeStepLimitScale * std::min(timeStepLimitBySpeed, timeStepLimitByForce); return static_cast&lt;unsigned int&gt;(std::ceil(timeIntervalInSeconds / desiredTimeStep));&#125; &emsp;&emsp;真实世界中的声波在流体中的传播速度很快，这导致通过公式$(28)$计算得到的时间步长是非常小的。举个例子，假设我们的光滑核半径为$0.1m$，质量为$0.01kg$，声波传播速度为$1482m/s$，在只有重力这个外力的作用下，根据公式$(28)$计算可得$\\Delta t_v=0.00002699055331s$，而$\\Delta t_f=0.0007985957062$，因此最大的时间步长为$0.00002699055331$，这意味着如果模拟$60$帧率的话，那么一秒需要计算$618$个子时间步长，耗费大量的时间和计算资源，即便是离线模拟，其代价也太大了。这个就是传统的SPH算法的缺点，被称为WCSPH（Weakly Compressible SPH），针对这个缺点，目前已经有不少研究者提出了不同的改进方法，其中比较优秀的算法就是PCISPH。 9、模拟效果&emsp;&emsp;整个模拟系统比较大，关于粒子数据的结构、邻域搜索算法、多线程并行、系统结构等方面不再赘述。我们模拟的结果是粒子群的位置向量，为了渲染出来需要采用一个算法根据粒子点云提取流体表面的三角网格，通常的流程就是根据粒子点云计算一个水平集，关于这方面目前已经不少学者研究并提出了几种算法，目前先采用最简单的一种提取表面算法，以后再回过头来查看这方面的算法。提取出来的表面之后采用了一个Mitsuba光追渲染器，模拟并渲染了两个场景，一个是Fluid Drop场景，一个是Dam Breaking场景。 &emsp;&emsp;由于传统的SPH模拟太过耗时，所以我就没有提取出流体的表面了，而是直接渲染流体的粒子。后面采用了改进的算法再提取流体表面，渲染极度真实的流体。下面的图5和图6分别展示了Fluid Drop场景和Dam Breaking场景的第0、10、20、30、40、50帧的流体粒子渲染结果。 图5 Fluid Drop的第0、10、20、30、40、50帧 图6 Dam Breaking的第0、10、20、30、40、50帧 三、PCISPH流体模拟算法&emsp;&emsp;传统的SPH算法采用了泰特的状态方程来计算流体压强，避免了求解大型稀疏矩阵的泊松方程，但是受限于状态方程中的声波速度，我们不能取太大的刚度系数，否则将导致时间步长非常小，极大地耗费计算资源。因此，后来又有学者提出了传统SPH算法的改进——PCISPH（全称为Predictive-Corrective Incompressible SPH，译为预测-矫正的不可压缩SPH，简称PCISPH）。与WCSPH算法不同，PCISPH不再采用泰特方程来计算流体的压强值，而是采用了一种非线性的迭代方法，尽可能地计算一个使得流体密度守恒的压强值。 &emsp;&emsp;PCISPH总体算法如下图7所示。可以看到，在求解流体的压强梯度力时，它是一个循环迭代的过程，迭代的终止条件为流体的当前密度与静止密度之差小于给定的阈值或者达到最大的迭代次数。 图7 PCISPH算法纵览 &emsp;&emsp;在每一重迭代的过程种，首先根据粒子的净合力计算加速度，并计算在当前加速度下的速度值、位置向量，这一过程称为预测过程。紧接着计算前面预测得到的粒子群的密度值以及当前的密度值与静止密度的差距，并根据这个密度差计算压强，使得在这个压强的作用下，我们的粒子能够移动到一个位置上从而缩小这个密度差距。最后根据压强计算压强梯度力，将其迭代到我们的合力当中，用以下一次的迭代。 &emsp;&emsp;迭代过程的关键一步就是如何根据密度差计算出所需的压强值，这涉及到一些数学内容。设光滑核半径长度为$h$，给定一点$i$在第$t+1$时间点的密度值通过下面的公式计算得到： \\begin{align} \\rho_i(t+1)&= m\\Sigma_j W(x_i(t+1)-x_j(t+1))\\\\ &= m\\Sigma_jW(x_i(t)+\\Delta x_i(t)-x_j(t)-\\Delta x_j(t))\\\\ &= m\\Sigma_jW(d_{ij}(t)+\\Delta d_{ij}(t)) \\end{align} \\tag {29}&emsp;&emsp;其中$d_{ij}(t)=x_i(t)-x_j(t)$，$\\Delta d_{ij}(t)=\\Delta x_i(t)-\\Delta x_j(t)$。假设$\\Delta d_{ij}$足够小，对$W(d_{ij}(t)+\\Delta d_{ij}(t))$做一阶泰勒展开： \\begin{align} \\rho_i(t+1) &= m\\Sigma_j[W(d_{ij}(t))+\\nabla W(d_{ij}(t))\\cdot \\Delta d_{ij}(t)]\\\\ &= m\\Sigma_j W(x_i(t)-x_j(t))+m\\Sigma_j \\nabla W(x_i(t)-x_j(t))\\cdot(\\Delta x_i(t)- \\Delta x_j(t))\\\\ &= \\rho_i(t)+\\Delta \\rho_i(t) \\end{align} \\tag {30}&emsp;&emsp;上式中，$\\Delta \\rho_i(t)$是未知项，我们对其做一些展开： \\begin{align} \\Delta \\rho_i(t) &=m\\Sigma_j\\nabla W_{ij}\\cdot (\\Delta x_i(t)-\\Delta x_j(t))\\\\ &=m(\\Sigma_j \\nabla W_{ij}\\Delta x_i(t)-\\Sigma_j\\nabla W_{ij}\\Delta x_j(t))\\\\ &=m(\\Delta x_i(t)\\Sigma_j \\nabla W_{ij}-\\Sigma_j \\nabla W_{ij}\\Delta x_j(t)) \\end{align} \\tag {31}&emsp;&emsp;令$\\Delta x_i$为在仅仅考虑流体压强梯度力的情况下的位移向量，则有： \\Delta x_i=\\Delta t^2\\frac{F_i^p}{m} \\tag {32}&emsp;&emsp;在理想的情况下，我们的流体应该处于一种这样的平衡状态：流体粒子的压强值均等于$\\overline p_i$，流体粒子的密度值均为静止时的密度$\\rho_0$，即有： F_i^p=-m^2\\Sigma_j(\\frac{\\overline p_i}{\\rho_0^2}+\\frac{\\overline p_i}{\\rho_0^2})\\nabla W_{ij}=-m^2\\frac{2\\overline p_i}{\\rho_0^2}\\Sigma_j \\nabla W_{ij} \\tag {33}&emsp;&emsp;将公式$(33)$代入公式$(32)$，可得： \\Delta x_i=-\\Delta t^2 m\\frac{2\\overline p_i}{\\rho_0^2}\\Sigma_j\\nabla W_{ij} \\tag {34}&emsp;&emsp;记在粒子$i$的压强作用下，邻域粒子$j$的位移为$\\Delta x_{j|i}$，因力是相互作用、相互对称的，则粒子$j$受到来自$i$的压强梯度力为： F_{j|i}^p=m^2(\\frac{\\overline p_i}{\\rho_0^2}+\\frac{\\overline p_i}{\\rho_0^2}) \\nabla W_{ij}=m^2\\frac{2\\overline p_i}{\\rho_0^2}\\nabla W_{ij} \\tag {35}&emsp;&emsp;在上述的压强梯度力作用下： \\Delta x_{j|i}=\\Delta t^2m\\frac{2\\overline p_i}{\\rho_0^2}\\nabla W_{ij} \\tag {36}&emsp;&emsp;公式$(34)$和公式$(36)$给出了$\\Delta x_i$和$\\Delta x_{j|i}$，将其代入公式$(31)$中，得到了$\\Delta \\rho_i(t)$项的表达式： \\begin{align} \\Delta \\rho_i(t) &=m(-\\Delta t^2m\\frac{2\\overline p_i}{\\rho_0^2}\\Sigma_j \\nabla W_{ij}\\cdot \\Sigma_j \\nabla W_{ij}-\\Sigma_j(\\nabla W_{ij}\\cdot \\Delta t^2 m\\frac{2\\overline p_i}{\\rho_0^2}\\nabla W_{ij}))\\\\ &=\\Delta t^2m^2\\frac{2\\overline p_i}{\\rho_0^2}(-\\Sigma_j \\nabla W_{ij}\\cdot \\Sigma_j \\nabla W_{ij}-\\Sigma_j(\\nabla W_{ij}\\cdot \\nabla W_{ij})) \\end{align} \\tag {37}&emsp;&emsp;对公式$(37)$做一些变化，可得关于$\\overline p_i$的一个表达式： \\overline p_i=\\frac{\\Delta \\rho_i(t)} {\\beta (-\\Sigma_j\\nabla W_{ij}\\cdot \\Sigma_j\\nabla W_{ij}-\\Sigma_j(\\nabla W_{ij}\\cdot \\nabla W_{ij}))}\\\\ \\beta=\\Delta t^2m^2\\frac{2}{\\rho_0^2} \\tag {38}&emsp;&emsp;而$\\Delta \\rho_i(t)=-\\rho_{err_i}^t=-(\\rho_i^t-\\rho_0)$，即当前粒子的密度值与静止密度的差。公式$(38)$中的系数可以提出来提前计算，即下面的公式$(39)$，一方面是为了性能考虑，另一方面是因为表面粒子的邻域粒子数量少，其计算的系数是错的。 \\delta =\\frac{-1}{\\beta(-\\Sigma_j\\nabla W_{ij}\\cdot \\Sigma_j\\nabla W_{ij}-\\Sigma_j(\\nabla W_{ij}\\cdot \\nabla W_{ij}))} \\tag {39}12345678910111213141516171819202122232425262728293031323334353637383940414243444546double PciSphSolver3::computeBeta(double timeStepInSeconds) &#123; auto particles = sphSystemData(); return 2.0 * square(particles-&gt;mass() * timeStepInSeconds / particles-&gt;targetDensity());&#125;double PciSphSolver3::computeDelta(double timeStepInSeconds)&#123; auto particles = sphSystemData(); const double kernelRadius = particles-&gt;kernelRadius(); Array1&lt;Vector3D&gt; points; BccLatticePointGenerator pointsGenerator; Vector3D origin; BoundingBox3D sampleBound(origin, origin); sampleBound.expand(1.5 * kernelRadius); pointsGenerator.generate(sampleBound, particles-&gt;targetSpacing(), &amp;points); SphSpikyKernel3 kernel(kernelRadius); double denom = 0; Vector3D denom1; double denom2 = 0; for (size_t i = 0; i &lt; points.size(); ++i) &#123; const Vector3D&amp; point = points[i]; double distanceSquared = point.lengthSquared(); if (distanceSquared &lt; kernelRadius * kernelRadius) &#123; double distance = std::sqrt(distanceSquared); Vector3D direction = (distance &gt; 0.0) ? point / distance : Vector3D(); // grad(Wij) Vector3D gradWij = kernel.gradient(distance, direction); denom1 += gradWij; denom2 += gradWij.dot(gradWij); &#125; &#125; denom += -denom1.dot(denom1) - denom2; return (std::fabs(denom) &gt; 0.0) ? -1 / (computeBeta(timeStepInSeconds) * denom) : 0;&#125; &emsp;&emsp;然后$\\overline p_i=\\delta \\rho_{err_i}^t$，在每一次的迭代中更新粒子的压强，$p_i+=\\overline p_i$。PCISPH算法与WCSPH算法的不同就在计算压强梯度力上，除了更新粒子的压强值，还要更新粒子的密度值，并根据粒子的预测位置计算压强梯度力，整个迭代的流程不难理解。迭代结束之后，需要将迭代得到的压强梯度力加到粒子的合力上。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104void PciSphSolver3::accumulatePressureForce(double timeIntervalInSeconds) &#123; auto particles = sphSystemData(); const size_t numberOfParticles = particles-&gt;numberOfParticles(); const double delta = computeDelta(timeIntervalInSeconds); const double targetDensity = particles-&gt;targetDensity(); const double mass = particles-&gt;mass(); auto p = particles-&gt;pressures(); auto d = particles-&gt;densities(); auto x = particles-&gt;positions(); auto v = particles-&gt;velocities(); auto f = particles-&gt;forces(); // Predicted density ds Array1&lt;double&gt; ds(numberOfParticles, 0.0); SphStdKernel3 kernel(particles-&gt;kernelRadius()); // Initialize buffers parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; p[i] = 0.0; _pressureForces[i] = Vector3D(); _densityErrors[i] = 0.0; ds[i] = d[i]; &#125;); unsigned int maxNumIter = 0; double maxDensityError; double densityErrorRatio = 0.0; for (unsigned int k = 0; k &lt; _maxNumberOfIterations; ++k) &#123; // Predict velocity and position parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; _tempVelocities[i] = v[i] + timeIntervalInSeconds / mass * (f[i] + _pressureForces[i]); _tempPositions[i] = x[i] + timeIntervalInSeconds * _tempVelocities[i]; &#125;); // Resolve collisions resolveCollision(_tempPositions.accessor(), _tempVelocities.accessor()); // Compute pressure from density error parallelFor(kZeroSize, numberOfParticles, [&amp;](size_t i) &#123; double weightSum = 0.0; const auto&amp; neighbors = particles-&gt;neighborLists()[i]; for (size_t j : neighbors) &#123; double dist = _tempPositions[j].distanceTo(_tempPositions[i]); weightSum += kernel(dist); &#125; weightSum += kernel(0); double density = mass * weightSum; double densityError = (density - targetDensity); double pressure = delta * densityError; if (pressure &lt; 0.0) &#123; pressure *= negativePressureScale(); densityError *= negativePressureScale(); &#125; p[i] += pressure; ds[i] = density; _densityErrors[i] = densityError; &#125;); // Compute pressure gradient force _pressureForces.set(Vector3D()); SphSolver3::accumulatePressureForce(x, ds.constAccessor(), p, _pressureForces.accessor()); // Compute max density error maxDensityError = 0.0; for (size_t i = 0; i &lt; numberOfParticles; ++i) maxDensityError = absmax(maxDensityError, _densityErrors[i]); densityErrorRatio = maxDensityError / targetDensity; maxNumIter = k + 1; if (std::fabs(densityErrorRatio) &lt; _maxDensityErrorRatio) break; &#125; std::cout &lt;&lt; \"Number of PCI iterations: \" &lt;&lt; maxNumIter &lt;&lt; std::endl; std::cout &lt;&lt; \"Max density error after PCI iteration: \" &lt;&lt; maxDensityError &lt;&lt; std::endl; if (std::fabs(densityErrorRatio) &gt; _maxDensityErrorRatio) &#123; std::cout &lt;&lt; \"Max density error ratio is greater than the threshold!\\n\"; std::cout &lt;&lt; \"Ratio: \" &lt;&lt; densityErrorRatio &lt;&lt; \" Threshold: \" &lt;&lt; _maxDensityErrorRatio &lt;&lt; std::endl; &#125; // Accumulate pressure force parallelFor(kZeroSize, numberOfParticles, [this, &amp;f](size_t i) &#123; f[i] += _pressureForces[i]; &#125;);&#125; &emsp;&emsp;与传统的WCSPH相比，PCISPH看起来步骤更多，因为它需要迭代多次，但是其不再依赖泰特方程，从而支持更大的时间步长，因此速度比WCSPH快很多。下图8和图9展示了Fluid Drop场景和Dam Breaking的场景模拟效果，采用了Marching Cubes重建流体表面进行渲染，效果与WCSPH几乎无差别，但是加速比起码有10倍以上，而且不可压缩性更好。这就是PCISPH算法的优势。 图8 Fluid Drop的第0、10、20、30、40、50、60、70、80帧 图9 Dam Breaking的第0、10、20、30、40、50、60、70、80帧 &emsp;&emsp;最后，用PCISPH的流体求解算法模拟了一个稍微比较复杂的流体场景，模拟了600帧，不敢用WCSPH模拟600帧，因为WCSPH算法实在是太慢了。下图10展示了第0、20、40、60、80、100、120、140、160帧的效果。 图10 一个复杂场景的第0、20、40、60、80、100、120、140、160帧 参考资料：$[1]$ Müller M, Charypar D, Gross M. Particle-based fluid simulation for interactive applications[C]//Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation. Eurographics Association, 2003: 154-159. $[2]$ Becker M, Teschner M. Weakly compressible SPH for free surface flows[C]//Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer animation. Eurographics Association, 2007: 209-217. $[3]$ Schechter H, Bridson R. Ghost SPH for animating water[J]. ACM Transactions on Graphics (TOG), 2012, 31(4): 61. $[4]$ Kim, D. (2017). Fluid engine development. Boca Raton: Taylor &amp; Francis, a CRC Press, Taylor &amp; Francis Group. $[5]$ Adams and Wicke, Meshless approximation methods and applications in physics based modeling and animation, Eurographics tutorials 2009. $[6]$ Dan Koschier, Jan Bender. Smoothed Particle Hydrodynamics Techniques for the Physics Based Simulation of Fluids and Solids, Eurographics Tutorial 2019. $[7]$ Solenthaler B, Pajarola R. Predictive-corrective incompressible SPH[C]// Acm Siggraph. 2009.","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/tags/Fluid-Simulation/"}]},{"title":"流体模拟Fluid Simulation：求解流体不可压缩的泊松方程","slug":"MakingFluidImcompressible","date":"2019-08-03T03:06:23.934Z","updated":"2019-09-12T07:45:21.455Z","comments":true,"path":"2019/08/03/MakingFluidImcompressible/","link":"","permalink":"https://yangwc.com/about/2019/08/03/MakingFluidImcompressible/","excerpt":"本文主要是流体不可压缩投影，在进行半拉格朗日对流之后，还需要对流体速度进行修正，使流体的速度场散度为零。主要是关于流体压强的泊松方程求解，通常采用共轭梯度法求解大规模稀疏矩阵的线性方程组，辅以不完全的Cholesky因子化。涉及到的数学内容比较多。","text":"本文主要是流体不可压缩投影，在进行半拉格朗日对流之后，还需要对流体速度进行修正，使流体的速度场散度为零。主要是关于流体压强的泊松方程求解，通常采用共轭梯度法求解大规模稀疏矩阵的线性方程组，辅以不完全的Cholesky因子化。涉及到的数学内容比较多。 离散的压力梯度 离散的速度场散度 求解关于压强的泊松方程 不可压缩投影 参考资料 流体的不可压缩投影 &emsp;&emsp;在流体模拟中，确保流体的不可压缩性是非常重要、关键的一步，也是最耗时的一步，这是整个流体模拟过程中的核心，涉及到模拟效果的真实性、模拟过程的效率快慢两方面的内容，因而相关的数学内容比较多。在流体模拟中，确保流体的不可压缩性的算法我们通常用$project(\\Delta t, \\vec u)$来表示，它输入时间步长$\\Delta t$和流体的速度场$\\vec u$，然后在流体内部的压力的作用下更新速度场： \\vec u^{n+1}=\\vec u-\\Delta t\\frac1\\rho \\nabla p \\tag {1}&emsp;&emsp;上述公式中，$\\vec u^{n+1}$就是新的速度场，$\\rho$是流体密度，$p$为压力。通过公式$(1)$，我们就可以得到满足流体不可压缩性的流体，此时得到的新速度场$\\vec u^{n+1}$的散度为0： \\nabla \\cdot \\vec u^{n+1}=0 \\tag {2}&emsp;&emsp;且在固体边界处，其速度场在法线方向上的投影等于固定墙的速度场在法线方向上的投影： \\vec u^{n+1}\\cdot \\vec n=\\vec u_{solid}\\cdot \\vec n \\tag {3}&emsp;&emsp;在前一步展开如何求解上述的几个公式之前，我们先把流体模拟区域划分成一个一个格子，有流体的格子标记为F，固体边界区域的格子标记为$S$，空白的区域则标记为$E$。下图1展示了一个二维的例子。 图1 模拟区域标记 &emsp;&emsp;模拟的网格依旧是MAC网格，即交错的网格结构，如下图2所示。格子中心存储流体压力值，边缘部分存储流体的速度场向量，依次交错存储。 图2 MAC网格 一、离散的压力梯度&emsp;&emsp;MAC网格解决了普通的网格的零域（null space）问题，在使用中心差分法计算梯度时鲁棒性更强。目前暂时假设流体的压力值已经知道且存储在MAC网格上，那么二维和三维的压力梯度计算采用中心差分法如下所示： \\nabla p=(\\frac{p_{i+1,j}-p_{i,j}}{\\Delta x},\\frac{p_{i,j+1}-p_{i,j}}{\\Delta x})\\\\ \\nabla p=(\\frac{p_{i+1,j,k}-p_{i,j,k}}{\\Delta x}, \\frac{p_{i,j+1,k}-p_{i,j,k}}{\\Delta x}, \\frac{p_{i,j,k+1}-p_{i,j,k}}{\\Delta x}) \\tag {4}&emsp;&emsp;然后将公式$(4)$中的离散压力梯度公式代入公式$(1)$，可以得到实际上的速度场更新公式： \\begin{cases} u_{i+1/2,j}^{n+1}=u_{i+1/2,j}-\\Delta t\\frac1\\rho\\frac{p_{i+1,j}-p_{i,j}}{\\Delta x}\\\\ v_{i,j+1/2}^{n+1}=v_{i,j+1/2}-\\Delta t\\frac1\\rho\\frac{p_{i,j+1}-p_{i,j}}{\\Delta x} \\end{cases}\\\\ \\begin{cases} u_{i+1/2,j,k}^{n+1}=u_{i+1/2,j,k}-\\Delta t\\frac1\\rho\\frac{p_{i+1,j,k}-p_{i,j,k}}{\\Delta x}\\\\ v_{i,j+1/2,k}^{n+1}=v_{i,j+1/2,k}-\\Delta t\\frac1\\rho\\frac{p_{i,j+1/2,k}-p_{i,j,k}}{\\Delta x}\\\\ w_{i,j,k+1/2}^{n+1}=w_{i,j,k+1/2}-\\Delta t\\frac1\\rho\\frac{p_{i,j,k+1/2}-p_{i,j,k}}{\\Delta x} \\end{cases} \\tag {5}&emsp;&emsp;公式$(5)$给出了二维和三维的流体不可压缩速度场更新公式。注意，固体墙边界并没有压力一说，而自由面边界压力我们设置为零，这样上述的公式仅适用于那些不邻近固体边界的速度场分量，一般情况下至少需要邻近一个流体区域。 &emsp;&emsp;现在我们把目标放到边界条件的处理上。首先是自由面，这部分通常就是空气，我们直接设置其压力值为零，这类边界处理我们称之为第一类边界条件——狄利克雷边界条件（Dirichlet boundary condition）$^{[1]}$。所谓狄利克雷边界条件，就是我们直接设定边界处的值，这是在数值方法中最简单的一种边界处理方法。 &emsp;&emsp;然后就是固体墙边界的处理。在固体墙边界部分，我们需要将流体的速度场在法线方向上的投影等于固体边界在法相方向上的速度场向量值，一种方法就是直接设置，例如在$(i+1/2,j,k)$处的速度场分量$u$，假设其左边$(i,j,k)$为固体边界区域，右边$(i+1,j,k)$为流体区域，那么直接赋值$u$： u_{i+1/2,j,k}^{n+1}=u_{i+1/2,j,k}^{solid} \\tag {6}&emsp;&emsp;直接赋值需要一个额外的分支进行。另外一种方案就是给固定墙区域设定一个ghost压强值，只要通过公式$(5)$更新后固体边界处的速度场为公式$(6)$即可，此时不需要额外的分支。通过一些变换，可得固体墙区域的ghost压强计算公式： p_{i,j,k}^{ghost}=p_{i+1,j,k}-\\frac{\\rho\\Delta x}{\\Delta t}(u_{i+1/2,j,k}-u_{i+1/2,j,k}^{solid}) \\tag {7}&emsp;&emsp;这种方案在进行不可压缩投影时不需要考虑邻域是否是固体边界，统一用公式$(5)$计算，在固体边界处用ghost压强值计算，依旧以上面为例： u_{i+1/2,j,k}^{n+1}=u_{i+1/2,j,k}-\\Delta t\\frac{1}{\\rho}\\frac{p_{i+1,j,k}-p_{i,j,k}^{ghost}}{\\Delta x} \\tag {8}&emsp;&emsp;实际上，将公式$(7)$代入公式$(8)$中，就可以得到公式$(6)$，通过这种方式不再将固体边界的处理作为一个单独考虑的情况。将公式$(8$做一些移项，重写为： \\frac{\\Delta t}{\\rho}\\frac{p_{i+1,j,k}-p_{i,j,k}^{ghost}}{\\Delta x}= u_{i+1/2,j,k}-u_{i+1/2,j,k}^{solid} \\tag {9}&emsp;&emsp;上述公式中的左边压强部分可以看成是关于压强梯度的有限差分，即关于下面公式的一个近似： \\frac{\\Delta t}{\\rho}\\frac{\\partial p}{\\partial x}=u-u^{solid} \\tag {10}&emsp;&emsp;公式$Math Processing Error$仅仅是关于一个方向，同样地，我们可以取其他方向的固体边界条件，得到更一般的公式： \\frac{\\Delta t}{\\rho}\\nabla p\\cdot \\vec n=(\\vec u-\\vec u^{solid})\\cdot \\vec n \\tag {11}&emsp;&emsp;公式$(11)$给出了固体边界处的压力梯度值，与狄利克雷条件处理方式不同，这里给出的是梯度的取值，这种边界处理方式被称为第二类边界条件——冯诺依曼边界条件（Neumannn boundary condition）$^{[1]}$。最后，假设流体的压力已知（实际上未知），根据压力梯度更新流体的速度场，算法伪代码如下： 1234567891011121314151617181920212223242526scale = dt/(density*dx);loop over i,j,k: #update u component. if label(i-1,j,k)==FLUID or label(i,j,k)==FLUID: if label(i-1,j,k)==SOLID or label(i,j,k)==SOLID: u(i,j,k) = usolid(i,j,k); else u(i,j,k) -= scale * (p(i,j,k) - p(i-1,j,k)); else mark u(i,j,k) as unknown; #update v component. if label(i,j-1,k)==FLUID or label(i,j,k)==FLUID: if label(i,j-1,k)==SOLID or label(i,j,k)==SOLID: v(i,j,k) = vsolid(i,j,k); else v(i,j,k) -= scale * (p(i,j,k) - p(i,j-1,k)); else mark v(i,j,k) as unknown; #update w component. if label(i,j,k-1)==FLUID or label(i,j,k)==FLUID: if label(i,j,k-1)==SOLID or label(i,j,k)==SOLID: w(i,j,k) = wsolid(i,j,k); else w(i,j,k) -= scale * (p(i,j,k) - p(i,j,k-1)); else mark w(i,j,k) as unknown; 二、离散的速度场散度&emsp;&emsp;流体不可压缩的最终表现就是速度场的散度为零，即$\\nabla \\cdot \\vec u=0$。经过不可压缩投影之后新的速度场$\\vec u^{n+1}$应该满足散度为零的条件，我们通过有限差分法估算新速度场$\\vec u^{n+1}$的散度。首先是散度的数学定义，二维和三维的散度定义如下所示： \\nabla \\cdot \\vec u=\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}\\\\ \\nabla \\cdot \\vec u=\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}+ \\frac{\\partial w}{\\partial z} \\tag {12}&emsp;&emsp;给定一个二维欧拉网格$(i,j)$，采用中心差分法计算离散的散度公式如下： (\\nabla\\cdot \\vec u)_{i,j}\\approx \\frac{u_{i+1/2,j}-u_{i-1/2,j}}{\\Delta x}+\\frac{v_{i,j+1/2}-v_{i,j-1/2}}{\\Delta x} \\tag {13}&emsp;&emsp;同理，三维$(i,j,k)$的离散散度计算公式为： (\\nabla\\cdot \\vec u)_{i,j,k}\\approx \\frac{u_{i+1/2,j,k}-u_{i-1/2,j,k}}{\\Delta x} +\\frac{v_{i,j+1/2,k}-v_{i,j-1/2,k}}{\\Delta x} +\\frac{w_{i,j,k+1/2}-w_{i,j,k-1/2}}{\\Delta x} \\tag {14}&emsp;&emsp;事实上，我们更关心散度的相反数，且仅对于流体区域计算其散度值，其余的固体边界、自由面部分不需要计算散度值。下面是一个计算流体散度的伪代码。 12345scale = 1/dx;loop over i,j,k where label (i,j,k)==FLUID: rhs(i,j,k)=-scale * (u(i+1,j,k)-u(i,j,k) +v(i,j+1,k)-v(i,j,k) +w(i,j,k+1)-w(i,j,k)); &emsp;&emsp;给定一个流体区域及其表面$\\partial cell$，流体散度的意义就是流体流进流出的总比例： \\int\\int_{\\partial cell}\\vec u \\cdot \\vec n \\tag {15}&emsp;&emsp;若流体区域为一个欧拉网格的立方体格子，那么上述的积分就是对立方体六个面上的速度场进行积分。这种方法与我们前面讨论的有限差分法不同，这是有限体积法（Finite volume method）。这里只是顺带一提。 三、求解关于压强的泊松方程&emsp;&emsp;在前面我们讨论了如何在已知流体内部压强的情况下进行不可压缩投影，更新速度场，并估算速度场的散度值。实际上，流体内部压强并不知道，这需要我们计算得到。接下来的内容就是关于如何求解流体的压力数值，这部分涉及的数学内容比较多，是比较难的一部分。 1、泊松方程&emsp;&emsp;我们知道新的速度场$\\vec u^{n+1}$需要满足散度为零，从而保证流体的不可压缩条件。又已知新的速度场$\\vec u^{n+1}$计算公式为上面的公式$(5)$，离散的散度计算公式为公式$(13)$和公式$(14)$，我们将公式$(5)$得到的速度场代入公式$(13)$、$(14)$中，就可以得到以流体压力为未知量的线性方程。 &emsp;&emsp;我们先来看二维的情况，关于$\\vec u^{n+1}$的散度为零表示成如下： \\frac{u_{i+1/2,j}^{n+1}-u_{i-1/2,j}^{n+1}}{\\Delta x} + \\frac{v_{i,j+1/2}^{n+1}-v_{i,j-1/2}^{n+1}}{\\Delta x} = 0 \\tag {16}&emsp;&emsp;将公式$(5)$中二维的速度更新公式代入上面的公式$(16)$中，可得： \\frac{1}{\\Delta x}[(u_{i+1/2,j}-\\Delta t\\frac1\\rho\\frac{p_{i+1,j}-p_{i,j}}{\\Delta x}) -(u_{i-1/2,j}-\\Delta t\\frac1\\rho\\frac{p_{i,j}-p_{i-1,j}}{\\Delta x})\\\\ +(v_{i,j+1/2}-\\Delta t\\frac1\\rho\\frac{p_{i,j+1}-p_{i,j}}{\\Delta x}) -(v_{i,j-1/2}-\\Delta t\\frac1\\rho\\frac{p_{i,j}-p_{i,j-1}}{\\Delta x})]=0\\\\ \\to \\\\ \\frac{\\Delta t}{\\rho}(\\frac{4p_{i,j}-p_{i+1,j}-p_{i,j+1}-p_{i-1,j}-p_{i,j-1}}{\\Delta x^2}) =-(\\frac{u_{i+1/2,j}-u_{i-1/2,j}}{\\Delta x}+\\frac{v_{i,j+1/2}-v_{i,j-1/2}}{\\Delta x}) \\tag {17}&emsp;&emsp;三维的情况同理，只不过多了一维需要处理： \\frac{u_{i+1/2,j,k}^{n+1}-u_{i-1/2,j,k}^{n+1}}{\\Delta x} + \\frac{v_{i,j+1/2,k}^{n+1}-v_{i,j-1/2,k}^{n+1}}{\\Delta x} + \\frac{w_{i,j,k+1/2}^{n+1}-w_{i,j,k-1/2}^{n+1}}{\\Delta x} = 0 \\tag {18} \\frac{1}{\\Delta x}[(u_{i+1/2,j,k} -\\Delta t\\frac1\\rho\\frac{p_{i+1,j,k}-p_{i,j,k}}{\\Delta x}) -(u_{i-1/2,j,k}-\\Delta t\\frac1\\rho\\frac{p_{i,j,k}-p_{i-1,j,k}}{\\Delta x})\\\\ +(v_{i,j+1/2,k}-\\Delta t\\frac1\\rho\\frac{p_{i,j+1,k}-p_{i,j,k}}{\\Delta x}) -(v_{i,j-1/2,k}-\\Delta t\\frac1\\rho\\frac{p_{i,j,k}-p_{i,j-1,k}}{\\Delta x})]\\\\ +(w_{i,j,k+1/2}-\\Delta t\\frac1\\rho\\frac{p_{i,j,k+1}-p_{i,j,k}}{\\Delta x}) -(w_{i,j,k-1/2}-\\Delta t\\frac1\\rho\\frac{p_{i,j,k}-p_{i,j,k-1}}{\\Delta x})]=0\\\\ \\to \\\\ \\frac{\\Delta t}{\\rho}(\\frac{6p_{i,j,k}-p_{i+1,j,k}-p_{i,j+1,k}-p_{i,j,k+1}-p_{i-1,j,k}-p_{i,j-1,k}-p_{i,j,k-1}}{\\Delta x^2}) \\\\ = -( \\frac{u_{i+1/2,j,k}-u_{i-1/2,j,k}}{\\Delta x} + \\frac{v_{i,j+1/2,k}-v_{i,j-1/2,k}}{\\Delta x} + \\frac{w_{i,j,k+1/2}-w_{i,j,k-1/2}}{\\Delta x} ) \\tag {19}&emsp;&emsp;仔细观察上面的公式$(17)$和公式$(19)$，可以发现他们都是泊松问题$-\\Delta t/\\rho \\nabla \\cdot \\nabla p=-\\Delta \\cdot \\vec u$的一个离散计算公式，左边是关于压力的拉普拉斯算子的数值近似，右边是关于速度场散度的数值近似，我们最终要求解的就是这么一个关于压力的方程。方程右边是散度的相反数，这就是我们前面计算并保存的是散度的相反数的原因。当邻居格子是自由面边界时，我们直接将其相应的压力值取零；当邻居格子是固体墙边界时，我们根据冯诺依曼边界条件计算得到的压强替换相应的压力项。 2、写成矩阵形式&emsp;&emsp;上面讨论的仅仅是一个格子的压力项求解方程，事实上我们要求解的是整个流体区域的压力项值。为此，为了便于阐述和求解，我们将上面的线性方程写成矩阵乘向量的形式。我们将方程中所有压力项的系数提出来，写成一个系数矩阵$A$，所有流体区域的压力项写成一个未知变量的向量$p$，相应的所有流体区域的负散度作为线性方程组右边的向量$b$，那么我们要求解的就是下面的一个大规模非齐次线性方程组： Ap=b \\tag {20}&emsp;&emsp;矩阵$A$每一行存储的是一个流体格子求解压力项的系数。仔细观察公式$(17)$和$(19)$，实际上每一行的系数中仅仅只有几个不为0，不为0的系数是其周围邻居和自身的。在三维情况下，给定$(i,j,k)$这个流体格子，那么其对应的矩阵$A$中的那一行中，仅仅是$p_{i,j,k}$、$p_{i\\pm 1,j,k}$、$p_{i,j\\pm 1,k}$和$p_{i,j,k\\pm 1}$对应的系数不为0，即又7个不为的系数，剩下的系数全部为0。二维就是一行有5个系数不为0。可以看到矩阵$A$是一个大规模的稀疏矩阵，大部分的矩阵元素都为0，所以我们没有必要直接存储矩阵$A$，因为这样将会极大地耗费内存，甚至出现内存不足的情况，因为矩阵A实在是太大了。 &emsp;&emsp;仔细观察前面公式$(17)$和$(19)$的线性方程，对于给定的流体格子$(i,j,k)$，其邻域压力项的系数为$-\\Delta t/(\\rho \\Delta x^2)$，自身压力项的系数为邻居格子中流体格子或者空气（自由面）格子的数量$n_{i,j,k}$在乘上$\\Delta t/(\\rho\\Delta x^2)$，即$n_{i,j,k}\\Delta t/(\\rho \\Delta x^2)$，可以看到在三维时$n_{i,j,k}$至多为6、在二维时至多为$4$。 &emsp;&emsp;同时，系数矩阵$A$也是一个对阵矩阵。举个例子，$A_{(i,j,k)(i+1,j,k)}$是方程组中关于$p_{i+1,j,k}$的系数，那么它必然等于$A_{(i+1,j,k)(i,j,k)}$，这是因为$(i+1,j,k)$是$(i,j,k)$的邻居，那么$(i,j,k)$必然也是$(i+1,j,k)$的邻居。系数矩阵$A$既然是对阵矩阵，那么我们存储系数矩阵$A$时又可以减少一半的存储量了，只需存储上三角形、或者下三角矩阵。 &emsp;&emsp;经过上述的讨论，对于大规模的稀疏矩阵$A$，以二维为例，我们采用的存储方案为：每一个流体格子存储关于自身压力项的系数$A_{(i,j)(i,j)}$，这个就是矩阵$A$中的对角线元素，而关于邻居流体格子压力项的系数我们仅存储正向方向上的（因为对称）$A_{(i,j),(i+1,j)}$和$A_{(i,j)(i,j+1)}$。我们记矩阵$A$的对角线元素为Adiag(i,j)，$x$轴方向的矩阵元素Ax(i,j)，$y$轴方向的矩阵元素Ay(i,j)。三维同理，分别存储到Adiag(i,j,k)、Ax(i,j,k)、Ay(i,j,k)、Az(i,j,k)，一个三维的系数矩阵$A$的计算并存储的伪代码如下所示： 123456789101112131415161718192021222324252627282930scale = dt / (density*dx*dx);loop over i,j,k: if label(i,j,k)==FLUID: # handle negative x neighbor if label(i-1,j,k)==FLUID: Adiag(i,j,k) += scale; # handle positive x neighbor if label(i+1,j,k)==FLUID: Adiag(i,j,k) += scale; Ax(i,j,k) = -scale; else if label(i+1,j,k)==EMPTY: Adiag(i,j,k) += scale; # handle negative y neighbor if label(i,j-1,k)==FLUID: Adiag(i,j,k) += scale; # handle positive y neighbor if label(i,j+1,k)==FLUID: Adiag(i,j,k) += scale; Ay(i,j,k) = -scale; else if label(i,j+1,k)==EMPTY: Adiag(i,j,k) += scale; # handle negative z neighbor if label(i,j,k-1)==FLUID: Adiag(i,j,k) += scale; # handle positive z neighbor if label(i,j,k+1)==FLUID: Adiag(i,j,k) += scale; Az(i,j,k) = -scale; else if label(i,j,k+1)==EMPTY: Adiag(i,j,k) += scale; 3、共轭梯度算法&emsp;&emsp; 现在我们把目标放到求解大规模的非齐次线性方程组上，即上面的公式$(20)$。系数矩阵$A$除了是一个大规模的稀疏对称矩阵之外，还是一个正定（Positive definite）矩阵，即对于任意的非零向量$q$，$q^TAq&gt;0$均成立，且系数矩阵$A$的所有特征值均大于0。当然在实际情况下，系数矩阵$A$可能不是一个严格的正定矩阵，而是一个半正定矩阵，即对于任意非零向量$q$，有$q^TAq\\geq0$。这种情况出现在流体模拟区域中，存在一些流体区域完全被固体墙边界包围，此时的系数矩阵$A$不是一个严格的正定矩阵，甚至此时的矩阵$A$是一个奇异矩阵，它不存在逆矩阵，从而导致方程$(20)$不一定有解。因而在一些特殊情况下，我们求解的是兼容条件下的解，即只要使得流体-固体边界处流体的流进和流出保持平衡（流进量等于流出量）即可。 &emsp; &emsp; 求解一个线性方程组最常用、最暴力的方法就是高斯消元法，逐行进行消元，最后使得矩阵变成一个上三角矩阵，从最后一行往回代即可求解出方程的解（只要解存在）。但是高斯消元法实在是太慢了，小规模的矩阵来说还好，大规模的矩阵方程求解一般不会这么直接暴力解。这里我们采用的求解方程$(20)$的方法是共轭梯度算法（Conjugate gradient method，简称为CG）$^{[2]}$。相比于普通的高斯消元法，共轭梯度算法采用了一个迭代的流程，迭代到给定的收敛程度即可，整个算法流程只涉及到矩阵-向量乘法、向量加减、向量-标量乘法以及非常少的点乘操作，这些都可以快速并行地计算。 &emsp; &emsp; 共轭梯度算法就是针对对称正定矩阵的线性方程组的求解，给定一个线性方程组如下$(21)$所示，其中系数矩阵$A$是一个元素均已知的实对称正定矩阵，右边的向量$\\vec b$也已知，我们要求解的就是未知向量$\\vec x$。这个就是共轭梯度所要解决的线性方程组。 A x= b \\tag {21}&emsp;&emsp;给定两个非零向量$u$和$v$，我们说它们是共轭的（相对于$A$），若： u^TAv = 0 \\tag {22}&emsp;&emsp;而矩阵$A$是一个对称的正定矩阵（故有$A^T=A$），那么上式的左边可以写成多种向量内积的形式，它们完全是等价的。这样两个非零向量是共轭的，当且仅当这两个向量相对于下面的内积是正交的。共轭是一个对称的相互关系，$u$共轭于$v$，那么$v$也共轭于$u$。 u^TAv=_A=== \\tag {23}&emsp;&emsp;令$P=\\{p_1,…,p_n\\}$是一个这样的向量集合，集合中的所有向量都相对于$A$互相共轭，那么$P$构成了$R^n$空间的一个基，方程$(21)$的解$x_*$可以用这组基的线性组合来表示： x_*=\\Sigma_{i=1}^n\\alpha_i p_i \\tag {24}&emsp;&emsp;将上述的$x_*$表达式代入方程$(21)$中，有： Ax_*=\\Sigma_{i=1}^n\\alpha_iAp_i \\tag {25}&emsp;&emsp;再左乘上$P$中的一个向量$p_k^T$： p_k^TAx_*=\\Sigma_{i=1}^n\\alpha_ip_k^TAp_i \\tag {26}&emsp;&emsp;然后将$Ax_*=b$、$u^TAv=(u,v)_A$代入公式$(26)$中： p_k^Tb=\\Sigma_{i=1}^n\\alpha_i _A \\tag {27}&emsp;&emsp;最后根据$u^Tv=(u,v)$以及$\\forall i\\neq k: (p_k,p_i)_A=0$，可得： =\\alpha_k _A \\\\ \\to \\\\ \\alpha_k=\\frac{}{_A} \\tag {28}&emsp;&emsp;由此，我们可以得到一个求解方程$Ax=b$的方法，首先找到$n$个互相共轭的向量，然后计算其组合系数$\\alpha_k$，最后未知向量就由这$n$个互相共轭的向量线性组合而成（即公式$(24)$）。但显然直接寻找$n$个互相共轭的向量在$n$比较大的时候不现实，因为这将耗费大量的时间。 &emsp;&emsp;共轭梯度采用的是迭代法，我们给出了一个初始预测解$x_0$，致力于寻找最终解$x_d$，通过迭代的方法使得初始的预测解$x_0$逐步向$x_d$逼近，每一次的迭代中我们挑选出一个共轭向量$p$。迭代过程中的目标就是最小化预测向量与最终解$x_d$之间的差距，此时我们的问题就转换为如下的二次方程的最优化问题： f(x)=x^T(\\frac12Ax-b)=\\frac12x^TAx-x^Tb, x\\in R^n \\tag {29}&emsp;&emsp;公式$(29)$是一个关于$n$维向量的二次方程，是一个$n$元函数，其一阶导数和二阶导数分别如下所示： \\nabla f(x)=Ax-b \\\\ \\nabla^2 f(x)=A \\tag {30}&emsp;&emsp;可以看到当二次方程的一阶导数$Ax-b=0$时，此时的$x$就是我们要求的线性方程组的解，此时二次方程取得最小值。共轭梯度法在初次迭代时直接取第一个共轭向量$p_0=b-Ax_0$，即负梯度方向，这一步与梯度下降法类似。但是后续迭代取的向量就不再是负梯度向量了，而是取一个与梯度向量共轭的向量，这就是名称共轭梯度法的由来。实际上，每一次迭代时我们取的负梯度向量就是当前预测解与最终解之间的残差（Residual），第$k$次迭代时的残差为： r_k=b-Ax_k \\tag {31}&emsp;&emsp;为了确保在每一次迭代中取得的向量共轭于梯度以及前面迭代得到的向量，我们结合残差向量以及前面迭代的向量计算当前迭代挑选的向量： p_k=r_k-\\Sigma_{i","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/tags/Fluid-Simulation/"}]},{"title":"流体模拟Fluid Simulation：Level Set & Marching Cube","slug":"LevelSet","date":"2019-07-30T03:38:07.238Z","updated":"2019-09-23T11:13:49.998Z","comments":true,"path":"2019/07/30/LevelSet/","link":"","permalink":"https://yangwc.com/about/2019/07/30/LevelSet/","excerpt":"本文主要是基于欧拉网格流体模拟中的算法常客——Level Set和Marching Cube，Level Set算法负责构建复杂的边界几何体，而Marching Cube负责从欧拉网格中重建流体表面。","text":"本文主要是基于欧拉网格流体模拟中的算法常客——Level Set和Marching Cube，Level Set算法负责构建复杂的边界几何体，而Marching Cube负责从欧拉网格中重建流体表面。 符号距离场 三角形网格的SDF计算 Marching Cube重建算法 参考资料 水平集（Level Set） &emsp;&emsp;在流体模拟中，我们需要处理流体与固体边界的交互作用。这对于流体模拟来说至关重要，因为流体通常是作用到固体以及其他边界上才会体现流体的特性，让人看起来是流体。这就涉及到两个问题： &emsp;&emsp;1、给定一个点（例如用半拉格朗日后向追踪得到的点），如何判断它是否在固体边界内？ &emsp;&emsp;2、如何计算给定一点到几何体表面上的最近的点？ &emsp;&emsp;上面的问题在流体模拟中非常常见，在基于欧拉网格的流体模拟中，我们常用的是水平集的方法。我们将几何体表示成一个隐式的表面，给定关于这个几何体的连续标量函数$\\phi(x,y,z)$，我们定义该几何体构成的空间如下： &emsp;&emsp;1、当$\\phi(x,y,z)>0$时，点$x$在几何体外部，此时满足该条件的$x$的点集构成几何体的外部空间； &emsp;&emsp;2、当$\\phi(x,y,z)","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/tags/Fluid-Simulation/"}]},{"title":"二维渲染2D Rendering：2D Lighting","slug":"2DLighting","date":"2019-07-23T10:36:10.868Z","updated":"2019-09-27T13:07:40.063Z","comments":true,"path":"2019/07/23/2DLighting/","link":"","permalink":"https://yangwc.com/about/2019/07/23/2DLighting/","excerpt":"偶然在知乎上看到图形学大佬的几篇关于二维渲染绘制的文章，感觉非常有趣。与普通的光线求交不同，这里的二维渲染采用了光线步进法和符号距离场。下图就是渲染出来的效果，二维虽然少了一维，但渲染开销依旧大，渲染下面的这张图开启了多线程也花费了1个半小时多的时间。","text":"偶然在知乎上看到图形学大佬的几篇关于二维渲染绘制的文章，感觉非常有趣。与普通的光线求交不同，这里的二维渲染采用了光线步进法和符号距离场。下图就是渲染出来的效果，二维虽然少了一维，但渲染开销依旧大，渲染下面的这张图开启了多线程也花费了1个半小时多的时间。 二维渲染 SDF构造实体几何 比尔-郎伯定律 实现效果 参考资料 2D Lighting &emsp;&emsp;偶然在知乎上看到图形学大佬的几篇关于二维渲染绘制的文章，感觉非常有趣，二维的相对而言比较简单但是涉及到的知识也不少，遂记录下来一些自己的学习总结。一些内容跟前面光线追踪的重合了，所以就不细细展开了。本文主要是关于SDF方面的内容。 ## 一、二维渲染 &emsp;&emsp;我们要绘制的世界是一个二维的场景，场景中有发光的二维体。对于一个二维屏幕上的像素，我们要求在360度方向上接收的光照值，故给定二维空间的坐标$(x,y)$，渲染方程为对$[0,2\\pi]$方向的单重积分： $$ L_o(x,y)=\\int_0^{2\\pi}L_i(x,y,\\theta)d\\theta \\tag {1} $$ &emsp;&emsp;在这里我们就对二维图像的每个像素求解方程$(1)$的积分公式，用tbb多线程库加速渲染，然后将像素矩阵用stb_image保存成png文件。原点在图像的左上角。 123456789101112131415161718192021unsigned char * Renderer::render()&#123; parallel_for(blocked_range&lt;size_t&gt;(0, m_height * m_width, 5000), [&amp;](blocked_range&lt;size_t&gt;&amp; r) &#123; for (size_t i = r.begin(); i != r.end(); ++i) &#123; size_t col = i % m_width; size_t row = i / m_width; glm::vec3 color(0.0f); // sampling and lighting. color.x = color.y = color.z = sample(static_cast&lt;float&gt;(col) / m_width, static_cast&lt;float&gt;(row) / m_height); // save to pixel. drawPixel(row, col, color); &#125; &#125;); return m_image;&#125; &emsp;&emsp;由于被积函数$L(x,y,\\theta)$并没有一个显示的数学表达式，我们无法求解公式$(1)$单重积分的解析解，因此采用蒙特卡洛数值积分法。我们首先考虑随机均匀采样$N$个方向，每个方向用$\\theta_1,\\theta_2,...,\\theta_N$表示，因为是均匀采样，所以概率密度函数$pdf=\\frac {1}{2\\pi}$，蒙特卡洛数值积分公式如下： $$ L_o(x,y)=\\frac 1N\\Sigma_{i=1}^N\\frac{L_i(x,y,\\theta)}{\\frac{1}{2\\pi}} =\\frac {2\\pi}N\\Sigma_{i=1}^N{L_i(x,y,\\theta)} \\tag {2} $$ &emsp;&emsp;因此一个随机均匀采样的蒙特卡洛积分如下所示，此外在这里我们不考虑实际单位，所以实现时把系数$2\\pi$去掉了。 1234567891011float Renderer::sample(float x, float y)&#123; float sum = 0.0f; for (int i = 0; i &lt; m_samples; ++i) &#123; // randome sampling. float angle = M_2PI * rand() / RAND_MAX; sum += trace(x, y, cosf(angle), sinf(angle)); &#125; return sum / m_samples;&#125; &emsp;&emsp;紧接着我们将实现trace函数，这个函数返回给定方向上的光照值。我们采用光线步进法（Ray Marching），场景中的物体以符号距离场（Signed Distance Field，简称SDF）表示，符号距离场是一个这样的映射：$\\phi:R^2\\to R$，具有如下的性质： &emsp;&emsp;(1).当$\\phi(x)>0$时，坐标$x$位于场景的物体外面，$x$到最近物体表面的距离为$\\phi(x)$； &emsp;&emsp;(2).当$\\phi(x)","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"2D Rendering","slug":"2D-Rendering","permalink":"https://yangwc.com/about/categories/2D-Rendering/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"2D Rendering","slug":"2D-Rendering","permalink":"https://yangwc.com/about/tags/2D-Rendering/"}]},{"title":"实时渲染Real-time Rendering：Image Based Lighting","slug":"ImageBasedLighting","date":"2019-07-21T05:05:21.069Z","updated":"2019-09-27T13:05:02.527Z","comments":true,"path":"2019/07/21/ImageBasedLighting/","link":"","permalink":"https://yangwc.com/about/2019/07/21/ImageBasedLighting/","excerpt":"本文在前一篇PBR渲染器的基础上，进一步深入理解并实现了Image Based Lighting的全局光照。Image Based Lighting预先烘培辐照度贴图、构建BRDF积分查找表，然后在实时渲染中直接查询查找表快速计算渲染方程，实现的效果高效且真实。这种技术已经集成在虚幻4引擎中。","text":"本文在前一篇PBR渲染器的基础上，进一步深入理解并实现了Image Based Lighting的全局光照。Image Based Lighting预先烘培辐照度贴图、构建BRDF积分查找表，然后在实时渲染中直接查询查找表快速计算渲染方程，实现的效果高效且真实。这种技术已经集成在虚幻4引擎中。 渲染方程的求解 hdr文件转成cubemap 预计算漫反射积分 预计算镜面反射积分 计算渲染方程 实现效果 参考资料 Image Based Lighting &emsp;&emsp;在前面一篇基于物理的渲染中我们仅仅考虑了直接光照部分，仅仅考虑直接光照时求解渲染方程非常简单，只需需根据给定的光源直接计算并叠加，而不需要在法线轴向的半球方向做积分。Image Based Lighting（以下简称IBL）翻译过来的意思就是基于图像的光照，这种技术专门计算来自于周围环境的间接光照，周围环境的光影信息存储在一张环境贴图中，这个环境贴图可以是预先生成好的，也可以是运行时动态生成好的。IBL技术使得物体的光照效果与周围的环境更加融合，大大增强沉浸感。 ## 一、渲染方程的求解 &emsp;&emsp;IBL的渲染方程与PBR一样，即特化的渲染方程——反射方程，如下所示： $$ L_o(p,\\omega_o)=\\int_{\\Omega}(k_d\\frac{c}{\\pi}+\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\tag {1} $$ &emsp;&emsp;我们实现基于物理的光照效果的本质就是要计算上面的渲染方程，它是对半球方向$\\Omega$的所有$\\omega_i$积分。在前面一偏PBR中我们求解这个积分方程非常容易，因为仅考虑直接光照且光源为非面积光源时，上述的被积函数部分是一个狄拉克函数，即除了在光源方向上函数值不为$0$，在半球内的其余定义域该被积函数均为$0$，因此没有必要求半球积分，或者说半球积分的值就等于在光源方向上的被积函数值。但是在IBL中，就没有这么简单了。我们把环境贴图中的每一个像素都当作一个光源，这使得我们必须要求解半球方向的积分，即在整个半球方向采样光照信息。如果我们直接暴力求解渲染方程$(1)$的话，在每一个片元着色器上我们都要计算渲染方程的半球积分，耗费极大的性能，这对于实时性应用来说几乎不可能。 &emsp;&emsp;因此，为了高效地求解渲染积分方程，IBL方法将渲染方程中的积分项预先计算出来并存储到指定的纹理图片上，在后面进行光照计算的时候根据相关的信息索引预先计算好的积分值，从而快速高效地求解积分。这其实就是以空间换时间的思想。为了方便预先计算积分值，我们把公式$(1)$写成如下的形式： $$ L_o(p,\\omega_o) =\\int_{\\Omega}(k_d\\frac{c}{\\pi}+\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i\\\\ =\\int_{\\Omega}k_d\\frac{c}{\\pi}L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i +\\int_{\\Omega}(\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\tag {2} $$ &emsp;&emsp;上面公式$(2)$的两个项分别对应光照的漫反射部分和镜面反射部分，我们把这两部分分开来，然后分别单独计算漫反射的积分值、镜面反射的积分值，这个其实相当于对立方体环境贴图做一个特殊的卷积操作，计算得到的结果分别存储到各自指定查找表中供后续的渲染使用。这个就是IBL算法的核心思想，接下来我们就围绕这个核心思想展开相应的实现细节。 ## 二、hdr文件转成cubemap &emsp;&emsp;在实现积分预计算之前我们还有一件事要完成，就是获取周围环境贴图的辐射率值，因为我们把环境贴图的每一个像素都当作一个光源。在进行积分计算时，我们要根据给定的入射方向$\\omega_i$去获取该方向上辐射过来的辐射率值。如果环境贴图是一个立方体贴图的话，这很容易实现，因为立方体贴图就是根据三维向量进行索引的。但是在IBL中我们的环境贴图并不是一个普通的环境贴图，普通的环境贴图的像素值在低动态范围（Low Dynamic Range），即每个像素的分量不超过1.0。IBL同样是PBR渲染方法，为了能够捕捉真实的物理光影和细节，我们的环境贴图应该是高动态范围的（High Dynamic Range）。 &emsp;&emsp;目前有一种以.hdr为后缀的图像格式保存了高动态范围的像素值，这种格式的文件采用一种巧妙的方法将超出$0.0$到$1.0$的正确的颜色值保存下来。高动态范围的环境贴图就采用了这种格式保存，stb_image支持.hdr文件加载。[这里](http://www.hdrlabs.com/sibl/archive.html)收集了一些hdr格式的环境贴图，stb_image加载.hdr文件也非常简单，如下所示： 1float *data = stbi_loadf(path.c_str(), &amp;m_width, &amp;m_height, &amp;m_channel, 0); &emsp;&emsp;加载后得到的图片如下图1所示，这是一个二维的图片，类似于广角镜头拍摄得到的画面。.hdr格式保存的不是六张独立的图片，它将6张图片从球面投影到一个二维的平面上，从而构成下面这张略带扭曲的矩形纹理图。为了后续方便获取指定像素的纹理值，我们需要把这张矩形的hdr图转换成立方体贴图。 图1 .hdr贴图 &emsp;&emsp;从hdr转换到cubemap的过程其实就是利用球面投影到二维平面，可以理解为球面的纹理映射为二维的纹理（或者说球面上的点映射到二维平面上的点）。这个映射过程就是利用了球坐标和笛卡尔坐标的关系，对于一个球心在原点的单位球体上的点$(x,y,z)$，我们也可以用采用天顶角$\\theta$（与$xz$平面的夹角）和方位角$\\phi$（在$xz$平面上与$x$轴的夹角）来唯一地表示，他们的关系如下： x=cos(\\theta)cos(\\phi)\\\\ y=sin(\\theta) \\\\ z=cos(\\theta)sin(\\phi) \\tag {3}&emsp;&emsp;其中$\\theta$的取值范围为$[-\\pi/2,+\\pi/2]$，$\\phi$的取值范围为$[-\\pi,+\\pi]$，我们很容易地可以分别将其映射到二维纹理坐标uv的$[0,1]$，这样我们就将一个三维立方体贴图纹理坐标映射到了二维的纹理坐标，这个其实就是构建.hdr时的映射过程。现在我们要再现这个过程，获取三维纹理坐标对应的二维纹理坐标，然后索引上面的hdr贴图，从而得到三维纹理坐标对应的像素值，并将其保存到cubemap当中。我们采用的方法就是绘制6次立方体，每次保存1个立方体面的像素值： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051void IBLAuxiliary::convertToCubemap(int width, int height, unsigned int hdrTexIndex, unsigned int cuebmapTexIndex)&#123; // manager. TextureMgr::ptr texMgr = TextureMgr::getSingleton(); ShaderMgr::ptr shaderMgr = ShaderMgr::getSingleton(); // load shader. unsigned int shaderIndex = shaderMgr-&gt;loadShader(\"convertToCubemap\", \"./glsl/convertToCubemap.vert\", \"./glsl/convertToCubemap.frag\"); // load cube mesh. Mesh::ptr cubeMesh = std::shared_ptr&lt;Mesh&gt;(new Cube(1.0f, 1.0f, 1.0f)); // load framebuffer. FrameBuffer::ptr framebuffer = std::shared_ptr&lt;FrameBuffer&gt;(new FrameBuffer(width, height, \"convertDepth\", &#123;&#125;, true)); // projection matrix and view matrix. glm::mat4 captureProjectMatrix = glm::perspective(glm::radians(90.0f), 1.0f, 0.1f, 10.0f); glm::mat4 captureViewMatrix[] = &#123; glm::lookAt(glm::vec3(0.0f), glm::vec3(+1.0f, 0.0f, 0.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(-1.0f, 0.0f, 0.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f,+1.0f, 0.0f), glm::vec3(0.0f, 0.0f,+1.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f,-1.0f, 0.0f), glm::vec3(0.0f, 0.0f,-1.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f, 0.0f,+1.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f, 0.0f,-1.0f), glm::vec3(0.0f, -1.0f, 0.0f)), &#125;; // convert. framebuffer-&gt;bind(); glDisable(GL_BLEND); glDisable(GL_CULL_FACE); glEnable(GL_DEPTH_TEST); glDepthFunc(GL_LEQUAL); glClearColor(0.0f, 1.0f, 0.0f, 1.0f); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); GLuint cubemapId = texMgr-&gt;getTexture(cuebmapTexIndex)-&gt;getTextureId(); Shader::ptr shader = shaderMgr-&gt;getShader(shaderIndex); shader-&gt;bind(); shader-&gt;setInt(\"hdrMap\", 0); shader-&gt;setMat4(\"projectMatrix\", captureProjectMatrix); texMgr-&gt;bindTexture(hdrTexIndex, 0); for (unsigned int i = 0; i &lt; 6; ++i) &#123; shader-&gt;setMat4(\"viewMatrix\", captureViewMatrix[i]); glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_CUBE_MAP_POSITIVE_X + i, cubemapId, 0); cubeMesh-&gt;draw(false, 0); &#125; shader-&gt;unBind(); texMgr-&gt;unBindTexture(hdrTexIndex); framebuffer-&gt;unBind();&#125; &emsp;&emsp;相应的，在着色器中我们将三维纹理坐标投影到二维纹理坐标，索引hdr值，输出为片元着色器的值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344=======================Vertex Shader=======================#version 330 corelayout (location = 0) in vec3 position;layout (location = 1) in vec3 normal;layout (location = 2) in vec2 texcoord;layout (location = 3) in vec3 color;out vec3 worldPos;uniform mat4 viewMatrix;uniform mat4 projectMatrix;void main()&#123; worldPos = position; gl_Position = projectMatrix * viewMatrix * vec4(position,1.0f);&#125;=======================Fragment Shader=====================#version 330 corein vec3 worldPos;out vec4 fragColor;uniform sampler2D hdrMap;// (1/(pi/2), 1/(pi))const vec2 invAtan = vec2(0.1591, 0.3183);vec2 sampleSphericalMap(vec3 v)&#123; vec2 uv = vec2(atan(v.z, v.x), asin(v.y)); // to [0,1]. uv *= invAtan; uv += vec2(0.5f); return uv;&#125;void main()&#123; // map 3d texcoord to 2d texcoord. vec2 uv = sampleSphericalMap(normalize(worldPos)); // sample hdr map. vec3 sampler = texture(hdrMap, uv).rgb; // save to one face of cubemap. fragColor = vec4(sampler, 1.0f);&#125; &emsp;&emsp;通过上面的步骤，我们就将一个二维的hdr贴图转换成cubemap，如下图2所示，方便我们后续的使用。有了这个高动态范围的环境贴图，接下来我们就展开漫反射积分和镜面反射积分的预计算。 图2 hdr立方体贴图 三、预计算漫反射积分&emsp;&emsp;首先来看渲染方程中的漫反射积分部分，把它单独拎出来： \\int_{\\Omega}k_d\\frac{c}{\\pi}L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i =k_d\\frac{c}{\\pi}\\int_{\\Omega}L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\tag {4}&emsp;&emsp;上面的积分公式中，积分变量为$\\omega_i$，和$\\frac{c}{\\pi}$项都与$\\omega_i$无关，故可以提出积分外。我们实际上要求积分的被积函数是$L_i(p,\\omega_i)n\\cdot \\omega_i$。假设点$p$在原点，则对于每一个$n$确定的半球方向，积分值仅仅取决于$\\omega_i$。因此，我们遍历所有的$n$，然后预先计算其积分值$\\int_{\\Omega}L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i$到一张cubemap当中，最后渲染时直接用$n$去采样这个cubemap得到预先计算好的积分值。这个就是预计算漫反射积分的思路。 &emsp;&emsp;$n$就是给定法线向量，它的取值范围就是所有方向，相当于一个球心在原点的单位球体上的所有点。为了遍历所有的$n$，我们送入一个立方体进行预计算的渲染，不需要球体，因为立方体的顶点归一化normalize之后也就是对应的球面上的点。然后在片元着色器根据这个$n$进行半球方向的积分，最后存储到cubemap当中。同样的，我们需要绘制6次，每次绘制保存到cubemap的六个面中的一面。 &emsp;&emsp;然后就是关于公式$(4)$当中的积分数值计算。首先需要注意的是公式$(4)$当中的积分变量$\\omega_i$是立体角，为了方便计算，我们需要把他转成以天顶角$\\theta$和方位角$\\phi$为变量的表示形式。前面在光线追踪器Ray Tracer：进阶篇已经提到过微分立体角$d\\omega_i$如何转成用$d\\theta$和$d\\phi$表示，即$d\\omega_i=sin\\theta d\\theta d\\phi$。因此，公式$(4)$中的积分项转成如下的二重积分： k_d\\frac{c}{\\pi}\\int_{\\Omega}L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i = k_d\\frac{c}{\\pi}\\int_{\\phi=0}^{2\\pi}\\int_{\\theta=0}^{\\frac12\\pi}L_i(p,\\phi_i,\\theta_i)cos\\theta sin \\theta d\\theta d\\phi \\tag {5}&emsp;&emsp;这里要提一下，公式$(4)$中的$n\\cdot \\omega_i$就是$cos\\theta$，半球方向内$\\theta$取值$[0,\\frac12\\pi]$，$\\phi$取值$[0,2\\pi]$。公式$(5)$等号两边完全是等价的。然后我们需要采用数值方法近似计算公式$(5)$的积分值，在半球方向做均匀的离散采样$(\\phi,\\theta)$，计算采样的方向对应的被积函数值，最后叠加起来，叠加的结果就是黎曼积分和。公式$(5)$的黎曼积分近似公式为： k_d\\frac{c}{\\pi}\\int_{\\phi=0}^{2\\pi}\\int_{\\theta=0}^{\\frac12\\pi}L_i(p,\\phi_i,\\theta_i)cos\\theta sin \\theta d\\theta d\\phi \\approx k_d\\frac{c}{\\pi}\\Sigma_0^{2\\pi}\\Sigma_0^{\\frac12\\pi}L_i(p, \\phi_i, \\theta_i)cos\\theta sin\\theta d\\theta d\\phi \\tag{6}&emsp;&emsp;对于公式$(6)$中的黎曼积分，我们需要确定$d\\theta$和$d\\phi$，即数值积分步长。这个步长越小，则计算结果越接近于理论值，但耗费的时间也越多。此外还需要提的是，我们是在切线空间的半球方向进行采样的，在切线空间获取采样的方向向量之后，我们需要把它转换到世界空间，这里我们没有构建变换矩阵，直接计算切线空间的三个基向量。计算近似的黎曼积分的着色器代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152============================Vertex Shader=======================#version 330 corelayout (location = 0) in vec3 position;layout (location = 1) in vec3 normal;layout (location = 2) in vec2 texcoord;layout (location = 3) in vec3 color;out vec3 worldPos;uniform mat4 viewMatrix;uniform mat4 projectMatrix;void main()&#123; worldPos = position; gl_Position = projectMatrix * viewMatrix * vec4(position,1.0f);&#125; ============================Fragment Shader=======================#version 330 corein vec3 worldPos;out vec4 fragColor;uniform samplerCube environmentMap;const float PI = 3.14159265359;void main()&#123; vec3 normal = normalize(worldPos); vec3 irradiance = vec3(0.0f); // tangent space. vec3 up = vec3(0.0f, 1.0f, 0.0f); vec3 right = cross(up, normal); up = cross(normal, right); float sampleDelta = 0.025; float sampleDeltaSquared = sampleDelta * sampleDelta; for(float phi = 0.0f; phi &lt; 2.0 * PI;phi += sampleDelta) &#123; for(float theta = 0.0f; theta &lt; 0.5 * PI;theta += sampleDelta) &#123; vec3 sampleDir = vec3(sin(theta) * cos(phi), sin(theta) * sin(phi), cos(theta)); // tangent space to world space. sampleDir = sampleDir.x * right + sampleDir.y * up + sampleDir.z * normal; irradiance += texture(environmentMap, sampleDir).rgb * cos(theta) * sin(theta) * sampleDeltaSquared; ++ nSamples; &#125; &#125; irradiance = (1.0f / PI) * irradiance ; fragColor = vec4(irradiance, 1.0f);&#125; &emsp;&emsp;在片元着色器中，我们取采样步长$d\\phi$和$d\\theta$均为$0.025$，然后是两重循环，获取采样方向向量的$(\\phi,\\theta)$后我们需要把它转换成笛卡尔坐标下的方向向量，用这个方向相应去获取该方向对应的hdr立方体纹理。最后结果除以一个$\\pi$，对应公式$(6)$中的$\\pi$，而剩下的漫反射因子$k_d$和反照率$c$由渲染时确定。上面的过程相当于对hdr立方体贴图做一个卷积操作，存储结果的纹理我们称之为辐照度纹理（Irradiance Map）。由于辐照度纹理没有高频的细节，因此我们不需要设置太大分辨率，在cpu端创建一个$64\\times 64$大小的cubemap，然后调用着色器绘制6次写入这个cubemap当中，具体如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950void IBLAuxiliary::convoluteDiffuseIntegral(int width, int height, unsigned int cubemapTexIndex, unsigned int irradianceTexIndex)&#123; // manager. TextureMgr::ptr texMgr = TextureMgr::getSingleton(); ShaderMgr::ptr shaderMgr = ShaderMgr::getSingleton(); // load shader. unsigned int shaderIndex = shaderMgr-&gt;loadShader(\"diffuseIntegral\", \"./glsl/diffuseIntegral.vert\", \"./glsl/diffuseIntegral.frag\"); // load cube mesh. Mesh::ptr cubeMesh = std::shared_ptr&lt;Mesh&gt;(new Cube(1.0f, 1.0f, 1.0f)); // load framebuffer. FrameBuffer::ptr framebuffer = std::shared_ptr&lt;FrameBuffer&gt;( new FrameBuffer(width, height, \"irradianceDepth\", &#123;&#125;, true)); // projection matrix and view matrix. glm::mat4 captureProjectMatrix = glm::perspective(glm::radians(90.0f), 1.0f, 0.1f, 10.0f); glm::mat4 captureViewMatrix[] = &#123; glm::lookAt(glm::vec3(0.0f), glm::vec3(+1.0f, 0.0f, 0.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(-1.0f, 0.0f, 0.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f,+1.0f, 0.0f), glm::vec3(0.0f, 0.0f,+1.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f,-1.0f, 0.0f), glm::vec3(0.0f, 0.0f,-1.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f, 0.0f,+1.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f, 0.0f,-1.0f), glm::vec3(0.0f, -1.0f, 0.0f)), &#125;; // begin to convolute. framebuffer-&gt;bind(); glDisable(GL_BLEND); glDisable(GL_CULL_FACE); glEnable(GL_DEPTH_TEST); glDepthFunc(GL_LEQUAL); glClearColor(0.0f, 1.0f, 0.0f, 1.0f); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); GLuint irradianceTexId = texMgr-&gt;getTexture(irradianceTexIndex)-&gt;getTextureId(); Shader::ptr shader = shaderMgr-&gt;getShader(shaderIndex); shader-&gt;bind(); shader-&gt;setInt(\"environmentMap\", 0); shader-&gt;setMat4(\"projectMatrix\", captureProjectMatrix); texMgr-&gt;bindTexture(cubemapTexIndex, 0); for (unsigned int i = 0; i &lt; 6; ++i) &#123; shader-&gt;setMat4(\"viewMatrix\", captureViewMatrix[i]); glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_CUBE_MAP_POSITIVE_X + i, irradianceTexId, 0); cubeMesh-&gt;draw(false, 0); &#125; shader-&gt;unBind(); texMgr-&gt;unBindTexture(cubemapTexIndex); framebuffer-&gt;unBind();&#125; &emsp;&emsp;以上的预计算或者说预烘培步骤只需执行一次，然后我们就得到了一张辐照度立方体贴图，如下图3所示，可以看到类似对原来的立方体贴图做了一个模糊处理，但这并不是模糊处理。 图3 预烘培得到的辐照度纹理 四、预计算镜面反射积分&emsp;&emsp;接下来我们就把目标转到镜面反射的积分预计算上面。我们要预计算的就是渲染方程$(2)$中的镜面反射部分，如下所示： \\int_{\\Omega}(\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i =\\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\tag {7}&emsp;&emsp;被积函数中的$f_r(p,\\omega_i,\\omega_o)$是之前提到的brdf函数，它与积分变量$\\omega_i$有关，因此它不是一个常量，我们不能像前面的漫反射积分那样把brdf函数当作常数项并提出积分外。除此之外，我们还注意到最终的积分值还取决于出射方向或者说观察方向$\\omega_o$，因此公式$(7)$的积分值取决于$n$和$\\omega_o$这两个三维的向量。前面的漫反射积分仅取决于$n$，所以我们可以很容易地遍历所有的$n$，然后将积分值存储到立方体贴图中。但是在镜面反射积分这里，积分值取决于$n$和$\\omega_o$，这使得问题变得复杂起来，这是因为我们不能同时用$n$和$\\omega_o$去索引立方体贴图。Epic Games公司提出了提出了一种近似的方案，这种方法就是将公式$(7)$中的积分分成两部分： \\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\approx \\int_{\\Omega}L_i(p,\\omega_i)d\\omega_i * \\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)n\\cdot \\omega_i d\\omega_i \\tag {8}&emsp;&emsp;其中$\\int_{\\Omega}L_i(p,\\omega_i)d\\omega_i$是对半球方向的辐射率进行积分，$\\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)n\\cdot\\omega_id\\omega_i$则对半球方向的brdf函数进行积分，我们将原来的积分分成了这两部分，预计算然后保存到两张纹理当中。可以看到前面一部分的积分仅仅取决于法线向量$n$，后面一部分表面上取决于$n$和$\\omega_o$，后面深入了解之后就会发现不用$n$和$\\omega_o$去索引brdf函数的积分值。 1、Pre-Filtered Environment Map&emsp;&emsp;首先我们来看前面一部分的积分值，即$\\int_{\\Omega}L_i(p,\\omega_i)d\\omega_i$，这部分预计算得到的纹理为预过滤的环境贴图，它返回的是反射的像素值。我们知道，给定一个物体表面，若其表面的粗糙程度越高，则反射的内容越模糊。因此，为了把物体的粗糙度考虑进去，我们将考虑物体的粗糙度划分成几个等级，每个等级预计算一遍积分，并存储到cubemap的一个mipmap当中。我们为生成的预过滤环境贴图构建mipmap，越粗糙则存储到mipmap的等级越高，正如如下图4所示： 图4 不同粗糙度对应的mipmap &emsp;&emsp;表面越粗糙则反射得越模糊得现象其实涉及到一个反射波瓣的问题，所以的反射波瓣就是反射光线的分布范围。如下图5所示，对于一个光滑的完美镜面，其反射波瓣就是反射向量，越粗糙则反射波瓣越大，且基本上都是以反射向量为中心。 图5 不同粗糙度下的反射波瓣 &emsp;&emsp;因此，当我们进行积分采样时，我们没有必要再在半球方向做一个均匀的采样，因为超出反射波瓣的采样是无效的。我们应该偏向反射波瓣采样，这里就涉及到了一个重要性采样的问题，重要性采样的内容在前面的光线追踪器Ray Tracer：进阶篇已经详细地展开过，这里就不再赘述。采用了重要性采样方法，我们求解积分方程的数值方法不再是黎曼积分法，而是蒙特卡洛积分法，蒙特卡洛积分同样在光线追踪渲染器中提及过。 &emsp;&emsp;为了加速蒙特卡洛积分方法的收敛速度，Epic Games公司提出使用超均匀分布序列（Low-discrepancy Sequence）——Hammersley序列。相对于普通的伪随机数，Hammersley序列的随机数分布更加均匀，如下图6所示，将其应用到蒙特卡洛采样能够提升收敛速度。 图6 伪随机数vs超均匀分布随机数 &emsp;&emsp;我们将使用的是二维的Hammersley序列。一个二维的Hammersley序列$H_N=\\{x_1,…,x_N\\}, N\\geq 1$是散落分布在单位正方形内的点集，其数学定义为： H_N=\\{x_i= (\\begin{matrix} i/N\\\\ \\Phi_2(i) \\end{matrix} ),\\ \\ for\\ \\ i=0,...,N-1 \\} \\tag {9}&emsp;&emsp;其中$\\Phi_2(i)$是Van der Corput序列，它输入$i$，然后将$i$的二进制编码以小数点为对称做一个镜像操作，返回$[0,1)$的浮点数，其数学定义为： \\Phi_2(i)=\\frac{a_0}{2}+\\frac{a_1}{2^2}+...+\\frac{a_r}{2^{r+1}} \\tag {10}&emsp;&emsp;其中的$a_0a_1…a_n$是$i$的二进制编码每一位二进制位，即$i=a_0+a_1\\cdot 2+a_2\\cdot 2^2+a_3\\cdot 2^3+…+a_r\\cdot 2^r$。然后我们需要将这个二维的序列转换成我们对半球方向的三维采样，同样我们利用球面坐标和笛卡尔坐标之间的关系，首先将Hammersley序列$x_i=(u,v)^T\\in H_N$映射到$(\\phi,\\theta)$，然后转换成笛卡尔坐标下的向量形式。一个均匀映射和一个余弦映射公式如下： Uniform\\ \\ mapping= \\{\\begin{matrix} \\theta=cos^{-1}(1-u)\\\\ \\phi=2\\pi v \\end{matrix} \\\\ Cosinus\\ \\ mapping= \\{\\begin{matrix} \\theta=cos^{-1}(\\sqrt{(1-u)})\\\\ \\phi=2\\pi v \\end{matrix} \\tag {11}&emsp;&emsp;均匀映射就是将序列映射到一个均匀的分布，余弦映射则将序列映射到一个更偏向于半球中心轴上的分布。两者的区别如下图7所示。Hammersley序列可以通过位移操作快速地实现，具体代码见下面。 图7 均匀映射vs余弦映射 1234567891011121314float radicalInverseVdc(uint bits) &#123; bits = (bits &lt;&lt; 16u) | (bits &gt;&gt; 16u); bits = ((bits &amp; 0x55555555u) &lt;&lt; 1u) | ((bits &amp; 0xAAAAAAAAu) &gt;&gt; 1u); bits = ((bits &amp; 0x33333333u) &lt;&lt; 2u) | ((bits &amp; 0xCCCCCCCCu) &gt;&gt; 2u); bits = ((bits &amp; 0x0F0F0F0Fu) &lt;&lt; 4u) | ((bits &amp; 0xF0F0F0F0u) &gt;&gt; 4u); bits = ((bits &amp; 0x00FF00FFu) &lt;&lt; 8u) | ((bits &amp; 0xFF00FF00u) &gt;&gt; 8u); return float(bits) * 2.3283064365386963e-10; // / 0x100000000&#125;vec2 hammersley(uint i, uint N)&#123; return vec2(float(i) / float(N), radicalInverseVdc(i));&#125; &emsp;&emsp;紧接着我们要用Hammersley序列实现我们的重要性采样。前面已经提到过，我们将考虑物体的粗糙度，因为不同粗糙度下的反射波瓣大小不同。我们将粗糙度换分成5个等级，每个等级根据当前的粗糙度进行重要性采样。因此做重要性采样时我们需要根据粗糙度确定当前的反射波瓣大小，反射波瓣越大则采样范围越大。我们将结合之前PBR提到的法线分布函数，法线分布函数给定一个法线向量，它返回微平面法线与给定法线朝向一致的分布概率。Trowbridge-Reitz GGX法线分布函数的数学定义为： NDF_{GGXTR}(n,h,\\alpha)=\\frac{\\alpha^2}{\\pi((n\\cdot h)^2(\\alpha^2-1)+1)^2} \\tag {12}&emsp;&emsp;我们将法线分布函数与余弦映射结合起来做重要性采样： Important\\ \\ sampling= \\{\\begin{matrix} \\theta=cos^{-1}(\\sqrt{\\frac{1-u}{u(\\alpha^2-1)+1}})\\\\ \\phi=2\\pi v \\end{matrix} \\tag {13}&emsp;&emsp;同样这也是Epic Games公司提出的映射方法，注意到与公式$(11)$中的余弦映射相比，公式$(13)$多了一个分母$u(\\alpha^2-1)+1$取自公式$(12)$中的法线分布函数。当粗糙度$\\alpha$增大时，余弦值减小，$\\theta$取值范围越大，反射波瓣也就越大，这个就是公式$(13)$的大体思路。对于给定的Hammersley序列、法线向量N、粗糙度roughness，一个重要性采样代码如下所示： 12345678910111213141516171819vec3 importanceSampleGGX(vec2 Xi, vec3 N, float roughness)&#123; float a = roughness * roughness; float phi = 2.0 * PI * Xi.x; float cosTheta = sqrt((1.0 - Xi.y) / (1.0 + (a * a - 1.0) * Xi.y)); float sinTheta = sqrt(1.0 - cosTheta * cosTheta); vec3 H; H.x = cos(phi) * sinTheta; H.y = sin(phi) * sinTheta; H.z = cosTheta; // from tangent space to world space. vec3 up = abs(N.z) &lt; 0.999 ? vec3(0.0, 0.0, 1.0) : vec3(1.0, 0.0, 0.0); vec3 tangent = normalize(cross(up, N)); vec3 bitangent = cross(N, tangent); vec3 sampleVec = H.x * tangent + H.y * bitangent + H.z * N; return normalize(sampleVec);&#125; &emsp;&emsp;最后我们就利用重要性采样进行数值积分的计算，如下所示： \\int_{\\Omega}L_i(p,\\omega_i)d\\omega_i \\approx \\frac1N\\Sigma_{k=1}^NL_i(l_k) \\tag {14}&emsp;&emsp;然后在实现时，Ephic Games公司发现再乘上一个权重$cos\\theta_{l_k}$效果更加，因而实现的积分计算代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940#version 330 corein vec3 worldPos;out vec4 fragColor;uniform float roughness;uniform samplerCube environmentMap;const float PI = 3.14159265359;float radicalInverseVdc(uint bits);vec2 hammersley(uint i, uint N);vec3 importanceSampleGGX(vec2 Xi, vec3 N, float roughness);void main()&#123; vec3 N = normalize(worldPos); vec3 V = N; const uint sampleCount = 1024u; float totalWeight = 0.0f; vec3 prefilteredColor = vec3(0.0f); for(uint i = 0u;i &lt; sampleCount;++ i) &#123; vec2 Xi = hammersley(i, sampleCount); // sample halfway vector. vec3 H = importanceSampleGGX(Xi, N, roughness); // reflect vector. vec3 L = normalize(2.0 * dot(V, H) * H - V); float NdotL = max(dot(N, L), 0.0); if(NdotL &gt; 0.0f); &#123; prefilteredColor += texture(environmentMap, L).rgb * NdotL; totalWeight += NdotL; &#125; &#125; prefilteredColor = prefilteredColor / totalWeight; fragColor = vec4(prefilteredColor, 1.0f);&#125; &emsp;&emsp;在cpu端生成多个mipmap层次的cubemap，对于每一个粗糙度等级，我们将预计算的结果存储到对应mipmap等级的cubemap纹理当中，最后我们就得到如图4所示的多个mipmap等级的Pre-Filtered Environment Map。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162void IBLAuxiliary::convoluteSpecularIntegral(int width, int height, unsigned int cubemapTexIndex, unsigned int prefilteredTexIndex)&#123; // manager. TextureMgr::ptr texMgr = TextureMgr::getSingleton(); ShaderMgr::ptr shaderMgr = ShaderMgr::getSingleton(); // load shader. unsigned int shaderIndex = shaderMgr-&gt;loadShader(\"prefilterEnvMap\", \"./glsl/prefilterEnvMap.vert\", \"./glsl/prefilterEnvMap.frag\"); // load cube mesh. Mesh::ptr cubeMesh = std::shared_ptr&lt;Mesh&gt;(new Cube(1.0f, 1.0f, 1.0f)); // projection matrix and view matrix. glm::mat4 captureProjectMatrix = glm::perspective(glm::radians(90.0f), 1.0f, 0.1f, 10.0f); glm::mat4 captureViewMatrix[] = &#123; glm::lookAt(glm::vec3(0.0f), glm::vec3(+1.0f, 0.0f, 0.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(-1.0f, 0.0f, 0.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f,+1.0f, 0.0f), glm::vec3(0.0f, 0.0f,+1.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f,-1.0f, 0.0f), glm::vec3(0.0f, 0.0f,-1.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f, 0.0f,+1.0f), glm::vec3(0.0f, -1.0f, 0.0f)), glm::lookAt(glm::vec3(0.0f), glm::vec3(0.0f, 0.0f,-1.0f), glm::vec3(0.0f, -1.0f, 0.0f)), &#125;; // begin to filter. GLuint prefilteredTexId = texMgr-&gt;getTexture(prefilteredTexIndex)-&gt;getTextureId(); Shader::ptr shader = shaderMgr-&gt;getShader(shaderIndex); shader-&gt;bind(); shader-&gt;setInt(\"environmentMap\", 0); shader-&gt;setMat4(\"projectMatrix\", captureProjectMatrix); texMgr-&gt;bindTexture(cubemapTexIndex, 0); unsigned int maxMipLevels = 5; for (unsigned int mip = 0; mip &lt; maxMipLevels; ++mip) &#123; unsigned int mipWidth = width * std::pow(0.5, mip); unsigned int mipHeight = height * std::pow(0.5, mip); std::stringstream ss; ss &lt;&lt; mip; FrameBuffer::ptr framebuffer = std::shared_ptr&lt;FrameBuffer&gt;( new FrameBuffer(mipWidth, mipHeight, \"prefilteredDepth\" + ss.str(), &#123;&#125;, true)); framebuffer-&gt;bind(); glDisable(GL_BLEND); glDisable(GL_CULL_FACE); glEnable(GL_DEPTH_TEST); glEnable(GL_TEXTURE_CUBE_MAP_SEAMLESS); glDepthFunc(GL_LEQUAL); glClearColor(0.0f, 1.0f, 0.0f, 1.0f); float roughness = (float)mip / (float)(maxMipLevels - 1); shader-&gt;setFloat(\"roughness\", roughness); for (unsigned int i = 0; i &lt; 6; ++i) &#123; shader-&gt;setMat4(\"viewMatrix\", captureViewMatrix[i]); glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_CUBE_MAP_POSITIVE_X + i, prefilteredTexId, mip); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); cubeMesh-&gt;draw(false, 0); &#125; framebuffer-&gt;unBind(); &#125; shader-&gt;unBind(); texMgr-&gt;unBindTexture(cubemapTexIndex);&#125; 2、Pre-computing the BRDF&emsp;&emsp;接下来我们把目标方法第二部分的积分计算，即brdf函数积分： \\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)n\\cdot \\omega_id\\omega_i \\tag {15}&emsp;&emsp;公式$(15)$的积分值取决于三个变量，即$n\\cdot \\omega_o$、表面粗糙度以及$F_0$，$F_0$是菲涅尔方程的一个输入值。三个变量太多了，为了简化且方便预计算，我们设法将一些变量提出积分符号外： \\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)n\\cdot \\omega_id\\omega_i =\\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)\\frac{F(\\omega_o,h)}{F(\\omega_o,h)}n\\cdot \\omega_id\\omega_i\\\\ =\\int_{\\Omega}\\frac{f_r(p,\\omega_i,\\omega_o)}{F(\\omega_o,h)}F(\\omega_o,h)n\\cdot \\omega_id\\omega_i \\tag{16}&emsp;&emsp;将菲涅尔项$F(\\omega_o,h)=(F_0+(1-F_0)(1-\\omega_o\\cdot h)^5)$代入上式，得： \\int_{\\Omega}\\frac{f_r(p,\\omega_i,\\omega_o)}{F(\\omega_o,h)}(F_0+(1-F_0)(1-\\omega_o\\cdot h)^5)n\\cdot \\omega_id\\omega_i \\\\ = \\int_{\\Omega}\\frac{f_r(p,\\omega_i,\\omega_o)}{F(\\omega_o,h)}(F_0(1-(1-\\omega_o\\cdot h)^5)+(1-\\omega_o\\cdot h)^5)n\\cdot \\omega_id\\omega_i \\tag {17}​ &emsp;&emsp;然后将公式$(17)$得到的结果分成两部分： \\int_{\\Omega}\\frac{f_r(p,\\omega_i,\\omega_o)}{F(\\omega_o,h)}(F_0(1-(1-\\omega_o\\cdot h)^5)+(1-\\omega_o\\cdot h)^5)n\\cdot \\omega_id\\omega_i\\\\ =F_o\\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)(1-(1-\\omega_o\\cdot h)^5)n\\cdot \\omega_id\\omega_i +\\int_{\\Omega}f_r(p,\\omega_i,\\omega_o)(1-\\omega_o\\cdot h)^5 n\\cdot \\omega_i d\\omega_i \\tag {18}&emsp;&emsp;注意，公式$(18)$中的是去掉了菲涅尔项的brdf函数，因为菲涅尔项被消去了。现在我们把$F_0$提出积分外了，公式$(18)$中的两项积分取决于$n\\cdot \\omega_o$和粗糙度roughness。我们构建这样的一个二维查找表，它的横轴坐标取值为$n\\cdot \\omega_o$，纵轴坐标取值为粗糙度roughness，我们渲染屏幕空间大小的四边形，遍历$n\\cdot \\omega_o$和粗糙度的所有取值，计算其对应的公式$(18)$中的两项积分的结果，存储为纹理的像素值。最后渲染时使用纹理坐标$(n\\cdot \\omega_o, roughness)$去索引像素值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859float geometrySchlickGGX(float NdotV, float roughness)&#123; float a = roughness; float k = (a * a) / 2.0f; float nom = NdotV; float denom = NdotV * (1.0 - k) + k; return nom / denom;&#125;float geometrySmith(vec3 N, vec3 V, vec3 L, float roughness)&#123; float NdotV = max(dot(N, V), 0.0); float NdotL = max(dot(N, L), 0.0); float ggx2 = geometrySchlickGGX(NdotV, roughness); float ggx1 = geometrySchlickGGX(NdotL, roughness); return ggx1 * ggx2;&#125;vec2 integrateBRDF(float NdotV, float roughness)&#123; vec3 V; V.x = sqrt(1.0 - NdotV * NdotV); V.y = 0.0f; V.z = NdotV; float A = 0.0; float B = 0.0; vec3 N = vec3(0.0, 0.0, 1.0); const uint sampleCount = 1024u; for(uint i = 0u;i &lt; sampleCount;++i) &#123; vec2 Xi = hammersley(i, sampleCount); vec3 H = importanceSampleGGX(Xi, N, roughness); vec3 L = normalize(2.0 * dot(V, H) * H - V); float NdotL = max(L.z, 0.0); float NdotH = max(H.z, 0.0); float VdotH = max(dot(V, H), 0.0); if(NdotL &gt; 0.0) &#123; float G = geometrySmith(N, V, L, roughness); float G_Vis = (G * VdotH) / (NdotH * NdotV); float Fc = pow(1.0 - VdotH, 5.0); A += (1.0 - Fc) * G_Vis; B += Fc * G_Vis; &#125; &#125; A /= float(sampleCount); B /= float(sampleCount); return vec2(A, B);&#125;void main()&#123; vec2 value = integrateBRDF(Texcoord.x, Texcoord.y); fragColor = vec4(value, 0.0f, 1.0f);&#125; &emsp;&emsp;在cpu端创建一个二维纹理，并送入一个屏幕大小的四边形进行预计算的渲染。 1234567891011121314151617181920212223242526272829void IBLAuxiliary::convoluteSpecularBRDFIntegral(int width, int height, unsigned int brdfLutTexIndex)&#123; // manager. TextureMgr::ptr texMgr = TextureMgr::getSingleton(); ShaderMgr::ptr shaderMgr = ShaderMgr::getSingleton(); // load shader. unsigned int shaderIndex = shaderMgr-&gt;loadShader(\"genBrdfLUT\", \"./glsl/genBrdfLUT.vert\", \"./glsl/genBrdfLUT.frag\"); // load quad mesh. Mesh::ptr quadMesh = std::shared_ptr&lt;Mesh&gt;(new ScreenQuad()); FrameBuffer::ptr framebuffer = std::shared_ptr&lt;FrameBuffer&gt;( new FrameBuffer(width, height, \"brdfDepth\", &#123;&#125;, true)); Shader::ptr shader = shaderMgr-&gt;getShader(shaderIndex); framebuffer-&gt;bind(); glDisable(GL_BLEND); glDisable(GL_CULL_FACE); glDisable(GL_DEPTH_TEST); glDepthFunc(GL_LEQUAL); glClearColor(0.0f, 1.0f, 0.0f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); GLuint brdfLutTexId = texMgr-&gt;getTexture(brdfLutTexIndex)-&gt;getTextureId(); glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, brdfLutTexId, 0); shader-&gt;bind(); quadMesh-&gt;draw(false, 0); shader-&gt;unBind(); framebuffer-&gt;unBind();&#125; &emsp;&emsp;然后我们就可以得到下面这张纹理。 图8 brdf积分查找表 五、计算渲染方程&emsp;&emsp;在前面的步骤中我们预计算获取了Irradiance Map、Pre-Filtered Environment Map以及BRDF Lookup Texture，最后我们就直接查找这些纹理，用以我们的光照计算。光照计算部分直接就是一开始提到的渲染方程，渲染方程中的积分项从纹理中直接获取，不再需要实时计算。 12345678910111213141516// ambient lighting.vec3 ambientS = fresnelSchlickRoughness(max(dot(normal, viewDir), 0.0f), F0, roughness);vec3 ambientD = vec3(1.0f) - ambientS;ambientD *= (1.0 - metallic);vec3 irradiance = texture(irradianceMap, normal).rgb;vec3 R = normalize(reflect(-viewDir, normal));const float MAX_REFLECTION_LOD = 4.0;vec3 prefilteredColor = texture(prefilteredMap, R, roughness * MAX_REFLECTION_LOD).rgb;vec2 envBrdf = texture(brdfLutMap, vec2(max(dot(normal, viewDir), 0.0f), roughness)).rg;vec3 envSpecular = prefilteredColor * (ambientS * envBrdf.x + envBrdf.y);vec3 ambient = (albedo * irradiance * ambientD + envSpecular) * ao;fragColor.xyz = ambient + fragColor.xyz * shadow + pointLightRadiance; 六、实现效果 参考资料：$[1]$ Hammersley Points on the Hemisphere $[2]$ Real Shading in Unreal Engine 4, by Brian Karis, Epic Games $[3]$ https://learnopengl.com/PBR/IBL/Diffuse-irradiance $[4]$ https://learnopengl.com/PBR/IBL/Specular-IBL","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Real-time Rendering","slug":"Real-time-Rendering","permalink":"https://yangwc.com/about/categories/Real-time-Rendering/"},{"name":"Physically Based Rendering","slug":"Physically-Based-Rendering","permalink":"https://yangwc.com/about/categories/Physically-Based-Rendering/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Real-time Rendering","slug":"Real-time-Rendering","permalink":"https://yangwc.com/about/tags/Real-time-Rendering/"},{"name":"Physically Based Rendering","slug":"Physically-Based-Rendering","permalink":"https://yangwc.com/about/tags/Physically-Based-Rendering/"}]},{"title":"实时渲染Real-time Rendering：基于物理的光照模型","slug":"PhysicallyBasedRendering","date":"2019-07-14T09:25:05.766Z","updated":"2019-09-27T13:04:01.396Z","comments":true,"path":"2019/07/14/PhysicallyBasedRendering/","link":"","permalink":"https://yangwc.com/about/2019/07/14/PhysicallyBasedRendering/","excerpt":"本文采用OpenGL搭建了一个基于物理着色的渲染器，目前大多数的实时应用都是采用了PBR，相对于传统的Phong等基于经验的光照模型，基于物理着色的渲染方法更为真实。","text":"本文采用OpenGL搭建了一个基于物理着色的渲染器，目前大多数的实时应用都是采用了PBR，相对于传统的Phong等基于经验的光照模型，基于物理着色的渲染方法更为真实。 能量守恒 微平面模型 基于物理的BRDF 屏幕空间环境光遮蔽 实现效果 参考资料 基于物理的渲染 &emsp;&emsp;基于物理的渲染（Physically Based Rendering，简称PBR）技术致力于渲染出更贴近于真实物理世界的光影效果，它倾向于探索光影背后的物理规律，然后在此基础上构建一个基于物理规律的光照模型，最后应用到光照计算中。基于物理的渲染除了更为真实，它也给光照计算的赋予了更多的物理意义，从而使得设计师们摆脱基于经验的参数调整，只要设置的物理量正确，则最终光照效果也将会是正确的。即便如此，基于物理的渲染技术依然只是现实物理世界的一个逼近。对于一个基于物理渲染的光照模型，它通常需要满足以下的三个条件： &emsp;&emsp;1、能量守恒 &emsp;&emsp;2、基于微平面的表面模型 &emsp;&emsp;3、使用基于物理的BRDF &emsp;&emsp;接下来我们就按照上面的顺序一一展开。 ## 一、能量守恒（Energy Conservation） &emsp;&emsp;基于物理的光照模型必须遵守这样的一个能量守恒原则：对于一个非自发光的物体，出射光线的能量永远不能超过入射光线的能量。当一束光线照射到物体表面，它就被分割成两个部分，分别是折射部分和反射部分。反射部分的光线则是直接撞击到表面然后反弹开来的那部分光线，这部分构成中我们日常生活中常见的镜面高光。折射部分的光线则进入物体内部，光线在内部与物体的粒子发生碰撞，此时光线的一部分能量就转变成热能。一般情况下，并非所有光能都被转化成热能，还有一些光线在内部经过多次散射最终又离开物体表面，这部分的光线构成了物体的漫反射光。这里还要特别区分一下金属材质，与非金属材质和电介质不同，金属材质会直接吸收折射光而不会散开，只表现出镜面反射光。即金属表面不会显示出漫反射的颜色。 &emsp;&emsp;根据能量守恒的原则，反射光与折射光是相互排斥的，因此我们只要知道其中一部分占总入射光的百分比，就能立马得到另外一部分的能量占比。通过这样的一个方案，我们就能保证出射光的总能量小于等于入射光的总能量。下面的图1展示Blin-Phong的光照渲染结果，Blin-Phong光照模型并不是一个基于物理的光照模型，它并不满足能量守恒的原则，可以看到，图1场景看起来太亮了，而我仅仅将光照的辐射率设置为vec3(0.6)。 图1 Blin-Phong光照模型的渲染结果 二、微平面模型（Microfacet Model）&emsp;&emsp;PBR技术采用了微平面理论：在微观尺度下，任意的一个平面都可以用一组微小的光滑镜面来描述，这个微小的光滑镜面就是微平面（Microfacet）。根据平面粗糙程度的不同，这些微平面的排列取向也各不相同。一个平面越是粗糙，则其平面上的微平面排列就越混乱。微平面的排列越混乱，则入射光线照射到该平面上时更趋向于朝向完全不同的方向散射开来，从而产生更大范围的镜面光。同理，若平面越光滑，则其微平面排列取向越规整，入射光线大体上越趋向于向同一个方向反射，产生更小范围、更加锐利的镜面高光。正如如下图2所示。 图2 粗糙和光滑的微平面 &emsp;&emsp;微平面的取向排列混乱程度我们采用一个粗糙度（Roughness）的参数来衡量。直接在微观尺度下操作显然不可行，因为我们将在宏观尺度下采用统计学的方法来估算微平面的粗糙度。这个粗糙度我们定义为某个向量的方向与微平面平均取向方向一致的概率，这个向量便是光线向量$l$和视线向量$v$之间的中间向量$h$： h=\\frac{l+v}{||l+v||} \\tag {1}&emsp;&emsp;在一个表面的微平面中，越多微平面的法线方向与中间向量的方向一致，则镜面的反射效果就越强烈、越锐利。 图3 不同粗糙度的镜面高光 三、基于物理的BRDF&emsp;&emsp;在前面的能量守恒原则和微平面理论的基础上，我们将展开基于物理的光照计算。首先我们要了解的是渲染方程，PBR采用的渲染方程是一个特化版本，也被称为反射方程，如下所示： L_o(p,\\omega_o)=\\int _{\\Omega}f_r(p,\\omega_i,\\omega_o)L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\tag {2}&emsp;&emsp;上式中，$L_o$是反射辐射率，$L_i$是入射辐射率，$p$为物体表面上的一点，$\\omega_o$为出射方向向量，$\\omega_i$为入射方向向量，$n$是表面法线向量，$f_r(p,\\omega_i,\\omega_o)$是后面我们将要提到的BRDF函数。积分区域$\\Omega$是以表面法线$n$为轴的半球领域。PBR渲染方程主要是关于光能辐射度量学的（Radiometry）的内容，这里简单介绍一些辐射度量学的物理量。 &emsp;&emsp;辐射通量（Radiant Flux）：辐射通量以瓦特为单位，符号为$\\Phi$，它衡量一个光源所辐射的能量。光是由多种不同波长的能量所集合而成的，一个光源所放射出来的能量可以被视作这个光源包含的所有各种波长的一个函数。但是在计算机图形学中，我们通常采用三原色编码即RGB来简化辐射通量的表示，这套编码带来的损失基本可以忽略。 &emsp;&emsp;立体角（Solid Angle）：这个物理量在前面的文章都有提及过了，立体角符号为$\\omega$，它描述了一个几何体投影到单位球面上的大小，立体角可以看成是带有体积的方向向量。 &emsp;&emsp;辐射强度（Radiant Intensity）：辐射强度衡量了在单位球面上，一个光源每单位立体角所辐射的辐射通量。其定义公式为$I=\\frac{d\\Phi}{d\\omega}$，即微分辐射通量除以微分立体角。对于一个全向且向所有方向均匀辐射的光源，辐射强度表示了光源在一个单位球面上单位立体角的辐射能量。 &emsp;&emsp;辐射率（Radiance）：辐射率就是具有辐射强度$\\Phi$的光源在单位面积$A$、单位立体角$\\omega$下的辐射总能量，其定义见下面的公式$(3)$。$d\\omega cos\\theta$将单位立体角（也就是单位球体上的面积）投影到法线方向。 L=\\frac{d^2\\Phi}{dAd\\omega cos\\theta} \\tag {3} 图4 辐射率示意图 &emsp;&emsp;辐射率是辐射度量学上表示一个区域平面上光线总量的物理量，它受到入射光线与平面法线间的夹角$\\theta$的余弦值$cos\\theta$的影响：当直接辐射到平面上的程度越低时，光线就越弱，而当光线完全垂直于平面时强度最高。当立体角$\\omega$和面积$A$趋向于无穷小时，我们能用辐射率来表示单束光线穿过空间中的一个点的通量。这就使我们可以计算得出作用于单个片段或点上的单束光线的辐射率，即把立体角$\\omega$转变为方向向量然后把面$A$转换为点$p$，这样我们就能直接在我们的着色器中使用辐射率来计算单束光线对每个片段的作用。 &emsp;&emsp;上面讨论的仅仅是一束光线投射到点$p$上，但是通常我们需要计算的是所有投射到点$p$上的光线总和，这个和就是辐照度（Irradiance）。注意到反射方程$(2)$对半球领域$\\Omega$进行积分，这是因为我们要计算的不只是单一一个方向上的入射光，而是一个以点$p$为球心、以法向为中轴的半球领域$\\Omega$内所有方向上的入射光。 图5 半球领域 &emsp;&emsp;由于渲染方程都没有解析解，求解公式$(2)$即反射方程时我们将采用离散的方法来积分的数值解。目前常用的就是梯形法，在半球领域$\\Omega$按一定的步长将反射率方程分散求解，然后再按照步长大小将所得的结果平均化，这个就是黎曼和（Riemann Sum）。 &emsp;&emsp;然后剩下的就是BRDF函数，也就是公式$(2)$中的$f_r(p,\\omega_i,\\omega_o)$部分。BRDF的全称为Bidirectional Reflective Distribution Function，即双向反射分布函数。对于一个给定材质属性，BRDF函数给出了入射光和反射光的关系，一束给定入射方向的入射光照射到物体表面时，会被反射到表面半球范围内的各个方向，不同反射角度的反射光线在入射光线中的占比各不相同，BRDF函数就用来表示这种比例关系，其定义如下： f(l,v)=\\frac{dL_o(v)}{dE(l)} \\tag {4}&emsp;&emsp;Cook-Torrance模型是目前应用最为广泛的基于物理的BRDF模型，它被用于很多实时渲染管线的材质和光照环境下。Cook-Torrance的BRDF包含漫反射和镜面反射两个部分，其中的镜面反射部分比较复杂： f_r=k_df_{lambert}+k_sf_{cook-torrance} \\tag {5}&emsp;&emsp;其中，$k_d$就是前面提到的入射光线中被折射的光线部分的能量占比，而$k_s$则是被反射的光线部分所占的比例。$f_{lambert}$是BRDF的漫反射部分，这个是Lambertian漫反射模型，其计算公式如下所示： f_{lambert}=\\frac{c}{\\pi} \\tag {6}&emsp;&emsp;其中$c$是物体的反照率（Albedo），大部分的实时渲染应用都采用了Lambertian漫反射模型。然后就是镜面反射部分，镜面反射部分就是Cook-Torrance的各向同性光照模型： f_{cook-torrance} = \\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)} \\tag {7}&emsp;&emsp;其中$\\omega_i$是入射方向，$\\omega_o$是观察方向。Cook-Torrance的模型包含三个函数，D、F、G分别是法线分布函数、菲尼尔方程、几何函数。接下来我们将讨论的是Trowbridge-Reitz GGX法线分布函数，Fresnel-Schlick菲涅尔方程以及Smith’s Schlick-GGX几何函数。 &emsp;&emsp;首先是法线分布函数（Normal Distribution Function），给定表面的粗糙度，法线分布函数估算平面法线取向与中间向量一致的微平面数量。从统计学上讲，法线分布函数近似地描述了与中间向量$h$取向相同的微平面占全部微平面的比例。例如，给定中间向量$h$，若我们要估算的微平面中有$35\\%$与向量$h$取向相同，那么法线分布函数将返回$0.35$。Trowbridge-Reitz GGX法线分布函数的数学定义如下所示： NDF_{GGXTR}(n,h,\\alpha)=\\frac{\\alpha ^2}{\\pi ((n\\cdot h)^2(\\alpha^2-1)+1)^2} \\tag {8}&emsp;&emsp;其中，$n$为宏观法线，$h$是中间向量，而$\\alpha$则表示平面的粗糙度。当粗糙度$\\alpha$值很低时，即表面比较光滑时，与中间向量$h$取向相同的微平面会高度地集中在一个小半径范围内。此时镜面反射会形成一个非常明亮的光斑。相反，当表面的粗糙度值较高时，与$h$向量取向一致的微平面分布在一个比较大的半径范围内，这使得最终的镜面反射效果显得较为灰暗。 &emsp;&emsp;然后就是菲涅尔方程（Fresnel Equation），描述了指定角度下表面反射的光线所占的比例。当一束光线照射到表面时，菲涅尔方程会依据观察角度给出反射光线所占的百分比。然后根据这个反射光所占的百分比和能量守恒定律就可以得出光线折射部分所占的比率。当我们垂直观察物体的时候，任何表面都有一个基础的反射率。例如，用垂直的视角看向木制桌面或者金属桌面，此时只有最基本的反射，但若近乎与平面平行的角度去观察的话就会看到非常明显的反光效果。Fresnel-Schlick近似菲涅尔方程如下所示： F_{schlick}(h,v,F_0)=F_0+(1-F_0)(1-(h\\cdot v))^5 \\tag {9}&emsp;&emsp;其中，$F_0$就是表面的基础反射率，它是通过折射系数计算得到的，$h$即前面提到的中间向量，$v$为观察方向向量。 &emsp;&emsp;最后就是几何函数（Geometry Function），几何函数描述了微平面自我遮挡的属性。当一个平面比较粗糙的时候，表面上的微平面可能会挡住其他的微平面从而减弱表面反射光的强度。与法线分布函数类似，几何函数也是从统计学的角度近似求出微平面之间相互遮蔽的比率。 图6 微平面的相互遮蔽现象 &emsp;&emsp;几何函数也采用一个材质的粗糙度作为输入的参数，越粗糙的表面其微平面之间相互遮挡的概率也就越高。Schlick-GGX几何函数的数学定义如下： G_{schlickGGX}(n,v,k)=\\frac{n\\cdot v}{(n\\cdot v)(1-k)+k} \\tag {10}&emsp;&emsp;公式$(10)$中的$k$是关于粗糙度$\\alpha$的重映射，取决于几何函数是针对直接光照还是针对IBL（Image Based Lighting）光照： k_{direct}=\\frac{(\\alpha+1)^2}{8} \\\\ k_{IBL}=\\frac{\\alpha^2}{2} \\tag {11}&emsp;&emsp;微平面的相互遮蔽主要有两个方面，分别是几何遮蔽（Geometry Obstruction）和几何阴影（Geometry Shadowing），几何遮蔽与视线向量有关，而几何阴影则于入射方向向量相关。我们采用史密斯法将两者纳入其中： G(n,v,l,k)=G_{schlickGGX}(n,v,k)G_{schlickGGX}(n,l,k) \\tag {12}&emsp;&emsp;最终，我们得到反射方程$(2)$中的BRDF计算公式： f_r=k_d\\frac{c}{\\pi}+k_s\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)} \\tag {13}&emsp;&emsp;实际上，BRDF的计算公式$(13)$有个错误，公式中的$F$即菲涅尔项就是$k_s$，因为菲涅尔项表示的就是反射光线的占比，因此应该把$k_s$去掉，然后$k_d=1-F$。 f_r=k_d\\frac{c}{\\pi}+\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)} \\tag {14}&emsp;&emsp;将公式$(14)$代入到公式$(2)$中，我们最终得到一个具体的渲染方程： L_o(p,\\omega_o)=\\int _{\\Omega}(k_d\\frac{c}{\\pi}+\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot \\omega_id\\omega_i \\tag {15}三、PBR渲染器的实现&emsp;&emsp;有了前面的理论基础，接下来我们就展开相关的PBR实现。首先我们要讨论的是PBR材质，仔细观察渲染方程公式$(15)$，求解这个渲染方程我们需要获取物体的反照率向量、法线向量、粗糙度。除此之外，我还需要物体的金属度参数，这是因为Fresnel-Schlick近似仅仅对电介质或者说非金属表面有定义，对于导体(Conductor)表面（金属），使用它们的折射指数计算基础折射率并不能得出正确的结果。金属度用来描述一个材质表面是金属还是非金属的，基于金属表面特性，我们要么使用电介质的基础反射率要么就使用物体的表面颜色。因为金属表面会吸收所有折射光线而没有漫反射，所以我们可以直接使用表面颜色纹理来作为它们的基础反射率。 &emsp;&emsp;因此，对于一个PBR渲染器，我们需要获取物体的PBR材质，PBR材质包含了反照率（Albedo）纹理、法线（Normal）纹理、粗糙度（Roughness）纹理以及金属度（Metallic）纹理，正如如下图7所示。在一些PBR渲染器中，还有一个环境遮蔽光贴图（Ambient Occulsion），这里我们不考虑AO贴图，而是考虑在后面采用SSAO实现环境遮蔽光的效果。 图7 PBR材质 &emsp;&emsp;然后还需要提一点的是，我们目前仅考虑直接光照部分，不考虑反弹多次的间接光照。仅考虑直接光照时，对于渲染方程$(15)$，我们不需要对整个半球领域进行积分，因为此时的被积函数是一个狄拉克函数。也就是被积函数仅在某一个特定的方向上才不为0，剩余部分函数值全为0，因此没有必要进行积分。在光照计算中，我们是可以直接知道空间中的光源位置，因此可以直接计算。当空间中有多个光源时，渲染方程的值就是直接计算点与这些光源之间的被击函数值最后累加起来。 L_o(p,\\omega_o)=\\Sigma_{i}^m(k_d\\frac{c}{\\pi}+\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot \\omega_i \\tag {16}&emsp;&emsp;公式$(16)$就是场景中有$m$个光源时的实际渲染方程。为了支持大量的光源，我采用了延迟渲染，将物体空间位置和PBR材质信息存储到多张纹理中，然后在屏幕空间计算光照。首先将物体的信息渲染到纹理中，因为采用了法线贴图，所以要在顶点着色器中构建TBN矩阵提取出法线向量： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768---------------Vertex Shader-------------------#version 330 corelayout (location = 0) in vec3 position;layout (location = 1) in vec3 normal;layout (location = 2) in vec2 texcoord;layout (location = 3) in vec3 color;layout (location = 4) in vec3 tangent;layout (location = 5) in vec3 bitangent;layout (location = 6) in mat4 instanceMatrix;out vec3 FragPos;out vec2 Texcoord;out mat3 TBNMatrix;uniform bool instance;uniform mat4 modelMatrix;uniform mat4 viewMatrix;uniform mat4 projectMatrix;uniform mat4 normalMatrix;uniform mat4 lightSpaceMatrix;void main()&#123; vec3 T = normalize(vec3(modelMatrix * vec4(tangent, 0.0f))); vec3 B = normalize(vec3(modelMatrix * vec4(bitangent, 0.0f))); vec3 N = normalize(vec3(modelMatrix * vec4(normal, 0.0f))); TBNMatrix = mat3(T, B, N); Texcoord = texcoord; if(!instance) FragPos = vec3(modelMatrix * vec4(position,1.0f)); else FragPos = vec3(modelMatrix * instanceMatrix * vec4(position,1.0f)); gl_Position = projectMatrix * viewMatrix * vec4(FragPos,1.0f);&#125;---------------Fragment Shader-------------------#version 330 corein vec3 FragPos;in vec2 Texcoord;in mat3 TBNMatrix;uniform float nearPlane;uniform float farPlane;uniform sampler2D albedoMap;uniform sampler2D normalMap;uniform sampler2D roughMap;uniform sampler2D metallicMap;uniform sampler2D depthMap;layout(location = 0) out vec3 dposition;layout(location = 1) out vec3 dnormal;layout(location = 2) out vec3 dalbedo;layout(location = 3) out vec3 droughness;void main()&#123; // sample albedo. vec3 albedo = texture(albedoMap, Texcoord).rgb; // sample normal. vec3 normal = normalize(2.0f * texture(normalMap, Texcoord).rgb - vec3(1.0f)); normal = TBNMatrix * normal; // sample roughness. float roughness = texture(roughMap, Texcoord).r; // sample metallic. float metallic = texture(metallicMap, Texcoord).r; dposition = FragPos; dnormal = normal; dalbedo = albedo; droughness = vec3(roughness, metallic, gl_FragCoord.z);&#125; &emsp;&emsp;然后在屏幕空间中实现我们的PBR算法。首先是BRDF的三个函数。根据公式$(8)$，法线分布函数如下所示： 1234567891011float NormalDistributionGGX(vec3 N, vec3 H, float roughness)&#123; float a = roughness * roughness; float aSquared = a * a; float NdotH = max(dot(N, H), 0.0f); float NdotHSquared = NdotH * NdotH; float nom = aSquared; float denom = (NdotHSquared * (aSquared - 1.0f) + 1.0f); denom = PI * denom * denom; return nom / denom;&#125; &emsp;&emsp;根据公式$(9)$，菲涅尔方程的计算代码： 1234vec3 fresnelSchlick(float cosTheta, vec3 F0)&#123; return F0 + (1.0f - F0) * pow(1.0 - cosTheta, 5.0f);&#125; &emsp;&emsp;根据公式$(10)$、$(11)$、$(12)$，几何函数的计算代码： 123456789101112131415161718float GeometrySchlickGGX(float NdotV, float roughness)&#123; float r = (roughness + 1.0f); float k = (r * r) / 8.0f; float nom = NdotV; float denom = NdotV * (1.0f - k) + k; return nom / denom;&#125;float GeometrySmith(vec3 N, vec3 V, vec3 L, float roughness)&#123; float NdotV = max(dot(N, V), 0.0f); float NdotL = max(dot(N, L), 0.0f); float ggx2 = GeometrySchlickGGX(NdotV, roughness); float ggx1 = GeometrySchlickGGX(NdotL, roughness); return ggx2 * ggx1;&#125; &emsp;&emsp;然后就是光照部分，我实现的渲染器支持一个平行光、多个点光源。首先来看平行光部分，平行光部分因为不用考虑衰减，因而更为简单： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465struct DirLight &#123; vec3 direction; vec3 radiance;&#125;;uniform DirLight dirLight;void main()&#123; // sample position. vec3 FragPos = texture(dposition, Texcoord).rgb; // sample albedo. vec3 albedo = texture(dalbedo, Texcoord).rgb; // sample normal. vec3 normal = texture(dnormal, Texcoord).rgb; // sample roughness. float roughness = texture(droughness, Texcoord).r; // sample metallic. float metallic = texture(droughness, Texcoord).g; // sample depth. float depth = texture(droughness, Texcoord).b; // sample ambient occlusion. float ao = texture(ddepth, Texcoord).r; // emssive if(normal.x == 0.0f &amp;&amp; normal.y == 0.0f &amp;&amp; normal.z == 0.0f) &#123; fragColor.rgb = albedo; // glow map. float brightness = dot(fragColor.rgb, vec3(0.2126, 0.7152, 0.0722)); brightColor = vec4(fragColor.rgb, 1.0f); // write depth to buffer for forwarding shading. gl_FragDepth = depth; return; &#125; // some directions. vec3 viewDir = normalize(cameraPos - FragPos); // index of refracted. vec3 F0 = vec3(0.04); F0 = mix(F0, albedo, metallic); // ************************directional light************************************ vec3 lightDir = dirLight.direction; vec3 halfwayDir = normalize(lightDir + viewDir); // fresnel factor. vec3 fresnel = fresnelSchlick(max(dot(halfwayDir, viewDir), 0.0f), F0); // normal distribution factor. float distribution = NormalDistributionGGX(normal, halfwayDir, roughness); // geometry facror. float geometryFactor = GeometrySmith(normal, viewDir, lightDir, roughness); // brdf function. vec3 brdf = distribution * fresnel * geometryFactor / (4.0f * max(dot(viewDir, normal), 0.0f) * max(dot(lightDir, normal), 0.0f) + 0.0001f); vec3 kSpecular = fresnel; vec3 kDiffuse = vec3(1.0f) - kSpecular; kDiffuse *= (1.0f - metallic); // rendering equation. fragColor.rgb = (kDiffuse * albedo / PI + brdf) * dirLight.radiance * max(dot(normal, lightDir), 0.0f); // ***************************************************************************** // other. ....&#125; &emsp;&emsp;需要特别说明的就是基础反射率部分，在上面的代码第41、42行，对于电介质我们令其基础反射率为0.04，然后根据材质的金属度在0.04和反照率直接做一个混合。然后就是点光源部分，点光源通常要有一个衰减的过程，这里我采用的衰减因子计算公式如下： attenuation = \\frac{1.0}{c\\cdot d^2}\\\\ d = length(lightPosition - fragPosition) \\tag {17}&emsp;&emsp;即点光的光照强度以距离的平方的倒数衰减，其中$c$是衰减系数，可由用户根据想要的效果指定。确定了衰减方程之后，我们还需要计算点光源的光体积，这是因为当光源与当前点的距离超过一定的值时，计算得到的光照值将小到可以忽略不计。因此，我们可以做这样的一个优化，当距离超过一定值时直接不计算光照，这对于拥有大量光源的场景来说是非常有意义的，它能够减少大量的计算。 &emsp;&emsp;那么如何知道这个距离的阈值呢？这个距离的阈值必须要刚刚好，太小则会产生明显的光照硬边，太大则优化又没有那么明显。事实上，这个距离阈值与上面的衰减因子计算（即公式$(17)$）息息相关。理想情况下，当$attenuation$变为0时，光照的贡献值也变为0。但是事实上$attenuation$不能为0，只能无限地趋于0，我们可以根据一个自己设置的阈值来求解$d$，我设置的阈值为$\\frac{1}{256}$，当光照贡献值小于这个值时，可以忽略不计了： \\frac{1}{256}=I_{max}\\cdot attenuation = I_{max}\\frac{1.0}{c\\cdot d^2}\\\\ \\to d=\\sqrt{\\frac{256I_{max}}{c}} \\tag {18}&emsp;&emsp;上式中的$I_{max}$是光照颜色中的最大分量，根据公式$(18)$我们就得到了点光源的光体积，这是一个以该$d$为半径的球体。当片段位置到光源位置的距离大于这个半径时，我们直接跳过该光源的光照计算。这个光体积直接在CPU上计算一次即可： 123456789101112131415void PointLight::setAttenuationCoff(float coff)&#123; m_atteunationCoff = coff; // calculate point light's volume. GLfloat lightMax = std::fmaxf(std::fmaxf(m_radiance.r, m_radiance.g), m_radiance.b); m_radius = sqrt(256.0f * lightMax / (1.0f * m_atteunationCoff));&#125;void PointLight::setLightColor(glm::vec3 radiance)&#123; Light::setLightColor(radiance); // calculate point light's volume. GLfloat lightMax = std::fmaxf(std::fmaxf(m_radiance.r, m_radiance.g), m_radiance.b); m_radius = sqrt(256.0f * lightMax / (1.0f * m_atteunationCoff));&#125; &emsp;&emsp;最后完整的着色器代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204#version 330 corein vec2 Texcoord;struct DirLight &#123; vec3 direction; vec3 radiance;&#125;;struct PointLight&#123; float radius; vec3 position; vec3 radiance;&#125;;uniform vec3 cameraPos;// lighting.#define MAX_POINT_LIGHT 128uniform int pointLightNum;uniform DirLight dirLight;uniform PointLight pointLight[MAX_POINT_LIGHT];uniform float lightAttenuationCoff;uniform mat4 lightSpaceMatrix;// pbr material texture.uniform sampler2D dposition;uniform sampler2D dnormal;uniform sampler2D dalbedo;uniform sampler2D droughness;uniform sampler2D ddepth;uniform sampler2D shadowDepth;layout(location = 0) out vec4 fragColor;layout(location = 1) out vec4 brightColor;// brdf auxiliary functions.float NormalDistributionGGX(vec3 N, vec3 H, float roughness);float GeometrySchlickGGX(float NdotV, float roughness);float GeometrySmith(vec3 N, vec3 V, vec3 L, float roughness);vec3 fresnelSchlick(float cosTheta, vec3 F0);float shadowCalculation(vec4 fragPosLightSpace, float bias);const float PI = 3.14159265359;void main()&#123; // sample position. vec3 FragPos = texture(dposition, Texcoord).rgb; // sample albedo. vec3 albedo = texture(dalbedo, Texcoord).rgb; // sample normal. vec3 normal = texture(dnormal, Texcoord).rgb; // sample roughness. float roughness = texture(droughness, Texcoord).r; // sample metallic. float metallic = texture(droughness, Texcoord).g; // sample depth. float depth = texture(droughness, Texcoord).b; // sample ambient occlusion. float ao = texture(ddepth, Texcoord).r; // emssive if(normal.x == 0.0f &amp;&amp; normal.y == 0.0f &amp;&amp; normal.z == 0.0f) &#123; fragColor.rgb = albedo; // glow map. float brightness = dot(fragColor.rgb, vec3(0.2126, 0.7152, 0.0722)); brightColor = vec4(fragColor.rgb, 1.0f); // write depth to buffer for forwarding shading. gl_FragDepth = depth; return; &#125; // some directions. vec3 viewDir = normalize(cameraPos - FragPos); // index of refracted. vec3 F0 = vec3(0.04); F0 = mix(F0, albedo, metallic); // *******************************directional light************************************ vec3 lightDir = dirLight.direction; vec3 halfwayDir = normalize(lightDir + viewDir); // fresnel factor. vec3 fresnel = fresnelSchlick(max(dot(halfwayDir, viewDir), 0.0f), F0); // normal distribution factor. float distribution = NormalDistributionGGX(normal, halfwayDir, roughness); // geometry facror. float geometryFactor = GeometrySmith(normal, viewDir, lightDir, roughness); // brdf function. vec3 brdf = distribution * fresnel * geometryFactor / (4.0f * max(dot(viewDir, normal), 0.0f) * max(dot(lightDir, normal), 0.0f) + 0.0001f); vec3 kSpecular = fresnel; vec3 kDiffuse = vec3(1.0f) - kSpecular; kDiffuse *= (1.0f - metallic); // rendering equation. fragColor.rgb = (kDiffuse * albedo / PI + brdf) * dirLight.radiance * max(dot(normal, lightDir), 0.0f); // ************************************************************************************ // *************************************point lights*********************************** vec3 pointLightRadiance = vec3(0.0f); for(int i = 0;i &lt; pointLightNum;++ i) &#123; vec3 lightDir = normalize(pointLight[i].position - FragPos); vec3 halfwayDir = normalize(viewDir + lightDir); float distance = length(pointLight[i].position - FragPos); if(distance &gt; pointLight[i].radius) continue; float attenuation = 1.0f / (lightAttenuationCoff * distance * distance + 0.00001); vec3 radiance = pointLight[i].radiance * attenuation; vec3 fresnel = fresnelSchlick(max(dot(halfwayDir, viewDir), 0.0f), F0); // normal distribution factor. float distribution = NormalDistributionGGX(normal, halfwayDir, roughness); // geometry facror. float geometryFactor = GeometrySmith(normal, viewDir, lightDir, roughness); // brdf function. vec3 brdf = distribution * fresnel * geometryFactor / (4.0f * max(dot(viewDir, normal), 0.0f) * max(dot(lightDir, normal), 0.0f) + 0.0001f); vec3 kSpecular = fresnel; vec3 kDiffuse = vec3(1.0f) - kSpecular; kDiffuse *= (1.0f - metallic); // rendering equation. pointLightRadiance += (kDiffuse * albedo / PI + brdf) * radiance * max(dot(normal, lightDir), 0.0f); &#125; // ************************************************************************************ // shadow float shadow = 1.0f; vec4 FragPosLightSpace = lightSpaceMatrix * vec4(FragPos, 1.0f); shadow = 1.0f - shadowCalculation(FragPosLightSpace, 0.0f); fragColor.xyz = ao * albedo * 0.02f + fragColor.xyz * shadow + pointLightRadiance; // glow map. float brightness = dot(fragColor.rgb / (fragColor.rgb + vec3(1.0f)), vec3(0.2126, 0.7152, 0.0722)); if(brightness &gt; 0.55f) brightColor = vec4(fragColor.rgb / (fragColor.rgb + vec3(1.0f)), 1.0f); // write depth to buffer for forwarding shading. gl_FragDepth = depth;&#125;float NormalDistributionGGX(vec3 N, vec3 H, float roughness)&#123; float a = roughness * roughness; float aSquared = a * a; float NdotH = max(dot(N, H), 0.0f); float NdotHSquared = NdotH * NdotH; float nom = aSquared; float denom = (NdotHSquared * (aSquared - 1.0f) + 1.0f); denom = PI * denom * denom; return nom / denom;&#125;float GeometrySchlickGGX(float NdotV, float roughness)&#123; float r = (roughness + 1.0f); float k = (r * r) / 8.0f; float nom = NdotV; float denom = NdotV * (1.0f - k) + k; return nom / denom;&#125;float GeometrySmith(vec3 N, vec3 V, vec3 L, float roughness)&#123; float NdotV = max(dot(N, V), 0.0f); float NdotL = max(dot(N, L), 0.0f); float ggx2 = GeometrySchlickGGX(NdotV, roughness); float ggx1 = GeometrySchlickGGX(NdotL, roughness); return ggx2 * ggx1;&#125;vec3 fresnelSchlick(float cosTheta, vec3 F0)&#123; return F0 + (1.0f - F0) * pow(1.0 - cosTheta, 5.0f);&#125;float shadowCalculation(vec4 fragPosLightSpace, float bias)&#123; // perspective division. vec3 projCoords = fragPosLightSpace.xyz / fragPosLightSpace.w; projCoords = projCoords * 0.5 + 0.5; if(projCoords.z &gt; 1.0) return 0.0f; // pcf. float shadowFactor = 0.0f; float currentDepth = projCoords.z; vec2 texelSize = 1.0 / textureSize(shadowDepth, 0); for(int x = -1; x &lt;= 1; ++x) &#123; for(int y = -1; y &lt;= 1; ++y) &#123; float pcfDepth = texture(shadowDepth, projCoords.xy + vec2(x, y) * texelSize).r; shadowFactor += ((currentDepth - bias) &gt; pcfDepth) ? 1.0 : 0.0; &#125; &#125; shadowFactor /= 9.0; return shadowFactor;&#125; 四、屏幕空间环境光遮蔽&emsp;&emsp;本文前面主要介绍了PBR的直接光照，这意味着在没有被光源直接照亮的区域，依然没有产生符合物理规律的光影效果，这是因为我们还没有考虑间接光照。在实时应用中，为了实现物体的相互遮蔽效果，通常采用SSAO（即Screen Space Ambient Occlusion），实际上这是一个比较tricky的做法，但是产生的效果非常不错。 &emsp;&emsp;SSAO采用的原理非常简单，：对于每一个片段，我们都会根据周边深度值计算一个遮蔽因子(Occlusion Factor)。这个遮蔽因子之后会被用来减少或者抵消片段的环境光照分量。遮蔽因子是通过采集片段周围球型核心(Kernel)的多个深度样本，并和当前片段深度值对比而得到的。高于片段深度值样本的个数就是我们想要的遮蔽因子。正如下图8所示。到这里文章篇幅有点太长了，SSAO也比较简单，因此我就不再赘述了。SSAO因子计算的核心代码： 图8 Occlusion Factor 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#version 430 corein vec2 Texcoord;uniform vec3 samples[64];uniform mat4 viewMatrix;uniform mat4 projectMatrix;uniform sampler2D dposition;uniform sampler2D dnormal;uniform sampler2D ddepth;uniform sampler2D randomNoise;uniform float farPlane;uniform float nearPlane;const float radius = 10.0f;const int sampleNum = 64;void main()&#123; // sample position. vec3 FragPos = texture(dposition, Texcoord).rgb; // sample normal. vec3 normal = texture(dnormal, Texcoord).rgb; // sample depth. float depth = texture(ddepth, Texcoord).r; // sample random vectors. vec2 depthTextureSize = textureSize(ddepth, 0); vec2 noiseTextureSize = textureSize(randomNoise, 0); vec2 noiseTexScale = vec2(depthTextureSize.x / noiseTextureSize.x, depthTextureSize.y / noiseTextureSize.y); vec3 randomVec = texture(randomNoise, Texcoord * noiseTexScale).rgb; vec3 tangent = normalize(randomVec - normal * dot(randomVec, normal)); vec3 bitangent = cross(normal, tangent); mat3 TBNMatrix = mat3(tangent, bitangent, normal); // calculate ambient occlusion. float occlusion = 0.0f; for(int i = 0;i &lt; sampleNum;++i) &#123; // change to world space. vec3 samplePoint = TBNMatrix * samples[i]; samplePoint = FragPos + samplePoint * radius; // change to view space. samplePoint = vec3(viewMatrix * vec4(samplePoint, 1.0f)); // change to ndc space &amp; screen space. vec4 tmp = vec4(samplePoint, 1.0f); tmp = projectMatrix * tmp; tmp.xyz /= tmp.w; tmp.xyz = tmp.xyz * 0.5f + 0.5f; // get sample point's depth. float sampleDepth = texture(ddepth, tmp.xy).r; samplePoint.z /= -farPlane; // range check and accumulate. float rangeCheck = smoothstep(0.0, 1.0, radius / (abs(depth - sampleDepth) * farPlane)); occlusion += (sampleDepth &gt; samplePoint.z ? 0.0 : 1.0) * rangeCheck; &#125; occlusion /= sampleNum; occlusion = 1.0f - occlusion; gl_FragDepth = occlusion;&#125; &emsp;&emsp;SSAO对于场景的真实感觉有着非常重要的作用，可能我们平时不会太过注意，但是却又是一个非常关键的点。下面左边就是计算得到的AO因子，最后将AO因子的乘上物体的反照率以及环境光缩放系数即可。 图9 ao因子计算结果 五、实现效果&emsp;&emsp;除了PBR、SSAO，其他如延迟渲染、HDR、Glow Effect、因子等不再赘述。 参考资料：$[1]$ https://learnopengl.com/PBR/Theory $[2]$ https://learnopengl.com/PBR/Lighting $[3]$ https://learnopengl.com/Advanced-Lighting/Deferred-Shading $[4]$ https://learnopengl.com/Advanced-Lighting/SSAO","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Real-time Rendering","slug":"Real-time-Rendering","permalink":"https://yangwc.com/about/categories/Real-time-Rendering/"},{"name":"Physically Based Rendering","slug":"Physically-Based-Rendering","permalink":"https://yangwc.com/about/categories/Physically-Based-Rendering/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Real-time Rendering","slug":"Real-time-Rendering","permalink":"https://yangwc.com/about/tags/Real-time-Rendering/"},{"name":"Physically Based Rendering","slug":"Physically-Based-Rendering","permalink":"https://yangwc.com/about/tags/Physically-Based-Rendering/"}]},{"title":"流体渲染Fluid Rendering：基于屏幕空间的液体渲染","slug":"FluidRendering","date":"2019-07-04T13:13:24.745Z","updated":"2019-07-14T09:18:21.952Z","comments":true,"path":"2019/07/04/FluidRendering/","link":"","permalink":"https://yangwc.com/about/2019/07/04/FluidRendering/","excerpt":"本文主要是关于屏幕空间的液体渲染算法，分别介绍了高斯滤波、双边滤波和曲率流方法平滑流体深度的方法。","text":"本文主要是关于屏幕空间的液体渲染算法，分别介绍了高斯滤波、双边滤波和曲率流方法平滑流体深度的方法。 基于屏幕空间的流体渲染 水体的光照计算 流体深度贴图的平滑处理 实现效果 参考资料 基于屏幕空间的流体渲染 &emsp;&emsp;在计算机图形学中，我们最后都要把模拟的物体渲染出来，这是图形学的最终目的。而目前对于流体渲染，无论是基于拉格朗日视角的还是基于欧拉视角的流体模拟都要经过流体表面重建这一步，然后再做进一步的光照着色计算。流体表面重建有个非常经典的算法——Marching Cubes$^{[2]}$，该方法采用水平集（Level Set），首先将空间划分称均匀的立方体网格，然后计算每个网格上8个顶点的密度。对于每一个立方体，如果立方体上的一条边两个端点的密度值大于给定的一个阈值$\\rho_{boundary}$，则这条边上存在着一个流体表面上的顶点。最后将每个立方体构造的多边形拼接，即可得到流体的表面网格。该方法基于这样的一个事实：流体表面处的密度应该等于某个固定的值，流体表面是一个三维的密度等高面，密度值为$\\rho_{boundary}$，这就是水平集的思想。 图1 Marching Cube的15种模式 &emsp;&emsp;Marching Cube是流体表面重建的传统做法，实现效果非常不错，但是算法的时间复杂度大，重建一次需要花费不少的时间。对于流动的流体来说，需要每帧构建流体表面，因而很难保证实时性。除了Marching Cube这类传统的流体表面重建方法，还有一些技巧性比较强的方法适合实时性应用，基于屏幕空间的流体渲染方法就是这一类。基于屏幕空间的流体渲染以一种新的思路角度展开流体的渲染，这种方法对并行友好，不涉及到直接对液体表面网格的重建，实现也相对简单。 一、基于屏幕空间的流体渲染&emsp;&emsp;首先介绍一些基于屏幕空间的流体渲染算法纵览。与在世界空间构建流体表面网格的思路不同，基于屏幕空间的流体渲染算法直接在屏幕空间做流体表面的复原工作，操作维度从三维降到了二维。这是一种与图形处理紧密结合的渲染算法。算法分为两个步骤，重点在于第一个步骤。第一个步骤是屏幕空间流体处理步骤，首先将流体粒子按照当前的投影矩阵和视图矩阵渲染到屏幕空间，获取流体粒子的深度轮廓信息、流体厚度信息，这些信息存储到纹理中，紧接着我们对存储深度信息的纹理做一些后处理操作，模糊掉球形粒子的坑坑洼洼，使得深度信息更为平滑，最后我们根据深度纹理和厚度纹理做流体的光照计算。 图2 Screen Space Fluid Rendering算法总览 &emsp;&emsp;算法的总流程如图2所示，其中的噪声图像生成不是必须的，实际上我觉得加上这个噪声处理反而是个败笔。对于水体来说，它的表面通常是比较光滑的而不是坑坑洼洼。背景图用于处理流体的折射计算，因此流体通常应该放到最后渲染。该算法可以看成是针对流体的延迟渲染，流体的表面法线信息并不能直接获得，需要从深度贴图中重建法线，而在这之前深度贴图也要经过一些特殊的处理。基于屏幕空间的流体渲染步骤中，如何对深度贴图做处理使之能真实反映流体的特性是算法的核心，剩下的光照部分直接采用Blin-Phong光照模型，并综合考虑水体的折射和反射，上面提到的厚度贴图用于水体的折射计算，因为越厚的流体其透光率越低。 二、水体的光照计算&emsp;&emsp;首先我们先实现一个不对深度贴图和厚度贴图做处理的Naive版的流体渲染算法，这有利于我们弄清整个算法的思路，而且使得实现的过程更加清晰、有条理、便于调试。去掉对深度贴图和厚度贴图的处理部分（噪声部分我们不考虑），剩下的算法流程就分为：渲染不包含流体的场景纹理、渲染粒子深度信息、渲染粒子厚度信息、光照着色计算。这里我们采用OpenGL的render to target，将场景、深度贴图、厚度贴图都渲染到纹理当中，供最后的光照着色计算使用。 &emsp;&emsp;一开始我们为了获取流体的深度信息和厚度信息，需要流体以球形粒子的形态进行绘制，这里我们采用OpenGL的点精灵Point Sprite做粒子绘制。在OpenGL中，点精灵Point Sprite就是一个内建的始终朝向摄像机视角的正方形，我们可以指定它的大小，但是它的大小是定义在屏幕空间的。因此，在利用点精灵绘制流体粒子时，一方面我们要根据世界空间的流体粒子大小设置点精灵的大小，另一方面要以将四边形变成圆形。 &emsp;&emsp;首先根据世界空间的流体粒子大小设置点精灵的大小，上面说了，点精灵的大小是定义在屏幕空间的，而通常我们设置的流体粒子大小是世界空间的。为了正确设置点精灵的大小，使之符合透视原理，需要计算世界空间的长度投影到屏幕空间的长度，这个比较基础，根据三角形相似的原理即可。 1234567891011121314151617181920212223242526// calculate particle size scale factor.float aspect = camera-&gt;getAspect();float fovy = camera-&gt;getFovy();float pointScale = 1.0f * m_screenWidth / aspect * (1.0f / tanf(glm::radians(fovy) * 0.5f));......shader-&gt;setFloat(\"pointScale\", pointScale);shader-&gt;setFloat(\"pointSize\", m_particleRadius);......#version 430 corelayout (location = 0) in vec4 position;uniform mat4 modelMatrix;uniform mat4 viewMatrix;uniform mat4 projectMatrix;uniform float pointScale;uniform float pointSize;out vec3 eyeSpacePos;void main()&#123; eyeSpacePos = (viewMatrix * modelMatrix * vec4(position.xyz, 1.0f)).xyz; gl_PointSize = -pointScale * pointSize / eyeSpacePos.z; gl_Position = projectMatrix * viewMatrix * modelMatrix * vec4(position.xyz, 1.0f);&#125; &emsp;&emsp;然后就是让正方形的点精灵变成一个圆形。在片元着色器中，点精灵提供了一个gl_PointCoord变量，这是一个内建的专用于点精灵的纹理坐标，已经经过光栅化插值之后的点精灵纹理坐标。我们根据这个坐标来获取每一个点精灵的像素所在的相对位置，gl_PointCoord纹理坐标以左上角为原点，我们需要将其变换到点精灵中心。接着获取每个像素的法线信息，并计算半径长度，半径超过1.0的像素我们给它discard掉。 12345678910111213141516#version 430 coreuniform float pointSize;uniform mat4 projectMatrix;in vec3 eyeSpacePos;void main()&#123; vec3 normal; normal.xy = gl_PointCoord.xy * vec2(2.0, -2.0) + vec2(-1.0,1.0); float mag = dot(normal.xy, normal.xy); if(mag &gt; 1.0) discard; normal.z = sqrt(1.0 - mag); ......&#125; &emsp;&emsp;点精灵给我们提供一个非常方便的球心粒子绘制方法，这种方法绘制的球体是基于解析解的，精度非常高，球体非常光滑，而如果采用球体网格则需要绘制很多个网格，球体精度受限于网格的精度，速度也慢。现在知道如何根据点精灵绘制，接下来我们需要从流体粒子中获取深度贴图和厚度贴图。 1、流体深度贴图&emsp;&emsp;绘制流体深度贴图需要开启深度测试，并创建一个帧缓冲，深度的信息绘制到给定帧缓冲的深度缓冲当中。 12345678910111213141516171819202122232425262728293031323334353637383940void LiquidDrawable::drawLiquidDepth(Camera3D::ptr camera, Light::ptr sunLight, Camera3D::ptr lightCamera, Shader::ptr shader)&#123; m_framebuffer-&gt;bind(); // calculate particle size scale factor. float aspect = camera-&gt;getAspect(); float fovy = camera-&gt;getFovy(); float pointScale = 1.0f * m_screenWidth / aspect * (1.0f / tanf(glm::radians(fovy) * 0.5f)); // render state. glEnable(GL_DEPTH_TEST); glDisable(GL_BLEND); glClear(GL_DEPTH_BUFFER_BIT); glEnable(GL_PROGRAM_POINT_SIZE); glEnable(GL_VERTEX_PROGRAM_POINT_SIZE); // shader. shader = m_shaderMgr-&gt;getShader(\"liquidDepth\"); shader-&gt;bind(); shader-&gt;setFloat(\"farPlane\", camera-&gt;getFar()); shader-&gt;setFloat(\"nearPlane\", camera-&gt;getNear()); shader-&gt;setFloat(\"pointScale\", pointScale); shader-&gt;setFloat(\"pointSize\", m_particleRadius); shader-&gt;setFloat(\"densityLowerBound\", m_densityLowerBound); shader-&gt;setMat4(\"modelMatrix\", m_transformation.getWorldMatrix()); shader-&gt;setMat4(\"viewMatrix\", camera-&gt;getViewMatrix()); shader-&gt;setMat4(\"projectMatrix\", camera-&gt;getProjectMatrix()); // draw glBindVertexArray(m_particleVAO); glDrawArrays(GL_POINTS, 0, m_numParticles); glBindVertexArray(0); // restore. m_shaderMgr-&gt;unBindShader(); glDisable(GL_PROGRAM_POINT_SIZE); m_framebuffer-&gt;unBind();&#125; &emsp;&emsp;在片元着色器，我们需要计算像素的深度信息，为了还原出粒子的深度信息，我们首先将粒子在摄像机空间的位置传到片元着色器中，这个其实就是点精灵在摄像机空间的中心点。然后计算每个像素的法线，每个像素在摄像机空间的点就等于$eyeSpacePos + normal * pointSize$，其中$pointSize$就是流体粒子的半径大小。得到每个像素在摄像机空间的点，我们再将其乘上投影矩阵，并做透视除法，得到标准化设备空间$[-1,+1]$的深度值。 123456789101112131415161718192021222324252627282930313233343536373839404142============vertex shader================#version 430 corelayout (location = 0) in vec4 position;uniform mat4 modelMatrix;uniform mat4 viewMatrix;uniform mat4 projectMatrix;uniform float pointScale;uniform float pointSize;uniform float densityLowerBound;out vec3 eyeSpacePos;void main()&#123; eyeSpacePos = (viewMatrix * modelMatrix * vec4(position.xyz, 1.0f)).xyz; gl_PointSize = -pointScale * pointSize / eyeSpacePos.z; // to prevent single or a few particles. if(position.w &lt; densityLowerBound) gl_PointSize = 0.0f; gl_Position = projectMatrix * viewMatrix * modelMatrix * vec4(position.xyz, 1.0f);&#125; ============fragment shader================#version 430 coreuniform float pointSize;uniform mat4 projectMatrix;in vec3 eyeSpacePos;void main()&#123; vec3 normal; normal.xy = gl_PointCoord.xy * vec2(2.0, -2.0) + vec2(-1.0,1.0); float mag = dot(normal.xy, normal.xy); if(mag &gt; 1.0) discard; normal.z = sqrt(1.0 - mag); vec4 pixelEyePos = vec4(eyeSpacePos + normal * pointSize, 1.0f); vec4 pixelClipPos = projectMatrix * pixelEyePos; float ndcZ = pixelClipPos.z / pixelClipPos.w; gl_FragDepth = ndcZ;&#125; &emsp;&emsp;经过上面的一个pass，我们得到了流体的深度信息。下面是一张近看的流体粒子深度贴图，为什么要近看？因为投影矩阵对深度信息做了一个非线性变换，这个非线性变换使得深度值密度在靠近1.0这一端，稍微远一点，就全白看不见了。（所以如果渲染出来一张全白的深度贴图，不要着急，拉近看看） 图3 流体深度贴图 2、流体厚度贴图&emsp;&emsp;然后我们需要获取流体的厚度贴图，获取厚度贴图不是必须的，这是因为厚度贴图是为了流体折射计算服务。有些流体并不透光（如牛奶），所以也不存在折射。与渲染流体深度贴图一样，我们以将流体粒子以球形的点精灵形态进行绘制。给定一个方向，流体厚度衡量在这个方向上有多少流体粒子，越多越厚。因此，为了计算流体的厚度，我们采用了OpenGL的blending技巧，也就是透明融合，每个流体粒子计算各自贡献的厚度值，然后将其输出到颜色缓冲中，借助OpenGL的透明融合，将厚度值累积起来，这样就能得到从摄像机方向看去的流体厚度信息。此外，由于我们是采用球形点精灵绘制一个流体粒子，一个流体粒子，其厚度贡献值从中心到边缘应该逐渐减少，为此我们采用计算得到的normal的z分量作为厚度值，并乘上一个缩放系数。具体如下所示： 123456789101112131415#version 430 coreuniform float pointSize;uniform mat4 projectMatrix;layout(location = 0) out vec4 fragColor;void main()&#123; vec3 normal; normal.xy = gl_PointCoord.xy * vec2(2.0, -2.0) + vec2(-1.0,1.0); float mag = dot(normal.xy, normal.xy); if(mag &gt; 1.0) discard; normal.z = sqrt(1.0 - mag); fragColor = vec4(normal.z*0.005, 0.0, 0.0, 1.0);&#125; &emsp;&emsp;在CPU端，我们关闭深度测试，开启透明融合，并设置融合函数为additive blending，即加法融合。所谓加法融合，就是原像素值和目标像素值直接相加，不乘上任何的缩放系数。在OpenGL的加法融合就是设置glBlendFunc的参数均为GL_ONE、GL_ONE。这样不同流体粒子之间的厚度值直接叠加，达到我们所需的效果。同时为了凸显厚度，我们设当地设大一点粒子的大小。 123456789101112131415161718192021222324252627282930313233343536373839404142void LiquidDrawable::drawLiquidThick(Camera3D::ptr camera, Light::ptr sunLight, Camera3D::ptr lightCamera, Shader::ptr shader)&#123; m_framebuffer-&gt;bind(); // calculate particle size scale factor. float aspect = camera-&gt;getAspect(); float fovy = camera-&gt;getFovy(); float pointScale = m_screenWidth / aspect * (1.0f / tanf(glm::radians(fovy) * 0.5f)); // render state. glEnable(GL_BLEND); glBlendFunc(GL_ONE, GL_ONE); glDepthMask(GL_FALSE); glEnable(GL_DEPTH_TEST); glDisable(GL_DEPTH_TEST); glEnable(GL_PROGRAM_POINT_SIZE); glEnable(GL_VERTEX_PROGRAM_POINT_SIZE); glClearColor(0.0, 0.0, 0.0, 1.0); glClear(GL_COLOR_BUFFER_BIT); // shader. shader = m_shaderMgr-&gt;getShader(\"liquidThick\"); shader-&gt;bind(); shader-&gt;setFloat(\"pointScale\", pointScale); shader-&gt;setFloat(\"pointSize\", 4.0f * m_particleRadius); shader-&gt;setMat4(\"modelMatrix\", m_transformation.getWorldMatrix()); shader-&gt;setMat4(\"viewMatrix\", camera-&gt;getViewMatrix()); shader-&gt;setMat4(\"projectMatrix\", camera-&gt;getProjectMatrix()); // draw glBindVertexArray(m_particleVAO); glDrawArrays(GL_POINTS, 0, m_numParticles); glBindVertexArray(0); // restore. glDepthMask(GL_TRUE); glDisable(GL_BLEND); glDisable(GL_PROGRAM_POINT_SIZE); m_shaderMgr-&gt;unBindShader(); m_framebuffer-&gt;unBind();&#125; 图4 流体厚度贴图 &emsp;&emsp;图4就是渲染出来的流体厚度贴图，越红的地方越厚。 3、水体折射、Blin-Phong光照&emsp;&emsp;在前面的步骤我们获取了两张纹理贴图：流体深度纹理、流体厚度纹理。然后我们需要根据这两张纹理计算水体的光照效果。前面已经提到过，这是一种类似于延迟渲染的技术，光照计算都是在屏幕空间的四边形上进行。我们首先要根据流体的深度纹理信息重建流体表面的法线向量。在屏幕空间中，我们已知当前像素的纹理坐标，和对应的流体深度信息，那么如何根据这些信息重建法线向量呢？这就涉及到了如何从设备标准空间ndc顶点转换到摄像机空间$^{[1]}$。 &emsp;&emsp;设摄像机空间的顶点坐标为$(vx,vy,vz,vw)$，其中$vw=1.0$，ndc空间的顶点坐标为$(nx,ny,nz)$，其中$x、y、z\\in[-1,+1]$。我们知道从摄像机空间变换到ndc空间，需要经过投影变换、透视除法这两个步骤： clip=projectMatrix \\cdot (vx,vy,vz,1.0)\\\\ (nx,ny,nz) = clip.xyz/clip.w \\tag {1}&emsp;&emsp;其中的clip就是裁剪空间的顶点坐标。所以我们的问题就是，已知ndc空间的$(nx,ny,nz)$、摄像机空间的$vw=1.0$以及投影变换矩阵，如何计算得到摄像机空间的$(vx,vy,vz)$顶点坐标。根据公式$(1)$，我们有： (vx,vy,vz,1.0)=(projectMatrix)^{-1}\\cdot clip \\tag {2}&emsp;&emsp;而根据ndc顶点坐标$(nx,ny,nz)$与裁剪空间顶点坐标$clip$的关系，我们又有： clip=(clip.xyz,clip.w)=((nx,ny,nz)\\cdot clip.w, clip.w)=clip.w \\cdot (nx,ny,nz,1.0) \\tag {3}&emsp;&emsp;联立公式$(2)$与公式$(3)$，我们有： (vx,vy,vz,1.0)=clip.w \\cdot (projectMatrix)^{-1}\\cdot (nx,ny,nz,1.0) \\tag {4}&emsp;&emsp;公式$(4)$直接表明了摄像机空间顶点坐标与ndc空间坐标之间的关系，其中只有$clip.w$是未知，这是一个关键点。我们注意到$vw=1.0$，也就是在摄像机空间，顶点的分量为1.0。这一点可以利用起来： clip.w\\cdot[(projectMatrix)^{-1}\\cdot (nx,ny,nz,1.0)].w = 1.0 \\\\ \\to\\\\ clip.w=\\frac{1.0}{[(projectMatrix)^{-1}\\cdot (nx,ny,nz,1.0)].w} \\tag {5}&emsp;&emsp;公式$(5)$意思就是$clip.w$等于ndc空间的坐标$(nx,ny,nz,1.0)$右乘逆投影矩阵所得向量的$w$分量的倒数。将其代入公式$(4)$中，我们可得： (vx,vy,vz,1.0)=\\frac{1.0}{[(projectMatrix)^{-1}\\cdot (nx,ny,nz,1.0)].w} \\cdot (projectMatrix)^{-1}\\cdot (nx,ny,nz,1.0)&emsp;&emsp;因此，从ndc空间的坐标到摄像机空间的坐标，我们首先计算$(projectMatrix)^{-1}\\cdot (nx,ny,nz,1.0)$，然后再将其$xyz$分量除以$w$分量即可。 1234567vec3 uvToEye(vec2 coord, float z)&#123; vec2 pos = coord * 2.0f - 1.0f; vec4 clipPos = vec4(pos, z, 1.0f); vec4 viewPos = invProjectMatrix * clipPos; return viewPos.xyz / viewPos.w;&#125; &emsp;&emsp;知道了摄像机空间的顶点坐标，紧接着我们需要根据顶点坐标重建顶点法线。法线向量可以根据沿着x方向和沿着y方向的偏导数做叉乘得到，这里的x方向和y方向均为屏幕空间中的纹理坐标方向。求偏导数可以根据有限差分法： \\frac{\\partial f}{\\partial s}\\approx\\frac{f(s+\\Delta s)-f(s)}{\\Delta s} \\tag {6}&emsp;&emsp;这里我们采用了单向差分法（前向差分法、后向差分法），没有采用中心差分法。为了避免计算出错误的法线向量，我们综合考虑前向差分法和后向差分法，取较小的那个差分值（差分值较大的很可能是在边界处与背景或距离比较远的流体深度值做了一个差分，这个时候得到的法线是错误的，因为深度值不连续）。我们对深度纹理图的周围四个像素分别做x方向和y方向的差分，$\\Delta s=1$。最后根据得到的偏导向量做叉乘得到法线。 123456789101112131415161718192021// -----------------reconstruct normal----------------------------vec2 depthTexelSize = 1.0 / textureSize(depthTex, 0);// calculate eye space position.vec3 eyeSpacePos = uvToEye(Texcoord, depth);// finite difference.vec3 ddxLeft = eyeSpacePos - uvToEye(Texcoord - vec2(depthTexelSize.x,0.0f), texture(depthTex, Texcoord - vec2(depthTexelSize.x,0.0f)).r);vec3 ddxRight = uvToEye(Texcoord + vec2(depthTexelSize.x,0.0f), texture(depthTex, Texcoord + vec2(depthTexelSize.x,0.0f)).r) - eyeSpacePos;vec3 ddyTop = uvToEye(Texcoord + vec2(0.0f,depthTexelSize.y), texture(depthTex, Texcoord + vec2(0.0f,depthTexelSize.y)).r) - eyeSpacePos;vec3 ddyBottom = eyeSpacePos - uvToEye(Texcoord - vec2(0.0f,depthTexelSize.y), texture(depthTex, Texcoord - vec2(0.0f,depthTexelSize.y)).r);vec3 dx = ddxLeft;vec3 dy = ddyTop;if(abs(ddxRight.z) &lt; abs(ddxLeft.z)) dx = ddxRight;if(abs(ddyBottom.z) &lt; abs(ddyTop.z)) dy = ddyBottom;vec3 normal = normalize(cross(dx, dy));vec3 worldPos = (invViewMatrix * vec4(eyeSpacePos, 1.0f)).xyz; &emsp;&emsp;下图5显示了重建得到的法线情况，需要特别注意的是，这里我们重建得到的法线是在摄像机空间而非世界空间，这是因为我们还原的顶点也是摄像机空间的，因此计算光照时需要将光线向量变换到摄像机空间。 图5 重建的法线 &emsp;&emsp;有了法线向量，接下来我们就计算水体折射、Blin-Phong光照。我这里只考虑水体折射，反射不考虑，因为一般清澈的水很少有反射现象。Beer-Lambert定律揭示了液体的光吸收现象。根据Beer-Lambert定律，液体的透光率随着液体的厚度增加呈指数衰减： I(d)=I_0\\cdot e^{-kd} \\tag {7}&emsp;&emsp;公式$(7)$中，$d$是液体的厚度值，$I_0$是光照强度rgb向量，$k$是液体的光能衰减因子向量，通常等于$1.0-liquidColor$。液体的透光率计算如下所示，厚度值从之前渲染得到的贴图中直接采样得到。 12float thickness = max(texture(thicknessTex, Texcoord).r, 0.3f);vec3 transmission = exp(-(vec3(1.0f) - liquidColor.xyz) * thickness); &emsp;&emsp;然后，我们还需要从背景纹理图中采样，因为我们要实现折射的效果。为了实现折射的效果，我们需要对采样的纹理坐标做一个偏移，使之产生折射的效果。 1234float refractScale = 1.33 * 0.025; // refracted index.refractScale *= smoothstep(0.1, 0.4, worldPos.y);vec2 refractCoord = Texcoord + normal.xy * refractScale;vec3 refractedColor = texture(backgroundTex, refractCoord).xyz * transmission; &emsp;&emsp;最后就是关于Blin-Phong光照部分，比较简单，不再赘述。唯一需要注意的是我们获取的法线是在摄像机空间的，需要将光的方向也变换到摄像机空间。 12345678910// -----------------Phong lighting----------------------------vec3 viewDir = -normalize(eyeSpacePos);vec3 lightDir = normalize((viewMatrix * vec4(dirLight.direction, 0.0f)).xyz);vec3 halfVec = normalize(viewDir + lightDir);vec3 specular = vec3(dirLight.specular * pow(max(dot(halfVec, normal), 0.0f), 400.0f));vec3 diffuse = liquidColor.xyz * max(dot(lightDir, normal), 0.0f) * dirLight.diffuse * liquidColor.w;// -----------------Merge all effect----------------------------fragColor.rgb = diffuse + specular + refractedColor;fragColor.a = 1.0f; &emsp;&emsp;实现出来的效果如下图6所示。可以看到，渲染的效果很差，液体看起来像是由很多粒果冻组成，流体的粒子感非常明显，这并不是我们想要的。产生果冻壮流体的根本原因是因为我们的法线向量并不平滑，仔细观察图5，我们重建后的法线还是原来的球面上的法线，这造成光照效果的失真。 图6 实现的果冻壮流体 &emsp;&emsp;因此，为了进一步提升渲染效果，我们要想办法使得流体表面得法线平滑。而法线来源于深度贴图，因为我们追溯到源头就是使得深度贴图尽可能地平滑。接下来我们将讨论在屏幕空间对深度贴图做的一些平滑后处理方案。 三、流体深度贴图的平滑处理&emsp;&emsp;我们现在需要对深度贴图做平滑处理，这其实属于图像处理的范畴。 1、采用高斯滤波平滑深度贴图&emsp;&emsp;首先是高斯模糊的平滑方案，我们把深度纹理当场一张图片，采用高斯权重做一个滤波操作。为了性能，同样我们将二维的高斯模糊分成水平方向和垂直方向两个pass。在之前实现辉光效果时已经介绍过，不再赘述。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 unsigned int DepthGaussianBlurFilter::blurTexture(unsigned int targetTexIndex, const glm::mat4 &amp;projectMat) &#123; m_framebuffer-&gt;bind(); glDepthMask(GL_TRUE); glEnable(GL_DEPTH_TEST); glClear(GL_DEPTH_BUFFER_BIT); Shader::ptr blurShader = ShaderMgr::getSingleton()-&gt;getShader(m_blurShaderIndex); blurShader-&gt;bind(); blurShader-&gt;setInt(\"image\", 0); TextureMgr::getSingleton()-&gt;bindTexture(targetTexIndex, 0); for (unsigned int index = 0; index &lt; 5; ++index) &#123; // horizontal blur. blurShader-&gt;setInt(\"horizontal\", 1); MeshMgr::getSingleton()-&gt;drawMesh(m_screenQuadIndex, false, 0); // copy to target texture. glCopyTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, 0, 0, m_width, m_height); // vertical blur. blurShader-&gt;setInt(\"horizontal\", 0); MeshMgr::getSingleton()-&gt;drawMesh(m_screenQuadIndex, false, 0); // copy to target texture. glCopyTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, 0, 0, m_width, m_height); &#125; TextureMgr::getSingleton()-&gt;unBindTexture(targetTexIndex); blurShader-&gt;unBind(); m_framebuffer-&gt;unBind(); return targetTexIndex; &#125;==============Fragment Shader==================#version 330 corein vec2 Texcoord;uniform int horizontal;uniform sampler2D image;const float weight[8] = float[] (0.197448, 0.174697, 0.120999, 0.065602, 0.02784, 0.009246, 0.002403, 0.000489);void main()&#123; // gets size of single texel. vec2 tex_offset = 1.0 / textureSize(image, 0); float result = texture(image, Texcoord).r * weight[0]; result += texture(image, Texcoord + vec2(tex_offset.x * 1 * horizontal, tex_offset.y * 1 * (1-horizontal))).r * weight[1]; result += texture(image, Texcoord - vec2(tex_offset.x * 1 * horizontal, tex_offset.y * 1 * (1-horizontal))).r * weight[1]; result += texture(image, Texcoord + vec2(tex_offset.x * 2 * horizontal, tex_offset.y * 2 * (1-horizontal))).r * weight[2]; result += texture(image, Texcoord - vec2(tex_offset.x * 2 * horizontal, tex_offset.y * 2 * (1-horizontal))).r * weight[2]; result += texture(image, Texcoord + vec2(tex_offset.x * 3 * horizontal, tex_offset.y * 3 * (1-horizontal))).r * weight[3]; result += texture(image, Texcoord - vec2(tex_offset.x * 3 * horizontal, tex_offset.y * 3 * (1-horizontal))).r * weight[3]; result += texture(image, Texcoord + vec2(tex_offset.x * 4 * horizontal, tex_offset.y * 4 * (1-horizontal))).r * weight[4]; result += texture(image, Texcoord - vec2(tex_offset.x * 4 * horizontal, tex_offset.y * 4 * (1-horizontal))).r * weight[4]; result += texture(image, Texcoord + vec2(tex_offset.x * 5 * horizontal, tex_offset.y * 5 * (1-horizontal))).r * weight[5]; result += texture(image, Texcoord - vec2(tex_offset.x * 5 * horizontal, tex_offset.y * 5 * (1-horizontal))).r * weight[5]; result += texture(image, Texcoord + vec2(tex_offset.x * 6 * horizontal, tex_offset.y * 6 * (1-horizontal))).r * weight[6]; result += texture(image, Texcoord - vec2(tex_offset.x * 6 * horizontal, tex_offset.y * 6 * (1-horizontal))).r * weight[6]; result += texture(image, Texcoord + vec2(tex_offset.x * 7 * horizontal, tex_offset.y * 7 * (1-horizontal))).r * weight[7]; result += texture(image, Texcoord - vec2(tex_offset.x * 7 * horizontal, tex_offset.y * 7 * (1-horizontal))).r * weight[7]; gl_FragDepth = result;&#125; &emsp;&emsp;经过高斯模糊之后的，重建得到的法线向量平滑了一下如下所示。 图7 高斯模糊平滑得到的法线 &emsp;&emsp;然后用高斯模糊平滑得到法线向量做光照着色，得到下面的效果。渲染得到的效果看起来明显比图6的果冻壮好很多了。 图8 高斯模糊-光照着色 &emsp;&emsp;然后高斯模糊的平滑方法存在一个问题，高斯模糊没有考虑深度值，它仅仅考虑当前像素到中心像素的距离。这就导致了高斯模糊会明显地模糊掉边界，使得边界与背景或较远处的流体看起来融合在一起了。如下图9所示，上面的这张图红框部分，流体的边界于后面的流体融合在了一起，看不出有一个边界在那里。下面这张图可以看到，流体是分开的，但是从上面的这个视角看起又是连在一起的。 图9 高斯滤波模糊掉了边界 3、采用双边滤波平滑深度贴图&emsp;&emsp;既然高斯模糊没有良好地保留边界，我们就选取一个能够保边去噪的滤波器，高斯双边滤波算法就是这样的一个滤波器。高斯模糊仅仅考虑了像素的空间分布，权重从中间向周边降低。而双边滤波则进一步考虑了图像的像素值，从而保证边缘部分不会被过滤掉。根据维基百科$^{[3]}$，一个双边滤波器定义为： I^{filtered}(x)=\\frac{1}{W_p}\\Sigma_{x_i\\in \\Omega}I(x_i)f_r(||I(x_i)-I(x)||)g_s(||x_i-x||) \\tag {8}&emsp;&emsp;其中，$I_{filtered}(x)$是过滤后的图像，$x$是被过滤的像素坐标，$\\Omega$是像素$x$的滤波核领域，$I(x)$是初始未被过滤的图像。然后$f_r$是域值权重函数，$g_r$是空间权重函数。而$W_p$是归一化因子，其计算方式如下： W_p=\\Sigma_{x_i\\in \\Omega}f_r(||I(x_i)-I(x)||)g_s(||x_i-x||) \\tag {9}&emsp;&emsp;可以看到$f_r$函数输入的是两个像素之间的差，而$g_s$输入的是两个像素坐标之间的差。考虑一个像素，其坐标为$(i,j)$，而其邻域像素的坐标为$(k,l)$，则计算与邻域像素$(k,l)$的滤波核函数为： w(i,j,k,l)=exp(-\\frac{(i-k)^2+(j-l)^2}{2\\sigma_d^2}-\\frac{||I(i,j)-I(k,l)||^2}{2\\sigma_r^2}) \\tag {10}&emsp;&emsp;公式$(10)$给出的核函数包含了$f_r$和$g_s$，其中左边部分就是$g_s$，而右边部分则为$f_r$。在片元着色器中，我实现的双边滤波函数如下所示。这是一个暴力的两重循环，因为图像的邻域是二维的。 1234567891011121314151617181920212223242526272829303132333435#version 330 corein vec2 Texcoord;uniform sampler2D image;uniform float filterRadius;const float blurScale = 0.05f;const float blurDepthFalloff = 500.0f;void main()&#123; // gets size of single texel. vec2 tex_offset = 1.0 / textureSize(image, 0); float sum = 0.0f; float wsum = 0.0f; float value = texture(image, Texcoord).r; for(float y = -filterRadius;y &lt;= filterRadius;y += 1.0f) &#123; for(float x = -filterRadius;x &lt;= filterRadius;x += 1.0f) &#123; float sample = texture(image, Texcoord + vec2(x, y) * tex_offset).r; // spatial domain. float r = length(vec2(x, y)) * blurScale; float w = exp(-r * r); // range domain. float r2 = (sample - value) * blurDepthFalloff; float g = exp(-r2 * r2); sum += sample * w * g; wsum += w * g; &#125; &#125; if(wsum &gt;= 0.0f) sum /= wsum; gl_FragDepth = sum;&#125; &emsp;&emsp;然后根据双边滤波平滑后的深度图重建的法线如下图10所示。 图10 双边滤波平滑深度重建的法线 &emsp;&emsp;由双边滤波平滑得到的法线做流体渲染如下图11，可以看到流体边界部分的保持得非常良好。 图11 双边滤波-保边良好 &emsp;&emsp;但是这里又出现了一个性能方面的问题。双边滤波并不能像高斯模糊一样，拆分成水平方向依次、垂直方向一次，这是因为双边滤波考虑了图像的像素内容。一个核直径为20的双边滤波，每个像素要处理$20\\times20=400$个邻域像素，这对于我的rtx2070显卡来说还好，但是在其他一些不那么好的显卡上会严重地拖慢渲染速度。Nvidia$^{[5]}$指出，可以强行将双边滤波分成两个pass，将$O(n^2)$的时间复杂度降到$O(n)$，一些失真可以勉强接受。然后我就尝试了将双边滤波分割成两个pass，水平方向和垂直方向各一次。效果如下图12所示，并不是非常好，在流体的一些边缘部分出现了拉伸的失真。 图12 双边分离滤波-边缘失真 4、采用曲率流平滑深度贴图&emsp;&emsp;由于双边滤波的不可分割性，Wladimir等人转向了另外一个不同的思路$^{[4]}$。我们的目标是寻转一个算法，能够平滑掉不同流体粒子之间的曲率突变，构建一个平滑、连续的流体表面。因此，一种思路就是最小化流体的曲率，这也跟流体的自然物理属性-流体表面张力相对应。我们称这个过程为曲率流（curvature flow）。 &emsp;&emsp;曲率流沿着表面法线方向扩展，其速度取决于表面平均曲率。在我们的这个流体渲染中，我们仅仅处理的是流体表面的深度。在一帧当中，视角固定了，我们同样可以通过沿着曲率修改深度值达到平滑的效果： \\frac{\\partial z}{\\partial t} = H \\tag {11}&emsp;&emsp;其中$t$是平滑时间步长，$z$就是我们的深度值，$H$就是流体表面的平均曲率，公式$(11)$意思是在每一次的平滑迭代中，深度值的变化梯度为流体表面的平均曲率。因此，我们首先要求流体表面的平均曲率，平均曲率的定义为表面单位法线的散度： 2H=\\nabla\\cdot \\overline n \\tag {12}&emsp;&emsp;给定屏幕空间的坐标以及深度值，视口宽高$V_x$、$V_y$，以及投影的x和y方向的焦距长（就是投影矩阵的mat[0][0]、mat[1][1]），我们得到摄像机空间顶点坐标与屏幕空间的x和y的关系： P(x,y)= \\left( \\begin{matrix} \\frac{\\frac{2x}{V_x}-1.0}{F_x}\\\\ \\frac{\\frac{2y}{V_y}-1.0}{F_y}\\\\ 1.0 \\end{matrix} \\right)z(x,y) = \\left( \\begin{matrix} W_x\\\\ W_y\\\\ 1.0 \\end{matrix} \\right)z(x,y) \\tag {13}&emsp;&emsp;然后法线向量由两个偏导叉乘得到： n(x,y)=\\frac{\\partial P}{\\partial x}\\times \\frac{\\partial P}{\\partial y} \\\\= \\left( \\begin{matrix} C_xz+W_x\\frac{\\partial z}{\\partial x}\\\\ W_y\\frac{\\partial z}{\\partial x}\\\\ \\frac{\\partial z}{\\partial x} \\end{matrix} \\right) \\times \\left( \\begin{matrix} W_x\\frac{\\partial z}{\\partial y}\\\\ C_yz+W_y\\frac{\\partial z}{\\partial y}\\\\ \\frac{\\partial z}{\\partial y} \\end{matrix} \\right)\\\\ \\approx \\left( \\begin{matrix} C_xz\\\\ 0\\\\ \\frac{\\partial z}{\\partial x} \\end{matrix} \\right) \\times \\left( \\begin{matrix} 0\\\\ C_yz\\\\ \\frac{\\partial z}{\\partial y} \\end{matrix} \\right) = \\left( \\begin{matrix} -C_y\\frac{\\partial z}{\\partial x}\\\\ -C_x\\frac{\\partial z}{\\partial y}\\\\ C_xC_yz \\end{matrix} \\right)z \\tag {14}&emsp;&emsp;其中$C_x=\\frac{2}{V_xF_x}$，$C_y=\\frac{2}{V_yF_y}$，公式$(14)$中我们忽略$W_x$和$W_y$是因为能够大大简化计算，且其贡献非常小。然后单位法线向量则为： \\overline n=\\frac{n(x,y)}{|n(x,y)|}=\\frac{(-C_y\\frac{\\partial z}{\\partial x},-C_x\\frac{\\partial z}{\\partial y},C_xC_yz)^T}{\\sqrt{D}} \\tag {15} D=C_y^2(\\frac{\\partial z}{\\partial x})^2+C_x^2(\\frac{\\partial z}{\\partial y})^2+C_x^2C_y^2z^2 \\tag {16}&emsp;&emsp;单位法线公式有了，现在回过头来看平均曲率的计算公式$(12)$，我们要求单位法线的散度。由于深度值z是关于屏幕空间坐标x和y的函数，因此$\\frac{\\partial \\overline n}{\\partial z}=0$，因为求偏导时x和y定为常数。故： 2H=\\frac{\\partial \\overline n_x}{\\partial x}+\\frac{\\partial \\overline n_y}{\\partial y} =\\frac{C_yE_x+C_xE_y}{D^{\\frac32}} \\tag {17} E_x=\\frac12\\frac{\\partial z}{\\partial x}\\frac{\\partial D}{\\partial x}-\\frac{\\partial^2z}{\\partial x^2}D \\tag {18} E_y=\\frac12\\frac{\\partial z}{\\partial y}\\frac{\\partial D}{\\partial y}-\\frac{\\partial^2z}{\\partial y^2}D \\tag {19}&emsp;&emsp;求得了平均曲率之后，我们就根据公式$(11)$采用简单的欧拉差分法修改深度值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#version 330 corein vec2 Texcoord;uniform float step;uniform sampler2D image;uniform mat4 projectMatrix;void main()&#123; float depth = texture(image, Texcoord).r; if(depth &gt;= 1.0f || depth &lt;= -1.0f) &#123; gl_FragDepth = depth; return; &#125; vec2 imageDim = textureSize(image, 0); vec2 texelSize = 1.0 / imageDim; // central differences. float depthRight = texture(image, Texcoord + vec2(texelSize.x, 0)).r; float depthLeft = texture(image, Texcoord - vec2(texelSize.x, 0)).r; float zdx = 0.5f * (depthRight - depthLeft); if(depthRight == 0.0f || depthLeft == 0.0f) zdx = 0.0f; float depthUp = texture(image, Texcoord + vec2(0, texelSize.y)).r; float depthDown = texture(image, Texcoord - vec2(0, texelSize.y)).r; float zdy = 0.5f * (depthUp - depthDown); if(depthUp == 0.0f || depthDown == 0.0f) zdy = 0.0f; float zdxx = depthRight + depthLeft - 2.0f * depth; float zdyy = depthUp + depthDown - 2.0f * depth; float depthFalloff = 0.00005f; if(abs(depth - depthRight) &gt; depthFalloff || abs(depth - depthLeft) &gt; depthFalloff) zdx = zdxx = 0.0f; if(abs(depth - depthDown) &gt; depthFalloff || abs(depth - depthUp) &gt; depthFalloff) zdy = zdyy = 0.0f; float Fx = projectMatrix[0][0]; float Fy = projectMatrix[1][1]; float Cx = -2.0f/(imageDim.x * Fx); float Cy = -2.0f/(imageDim.y * Fy); float D = Cy * Cy * zdx * zdx + Cx * Cx * zdy * zdy + Cx * Cx * Cy * Cy * depth; float Ex = 0.5f * zdx * dFdx(D) - zdxx * D; float Ey = 0.5f * zdy * dFdy(D) - zdyy * D; // curvature flow. float curvature = 0.5f * (Cy * Ex + Cx * Ey)/ pow(D, 1.5); if(curvature &gt; 1.0f) curvature = 1.0f; gl_FragDepth = depth + curvature * step;&#125; &emsp;&emsp;然后在CPU端设置迭代多次，注意平滑的时间步长不能设置得太长。 1234567891011121314151617181920212223242526272829unsigned int DepthCurvatureFlowBlurFilter::blurTexture(unsigned int targetTexIndex, const glm::mat4 &amp;projectMat)&#123; m_framebuffer-&gt;bind(); glDepthMask(GL_TRUE); glEnable(GL_DEPTH_TEST); glClear(GL_DEPTH_BUFFER_BIT); // blur. Shader::ptr blurShader = ShaderMgr::getSingleton()-&gt;getShader(m_blurShaderIndex); blurShader-&gt;bind(); blurShader-&gt;setInt(\"image\", 0); blurShader-&gt;setFloat(\"step\", 0.00070f); blurShader-&gt;setMat4(\"projectMatrix\", projectMat); TextureMgr::getSingleton()-&gt;bindTexture(targetTexIndex, 0); for (unsigned int iter = 0; iter &lt; m_iterations; ++iter) &#123; // blur. MeshMgr::getSingleton()-&gt;drawMesh(m_screenQuadIndex, false, 0); // copy to target texture. glCopyTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, 0, 0, m_width, m_height); &#125; TextureMgr::getSingleton()-&gt;unBindTexture(targetTexIndex); blurShader-&gt;unBind(); m_framebuffer-&gt;unBind(); return targetTexIndex;&#125; &emsp;&emsp;然而采用曲率流得方法我实现的效果并不是想象中的那么好，总体上不如采用双边滤波的方法，颗粒感较强。而且不知道为什么，流体的表面有一些抖动，看起来很奇怪，大概这就是买家秀跟买家秀的区别吧。 图13 采用曲率流的平滑效果 四、实现效果 参考资料：$[1]$ How to go from device coordinates back to worldspace in OpenGL $[2]$ W. E. Lorensen and H. E. Cline, “Marching cubes: A high resolution 3D surface construction algorithm,” in Proceedings of the 14th annual conference on Computer graphics and interactive techniques - SIGGRAPH ’87, 1987, pp. 163–169. $[3]$ Bilateral filter. From Wikipedia, the free encyclopedia $[4]$ W. J. van der Laan, S. Green, and M. Sainz, “Screen space fluid rendering with curvature flow” in Proceedings of the 2009 symposium on Interactive 3D graphics and games - I3D ’09, 2009, p. 91. $[5]$ Screen Space Fluid Rendering for Games - Nvidia","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/categories/Fluid-Simulation/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"https://yangwc.com/about/categories/Position-Based-Dynamics/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/tags/Fluid-Simulation/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"https://yangwc.com/about/tags/Position-Based-Dynamics/"}]},{"title":"流体模拟Fluid Simulation：基于CUDA的PBF实现","slug":"PBF_CUDA","date":"2019-06-26T02:54:32.694Z","updated":"2019-07-04T13:09:54.671Z","comments":true,"path":"2019/06/26/PBF_CUDA/","link":"","permalink":"https://yangwc.com/about/2019/06/26/PBF_CUDA/","excerpt":"继之前实现了CPU端的PBF算法之后，我去学了CUDA编程模型，本文就是关于用CUDA实现PBF算法在GPU上高速地模拟。","text":"继之前实现了CPU端的PBF算法之后，我去学了CUDA编程模型，本文就是关于用CUDA实现PBF算法在GPU上高速地模拟。 基于GPU的空间哈希 基于CUDA的PBF流体模拟 程序结果 参考资料 基于CUDA的PBF流体模拟 一、基于GPU的空间哈希&emsp;&emsp;在之前我们讨论了N-body的GPU并行实现，实际上这是一种比较极端的例子。在N-body系统中，每个粒子的都要与剩下的所有粒子计算两两之间的相互作用，这种相互作用通常有穿透力（如万有引力，称为体积力），无论远近、大小。但是在一些其他的物理模拟中，粒子只与周围邻近的粒子产生相互作用，如刚体、流体等，这时距离当前粒子比较远的其他粒子不会对当前的粒子产生任何的影响，因为刚体、流体之间的相互作用通常需要接触之后才产生（极端的例子不考虑，如万有引力，因为通常考虑的质量太小，万有引力几乎为0），所以我们没有必要采用N-body的方法逐个计算剩下的所有例子与当前粒子的物理作用，因为这会大大增加冗余计算。 &emsp;&emsp;对于流体、刚体、软体等的物理交互模拟，我们通常都是考虑的局部相互作用，这种局部相互作用要么就是接触才产生，要么就是随着距离的增大而迅速减小至消失。因此在这类的物理模拟中，我们需要采用一种快速的算法，该算法获取周围邻近的粒子，用以后续的物理模拟计算。这种算法就是空间哈希，在前面用CPU实现PBF时我们已经讨论过，它将整个空间做一个分割，每个粒子映射到一个空间数据结构，寻找邻域粒子时直接搜索周围空间的存储列表，算法复杂度只有$O(N)$。但是在GPU上实现该类算法稍微麻烦了点，因为GPU上的数据结构通常只有线性表，同时还要慎重考虑内存访问的开销等。 &emsp;&emsp;在GPU上实现该类算法有两种思路，先说说第一种方案。第一种方案是直接存储均匀分割的所有空间，为每个空间单元预先申请一个固定的大小的存储空间，这样需要提前申请固定大小的显卡内存空间，而且通常非常大，申请之后每个粒子计算哈希值索引，根据索引将其存储到对应的空间单元，由于流体和刚体通常是聚集的，因为这样将导致大部分的显卡内存空间都是空置状态，浪费了大量的显存空间，存储方式比较稀疏。 图1 方案一 &emsp;&emsp;如上图1的二维示例所示，方案一申请两个线性表，大小均为空间分割的分辨率，图中分割空间为$4\\times 4=16$个。一个线性表为Count，记录当前的空间单元存在的粒子数目（为了避免写冲突，必须调用CUDA的原子操作指令atomicAdd），另外一个线性表记录每个空间单元中的粒子索引。当然我们也可以采用两个pass将粒子存储到一个连续的空间中，充分利用存储空间，但这需要两个pass，第一个pass计算每个空间单元的粒子数，然后第二个pass采用前缀和的方法计算每个空间单元存储的粒子的起始地址，最后将粒子连续地存储到线性表中。这个方法和接下来提到的采用排序的方法非常类似。 &emsp;&emsp;第二种方法采用排序将所有的粒子存储到紧凑的连续空间，节省大量的显存开销。如下图2所示，首先我们根据粒子的位置计算粒子的对应的空间哈希值，在这里我们目前只是简单地将粒子对应的线性编号作为它的哈希值cell id，然后将cell id和particle id这对数据存储线性数组中，其中我们要存储particle id是因为我们计算当前的cell id的时是根据粒子的位置来确定的，在后面我们需要用到这个对应关系，相当于一个索引。根据上面的步骤我们就得到了一个cell id乱序的线性表，接下来我们就根据这个cell id对整个数组做一个并行排序，使得数组的顺序是以cell id的顺序来排列的。这个过程相当于一个基数排序，因为一开始数组是以particle id为序的。得到这个以cell id为序的数组之后，我们需要记录每个空间单元cell记录的粒子起始地址和结束地址。判断起始地址很简单，只需将当前的cell id与数组的前一个cell id做比较，若不相同，则说明当前的粒子地址是当前粒子所在空间单元cell的起始地址，而且是其前一个粒子所在空间单元cell的终止地址。这样就能准确地记录每个空间单元的粒子。 图2 方案二 &emsp;&emsp;这里我们采用方案二的做法。首先需要计算每个粒子的空间hash值，前面已经说过，我们直接采用粒子的所在空间单元的线性编号，注意防止越界访问内存。下面的kernel代码calcGridPosKernel计算粒子所在空间单元的三维编号，接着calcGridHashKernel根据这个三维编号计算一维编号，最后将粒子索引id和哈希值存入两个对应的数组。 123456789101112131415161718192021222324252627282930313233343536373839__device__int3 calcGridPosKernel(float3 p)&#123; int3 gridPos; gridPos.x = floor((p.x - params.m_worldOrigin.x) / params.m_cellSize.x); gridPos.y = floor((p.y - params.m_worldOrigin.y) / params.m_cellSize.y); gridPos.z = floor((p.z - params.m_worldOrigin.z) / params.m_cellSize.z); return gridPos;&#125;__device__unsigned int calcGridHashKernel(int3 gridPos)&#123; gridPos.x = gridPos.x &amp; (params.m_gridSize.x - 1); gridPos.y = gridPos.y &amp; (params.m_gridSize.y - 1); gridPos.z = gridPos.z &amp; (params.m_gridSize.z - 1); return gridPos.z * params.m_gridSize.x * params.m_gridSize.y + gridPos.y * params.m_gridSize.x + gridPos.x;&#125;__global__void calcParticlesHashKernel( unsigned int *gridParticleHash, unsigned int *gridParticleIndex, float4 *pos, unsigned int numParticles)&#123; unsigned int index = blockIdx.x * blockDim.x + threadIdx.x; if (index &gt;= numParticles) return; volatile float4 curPos = pos[index]; int3 gridPos = calcGridPosKernel(make_float3(curPos.x, curPos.y, curPos.z)); unsigned int hashValue = calcGridHashKernel(gridPos); gridParticleHash[index] = hashValue; gridParticleIndex[index] = index;&#125; &emsp;&emsp;紧接着，我们需要对前面计算得到的两个数组做一个key-value排序，就是根据key的顺序来排列。同样为了效率，需要使用并行排序。CUDA提供了一个thrust库，直接调用库中的sort_by_key方法帮我们省去了这一个比较繁琐的工作。 1234567891011void sortParticles( unsigned int *deviceGridParticleHash, unsigned int *deviceGridParticleIndex, unsigned int numParticles)&#123; thrust::sort_by_key( thrust::device_ptr&lt;unsigned int&gt;(deviceGridParticleHash), thrust::device_ptr&lt;unsigned int&gt;(deviceGridParticleHash + numParticles), thrust::device_ptr&lt;unsigned int&gt;(deviceGridParticleIndex));&#125; &emsp;&emsp;前面对cell id和particle id排序之后，我们需要对存储粒子位置和速度属性的数组做一个相应的调整，使得其顺序与前面拍好的顺序一一对应。与此同时，还需要计算每个空间单元cell的起始地址和终止地址。这里我们充分利用同一个线程块的共享内存，设线程数有$n$个，那么我们申请$n+1$个单位大小的共享内存，每个线程首先将自己对应的那个粒子哈希值存储到共享内存中，第一个线程还要将其前一个粒子对应的哈希值存储到该共享内存中，这样可以避免每个线程访问全局内存2次（共2n次共享内存的访问），访问全局内存数变为了n+1次。然后每个粒子线程将当前对应的哈希值与其前一个粒子哈希值做比较，若不相同，则表明当前线程的index是空间单元cell的起始地址，终止地址同理。具体过程看如下的代码。我们设置cellStart初始为0xffffffff来表示它是一个空的cell单元，即没有任何的粒子落到该空间单元cell中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980__global__void reoderDataAndFindCellRangeKernel( unsigned int *cellStart, // output: cell start index unsigned int *cellEnd, // output: cell end index float4 *sortedPos, // output: sorted positions float4 *sortedVel, // output: sorted velocities unsigned int *gridParticleHash, // input: sorted grid hashes unsigned int *gridParticleIndex, // input: sorted particle indices float4 *oldPos, // input: not sorted position array float4 *oldVel, // input: not sorted velocity array unsigned int numParticles)&#123; thread_block cta = this_thread_block(); extern __shared__ unsigned int sharedHash[]; unsigned int index = blockIdx.x * blockDim.x + threadIdx.x; unsigned int hashValue; if (index &lt; numParticles) &#123; hashValue = gridParticleHash[index]; sharedHash[threadIdx.x + 1] = hashValue; // first thread in block must load neighbor particle hash if (index &gt; 0 &amp;&amp; threadIdx.x == 0) sharedHash[0] = gridParticleHash[index - 1]; &#125; sync(cta); if (index &lt; numParticles) &#123; if (index == 0 || hashValue != sharedHash[threadIdx.x]) &#123; cellStart[hashValue] = index; if (index &gt; 0) cellEnd[sharedHash[threadIdx.x]] = index; &#125; if (index == numParticles - 1) cellEnd[hashValue] = index + 1; unsigned int sortedIndex = gridParticleIndex[index]; float4 pos = oldPos[sortedIndex]; float4 vel = oldVel[sortedIndex]; sortedPos[index] = pos; sortedVel[index] = vel; &#125;&#125;void reorderDataAndFindCellRange( unsigned int *cellStart, unsigned int *cellEnd, float *sortedPos, float *sortedVel, unsigned int *gridParticleHash, unsigned int *gridParticleIndex, float *oldPos, float *oldVel, unsigned int numParticles, unsigned int numCell)&#123; unsigned int numThreads, numBlocks; numThreads = 256; numBlocks = (numParticles % numThreads != 0) ? (numParticles / numThreads + 1) : (numParticles / numThreads); // set all cell to empty. cudaMemset(cellStart, 0xffffffff, numCell * sizeof(unsigned int)); unsigned int memSize = sizeof(unsigned int) * (numThreads + 1); reoderDataAndFindCellRangeKernel &lt;&lt; &lt;numBlocks, numThreads, memSize &gt;&gt; &gt; ( cellStart, cellEnd, (float4*)sortedPos, (float4*)sortedVel, gridParticleHash, gridParticleIndex, (float4*)oldPos, (float4*)oldVel, numParticles);&#125; &emsp;&emsp;以上就是基于GPU的空间哈希过程，经过以上的步骤，粒子被紧凑地存储到线性空间，后面做物理计算时能快速地得到邻域空间的粒子。下面代码是整个基于GPU的空间哈希调用代码。 123456789101112131415161718192021222324252627// calculate grid Hash.computeHash( m_deviceGridParticleHash, m_deviceGridParticleIndex, m_devicePos, m_params.m_numParticles);// sort particles based on hash value.sortParticles( m_deviceGridParticleHash, m_deviceGridParticleIndex, m_params.m_numParticles);// reorder particle arrays into sorted order// and find start index and end index of each cell.reorderDataAndFindCellRange( m_deviceCellStart, m_deviceCellEnd, m_deviceSortedPos, m_deviceSortedVel, m_deviceGridParticleHash, m_deviceGridParticleIndex, m_devicePos, m_deviceVel, m_params.m_numParticles, m_params.m_numGridCells); 二、基于CUDA的PBF流体模拟&emsp;&emsp;在之前我们实现了CPU的PBF流体模拟，受限于CPU的低并行度，我们只能实时模拟数量很少的流体粒子。为了能够快速模拟大量的粒子，我特意去学了CUDA，接下来就用CUDA实现之前讨论的PBF算法。暂时不用刚体粒子来实现流体碰撞边界。首先我们回顾一下之前提到的PBF（Position Based Fluid）算法，算法的伪代码如下所示。基于PBD的流体模拟算法大致可以分成几个部分：流体粒子对流、领域粒子搜索、不可压缩的压力约束投影、更新速度、涡轮修复、粘度计算以及最后的粒子位置更新。可以看到，第5行到第7行邻域粒子搜索的实现在本文的前面部分已经讨论过了，所以就不再赘述了。 \\begin{align} &1.\\ forall\\ \\ particles\\ \\ i\\ \\ do\\\\ &2.\\ \\ \\ \\ \\ apply\\ \\ force\\ \\ v_i\\leftarrow v_i+\\Delta tf_{ext}(x_i)\\\\ &3.\\ \\ \\ \\ \\ predict\\ \\ position\\ \\ x_i^*\\leftarrow x_i+\\Delta t v_i\\\\ &4.\\ endfor\\\\ &5.\\ forall\\ \\ particles\\ \\ i\\ \\ do\\\\ &6.\\ \\ \\ \\ \\ find\\ \\ neighboring\\ \\ particles\\ \\ N_i(x_i^*)\\\\ &7.\\ endfor\\\\ &8.\\ while\\ \\ iter\\ \\","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/categories/Fluid-Simulation/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"https://yangwc.com/about/categories/Position-Based-Dynamics/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/tags/Fluid-Simulation/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"https://yangwc.com/about/tags/Position-Based-Dynamics/"}]},{"title":"多体问题N-body：基于CUDA的快速N-body模拟","slug":"NbodySimulation","date":"2019-06-20T05:17:42.095Z","updated":"2019-06-26T02:51:59.924Z","comments":true,"path":"2019/06/20/NbodySimulation/","link":"","permalink":"https://yangwc.com/about/2019/06/20/NbodySimulation/","excerpt":"本篇文章主要是关于用cuda实现天体的N-body系统，以及粒子渲染的辉光效果。相关的完整代码请看这个链接。","text":"本篇文章主要是关于用cuda实现天体的N-body系统，以及粒子渲染的辉光效果。相关的完整代码请看这个链接。 天体的N-body系统 基于CUDA的N-body实现 粒子渲染-辉光特效 参考资料 基于CUDA的快速N-body模拟 &emsp;&emsp;N-body问题（或者说N体问题），是一个常见的物理模拟问题。在N-body系统中，每个粒子体都会与剩下的其他粒子产生交互作用（交互作用因具体问题而异），从而产生相应的物理现象。天体模拟就是一个非常经典的N-body系统，根据牛顿的万有引力定律，宇宙中的不同天体之间会产生相互作用的吸引力，吸引力根据两个天体之间的质量和距离的不同而各不相同，一个天体的运动轨迹最终取决于剩下的所有的天体对该天体的引力的合力。除了天体系统之外，N-body模拟在其他计算学科中也是常客。例如模拟蛋白质折叠现象就是通过计算N-body之间的静电和范德华力，**计算机图形学中的湍流流体的流动模拟和全局光照明计算都涉及到N-body问题的求解**。 &emsp;&emsp;这周主要学习CUDA（全称为Computer Unified Device Architecture），它是一个NVIDIA的GPU编程模型，搞图形学不免常要与GPU打交道，所以非常有必要学习这个统一的GPU编程框架。N-body问题是计算机图形学中物理模拟的常见问题，在此我采用CUDA实现了一个天体星系的N-body模拟系统，充分利用GPU的并行能力去加速N-body的巨额计算过程。下面这张图就是我实现的天体星系模拟效果。 图1 天体星系的N-body模拟 1、天体的N-body系统&emsp;&emsp;一种最简单的求解N-body的方法就是暴力法，被称为all-pairs法，它直接计算一个粒子体与剩下的所有粒子体的相互作用，对每一个粒子都做类似的处理，这样可以确保每个粒子体都与剩下的所有粒子体都产生交互作用，这种方法简单、暴力，但是算法的复杂度非常高，达到了$O(N^2)$量级，当模拟的N-body系统有$N=10000$个粒子时，算法就需要处理1亿次的相互作用计算。显然对于非常庞大的N-body系统，直接使用暴力法将非常耗时，因而通常不是简单地采用该算法。all-pairs法通常与一种基于长距离力的远场近似法结合，目前此类形式的算法包括Barnes-Hut法$^{[1]}$、快速多极法$^{[2]}$和粒子网格法$^{[3]}$等等。 &emsp;&emsp;上面提到的几种加速方法最耗时的部分依旧是all-pairs部分，因此这是一个非常关键的部分，如果能够加速这一部分，那么模拟的速度将大大提升。因此，目前我们只关注all-pairs算法部分，而且不是在CPU上实现该算法的串行，而是接用CUDA编程模型实行一个GPU并行的快速版本。接下来先介绍一下天体星系的N-body系统。 &emsp;&emsp;天体星系模拟主要考虑的是万有引力。给定$N$个天体，我们记每个天体$0\\leq i&lt;N$的位置向量为$x_i$、速度向量为$v_i$、质量为$m_i$，根据牛顿的万有引力定律，任意两个不同天体$i$和$j$之间的万有引力计算公式如下所示： f_{ij}=G\\frac{m_im_j}{||r_{ij}||^2}\\cdot \\frac{r_{ij}}{||r_{ij}||} \\tag {1}&emsp;&emsp;公式$(1)$中的$r_{ij}=x_j-x_i$为从天体$i$到$j$的一个方向向量，故其模长为两者之间的距离。$G$是万有引力常数。上面这个公式可能看起来跟我们高中时学的万有引力公式略有不同，这是因为高中时还没有将力是一个矢量这个概念显示地表达出来，公式$(1)$中的$\\frac{r_{ij}}{||r_{ij}||}$是一个从$i$到$j$的单位方向向量。上述公式描述的是天体$j$对天体$i$的引力，那么除$i$之外的所有天体对天体$i$的引力合力的计算公式为： F_i=\\Sigma_{0\\leq j","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"N-body","slug":"N-body","permalink":"https://yangwc.com/about/categories/N-body/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"N-body","slug":"N-body","permalink":"https://yangwc.com/about/tags/N-body/"},{"name":"Glow effect","slug":"Glow-effect","permalink":"https://yangwc.com/about/tags/Glow-effect/"}]},{"title":"体素化Voxelization：基于GPU的三维体素化","slug":"Voxelization","date":"2019-06-11T09:28:14.040Z","updated":"2019-06-20T04:58:59.100Z","comments":true,"path":"2019/06/11/Voxelization/","link":"","permalink":"https://yangwc.com/about/2019/06/11/Voxelization/","excerpt":"本篇文章主要是关于三维网格模型的基于GPU并行的体素化算法，这个算法我是偶然从NVIDIA官网上看到的。基于GPU的体素化算法巧妙地借助了渲染流程的光栅化处理，将整个体素化的过程并行化，速度极快，缺点就是占用的内存较高。相关的完整代码请看这个链接中的Renderer目录下的Voxelization.h文件和Voxelization.cpp文件。","text":"本篇文章主要是关于三维网格模型的基于GPU并行的体素化算法，这个算法我是偶然从NVIDIA官网上看到的。基于GPU的体素化算法巧妙地借助了渲染流程的光栅化处理，将整个体素化的过程并行化，速度极快，缺点就是占用的内存较高。相关的完整代码请看这个链接中的Renderer目录下的Voxelization.h文件和Voxelization.cpp文件。 体素化 修补裂缝 修补孔洞 参考资料 &emsp;&emsp;在基于位置动力学的物理模拟中，所有要模拟的物体都由一组粒子来表示，每个粒子都是一个给定半径大小的球体，对于固体这类的物体，粒子通常是紧密相连的。为此，为了实现基于位置动力学的物理模拟，我们需要采用一种算法将网格物体的三角网格模型用一个个粒子表示，这个并不是简单地取网格模型的所有顶点就行，因为我们需要紧密连接的粒子，面片网格模型的顶点通常是稀疏的。这个过程其实就是体素化，三维体素是二维像素的三维扩展，体素的基本单元不再是二维的正方形，而是三维的立方体，立方体的边长决定了体素化的分辨率，通常边长越长，则分辨率越低。将网格体素化后我们得到了一组体素的中心顶点位置，可将其用于后续的基于位置动力学的物理模拟当中。 &emsp;&emsp;目前常用的体素化方法大都是基于CPU的，这类方法通常是将射线与物体求交，根据是奇数个交点还是偶数个交点来判断当前的体素是否在物体的内部。在没有采用特殊的数据结构时，每次求交都要遍历一次网格模型的所有三角形，效率非常低。在采用了八叉树加速之后，速度有所提升，但随着模型的三角形面片数增加，串行的体素化算法耗费的时间越来越长。我没有采用CPU串行的体素化方法，而是采用了基于GPU并行的体素化算法，这个算法我是偶然从NVIDIA官网上看到的。基于GPU的体素化算法巧妙地借助了渲染流程的光栅化处理，将整个体素化的过程并行化，速度极快，缺点就是占用的内存较高。 图1 三维体素模型 一、体素化&emsp;&emsp;基于GPU的三维体素化大致思想就是：首先计算出需要体素化模型的AABB包围盒，然后将模型投影到AABB包围盒的某个平面上，经过渲染管线的光栅化插值操作，我们可以在片元着色器得到每个像素点对应的世界空间的顶点坐标，根据这个顶点坐标标记三维空间数组（这个三维空间数组就是根据体素划分的空间序列）的相应位置，最后在CPU端读出这个三维空间数组，若当前的数组位置有标记，则将该数组位置对应的立方体作为一个体素。可以看到，整个流程思路非常清晰，但是还需要借助一些手段修正算法存在的缺陷，这个在后面会提到。 &emsp;&emsp;首先就是计算网格模型的AABB包围盒，在导入模型时获取$x$、$y$、$z$轴分量的最大值和最小值，从而得到包围盒的最大顶点和最小顶点。这个比较简单，不再赘述： 12345678910111213// bounding box.if (mesh-&gt;mVertices[x].x &lt; m_min.x) m_min.x = mesh-&gt;mVertices[x].x;if (mesh-&gt;mVertices[x].y &lt; m_min.y) m_min.y = mesh-&gt;mVertices[x].y;if (mesh-&gt;mVertices[x].z &lt; m_min.z) m_min.z = mesh-&gt;mVertices[x].z;if (mesh-&gt;mVertices[x].x &gt; m_max.x) m_max.x = mesh-&gt;mVertices[x].x;if (mesh-&gt;mVertices[x].y &gt; m_max.y) m_max.y = mesh-&gt;mVertices[x].y;if (mesh-&gt;mVertices[x].z &gt; m_max.z) m_max.z = mesh-&gt;mVertices[x].z; 图2 模型包围盒 &emsp;&emsp;获取了模型的包围盒之后，我们就需要根据这个包围盒设置我们的观察角度和投影平面，这关系到后面的体素化结果。同时为了保证正确地体素化模型，我们采用的投影方式是正交投影。首先我们要选择一个观察方向和投影平面，AABB包围盒有六个面，其中前和后、上和下、左和右的投影结果是一样的，因此实际的选择只有三个平面，分别是前、上、右（或者后、下、左）。显然一个物体投影到这个三个平面上的结果都不一样，目前我们暂时先选择投影到前面这个平面上，摄像机的视线朝向z轴的负方向。注意正确地设置摄像机的位置，否则什么看不到。既然我们选择投影到前面这个平面上，我们就设置摄像机的位置在包围盒前面这个平面的中心再往前一点。同时了为了确保模型全部投影到屏幕上，我们设置的正交投影平面比选定的包围盒平面稍微大一点点。具体代码如下所示： 图3 三个面上的投影结果 12345678910111213141516171819// bounding box and resolution.glm::vec3 min, max;glm::ivec3 resolution;target-&gt;getAABB(min, max);glm::vec3 range(max.x - min.x, max.y - min.y, max.z - min.z);resolution.x = static_cast&lt;int&gt;(range.x / step);resolution.y = static_cast&lt;int&gt;(range.y / step);resolution.z = static_cast&lt;int&gt;(range.z / step);int length = static_cast&lt;int&gt;(resolution.x * resolution.y * resolution.z);// cameraglm::vec3 cameraPos;cameraPos.x = (min.x + max.x) * 0.5f;cameraPos.y = (min.y + max.y) * 0.5f;cameraPos.z = max.z + 0.2f;FPSCamera::ptr camera(new FPSCamera(cameraPos));camera-&gt;lookAt(glm::vec3(0.0f, 0.0f, -1.0f));camera-&gt;setOrthographicProject(-range.x * 0.51, +range.x * 0.51, -range.y * 0.51, +range.y * 0.51, 0.1, range.z * 1.2f + 0.2f); 图4 包围盒投影平面 &emsp;&emsp;设置好投影矩阵和视图矩阵之后，我们需要申请一个着色器可写的缓冲，这个缓冲的大小等于AABB包围盒的分辨率，在片元着色器阶段我们需要根据当前片元的世界空间位置对这个缓冲做标记，表示该缓冲位置上有一个体素。我们采用OpenGL的GL_SHADER_STORAGE_BUFFER，这是一个着色器可读写的缓冲类型。申请缓冲之后，将缓冲全部初始化为0。然后将需要体素化的网格模型送入渲染管线进行渲染。在片元着色器中，将每个片元的世界空间位置对应的缓冲位置加1。最后在CPU端读出缓冲内容，缓冲值大于0时，则表示该位置有一个体素。CPU端的整个流程代码如下所示。这里需要特别注意的是，我们应该关闭深度测试和背面剔除，保证模型的全面三角形都进入片元着色器，确保所有的三角形不被剔除，从而使得全部的三角形都被处理，最后得到正确的体素化结果。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788bool Voxelization::voxelize(Drawable* target, const float &amp; step, std::vector&lt;glm::vec3&gt;&amp; ret)&#123; // shader ShaderMgr::ptr shaderMgr = ShaderMgr::getSingleton(); unsigned int voxelizeCount = shaderMgr-&gt;loadShader(\"voxelizeCount\", \"./glsl/voxelizeCount.vert\", \"./glsl/voxelizeCount.frag\"); Shader::ptr shader = shaderMgr-&gt;getShader(voxelizeCount); // bounding box and resolution. glm::vec3 min, max; glm::ivec3 resolution; target-&gt;getAABB(min, max); glm::vec3 range(max.x - min.x, max.y - min.y, max.z - min.z); resolution.x = static_cast&lt;int&gt;(range.x / step) + 1; resolution.y = static_cast&lt;int&gt;(range.y / step) + 1; resolution.z = static_cast&lt;int&gt;(range.z / step) + 1; int length = static_cast&lt;int&gt;(resolution.x * resolution.y * resolution.z); // camera glm::vec3 cameraPos; cameraPos.x = (min.x + max.x) * 0.5f; cameraPos.y = (min.y + max.y) * 0.5f; cameraPos.z = max.z + 0.2f; FPSCamera::ptr camera(new FPSCamera(cameraPos)); camera-&gt;lookAt(glm::vec3(0.0f, 0.0f, -1.0f)); camera-&gt;setOrthographicProject(-range.x * 0.51, +range.x * 0.51, -range.y * 0.51, +range.y * 0.51, 0.1, range.z * 1.2f + 0.2f); // polygon mode. glPolygonMode(GL_FRONT_AND_BACK, GL_FILL); glDisable(GL_CULL_FACE); glDisable(GL_DEPTH_TEST); glClearColor(1.0f, 0.0f, 0.0f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); // generate ssbo. glGenBuffers(1, &amp;m_cntBuffer); glBindBuffer(GL_SHADER_STORAGE_BUFFER, m_cntBuffer); glBufferData(GL_SHADER_STORAGE_BUFFER, length * sizeof(int), nullptr, GL_STATIC_DRAW); glBindBufferBase(GL_SHADER_STORAGE_BUFFER, 0, m_cntBuffer); // bind shader and ssbo. shader-&gt;bind(); shader-&gt;setVec3(\"boxMin\", min); shader-&gt;setFloat(\"step\", step); shader-&gt;setVec3(\"resolution\", resolution); int *writePtr = reinterpret_cast&lt;int*&gt;(glMapBuffer(GL_SHADER_STORAGE_BUFFER, GL_WRITE_ONLY)); for (int x = 0; x &lt; length; ++x) &#123; writePtr[x] = 0; &#125; if (!glUnmapBuffer(GL_SHADER_STORAGE_BUFFER)) std::cout &lt;&lt; \"unMap error\\n\" &lt;&lt; std::endl; // draw and count. target-&gt;render(camera, nullptr, nullptr, shader); glMemoryBarrier(GL_SHADER_STORAGE_BARRIER_BIT); // get count buffer. glBindBuffer(GL_SHADER_STORAGE_BUFFER, m_cntBuffer); int *readPtr = reinterpret_cast&lt;int*&gt;(glMapBuffer(GL_SHADER_STORAGE_BUFFER, GL_READ_ONLY)); if (readPtr != nullptr) &#123; for (int x = 0; x &lt; length; ++x) &#123; if (*(readPtr + x) != 0) &#123; int iy = x / (resolution.x * resolution.z); int iz = (x - iy * resolution.x * resolution.z) / (resolution.x); int ix = x - iy * resolution.x * resolution.z - iz * resolution.x; ret.push_back(min + glm::vec3(ix * step, iy * step, iz * step)); &#125; &#125; &#125; else &#123; std::cout &lt;&lt; \"nullptr error!\\n\"; &#125; glUnmapBuffer(m_cntBuffer); glBindBuffer(GL_SHADER_STORAGE_BUFFER, 0); glDeleteBuffers(1, &amp;m_cntBuffer); return false;&#125; &emsp;&emsp;接下来就需要在着色器中做一些操作。首先是顶点着色器，在顶点着色器中并没有什么复杂的操作，我们需要将当前的顶点位置传到片元着色器，借助渲染管线的光栅化功能，从而在片元着色器中得到每个片元对应的世界空间位置。下面顶点着色器的代码，其余部分乘上视图矩阵和投影矩阵就不说了。 123456789101112#version 430 corelayout (location = 0) in vec3 position;out vec3 FragPos;uniform mat4 viewMatrix;uniform mat4 projectMatrix;void main()&#123; FragPos = position; gl_Position = projectMatrix * viewMatrix * vec4(position,1.0f);&#125; &emsp;&emsp;中间经过光栅化处理，我们在片元着色器得到每个片元的世界空间坐标。根据这个世界空间的坐标去索引计数缓冲，注意这里采用了GLSL的原子操作函数atmoicAdd，避免GPU线程之间的写冲突。缓冲下标索引的计算基本就是根据体素的大小和包围盒来确定。 12345678910111213141516171819202122#version 430 corein vec3 FragPos;layout (std430, binding = 0) buffer CountBuffer&#123; int cnts[];&#125;;uniform float step;uniform vec3 boxMin;uniform vec3 resolution;out vec4 color;void main()&#123; int x = int((FragPos.x - boxMin.x)/step); int y = int((FragPos.y - boxMin.y)/step); int z = int((FragPos.z - boxMin.z)/step); int index = int(y * (resolution.z * resolution.x) + z * resolution.x + x); atomicAdd(cnts[index], 1); color = vec4(0.0,1.0,0.0,1.0);&#125; &emsp;&emsp;然后下面就是我实现的体素化效果，每个体素用一个立方体绘制，当然也可以用球体绘制。看起来颇有游戏《我的世界》的风格。 二、修补裂缝&emsp;&emsp;上面的实现效果看起来貌似非常不错，但是却存在一个非常严重的问题。前面我们在选择投影平面的时候固定投影在了z轴方向的包围盒平面，这是问题产生的根源。因为模型的每个三角形面片在每个包围盒投影面上的投影结果都不同，若当前的三角形与选取的投影面垂直，那么三角形投影到平面上的将是一条直线，这丢失了很多信息，从而导致裂缝的产生。 图5 不同投影平面的体素化结果 &emsp;&emsp;图5中，左图选取的投影面是摄像机在右边，朝向坐标，这时光栅化得到的结果很好，因而体素化的结果也很好。但是右边的这张图选取的投影面是摄像机在上面，朝向下边，这时光栅化得到的几何面片较少，很多相邻的位置都被投影到了一个片元像素，一些地方没有被体素化，从而导致了裂缝的产生！下面是我实现的程序产生的裂缝，选取的投影方向是z轴方向，下图中的红框部分的几何面片几乎平行于xz平面，从而导致投影光栅化产生的是一个被“压缩“的结果。由于裂缝非常明显且几乎必然会产生（因为通常模型都很复杂，三角形面片朝向很随机），因此有必要采取一些措施来修补这些裂缝。 图6 根据前面步骤产生的裂缝 &emsp;&emsp;如前面的图3所示，每个三角形面片在不同包围盒投影面上的投影结果不同，根据三角形的朝向不同，投影到平面上的三角形大小也各不相同。裂缝产生的原因就是因为投影到平面上的三角形面积被”压缩“了，因此我们需要选取一个投影方向，在该投影方向上三角形的投影面积最大，这样就能够确保所有的三角形面片被充分地体素化，从而使得裂缝小时。 图7 分别投影到包围盒的右、上、前平面上 &emsp;&emsp;因此，我们首先创建三个投影摄像机，将物体分别投影到沿着$x$、$y$、$z$轴的平面上，如图7所示，用以后面着色器中根据三角形的投影面积做选择。代码如下： 1234567891011121314151617181920212223242526272829303132// Camerasfloat offset = 0.2f;glm::vec3 cameraPosZ, cameraPosX, cameraPosY;// looking along z axis.cameraPosZ.x = (min.x + max.x) * 0.5f;cameraPosZ.y = (min.y + max.y) * 0.5f;cameraPosZ.z = max.z + offset;FPSCamera::ptr cameraZ(new FPSCamera(cameraPosZ));cameraZ-&gt;lookAt(glm::vec3(0.0f, 0.0f, -1.0f), Camera3D::LocalUp);cameraZ-&gt;setOrthographicProject(-range.x * 0.51, +range.x * 0.51, -range.y * 0.51, +range.y * 0.51, 0.1, range.z * 1.2f + offset);// looking along x axis.cameraPosX.x = max.x + offset;cameraPosX.y = (min.y + max.y) * 0.5f;cameraPosX.z = (min.z + max.z) * 0.5f;FPSCamera::ptr cameraX(new FPSCamera(cameraPosX));cameraX-&gt;lookAt(glm::vec3(-1.0f, 0.0f, 0.0f), Camera3D::LocalUp);cameraX-&gt;setOrthographicProject(-range.z * 0.51, +range.z * 0.51, -range.y * 0.51, +range.y * 0.51, 0.1, range.x * 1.2f + offset);// looking along y axis.cameraPosY.x = (min.x + max.x) * 0.5f;cameraPosY.y = max.y + offset;cameraPosY.z = (min.z + max.z) * 0.5f;FPSCamera::ptr cameraY(new FPSCamera(cameraPosY));cameraY-&gt;lookAt(glm::vec3(0.0f, -1.0f, 0.0f), glm::vec3(0, 1.0, 0.001));cameraY-&gt;setOrthographicProject(-range.x * 0.51, +range.x * 0.51, -range.z * 0.51, +range.z * 0.51, 0.1, range.y * 1.2f + offset);......shader-&gt;setMat4(\"viewProject[0]\", cameraX-&gt;getProjectMatrix() * cameraX-&gt;getViewMatrix());shader-&gt;setMat4(\"viewProject[1]\", cameraY-&gt;getProjectMatrix() * cameraY-&gt;getViewMatrix());shader-&gt;setMat4(\"viewProject[2]\", cameraZ-&gt;getProjectMatrix() * cameraZ-&gt;getViewMatrix()); &emsp;&emsp;接下来我们将用到几何着色器，几何着色器阶段在顶点着色器之后、光栅化之前，它根据给定的输入图元和输出图元进行相关的几何图元操作，正好我们可以接用它来根据三角形的投影面积选择采用哪一个投影相机。这里有一个技巧，直观上我们说是根据三角形的投影面积来渲染采用哪个投影相机，实际上没有必要真正地去计算三角形的投影面积，我们可以直接根据当前三角形的世界空间法线朝向来决定投影方向。举个例子，当法线向量的x分量比其余两个分量大时，则当前的三角形肯定投影到x轴方向的投影平面上的面积更大。更深入的理解：设法线向量为$n=(nx,ny,nz)$，我们将法线向量$n$与$(1,0,0)$、$(0,1,0)$、$(0,0,1)$分别做点乘，结果为$nx$、$ny$、$nz$，而法线向量分别与该三个基向量点乘的意义为法线向量在$x$、$y$、$z$轴上的投影值，该值越大则三角形投影到该平面上的面积也越大。所以，我们直接根据最大的法线分量来选择采用哪个投影相机。如下所示： 12345678910111213141516171819202122uint selectViewProject()&#123; vec3 p1 = gl_in[1].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 p2 = gl_in[2].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 faceNormal = cross(p1, p2); float nDX = abs(faceNormal.x); float nDY = abs(faceNormal.y); float nDZ = abs(faceNormal.z); if( nDX &gt; nDY &amp;&amp; nDX &gt; nDZ ) &#123; return 0; &#125; else if( nDY &gt; nDX &amp;&amp; nDY &gt; nDZ ) &#123; return 1; &#125; else &#123; return 2; &#125;&#125; &emsp;&emsp;然后我们将上述的代码应用到我们的几何着色器中，因为视图投影过程挪到了几何着色器阶段，所以顶点着色器直接输入顶点的位置，不做任何变换。几何着色器设置输入图元为三角形，输出图元为最大顶点数为3的三角形带，设置一个viewProject的uniform数组。通过几何着色器，我们对模型的每个三角形面片都做了一个投影选择的处理。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// vertex shader#version 430 corelayout (location = 0) in vec3 position;void main()&#123; gl_Position = vec4(position, 1.0f);&#125;// geometry shader#version 430 corelayout (triangles) in;layout (triangle_strip, max_vertices = 3) out;out vec3 FragPos;uniform mat4 viewProject[3];uint selectViewProject()&#123; vec3 p1 = gl_in[1].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 p2 = gl_in[2].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 faceNormal = cross(p1, p2); float nDX = abs(faceNormal.x); float nDY = abs(faceNormal.y); float nDZ = abs(faceNormal.z); if( nDX &gt; nDY &amp;&amp; nDX &gt; nDZ ) &#123; return 0; &#125; else if( nDY &gt; nDX &amp;&amp; nDY &gt; nDZ ) &#123; return 1; &#125; else &#123; return 2; &#125;&#125; void main() &#123; uint projectIndex = selectViewProject(); FragPos = gl_in[0].gl_Position.xyz; gl_Position = viewProject[projectIndex] * gl_in[0].gl_Position; EmitVertex(); FragPos = gl_in[1].gl_Position.xyz; gl_Position = viewProject[projectIndex] * gl_in[1].gl_Position; EmitVertex(); FragPos = gl_in[2].gl_Position.xyz; gl_Position = viewProject[projectIndex] * gl_in[2].gl_Position; EmitVertex(); EndPrimitive();&#125; &emsp;&emsp;最终，我们成功的修补了体素的裂缝，如下图所示，先前的裂缝已经填上了体素。 图8 成功修补裂缝 三、修补孔洞&emsp;&emsp;然而，通过前面2部分的处理，另外一个问题出现了。由于模型的每个三角形都是各自根据在每个平面上的投影面积来选择投影相机，这意味着两个相邻的三角形片面可能选取了不同投影相机，使得三角形面片之间因为体素化投影平面的不同而产生过渡问题，从而出现孔洞，即有些部分没有被体素化到。如下图9所示。 图9 体素孔洞 &emsp;&emsp;孔洞的产生根源于光栅化处理，一个像素是否作为当前图元的光栅片元，是通过判断当前图元是否覆盖了该像素中心来完成的。对于那些没有覆盖像素中心的片元，不作为该图元的光栅片元送入片元着色器做进一步的处理，因而模型的一些部分可能会被丢失，从而造成孔洞。为了解决这个问题，我们将在几何着色器中实现一种被称为保守光栅化（Conservative Rasterization）的算法，依旧在几何着色器中实现。 &emsp;&emsp;通常的硬件光栅化，都是默认只取那些中心被图元覆盖的像素单元。而保守光栅化则将所有被图元覆盖（无论是否覆盖到像素单元的中心点）的像素单元都作为光栅化的片元，从而确保图元覆盖的所有区域都被光栅化，故名思意，这就是“保守”一词的由来。如下图10所示，通常情况下硬件默认的光栅片元是绿色部分，边缘红色部分的片元没有被光栅化，导致我们的体素化结果出现孔洞。为了修补体素化的孔洞，我们必须使得被图元哪怕一点点覆盖到的像素（就是下图中的红色部分）都作为当前图元的光栅化结果，这个过程就是保守光栅化算法。 图10 保守光栅化 &emsp;&emsp;那么怎么实现保守光栅化算法，使得上面的红色部分也被光栅化到呢？一个简单直观的思路就是手动扩充三角形图元面片。如上图10所示，里面的三角形是最初的我们要光栅化的三角形，为了使得边缘红色的像素也包含进来，我们扩张最初的三角形得到外面的那个三角形，这个三角形比原来的三角形稍微大一点，此时若将该扩大的三角形送入硬件默认的光栅化单元进行处理，则红色像素也被当作光栅片元，从而达到了我们的目的。注意，这里三角形的扩大程度非常关键，上面的扩大的三角形将我们不需要的像素单元也包含了进来，即黄色像素部分，我们将通过计算三角形的包围盒来剔除那些黄色像素单元，剔除像素部分我们将在片元着色器中实现。下图11是我实现的保守光栅化（图右）效果，图左是默认光栅化的效果。 图11 默认的光栅化和保守光栅化对比 &emsp;&emsp;扩大三角形和剔除像素整个过程都是在裁剪空间中进行的，也就是经过摄像机空间变换和投影变换之后。故而三角形的包围盒只需二维即可，然后需要适当地扩大一点，以免剔除红色的像素片元。一个裁剪空间的三角形包围盒计算如下所示，我们采用GLSL的vec4存储包围盒的最小顶点和最大顶点。 12345678910vec4 calcAABB(vec4 pos[3], vec2 pixelDiagonal)&#123; vec4 aabb; aabb.xy = min(pos[2].xy, min(pos[1].xy, pos[0].xy)); aabb.zw = max(pos[2].xy, max(pos[1].xy, pos[0].xy)); // enlarge by half-pixel aabb.xy -= pixelDiagonal; aabb.zw += pixelDiagonal; return aabb;&#125; &emsp;&emsp;接下来对于给定的三角形的三个顶点，我们要适当地扩大三角形。总体的思路就是：首先计算三角形的三条边与原点构成的齐次空间的平面，然后适当挪动这三个平面，接着就计算偏移后的这三个齐次平面的交线，最后计算三条交线与三角形平面的交点，从而得到扩大后的三角形的三个顶点。整个计算过程都是在裁剪空间中进行的，所以我们忽略顶点的$z$分量，但是上面又提到了齐次平面一词，我们采用一个齐次平面来描述三角形边的线段。所谓齐次平面，就是我们把顶点的齐次分量$w$和$x$、$y$分量合并一起来表示一条线段，直观来看，这就是一个齐次空间的平面，但实际上就是一段二维空间的直线。如下所示： Ax_c+By_c+Cw_c=0 \\tag {1}&emsp;&emsp;公式$(1)$就是一个齐次空间的过原点的平面方程，但是它实际上就是一个二维空间的直线方程。这是因为我们采用的都是正交投影，正交投影并没有透视除法之类的处理，因为正交投影都是线性变换，故而$w_c=1$，所以公式$(1)$表示的过原点的齐次空间的平面方程就是如下所示的二维直线方程： Ax_c+By_c+C=0 \\tag {2}&emsp;&emsp;之所以采用齐次空间的平面方程，是为了方便我们的计算。首先我们根据三角形的三条边计算三个齐次空间的平面，我们已知该齐次空间的平面过原点，平面方程的$(A,B,C)$就是该平面的法线向量，我们直接做叉乘计算可得平面的法线，如下所示： 1234vec3 edgePlanes[3];edgePlanes[0] = cross(pos[0].xyw - pos[2].xyw, pos[2].xyw);edgePlanes[1] = cross(pos[1].xyw - pos[0].xyw, pos[0].xyw);edgePlanes[2] = cross(pos[2].xyw - pos[1].xyw, pos[1].xyw); &emsp;&emsp;然后对这三个平面分别进行偏移。直观上来说，我们分别令三角形的三条边在其法线的方向上挪一段距离，这个距离由像素单元格的大小（即下面的halfPixel）在法线方向的投影决定，如下所示： 123edgePlanes[0].z -= dot(halfPixel[projectIndex], abs(edgePlanes[0].xy));edgePlanes[1].z -= dot(halfPixel[projectIndex], abs(edgePlanes[1].xy)); edgePlanes[2].z -= dot(halfPixel[projectIndex], abs(edgePlanes[2].xy)); &emsp;&emsp;接着计算三个齐次平面的交线向量，这个不难理解，两个平面的交线必然垂直于这两个平面的法线向量，因而交线向量可由这两个平面的法线向量做叉乘得到： 1234567vec3 intersection[3];intersection[0] = cross(edgePlanes[0], edgePlanes[1]);intersection[1] = cross(edgePlanes[1], edgePlanes[2]);intersection[2] = cross(edgePlanes[2], edgePlanes[0]);intersection[0] /= intersection[0].z;intersection[1] /= intersection[1].z;intersection[2] /= intersection[2].z; &emsp;&emsp;最后我们根据上面的三条射线向量与初试三角形所在的平面求交点，从而得到最终扩大后的三角形的三个顶点。由于我们是正交投影，所以上面求到的三条射线向量的$x$分量和$y$分量就是扩大三角形顶点的$x$分量和$y$分量，即交点的$x$、$y$已知，需要求$z$值。一个三维平面方程如下所示，从直观的几何意义上来说，$(A,B,C)$就是平面的法线向量，$D$就是原点到平面的直线距离。 Ax+By+Cz+D=0 \\tag {3}&emsp;&emsp;已知初始三角形的三个点，我们可以求出它的法线向量，然后原点到平面的直线距离就等于平面上的点在法线向量方向上的投影长度，这里要特别注意符号，具体看下面的代码： 123vec4 trianglePlane;trianglePlane.xyz = normalize(cross(pos[1].xyz - pos[0].xyz, pos[2].xyz-pos[0].xyz));trianglePlane.w = -dot(pos[0].xyz, trianglePlane.xyz); &emsp;&emsp;然后还需要提一点的是，我们要确保输入的三角形的顶点环绕顺序都是逆时针方向，这个逆时针方向是针对当前的相机投影方向。对于背向的面片，我们要做一个纠正的过程。判断是否是背向面片很简单，只需通过计算三角形法线向量与$(0,0,1)$做点乘，判断其符号即可。 1234567// change winding, otherwise there are artifacts for the back faces.if (dot(trianglePlane.xyz, vec3(0.0, 0.0, 1.0)) &lt; 0.0)&#123; vec4 vertexTemp = pos[2]; pos[2] = pos[1]; pos[1] = vertexTemp;&#125; &emsp;&emsp;已知交点的$x$和$y$，我们代入平面方程$(3)$求得$z$值。 z=-\\frac{Ax+By+D}{C} \\tag {4}12345678// calculate dilated triangle verticesfloat z[3];z[0] = -(intersection[0].x * trianglePlane.x + intersection[0].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z;z[1] = -(intersection[1].x * trianglePlane.x + intersection[1].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z;z[2] = -(intersection[2].x * trianglePlane.x + intersection[2].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z;pos[0].xyz = vec3(intersection[0].xy, z[0]);pos[1].xyz = vec3(intersection[1].xy, z[1]);pos[2].xyz = vec3(intersection[2].xy, z[2]); &emsp;&emsp;最终，我们求得到扩大后的三角形的三个顶点，我们还需要对三个顶点做逆视图投影变换，将裁剪空间的顶点变换到世界空间，得到扩大后的三角形的世界坐标，因为我们最终目的是根据世界空间坐标做体素化的处理。与此同时，我们还将在裁剪空间的扩大三角形的顶点传到片元着色器，因为我们要剔除不必要的片元。以下是保守光栅化算法的几何着色器代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117#version 430 corelayout (triangles) in;layout (triangle_strip, max_vertices = 3) out;out vec3 FragPos;out vec3 ProjectPos;out vec4 BoundingBox;uniform vec2 halfPixel[3];uniform mat4 viewProject[3];uniform mat4 viewProjectInverse[3];uint selectViewProject()&#123; vec3 p1 = gl_in[1].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 p2 = gl_in[2].gl_Position.xyz - gl_in[0].gl_Position.xyz; vec3 faceNormal = cross(p1, p2); float nDX = abs(faceNormal.x); float nDY = abs(faceNormal.y); float nDZ = abs(faceNormal.z); if( nDX &gt; nDY &amp;&amp; nDX &gt; nDZ ) &#123; return 0; &#125; else if( nDY &gt; nDX &amp;&amp; nDY &gt; nDZ ) &#123; return 1; &#125; else &#123; return 2; &#125;&#125; vec4 calcAABB(vec4 pos[3], vec2 pixelDiagonal)&#123; vec4 aabb; aabb.xy = min(pos[2].xy, min(pos[1].xy, pos[0].xy)); aabb.zw = max(pos[2].xy, max(pos[1].xy, pos[0].xy)); // enlarge by half-pixel aabb.xy -= pixelDiagonal; aabb.zw += pixelDiagonal; return aabb;&#125;void main() &#123; uint projectIndex = selectViewProject(); vec4 pos[3] = vec4[3] ( viewProject[projectIndex] * gl_in[0].gl_Position, viewProject[projectIndex] * gl_in[1].gl_Position, viewProject[projectIndex] * gl_in[2].gl_Position ); vec4 trianglePlane; trianglePlane.xyz = normalize(cross(pos[1].xyz - pos[0].xyz, pos[2].xyz - pos[0].xyz)); trianglePlane.w = -dot(pos[0].xyz, trianglePlane.xyz); // change winding, otherwise there are artifacts for the back faces. if (dot(trianglePlane.xyz, vec3(0.0, 0.0, 1.0)) &lt; 0.0) &#123; vec4 vertexTemp = pos[2]; pos[2] = pos[1]; pos[1] = vertexTemp; &#125; if(trianglePlane.z == 0.0f) return; BoundingBox = calcAABB(pos, halfPixel[projectIndex]); vec3 edgePlanes[3]; edgePlanes[0] = cross(pos[0].xyw - pos[2].xyw, pos[2].xyw); edgePlanes[1] = cross(pos[1].xyw - pos[0].xyw, pos[0].xyw); edgePlanes[2] = cross(pos[2].xyw - pos[1].xyw, pos[1].xyw); edgePlanes[0].z -= dot(halfPixel[projectIndex], abs(edgePlanes[0].xy)); edgePlanes[1].z -= dot(halfPixel[projectIndex], abs(edgePlanes[1].xy)); edgePlanes[2].z -= dot(halfPixel[projectIndex], abs(edgePlanes[2].xy)); vec3 intersection[3]; intersection[0] = cross(edgePlanes[0], edgePlanes[1]); intersection[1] = cross(edgePlanes[1], edgePlanes[2]); intersection[2] = cross(edgePlanes[2], edgePlanes[0]); intersection[0] /= intersection[0].z; intersection[1] /= intersection[1].z; intersection[2] /= intersection[2].z; // calculate dilated triangle vertices float z[3]; z[0] = -(intersection[0].x * trianglePlane.x + intersection[0].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z; z[1] = -(intersection[1].x * trianglePlane.x + intersection[1].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z; z[2] = -(intersection[2].x * trianglePlane.x + intersection[2].y * trianglePlane.y + trianglePlane.w) / trianglePlane.z; pos[0].xyz = vec3(intersection[0].xy, z[0]); pos[1].xyz = vec3(intersection[1].xy, z[1]); pos[2].xyz = vec3(intersection[2].xy, z[2]); vec4 voxelPos; ProjectPos = pos[0].xyz; gl_Position = pos[0]; voxelPos = viewProjectInverse[projectIndex] * gl_Position; FragPos = voxelPos.xyz; EmitVertex(); ProjectPos = pos[1].xyz; gl_Position = pos[1]; voxelPos = viewProjectInverse[projectIndex] * gl_Position; FragPos = voxelPos.xyz; EmitVertex(); ProjectPos = pos[2].xyz; gl_Position = pos[2]; voxelPos = viewProjectInverse[projectIndex] * gl_Position; FragPos = voxelPos.xyz; EmitVertex(); EndPrimitive();&#125; &emsp;&emsp;最后的最后，我们还需要在片元着色器剔除无关的片元，具体原因我已经在前面说了，如果不做这一步的剔除操作，将出现如下图12所示的情况。在片元着色器中，我们根据传入的三角形包围盒与当前的片元位置判断是否需要丢弃该片元。具体看下面代码的第19行、第20行。 图12 保守光栅化出现的边边角角 12345678910111213141516171819202122232425262728#version 430 corein vec3 FragPos;in vec3 ProjectPos;in vec4 BoundingBox;layout (std430, binding = 0) buffer CountBuffer&#123; int cnts[];&#125;;uniform bool conservate;uniform float step;uniform vec3 boxMin;uniform vec3 resolution;out vec4 color;void main()&#123; if(ProjectPos.x &lt; BoundingBox.x || ProjectPos.y &lt; BoundingBox.y || ProjectPos.x &gt; BoundingBox.z || ProjectPos.y &gt; BoundingBox.w) discard; uint x = uint((FragPos.x - boxMin.x)/step); uint y = uint((FragPos.y - boxMin.y)/step); uint z = uint((FragPos.z - boxMin.z)/step); uint index = uint(y * (resolution.z * resolution.x) + z * resolution.x + x); atomicAdd(cnts[index], 1); color = vec4(0.0,1.0,0.0,1.0);&#125; &emsp;&emsp;最终，修复了孔洞的效果的如下图，可以看到，对比前面的图9，孔洞基本都被“补”上了。 图13 保守光栅化出现的边边角角 &emsp;&emsp;下面就是一些模型的体素化效果。 参考资料：$[1]$ The Basics of GPU Voxelization $[2]$ 《GPU Gems 2》： Chapter 42. Conservative Rasterization $[3]$ https://blog.csdn.net/xiewenzhao123/article/details/79875855","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"https://yangwc.com/about/categories/Position-Based-Dynamics/"},{"name":"Voxelization","slug":"Voxelization","permalink":"https://yangwc.com/about/categories/Voxelization/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Voxelization","slug":"Voxelization","permalink":"https://yangwc.com/about/tags/Voxelization/"}]},{"title":"流体模拟Fluid Simulation：Position Based Fluid","slug":"PositionBasedFluid","date":"2019-06-04T07:43:02.767Z","updated":"2019-06-26T01:48:47.815Z","comments":true,"path":"2019/06/04/PositionBasedFluid/","link":"","permalink":"https://yangwc.com/about/2019/06/04/PositionBasedFluid/","excerpt":"本篇文章主要是关于Position Based Dynamics的流体模拟方法，这类方法依旧采用基于拉格朗日的视角，把流体看成由一个一个粒子组成，易于并行化，适用于实时的流体模拟。目前实现的只是CPU版本，考虑在后面利用cuda挪到GPU上做模拟计算。相关的完整代码请看这里。","text":"本篇文章主要是关于Position Based Dynamics的流体模拟方法，这类方法依旧采用基于拉格朗日的视角，把流体看成由一个一个粒子组成，易于并行化，适用于实时的流体模拟。目前实现的只是CPU版本，考虑在后面利用cuda挪到GPU上做模拟计算。相关的完整代码请看这里。 基于位置动力学的物理模拟 基于位置动力学的流体模拟 流体模拟算法实现 实现效果 参考资料 一、基于位置动力学的物理模拟&emsp;&emsp;传统的物理模拟方法都是基于力的方法，这类方法通过计算内部力（如流体内部的粘性力、压力）和外部力（如重力和碰撞力）的合力，然后根据牛顿第二定律计算出加速度，最后根据数值计算方法求出物体的速度和位置。这种方法基本上针对每一种动态物体，会由一个独立的求解器，各种求解器按照一定的顺序计算，从而得到模拟的结果，这样会带来大量冗余的工作。基于位置动力学（Position Based Dynamics）的方法将这些物理运动通过约束表达出来，这样只需要一个求解器即可，更加方便地进行物理模拟。 &emsp;&emsp;下图1是基于力和基于位置动力学的物体碰撞更新过程的对比，可以看到基于力的碰撞检测首先在穿透发生时更新物体的速度，然后更新物体的位置。而基于位置动力学的碰撞检测首先只检测是否发生穿透，然后移动位置使之不发生穿透，最后再据此更新物体的速度信息。 图1 两种碰撞更新过程对比 1、基于位置动力学的模拟算法&emsp;&emsp;基于位置动力学英文全称为Position Based Dynamics，以下简称为PBD。接下来我们介绍经典的PBD算法。在PBD算法中，运动的物体由$N$个顶点和$M$个约束组成。顶点$i\\in [1,…,N]$的质量为$m_i$，位置为$x_i$，速度为$v_i$，每个约束$j\\in [1,…,M]$有如下五个性质： 约束的基数为$n_j$，即第$j$个约束所影响的顶点数目为$n_j$个； 约束函数$C_j:\\ R^{3n_j}\\to R$； 受约束影响的顶点索引值集合$\\{i_1,…,i_{n_j}\\},i_k\\in [1,…N]$； 每个约束都有对应的刚度参数$k_j\\in [0,1]$，这里我们可以理解为约束的强度； 约束分为两种，一类是等式约束即$C_j(x_{i1},x_{i_2},…,x_{i_{n_j}})=0$，另一类是不等式约束$C_j(x_{i_1},x_{i_2},…,x_{i_{n_j}})\\geq 0$。 &emsp;&emsp;给定时间步长$\\Delta t$，PBD的运动物体模拟的算法伪代码如下所示： \\begin{align} &1.forall\\ \\ vertices\\ \\ i:\\\\ &2.\\ \\ \\ \\ initialize\\ \\ x_i=x_i^0,v_i=v_i^0,w_i=1/m_i\\\\ &3.endfor\\\\ &4.loop\\\\ &5.\\ \\ \\ \\ forall\\ \\ vertices\\ \\ i\\ \\ do\\ \\ v_i\\leftarrow v_i+\\Delta tw_if_{ext}(x_i)\\\\ &6.\\ \\ \\ \\ dampVelocities(v_1,...,v_N)\\\\ &7.\\ \\ \\ \\ forall\\ \\ vertices\\ \\ i\\ \\ do\\ \\ p_i\\leftarrow x_i+\\Delta t v_i\\\\ &8.\\ \\ \\ \\ forall\\ \\ vertices\\ \\ i\\ \\ do\\ \\ generateCollisionConstraints(x_i\\to p_i)\\\\ &9.\\ \\ \\ \\ loop\\ \\ solverIterations\\ \\ times\\\\ &10.\\ \\ \\ \\ \\ \\ \\ \\ projectConstraints(C1,...,C_{M+M_{coll}},p_1,...,p_N)\\\\ &11.\\ \\ \\ endloop\\\\ &12.\\ \\ \\ forall\\ \\ vertices\\ \\ i\\\\ &13.\\ \\ \\ \\ \\ \\ \\ \\ v_i\\leftarrow (p_i-x_i)\\Delta t\\\\ &14.\\ \\ \\ \\ \\ \\ \\ \\ x_i\\leftarrow p_i\\\\ &15.\\ \\ \\ endfor\\\\ &16.\\ \\ \\ velocityUpdate(v1,...,v_N)\\\\ &17.endloop\\\\ \\end{align}&emsp;&emsp;在上面的算法第1步到第3步中，我们首先对顶点的位置、速度和质量倒数进行初始化，其中质量的倒数$w_i=1/m_i$，除了可以避免冗余的除法操作外，还可以使用于静态的物体，对于静态的物体我们设为$w_i=0$，这样在后续的更新中都不会产生位置和速度的变化量。第5步中的$f_{ext}$代表不能转换成约束形式的力（如重力），我们根据$f_{ext}$进行一次数值计算预测在$f_{ext}$的作用下的速度$v_i$。紧接着在第6步中我们添加阻尼的作用，阻尼可以理解为物体在运动中发生了能量耗散，从而导致速度有所衰减。第8行主要是生成碰撞约束，物体会与周围的环境发生碰撞，例如布料落在地板上，水碰上一面墙等，这些碰撞约束在每个时间步长都发生改变，所以每一次都需要重新生成碰撞约束。有了内部约束（如不可压缩流体的密度约束）和外部约束（如流体与地面的碰撞约束）之后，我们需要根据这些约束做一个迭代求解，也就是上面伪代码中的第9行到第11行，这里我们称为约束投影步骤。从约束投影步骤我们得到服从给定约束的粒子位置，然后再第12行到第15行更新顶点粒子的速度和位置信息。最后在第16行根据摩擦系数（friction）和恢复系数（restitution）更新速度，如下图2所示。这样，一个完整的PBD物理模拟步骤就完成了。 图2 friction和restitution 2、约束投影步骤&emsp;&emsp;接下来我们就针对约束投影步骤详细展开相关的内容，约束投影是PBD中的最难理解的核心部分，涉及的数学内容比较多一点。设有一个基数为n（也就是前面提到的$n_j$，受到该约束影响的顶点数目或者说粒子数目）的约束，关联的粒子点为$p_1,…,p_n$，约束函数记为$C$，刚度系数（stiffness）为$k$。记$p=[p_1^T,…,p_n^T]^T$，则等式约束函数表示为： C(p)=0 \\tag {1}&emsp;&emsp;我们的目标是计算这样的一个位移偏移量$\\Delta p$，使得粒子顶点在$p+\\Delta p$处约束条件依然满足，即： C(p+\\Delta p)=0 \\tag {2}&emsp;&emsp;对约束函数$C$做一阶泰勒展开（或者导数的定义），则可得: C(p+\\Delta p)\\approx C(p)+\\nabla_pC(p)\\cdot\\Delta p=0 \\tag {3}&emsp;&emsp;为了使粒子在$p+\\Delta p$处依然满足约束条件，我们要求解方程$(3)$得到$\\Delta p$。PBD算法的一个巧妙之处在于它将$\\Delta p$的方向限制在约束函数的梯度方向$\\nabla_p C(p)$上。如下图3所示，约束$C$所涉及到的粒子位置会形成一个高维空间，下图为该空间中满足不同约束条件的粒子位置形成的二维等值线示意图，其中满足$C$约束条件的是黑色等值线。故当粒子处于下图的黑色点的位置时，不满足约束条件，如果我们沿着点所在的等值线（灰色曲线）移动，此时刚体模态（Rigid body modes）的方向与该等值线相同，新得到的位置仍然在该灰色等值线上，依然不在黑色曲线 $C=0$上，即不满足约束条件。这可以理解为，约束中存在的误差依然没有得到修正。以两个粒子形成的距离约束为例，就好比同时移动了两个粒子或者该约束绕自身旋转，但是存在的误差并没有得到更正。而且这样一来还会引入系统中不存在的一种外力，导致系统动量不守恒。所以，我们希望该点的位移方向与刚体模态方向垂直，从而保证系统动量守恒，即从黑点指向红点的方向$\\nabla C$。 图3 约束等值线 &emsp;&emsp;因此，我们令位移向量$\\Delta p$为约束函数的梯度向量$\\nabla_p C$再乘上一个标量缩放系数$\\lambda$： \\Delta p=\\lambda \\nabla_p C(p) \\tag {4}&emsp;&emsp;其中的标量缩放系数$\\lambda$我们称之为拉格朗日乘子（Lagrange multiplier）。联立公式$(3)$和$(4)$我们可得： \\lambda=-\\frac{C(p)}{|\\nabla_pC(p)|^2} \\tag {5}&emsp;&emsp;然后将$\\lambda$再代入公式$(4)$我们可得$\\Delta p$的表达式： \\Delta p=\\lambda \\nabla_pC(p)=-\\frac{C(p)}{|\\nabla_pC(p)|^2}\\nabla_pC(p) \\tag {6}&emsp;&emsp;具体到粒子$i$，约束投影后其对应的位移向量为： \\Delta p_i=-s\\nabla_{p_i}C(p_1,...,p_n) \\tag {7}&emsp;&emsp;其中的$s$为如下所示，$s$的值对于约束函数$C$作用范围内的所有点都一样。 s=\\frac{C(p_1,...,p_n)}{\\Sigma_j|\\nabla_{p_j}C(p_1,...,p_n)|^2} \\tag {8}&emsp;&emsp;前面我们假定所有的粒子质量都相同，现在考虑粒子质量不同的情况。记粒子$i$的质量为$m_i$，其质量的倒数为$w_i=1/m_i$，则公式$(4)$变为： \\Delta p_i=\\lambda w_i\\nabla_{p_i}C(p) \\tag {9}&emsp;&emsp;公式$(7)$和公式$(8)$变为： \\Delta p_i=-s w_i\\nabla_{p_i}C(p_1,...,p_n) \\tag {10} s=\\frac{C(p_1,...,p_n)}{\\Sigma_jw_j|\\nabla_{p_j}C(p_1,...,p_n)|^2} \\tag {11}&emsp;&emsp;为了便于理解，接下来我们举个简单的例子应用约束投影方法。如下图4所示。 图4 简单的约束例子 &emsp;&emsp;上面的约束可以表示为$C(p_1,p_2)=|p_1-p_2|-d$，位移向量记为$\\Delta p_i$。根据约束投影方法，我们首先约束函数$C(p_1,p_2)$关于$p_1$和$p_2$的梯度，也就是求偏导数。注意到$C(p_1,p_2)=|p_1-p_2|-d=(\\sqrt{(p_1-p_2)^2})-d$，我们可以求得以下的梯度向量表达式： \\nabla_{p_1}C(p_1,p_2)=\\frac{p_1-p_2}{|p_1-p_2|}\\\\ \\nabla_{p_2}C(p_1,p_2)=-\\frac{p_1-p_2}{|p_1-p_2|} \\tag {12}&emsp;&emsp;注意，上面求到的是一个矢量，也就是我们说的梯度向量。将公式$(12)$代入公式$(11)$可得： \\begin{align} s=&\\frac{C(p_1,...,p_n)}{\\Sigma_jw_j|\\nabla_{p_j}C(p_1,...,p_n)|^2}\\\\ =&\\frac{|p_1-p_2|-d}{w_1|\\nabla_{p_1}C(p_1,p_2)|^2+w_2|\\nabla_{p_2}C(p_1,p_2)|^2}\\\\ =&\\frac{|p_1-p_2|-d}{w_1+w_2} \\tag {13} \\end{align}&emsp;&emsp;最后，将公式$(13)$代入到公式$(10)$，可得约束投影计算得到的位移： \\begin{align} \\Delta p_1=&-\\frac{|p_1-p_2|-d}{w_1+w_2}w_1\\nabla_{p_1}C(p_1,p_2)\\\\ =&-\\frac{w_1}{w_1+w_2}(|p_1-p_2|-d)\\frac{p_1-p_2}{|p_1-p_2|} \\end{align}&emsp;&emsp;同理$\\Delta p_2$如下所示： \\Delta p_2=+\\frac{w_2}{w_1+w_2}(|p_1-p_2|-d)\\frac{p_1-p_2}{|p_1-p_2|}&emsp;&emsp;前面我们提到每个约束都有对应的刚度系数$k$，令$k’=1-(1-k)^{1/n_s}$去乘$\\Delta p$，这里$n_s$迭代之后误差为$\\Delta p(1-k’)^{n_s}=\\Delta p(1-k)$，与刚度系数成线性关系，而与迭代次数$n_s$无关。下一个时间步的位置如下所示： p_1^{t+1}=p_1^t+k'\\Delta p_1\\\\ p_2^{t+1}=p_2^t+k'\\Delta p_23、约束投影求解器&emsp;&emsp;前面的伪代码中我们可以看到约束投影的输入为$M+M_{coll}$个约束和$N个$点的预测位置$p1,…,p_N$，所需要求解的方程组是非线性非对称方程组或不等式组（碰撞约束产生的）。约束投影步骤的主要任务就是修正预测位置使新得到的校正位置满足所有约束。但是一般情况下很难找到一个适当的$\\Delta p=[\\Delta p_1^T,…,\\Delta p_n^T]^T$恰好使得所有的约束都能够同时得到满足，故我们通常采用迭代的方法按顺序依次对约束进行求解。 &emsp;&emsp;我们可以采用非线性高斯-赛德尔（Non-Linear Gauss-Seidel，简称NGS）迭代方法。高斯赛德尔（Gauss-Sedel，简称GS）迭代方法只能求解线性方程组，NGS在依次求解德基础上，加入了约束投影求解这一非线性操作。与雅可比迭代方法（Jacobi method）不同，NGS求解器在一次迭代中对于顶点位置的修正立即被应用到下一个约束求解中，这样的好处就是显著加快了收敛速度。 &emsp;&emsp;但是NGS虽然稳定且容易实现，但是该方法收敛速度依然不是很快，不宜并行化。 二、基于位置动力学的流体模拟&emsp;&emsp;前面部分主要介绍了Position Based Dynamics算法相关的内容，接下来我们就看看如何将其PBD算法应用到流体模拟当中，主要是如何针对流体的物理特性构建相应的约束函数。基于位置动力学的流体全称为Position Based Fluid，简称PBF。 1、不可压缩约束&emsp;&emsp;在不可压缩性的流体模拟中，我们需要使粒子$i$的密度$\\rho_i$尽量与静态的密度$\\rho_0$相同，即$\\rho_i=\\rho_0$。因此，我们需要对每一个流体粒子都施加一个常量密度约束，PBF的常量密度约束如下所示： C_i(p_1,...,p_n)=\\frac{\\rho_i}{\\rho_0}-1 \\tag {14}&emsp;&emsp;公式$(14)$中，我们记粒子$i$的位置为$p_i$，$p_1,…,p_n$是与粒子$i$相邻的粒子。可以看到当密度约束$C_i(p_1,…,p_n)=0$时有$\\rho_i=\\rho_0$，此时流体的体积即不压缩也不膨胀，从而保证了流体的不可压缩条件，这就是公式$(14)$的由来。流体粒子$i$的密度根据SPH（Smoothed Particle Hydrodynamics，光滑粒子流体动力学，简称SPH）方法的计算公式如下所示： \\rho_i=\\Sigma_jm_jW(p_i-p_j,h) \\tag {15}&emsp;&emsp;在公式$(15)$中，$m_j$是邻居粒子$j$的质量，$h$是指定的光滑核半径。$W$函数我们接下来会提到。将公式$(15)$代入公式$(14)$，我们有： C_i(p_1,...,p_n)=\\frac{\\Sigma_j m_jW(p_i-p_j,h)}{\\rho_0}-1 \\tag {16}&emsp;&emsp;在公式$(15)$的密度计算中，PBF方法采用了Poly6核函数： W_{poly6}(r,h)=\\frac{315}{64\\pi h^9} \\begin{cases} (h^2-|r|^2)^3\\ \\ \\ \\ 0\\leq|r|\\leq h\\\\ 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ otherwise \\end{cases} \\tag {17}&emsp;&emsp;但是在计算密度的梯度时，却又采用了Spiky核函数： W_{spiky}(r,h)=\\frac{15}{\\pi h^6} \\begin{cases} (h-|r|)^3\\ \\ \\ \\ 0\\leq|r|\\leq h\\\\ 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 0therwise \\end{cases} \\tag {18}&emsp;&emsp;对公式$(18)$求关于$r$的导数（注意，$|r|=\\sqrt{r^2}$，不能直接对$|r|$求导），从而流体粒子密度的梯度如下所示： \\nabla W_{spiky}(r,h)=-\\frac{45}{\\pi h^6} \\begin{cases} (h-|r|)^2\\frac{r}{|r|}\\ \\ \\ \\ 0\\leq|r|\\leq h\\\\ 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ otherwise \\end{cases} \\tag {19}&emsp;&emsp;因此，粒子$i$的约束函数$(16)$是一个关于$p_1,…,p_n$的非线性方程组$C_i(p_1,…,p_n)=0$，所有粒子$i$的约束组成了一个非线性方程组。在PBF方法中，我们只考虑粒子质量相同的情况，故我们可以省去公式$(15)$和公式$(16)$中的质量$m_j$，即： \\rho_i=\\Sigma_jW(p_i-p_j,h) \\tag {20} C_i(p_1,...,p_n)=\\frac{\\Sigma_j W(p_i-p_j,h)}{\\rho_0}-1 \\tag {21}&emsp;&emsp;然后求约束函数$C_i$关于$p_k$的梯度如下，其中$k\\in\\{1,2,…,n\\}$： \\nabla_{p_k}C_i=\\frac1\\rho_0\\Sigma_j\\nabla_{p_k}W(p_i-p_j,h) \\tag {22}&emsp;&emsp;显然，针对$k$的不同，分为两种情况。当$k=i$也就是粒子本身的时候，连加符号中的$W$均为关于$p_k$的函数；当$k=j$即邻居粒子的时候，只有$W(p_i-p_k,h)$才有意义，其他相对于$p_k$来说都是常量，故导数为0（注意用到了求导的链式法则）： \\nabla_{p_k}C_i=\\frac1\\rho_0 \\begin{cases} \\Sigma_j\\nabla_{p_k}W(p_i-p_j,h)\\ \\ \\ \\ if\\ \\ k=i\\\\ -\\nabla_{p_k}W(p_i-p_j,h)\\ \\ \\ \\ \\ if\\ \\ k=j \\end{cases} \\tag {23}&emsp;&emsp;既然求出了约束函数的梯度，我们就把它应用到前面提到的拉格朗日乘子的计算公式中，联立公式$(5)$和公式$(23)$，我们有： \\lambda_i=-\\frac{C_i(p_1,...,p_n)}{\\Sigma_k|\\nabla_{p_k}C_i|^2} \\tag {24}2、混合约束&emsp;&emsp;如果一个约束条件不能被违背，我们称之为硬约束；而能一定程度上被违背的约束称为软约束。在理想的情况下，我们都希望约束始终是硬约束，但是由于误差或者数值方法的不稳定等原因，我们有时不得不向软约束妥协。 &emsp;&emsp;在PBF中，当$|r|=h$，粒子$i$与粒子$j$之间的距离等于光滑核半径时，粒子$i$和粒子$j$处于即将分离的状态。注意观察公式$(19)$的密度梯度计算公式，此时$\\nabla W_{spiky}(r,h)=0$。若所有的邻居粒子与粒子$i$都处于这种状态，那么必将导致约束函数的梯度即公式$(22)$取值为0： \\nabla_{p_k}C_i=\\frac1\\rho_0\\Sigma_j\\nabla_{p_k}W(p_i-p_j,h) = 0&emsp;&emsp;从而导致公式$(24)$中的分母$\\Sigma_k|\\nabla_{p_k}C_i|^2$为0，出现除零错误，这将导致PBF方法出现潜在的不稳定性。为了解决这个问题，PBF采用混合约束的方法，使密度硬约束转变成软约束。具体的做法就是将根据密度函数求解得到的约束力再加入到原始的约束函数中，这里在PBF的常量密度约束中得到的拉格朗日乘子$\\lambda$有类似的作用，故将$\\lambda$加入到初始的约束方程（即公式$(3)$）： C(p+\\Delta p)\\approx C(p)+\\nabla C^T\\nabla C \\lambda + \\epsilon\\lambda=0 \\tag {25}&emsp;&emsp;公式$(25)$中的$\\epsilon$是松弛参数，可以由用户指定。引入公式$(25)$后，拉格朗日乘子的计算公式$(24)$就变为： \\lambda_i=-\\frac{C_i(p_1,...,p_n)}{\\Sigma_k|\\nabla_{p_k}C_i|^2+\\epsilon} \\tag {26}&emsp;&emsp;从而可得粒子$i$在经过上述约束投影后对应的位移向量（包括自身密度约束以及邻居粒子密度约束共同作用的结果。注意，这里对应的上面的公式$(4)$，结合公式$(23)$）： \\begin{align} \\Delta p_i&=\\lambda_i \\nabla_{p_i}C_i+\\Sigma_j\\lambda_j\\nabla_{p_j}C_i\\\\ &=\\frac1\\rho_0\\Sigma_j\\lambda_i\\nabla_{p_i}W(r,h)+(-\\frac1\\rho_0\\Sigma_j\\lambda_j\\nabla_{p_j}W(r,h))\\\\ &=\\frac1\\rho_0\\Sigma_j\\lambda_i\\nabla_{p_i}W(r,h)+\\frac1\\rho_0\\Sigma_j\\lambda_j\\nabla_{p_i}W(r,h)\\\\ &=\\frac{1}{\\rho_0}\\Sigma_j(\\lambda_i+\\lambda_j)\\nabla_{p_i}W(r,h) \\end{align} \\tag {27}3、拉伸不稳定性&emsp;&emsp;PBF采用SPH的方法来计算流体粒子的密度，但是该方法通常需要30~40个邻居粒子才能使密度求值结果趋于静态密度。在邻居粒子数量较少的情况下，通过该方法计算得到的流体密度低于静态密度，由此会造成流体内部压强为负数，原本粒子间的压力变为吸引力，使得流体粒子凝聚在一起，导致流体表面的模拟效果失真。PBF采用了一种人工排斥力的计算模型，当流体粒子距离过近时该排斥力会使它们分开，避免产生粒子聚集的现象。在公式$(24)$的基础上，加入一个排斥项（repulsive term）$s_{corr}$： \\Delta p_i=\\frac1\\rho_0\\Sigma_j(\\lambda_i+\\lambda_j+s_{corr})\\nabla_{p_i}W(p_i-p_j,h) \\tag {28}&emsp;&emsp;其中的$s_{corr}$计算方式如下： s_{corr}=-k(\\frac{W(p_i-p_j,h)}{W(\\Delta q,h)})^n \\tag {29}&emsp;&emsp;公式$(29)$中，$\\Delta q$表示到粒子$i$的一个固定距离，通常取$|\\Delta q|=0.1h,…,0.3h$，$h$即前面提到的光滑核半径。此外，公式中的$k$可以看作表面张力参数，取值$k=0.1$，而$n=4$。公式$(28)$中的排斥项会使得流体粒子的密度稍微低于静态密度，从而产生类似于表面张力的效果，使得流体表面的的粒子分布均匀。通过这个排斥项，我们不再需要硬性规定流体的邻居数量必须在30~40个，进一步提升算法的流体模拟效率。 4、涡轮控制和人工粘性&emsp;&emsp;由于数值耗散，PBD的方法会引入额外的阻尼，使得整个系统的能量损耗太快，导致本来应该由的一些涡流细节迅速消失。在这里，PBF通过涡轮控制方法向整个系统重新注入能量： f_i^{vorticity}=\\epsilon (N\\times \\omega_i) \\tag {30}&emsp;&emsp;上述的公式中，$N=\\frac{\\eta}{|\\eta|},\\ \\eta=\\nabla|\\omega|_i$，而流体粒子的旋度$\\omega_i$计算公式如下： \\omega_i=\\nabla\\times v=\\Sigma_j(v_j-v_i)\\times \\nabla_{p_j}W(p_i-p_j,h) \\tag {31}&emsp;&emsp;涡轮控制方法的基本思路就是：通过添加一个体积力$f_i^{vorticity}$（在算法的第一步），在旋度粒子（可直观理解为比周围粒子旋转快的粒子，旋度$\\omega_i$指向粒子$i$的旋转轴）处加速粒子的旋转运动，通过这种方式来增加系统的旋度细节。公式$(30)$中的$\\epsilon$用于控制涡轮控制力的强度。 &emsp;&emsp;最后，PBF方法采用XSPH的粘度方法直接更新速度，从而产生粘性阻尼。人工粘性除了可以增加模拟的数值稳定性，还可以消除非物理的流体振荡。拉格朗日流体模拟方法中，人工粘性本质上会对流体粒子的相对运动产生阻尼作用，使流体的动能转化为热能： v_i^{new}=v_i+c\\Sigma_j(v_i-v_j)\\cdot W(p_i-p_j,h) \\tag {32}&emsp;&emsp;在流体模拟中，我们取公式$(32)$中的$c=0.01$。 5、PBF算法&emsp;&emsp;PBF算法的总体框架就是按照前面提到的PBD算法，只是经典PBD算法采用了顺序高斯-赛德尔（Sequential Gauss-Seidel，SGS）迭代求解，而SGS不容易被GPU并行化，因此基于CUDA实现的PBF求解器使用了雅克比（Jacobi）迭代方法并行求解。 &emsp;&emsp;PBF的算法伪代码如下所示： \\begin{align} &1.\\ forall\\ \\ particles\\ \\ i\\ \\ do\\\\ &2.\\ \\ \\ \\ \\ apply\\ \\ force\\ \\ v_i\\leftarrow v_i+\\Delta tf_{ext}(x_i)\\\\ &3.\\ \\ \\ \\ \\ predict\\ \\ position\\ \\ x_i^*\\leftarrow x_i+\\Delta t v_i\\\\ &4.\\ endfor\\\\ &5.\\ forall\\ \\ particles\\ \\ i\\ \\ do\\\\ &6.\\ \\ \\ \\ \\ find\\ \\ neighboring\\ \\ particles\\ \\ N_i(x_i^*)\\\\ &7.\\ endfor\\\\ &8.\\ while\\ \\ iter\\ \\","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/tags/Fluid-Simulation/"},{"name":"Position Based Dynamics","slug":"Position-Based-Dynamics","permalink":"https://yangwc.com/about/tags/Position-Based-Dynamics/"}]},{"title":"光线追踪器Ray Tracer：进阶篇","slug":"RayTracer-Advance","date":"2019-05-23T13:08:38.407Z","updated":"2019-05-26T03:37:23.595Z","comments":true,"path":"2019/05/23/RayTracer-Advance/","link":"","permalink":"https://yangwc.com/about/2019/05/23/RayTracer-Advance/","excerpt":"本篇文章在前面的基础上，丰富光线追踪器的各种特性。本篇内容主要包含添加纹理映射、平面光源和球形光源、三角网格模型渲染、增加天空盒背景、构建BVH树、tbb多线程渲染加速、蒙特卡罗积分方法、重要性采样，后面部分涉及的高等数学和概率论内容较多。相关的全部代码在这里。","text":"本篇文章在前面的基础上，丰富光线追踪器的各种特性。本篇内容主要包含添加纹理映射、平面光源和球形光源、三角网格模型渲染、增加天空盒背景、构建BVH树、tbb多线程渲染加速、蒙特卡罗积分方法、重要性采样，后面部分涉及的高等数学和概率论内容较多。相关的全部代码在这里。 纹理映射 三角网格模型 添加光源 天空盒背景 构建BVH树 tbb多线程渲染 蒙特卡罗积分 重要性采样 MC光线追踪 程序效果 参考资料 一、纹理映射&emsp;&emsp;纹理映射对渲染的重要性不言而喻，为了丰富物体表面的细节，我们在这里创建一个纹理加载和采样的类。实际上，除了图片纹理，还有一些过程式产生的纹理。我们创建一个虚类$Texture$，并将$sample$类作为虚接口。然后创建子类$ImageTexture$，图片的加载我采用了stb_image库。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Texture&#123;public: typedef std::shared_ptr&lt;Texture&gt; ptr; Texture() = default; virtual ~Texture() = default; virtual Vector3D sample(const float &amp;u, const float &amp;v, const Vector3D &amp;p) const = 0;&#125;;class ImageTexture : public Texture&#123;private: unsigned char *m_data; int m_width, m_height, m_channel; public: typedef std::shared_ptr&lt;ImageTexture&gt; ptr; ImageTexture() = default; ImageTexture(const std::string &amp;path); virtual ~ImageTexture(); virtual Vector3D sample(const float &amp;u, const float &amp;v, const Vector3D &amp;p) const; &#125;;ImageTexture::ImageTexture(const std::string &amp;path)&#123; m_data = stbi_load(path.c_str(), &amp;m_width, &amp;m_height, &amp;m_channel, 0); if (m_data == nullptr) std::cout &lt;&lt; \"Failed to load the image-&gt;\" &lt;&lt; path.c_str() &lt;&lt; std::endl;&#125;Vector3D ImageTexture::sample(const float &amp;u, const float &amp;v, const Vector3D &amp;p) const&#123; int i = static_cast&lt;int&gt;(u * m_width); int j = static_cast&lt;int&gt;((1.0f - v)*m_height) - 0.001; if (i &lt; 0) i = 0; if (j &lt; 0) j = 0; if (i &gt; m_width - 1) i = m_width - 1; if (j &gt; m_height - 1) j = m_height - 1; int index = (j * m_width + i) * m_channel; float r = static_cast&lt;int&gt;(m_data[index + 0]) / 255.0f; float g = static_cast&lt;int&gt;(m_data[index + 1]) / 255.0f; float b = static_cast&lt;int&gt;(m_data[index + 2]) / 255.0f; return Vector3D(r, g, b);&#125; &emsp;&emsp;纹理坐标转换为像素数组下标要注意是否越界了，这里实现的纹理环绕方式是clamp。然后对于球体，我们要计算球体上每个点的纹理坐标，这里采用球面坐标的一个技巧。球体的上每一个点，都对应着一组唯一的方向角和天顶角$(\\theta,\\phi)$，我们把$(\\theta,\\phi)$映射到二维纹理坐标即可。映射方法如下： u = \\phi/(2\\pi) \\\\ v = \\theta/\\pi \\tag {1}&emsp;&emsp;那么如何根据一个球面的点计算它的方向角和天顶角呢？从球面坐标$(\\theta, \\phi)$转到笛卡尔坐标$(x,y,z)$，不难理解，有如下关系： x = cos(\\phi)cos(\\theta) \\\\ y = sin(\\phi)cos(\\theta) \\\\ z = sin(\\theta) \\tag {2}&emsp;&emsp;注意到$y/x=tan(\\phi)$，所以我们可以采用下面的方式得到球面上点的天顶角和方位角： \\phi=atan2(y,x)\\\\ \\theta=asin(2) \\tag {3}&emsp;&emsp;需要注意的是，$atan2$函数返回的角度范围是$[-\\pi,+\\pi]$，$asin$返回的角度范围是$[-\\pi/2,\\pi/2]$。 1234567static void getSphereUV(const Vector3D &amp;p, Vector2D &amp;tex)&#123; float phi = atan2(p.z, p.x); float theta = asin(p.y); tex.x = 1 - (phi + M_PI) / (2*M_PI); tex.y = (theta + M_PI/2) / M_PI;&#125; 二、三角网格模型&emsp;&emsp;除了像球体、圆柱、圆锥等等这类有显示数学表达式的几何体，我们接触到更多的是没有表达式的网格模型。有显示的数学表达式当然好，因为我们直接直接求交点的解析解，非常准确。这里我们构建一个通用的网格模型类，它由一个个三角形构成。obj模型的导入我不再赘述，这里重点讲述了射线与三角形求交的推导过程。 &emsp;&emsp;一个三角形由空间中的三个顶点$P_0$、$P_1$、$P_2$的位置表示，三角形所在平面的法向量$N$可由下式计算而得： N=(P_1-P_0)\\times(P_2-P_0) \\tag {4}&emsp;&emsp;平面与原点的距离$d$等于平面法向量$N$与平面中任意一点的内积的负数，这里选$P_0$，则$d$为： d = -N\\cdot P_0 \\tag {5}&emsp;&emsp;则三角形所在的平面可以用四维向量$(N, -N\\cdot P_0)$表示，实际上三角形所在平面的表达式为$N\\cdot(x,y,z)+d=0$，首先我们求射线与该平面的交点，然后再判断交点是否在三角形内部。将射线方程$P(t) = S+tV$带入平面的方程，则有： N\\cdot P(t)+d=0\\\\ \\to N\\cdot S +(N\\cdot V)t +d=0\\\\ \\to t=-\\frac{(N\\cdot S+d)}{N\\cdot V}&emsp;&emsp;通过以上的方程我们就可以得到射线在平面$L$上的交点处的$t$值。需要注意的是，当$N\\cdot V=0$时，射线与平面平行，不存在交点。然后我们把$t$值带入射线方程即可求出射线与平面的交点$P$。接下来的问题是判断点$P$是否位于三角形内部，通过计算点$P$对于三角形的三个顶点$P_0$、$P_1$、$P_2$的重心坐标可以完成该判断。重心坐标是三角形顶点加权平均值，由三个标量$\\omega_0$、$\\omega_1$和$\\omega_2$组成，有： P=\\omega_0 P_0+\\omega_1 P_1 + \\omega_2 P_2 \\tag {6}&emsp;&emsp;其中，$\\omega_0+\\omega_1+\\omega_2 =1$，用$1-\\omega_1-\\omega_2$代替$\\omega_0$，可得： P=(1-\\omega_1-\\omega_2)P_0+\\omega_1P_1+\\omega_2P_2 \\\\ =P_0+\\omega_1(P_1-P_0)+\\omega_2(P_2-P_0) \\tag {7}&emsp;&emsp;定义以下的等式： R=P-P_0\\\\ Q_1=P_1-P_0\\\\ Q_2=P_2-P_0 \\tag {8}&emsp;&emsp;将公式$(9)$带入公式$(8)$，可得： R=\\omega_1Q_1+\\omega_2Q_2 \\tag {9}&emsp;&emsp;分别给式$(10)$两边乘$Q_1$和$Q_2$可得以下两个方程： R\\cdot Q_1=\\omega_1Q_1^2+\\omega_2(Q_1\\cdot Q_2)\\\\ R\\cdot Q_2=\\omega_1(Q_1\\cdot Q_2)+\\omega_2Q_2^2 \\tag {10}&emsp;&emsp;写成矩阵形式如下： \\left[ \\begin{matrix} Q_1^2 & Q_1\\cdot Q_2\\\\ Q_1\\cdot Q_2 & Q^2_2 \\end{matrix} \\right] \\left[ \\begin{matrix} \\omega_1\\\\ \\omega_2 \\end{matrix} \\right] = \\left[ \\begin{matrix} R\\cdot Q_1\\\\ R\\cdot Q_2 \\end{matrix} \\right] \\tag {11}&emsp;&emsp;解以上关于$\\omega_1$和$\\omega_2$的方程，可得： \\left[ \\begin{matrix} \\omega_1\\\\ \\omega_2 \\end{matrix} \\right] = \\left[ \\begin{matrix} Q_1^2 & Q_1\\cdot Q_2\\\\ Q_1\\cdot Q_2 & Q^2_2 \\end{matrix} \\right]^{-1} \\left[ \\begin{matrix} R\\cdot Q_1\\\\ R\\cdot Q_2 \\end{matrix} \\right] \\\\ =\\frac1{Q^2_1Q^2_2-(Q_1\\cdot Q_2)^2} \\left[ \\begin{matrix} Q_2^2 & -Q_1\\cdot Q_2 \\\\ -Q_1\\cdot Q_2 & Q_1^2 \\end{matrix} \\right] \\left[ \\begin{matrix} R\\cdot Q_1\\\\ R\\cdot Q_2 \\end{matrix} \\right] \\tag {12}&emsp;&emsp;当且仅当$\\omega_0$、$\\omega_1$和$\\omega_2$三个权值均为非负值时，点$R$位于三角形内部，由于$\\omega_0=1-\\omega_1-\\omega_2$，则此时应有$\\omega_1+\\omega_2\\leq 1$且$\\omega1 \\geq 0\\ and\\ \\omega_2\\geq0$。若顶点$P_0$、$P_1$和$P_2$上关联有一些属性信息，如颜色、法向量或者纹理坐标，则可以利用权值$\\omega_0$、$\\omega_1$和$\\omega_2$对这些属性信息进行插值。 123456789101112131415161718192021222324252627282930313233343536bool MeshHitable::triangleHit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret, const Vertex &amp;p0, const Vertex &amp;p1, const Vertex &amp;p2, const Vector3D &amp;normal) const&#123; float n_dot_dir = normal.dotProduct(ray.getDirection()); // no intersection. if (equal(n_dot_dir, 0.0)) return false; float d = -normal.dotProduct(p0.m_position); float t = -(normal.dotProduct(ray.getOrigin()) + d) / n_dot_dir; if (t &lt; t_min || t &gt; t_max) return false; ret.m_t = t; ret.m_position = ray.pointAt(t); ret.m_material = m_material; // judge inside or not. Vector3D r = ret.m_position - p0.m_position; Vector3D q1 = p1.m_position - p0.m_position; Vector3D q2 = p2.m_position - p0.m_position; float q1_squaredLen = q1.getSquaredLength(); float q2_squaredLen = q2.getSquaredLength(); float q1_dot_q2 = q1.dotProduct(q2); float r_dot_q1 = r.dotProduct(q1); float r_dot_q2 = r.dotProduct(q2); float determinant = 1.0f / (q1_squaredLen * q2_squaredLen - q1_dot_q2 * q1_dot_q2); float omega1 = determinant * (q2_squaredLen * r_dot_q1 - q1_dot_q2 * r_dot_q2); float omega2 = determinant * (-q1_dot_q2 * r_dot_q1 + q1_squaredLen * r_dot_q2); if (omega1 + omega2 &gt; 1.0f || omega1 &lt; 0.0f || omega2 &lt; 0.0f) return false; ret.m_normal = p0.m_normal * (1.0f - omega1 - omega2) + p1.m_normal * omega1 + p2.m_normal * omega2; ret.m_texcoord = p0.m_texcoord * (1.0f - omega1 - omega2) + p1.m_texcoord * omega1 + p2.m_texcoord * omega2; if (ret.m_normal.dotProduct(ray.getDirection()) &gt; 0.0f) ret.m_normal = -ret.m_normal; return true;&#125; &emsp;&emsp;既然模型是由一个个三角形组成，那么在判断射线与当前的模型是否存在交点时，我们就遍历所有的三角形，一个一个三角形与射线做相交判断： 1234567891011121314151617181920212223bool MeshHitable::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const&#123; HitRecord tmpRec; bool hitAny = false; float closestSoFar = t_max; for (int x = 0; x &lt; m_indices.size(); x += 3) &#123; int index1 = m_indices[x + 0]; int index2 = m_indices[x + 1]; int index3 = m_indices[x + 2]; if (triangleHit(ray, t_min, closestSoFar, tmpRec, m_vertices[index1], m_vertices[index2], m_vertices[index3], m_faceNormal[x / 3])) &#123; hitAny = true; closestSoFar = tmpRec.m_t; ret = tmpRec; &#125; &#125; return hitAny;&#125; 三、添加光源&emsp;&emsp;光源是一种特殊的物体，一般情况下它不反射、折射光线，而是自身发射光线。因此，为了实现一个光源，当我们的射线碰撞到光源表面时，我们直接返回光源的碰撞点的颜色，不再做折射和反射。我们将发光的逻辑放到材质中，并将发光这一行为抽象为$emitted$函数。对于非光源物体，我们可以看成发出的光rgb均为0。 123456789101112131415161718192021222324252627282930313233343536373839404142class Material&#123;public: typedef std::shared_ptr&lt;Material&gt; ptr; Material() = default; virtual ~Material() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, ScatterRecord &amp;srec) const &#123; return false; &#125; virtual float scattering_pdf(const Ray &amp;in, const HitRecord &amp;rec, const Ray &amp;scattered) const &#123; return 1.0f; &#125; virtual Vector3D emitted(const Ray &amp;in, const HitRecord &amp;rec, const float &amp;u, const float &amp;v, const Vector3D &amp;p) const &#123; return Vector3D(0.0f, 0.0f, 0.0f); &#125;&#125;;class DiffuseLight : public Material&#123;private: unsigned int m_emitTex; public: typedef std::shared_ptr&lt;DiffuseLight&gt; ptr; DiffuseLight(unsigned int a) : m_emitTex(a) &#123; &#125; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, ScatterRecord &amp;srec) const &#123; return false; &#125; virtual Vector3D emitted(const Ray &amp;in, const HitRecord &amp;rec, const float &amp;u, const float &amp;v, const Vector3D &amp;p) const;&#125;;Vector3D DiffuseLight::emitted(const Ray &amp;in, const HitRecord &amp;rec, const float &amp; u, const float &amp; v, const Vector3D &amp; p) const&#123; return TextureMgr::getSingleton()-&gt;getTexture(m_emitTex)-&gt;sample(u, v, p);&#125; &emsp;&emsp;这样，对于任意的物体，我们都可以把它当作一个光源，只要给这个物体赋予的材质为$DiffuseLight$即可，同时要注意给发光材质设置一个纹理。 四、天空盒背景&emsp;&emsp;之前在光线投射到背景中时，我们是直接返回设定的背景颜色（或通过插值、或直接指定背景）。同样，我们可以通过天空盒来丰富我们的场景细节。天空盒的相关原理比较简单，不再赘述。一个天空盒用边长为1的立方体表示，一个立方体我采用多个三角形构成立方体网格。这里有个问题，就是如何实现天空盒永远无法靠近的效果。在实时渲染时我们直接移除视图矩阵的位移，在光追这里我们直接将光源的出发点设为原点，方向保持不变，这样的一条射线再与天空盒立方体做求交并采样纹理即可。 1234567891011121314151617181920212223242526272829Vector3D Skybox::sampleBackground(const Ray &amp;ray)&#123; HitRecord rec; Ray r(Vector3D(0,0,0), ray.getDirection()); TextureMgr::ptr texMgr = TextureMgr::getSingleton(); int index = -1; for (int x = 0; x &lt; m_indices.size(); x += 3) &#123; int index1 = m_indices[x + 0]; int index2 = m_indices[x + 1]; int index3 = m_indices[x + 2]; if (triangleHit(r, 0.001f, FLT_MAX, rec, m_vertices[index1], m_vertices[index2], m_vertices[index3], m_vertices[index1].m_normal)) &#123; index = x; break; &#125; &#125; if(index != -1) &#123; int map = index / 6; return texMgr-&gt;getTexture(m_cubemap[map]) -&gt;sample(rec.m_texcoord.x, rec.m_texcoord.y, rec.m_position); &#125; else return Vector3D(0.0,0.0,0.0);&#125; 五、构建BVH树&emsp;&emsp;在整个光线追踪算法的渲染过程中，计算量最大的就是光线与场景图元的求交过程。如果不采用一些特殊的数据结构而只是用线性表存储场景物体的话，那么每一条射线都需要对这个存储场景物体的线性表遍历一次，这个射线碰撞检测的算法时间复杂度是$O(n)$的，当$n$比较大时，那么射线碰撞检测需要耗费绝大部分的光线追踪算法时间。射线相交检测的时间是目前光线追踪算法从理论到大规模实际应用过渡的主要瓶颈。为此，我们需要一些特殊的场景管理数据结构来加速这个过程，BVH树（全称为bounding volume hierachy，即层次包围体）是光线追踪领域常用的一种3D场景管理数据结构。它的启发思路就是通过一个简单的包围盒把物体包围起来，射线和场景中的物体求交之前，会先和这个包围盒进行求交，如果该射线没有碰到该包围盒，表明该直线一定不会和包围盒里的物体相交；如果该射线碰到该包围盒，那么再来计算射线是否和包围盒中的物体相交。我们采用包围体是AABB包围盒（即axis-aligned minimum bounding box，轴对齐的最小包围盒，简称轴向包围盒）。 &emsp;&emsp;BVH树本质上是对空间做分割，然后采用二分搜索快速判断射线会与哪些包围盒发生碰撞，从而使得算法的时间复杂度从$O(n)$降到了$O(log(n))$，这是一个非常明显的算法效率的提升，特别是当$n$数量逐渐增大的时候。每一次的判断过程如下列伪代码所示。如果射线与父节点的包围盒有交点，则进一步判断子节点与射线的相交情况，否则直接退出。 1234if (ray hits bounding object) return whether ray hits bounded objectselse return false &emsp;&emsp;BVH树全称是层次包围盒，故名思意，它是一个树形的层次结构，父节点的包围盒包围全部子节点所在的空间，正如下图11所示。蓝色和红色的包围盒被包含在紫色的大包围盒中，它们可能重叠，并且它们不是有序的，它们只是单纯地被包含在内部。 图1 层次包围盒 &emsp;&emsp;对于图1，检测的伪代码如下： 123456if (hits purple) hit0 = hits blue enclosed objects hit1 = hits red enclosed objectsif (hit0 or hit1) return true and info of closer hitreturn false 1、射线与包围盒相交判断&emsp;&emsp;我们采用的紧凑的包围盒是AABB包围盒，计算出了包围盒之后，我们还需要一个判断射线是否与包围盒相交的办法，不需要求出射线与包围盒的交点，只需判断是否存在交点即可！我们采用一种常见的”slab“方法，它是基于AABB包围盒的。三维的AABB包围盒由三个轴的区间表示，假设分别为$[x_0,x_1]$、$[y_0,y_1]$、$[z_0,z_1]$。 &emsp;&emsp;对于每一个区间，我们首先判断射线在边界的投影交点情况。三维空间中，$x=x_0$和$x=x_1$是一个平面，射线在这两个平面上的交点的$x$值可以通过将$x=x_0$和$x=x_1$带入射线的方程$P(t)=S+tV$的$x$分量得到： x_0=S_x+t_0*V_x \\\\ x_1=S_x+t_1*V_x \\tag {13} 图2 射线与边界的交点 &emsp;&emsp;从而可以求出$t_0$和$t_1$如下所示： t_0=\\frac{x_0-S_x}{V_x} \\\\ t_1=\\frac{x_1-S_x}{V_x} \\tag {14}&emsp;&emsp;关于$y$轴和$z$轴同理，我们求出了每条轴的交点分量，那么如何快速判断射线与包围盒区域是否存在相交的情况呢？为了便于理解，我们以二维的情况为例，则射线与二维的包围区域相交由如下三种情况： 图3 射线与边界相交的三种情况 &emsp;&emsp;我们求得$t$值是关于射线上的电到射线原点的距离，通过仔细观察上面的三张图片，我们可以发现在二维的情况下，当$max(t_0,t_2)&gt;min(t_1,t_3)$时，射线一定和区域存在交点，即射线与每个轴区间的左端点中的最大$t$值大于射线与每个轴区域间的右端点中的最小$t$值。 123456789101112131415161718192021222324252627282930313233343536bool hit(const Ray &amp;ray, float tmin, float tmax) const&#123; float t0, t1, invD; // x invD = 1.0f / ray.getDirection().x; t0 = (m_min.x - ray.getOrigin().x) * invD; t1 = (m_max.x - ray.getOrigin().x) * invD; if (invD &lt; 0.0f) std::swap(t0, t1); tmin = t0 &gt; tmin ? t0 : tmin; tmax = t1 &lt; tmax ? t1 : tmax; if (tmax &lt;= tmin) return false; // y invD = 1.0f / ray.getDirection().y; t0 = (m_min.y - ray.getOrigin().y) * invD; t1 = (m_max.y - ray.getOrigin().y) * invD; if (invD &lt; 0.0f) std::swap(t0, t1); tmin = t0 &gt; tmin ? t0 : tmin; tmax = t1 &lt; tmax ? t1 : tmax; if (tmax &lt;= tmin) return false; // z invD = 1.0f / ray.getDirection().z; t0 = (m_min.z - ray.getOrigin().z) * invD; t1 = (m_max.z - ray.getOrigin().z) * invD; if (invD &lt; 0.0f) std::swap(t0, t1); tmin = t0 &gt; tmin ? t0 : tmin; tmax = t1 &lt; tmax ? t1 : tmax; if (tmax &lt;= tmin) return false; return true;&#125; 2、BVH树的构建&emsp;&emsp;首先我们要考虑如何构建一颗BVH树，BVH数据结构本质就是一颗二叉树。每个树节点右两个子节点，当然子节点之间不存在空间上的顺序关系。树的内部节点都不存储实际的场景物体，仅存储一个包围盒，叶子节点才存储真正的场景物体。构建BVH树的工作考虑的是如何构造一棵可以有效描述当前场景信息的二叉树。这当中的关键是如何对毫无规律地散落在场景中的众物体进行划分，即决定哪些物体该划分到左子树上，哪些物体该划分到右子树上。我们可以把这个问题抽象成一个”划分策略“——我们总会按照某种”策略“划分场景的，待会再考虑具体有哪些策略。另外，由于我们是在3D空间中工作，为了将问题简化，用分而治之的角度看，我们可以首先建立一个”原则“：即决定在哪根轴（x,y,z）上进行划分。”原则“与”策略“的不同之处在于，不管用何种”策略“，总是遵守同一种”原则“。 &emsp;&emsp;决定在哪根轴（x,y,z）上进行划分，取决于场景中的物体在各个轴上分布的“散度”。如果这些物体沿着某根轴分布得最为“松散”（即落在该轴上靠一侧最近的物体与另一侧最近的物体，二者距离为最大），那么就沿该轴进行划分。还有一种方式，即采用随机的方式选取划分的轴，这样当场景物体分散的很随机时，实现的效果还不错。这里我采用随机选取一个轴的方法进行划分。 &emsp;&emsp;确定了以哪根轴进行划分，接下来就要考虑“怎么划分”。我们目前暂时实现按终点划分的策略，顾名思义，取中点划分的意思就是在先前选取的轴上取其中点作为划分点，中点以左划分到左子树，中点以右划分到右子树。这种划分的实现方式最为简单，但往往效果不是太好：因为物体的分布往往不是均匀的。其中一种糟糕的情况（a）是，某侧子树上可能会拥挤过多的物体，而另一侧子树上却太少，这对查找效率影响很大。另外还有一种糟糕的情况（b），就是包围盒之间互相“重叠”（overlapped）的情况。如果两棵子树对应的包围盒“重叠”的越多，那么一条射线穿过该区域时同时击中两子树的概率也就越大，这就意味着两棵子树都得进行相交测试。当然我们目前实现的BVH树没有考虑那么多。 1234567891011121314151617181920212223242526272829BVHNode::BVHNode(std::vector&lt;Hitable *&gt; &amp;list, int start, int end)&#123; // sort it randomly depend on int axis = static_cast&lt;int&gt;(3 * drand48()); if (axis == 0) sort(&amp;list[start], &amp;list[end], boxCompareX); else if (axis == 1) sort(&amp;list[start], &amp;list[end], boxCompareY); else if (axis == 2) sort(&amp;list[start], &amp;list[end], boxCompareZ); int length = end - start; if (length == 1) m_left = m_right = list[start]; else if (length == 2) &#123; m_left = list[start]; m_right = list[start + 1]; &#125; else &#123; m_left = new BVHNode(list, start, start + length / 2); m_right = new BVHNode(list, start + length / 2, end); &#125; // bounding box. AABB boxLeft, boxRight; if (!m_left-&gt;boundingBox(0, 0, boxLeft) || !m_right-&gt;boundingBox(0, 0, boxRight)) std::cerr &lt;&lt; \"no bounding box in BVHNode constructor\\n\"; m_box = AABB::surroundingBox(boxLeft, boxRight);&#125; 3、BVH树的遍历&emsp;&emsp;遍历BVH差不多是件直截了当的事情。在遍历的过程中，当发现射线与某个子节点相交的话，那么有无必要再检测下与另一子节点是否相交？答案是要的。因为两个节点无法保证完全“不重叠”，如下图所示，很有可能在检测另一子节点时发现了更近的交点。 123456789101112131415161718192021222324252627282930313233bool BVHNode::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const&#123; if (m_box.hit(ray, t_min, t_max)) &#123; HitRecord leftRec, rightRec; bool hitLeft = m_left-&gt;hit(ray, t_min, t_max, leftRec); bool hitRight = m_right-&gt;hit(ray, t_min, t_max, rightRec); // both hit. if (hitLeft &amp;&amp; hitRight) &#123; if (leftRec.m_t &lt; rightRec.m_t) ret = leftRec; else ret = rightRec; return true; &#125; // only left child. else if (hitLeft) &#123; ret = leftRec; return true; &#125; else if (hitRight) &#123; ret = rightRec; return true; &#125; else return false; &#125; else return false;&#125; 六、tbb多线程渲染&emsp;&emsp;到目前为止我们实现的光追渲染逻辑都是串行的，只能利用单核cpu运行我们的渲染程序。对于简单的场景来说，渲染的速度还是挺快的。但是当我们渲染复杂的模型时，单核光追的渲染速度慢到爆炸，渲染时间随着模型的面片数迅速增长，渲染时间动不动就数十小时！为此，我们迫切需要加速渲染程序。我们可以看到，每个像素着色之间是没有联系的，一个像素的着色值与其周围的像素计算无关，所以像素的着色计算是可以并行计算的。我们首先实现在cpu上利用多核加速我们的渲染程序。直接操纵原生的线程API不是非常好，因为这样的话我们必须知道当前电脑的核心数，并据此将循环做一个分割，以便充分利用每个cpu核心。Intel开发的TBB是非常有用的线程库，它屏蔽了底层的线程细节，自动根据我们给定的工作量做线程分割，充分利用电脑的全部cpu资源，而且使用起来也非常简单。这里利用多核线程的渲染速度加速比大致是当前电脑的核心数，也就是说，电脑的cpu核心越多，渲染速度越快。tbb的官方网站请看这里。 &emsp;&emsp;tbb的全称是Thread Building Blocks，这里我们只用了tbb的parallel_for接口，它对一个给定的for循环做划分，然后每个划分并行计算。我采用的parallel_for接口函数如下所示： 12template&lt;typename Range, typename Body&gt;void parallel_for( const Range&amp; range, const Body&amp; body, const auto_partitioner&amp; partitioner ) &emsp;&emsp;可以看到出现了三个参数：range、body和partitioner。range就是我们要做并行的for循环下标范围，通常采用一维的迭代器blocked_range指定。这里我把二重循环展开成一重循环。然后body就是函数执行体，这里我通过c++11的lambda表达式指定。最后的partitioner是线程的划分方法，通常直接采用auto_partitioner。并行版的光追渲染如下所示： 12345678910111213141516171819202122232425262728void Tracer::parallelThreadRender(Hitable *scene)&#123; parallel_for(blocked_range&lt;size_t&gt;(0, m_config.m_height * m_config.m_width, 10000), [&amp;](blocked_range&lt;size_t&gt;&amp; r) &#123; for (size_t i = r.begin(); i != r.end(); ++i) &#123; Vector4D color; size_t col = i % m_config.m_width; size_t row = i / m_config.m_width; for (int x = 0; x &lt; m_config.m_samplings; ++x) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = m_config.m_camera-&gt;getRay(u, v); color += deNan(tracing(ray, scene, &amp;m_samplingList,0)); &#125; color /= static_cast&lt;float&gt;(m_config.m_samplings); color.w = 1.0f; // gamma correction. color = Vector4D(sqrt(color.x), sqrt(color.y), sqrt(color.z), color.w); if(color.x &gt; 1.0f) color.x = 1.0f; if(color.y &gt; 1.0f) color.y = 1.0f; if(color.z &gt; 1.0f) color.z = 1.0f; drawPixel(col, row, color); &#125; &#125;, auto_partitioner());&#125; 七、蒙特卡罗积分&emsp;&emsp;蒙特卡罗积分方法（Monte Carlo method）是数值分析中的一个重要分支，它的核心概念是使用随机性来解决确定性的问题。大数定律告诉我们，对于满足某个概率分布的随机变量，其数学期望所描述的积分可以使用这个随机变量随机抽样的样本均值来近似，因此在一定的误差范围内，我们能够使用大量的随机数来近似积分运算的结果。在计算机图形学中， 蒙特卡罗方法主要被应用于物理模拟以及光照传输中的积分运算，在离线渲染领域， 渲染方程几乎只能使用蒙特卡洛方法来进行计算。为了深入理解蒙特卡罗方法，我们首先要复习概率论相关的一些基础内容。以下的内容主要参考秦春林的那本书《全局光照技术：从离线到实时渲染》。 1、概率密度函数、概率分布函数&emsp;&emsp;概率密度函数（probability density function, 简称PDF）用于描述连续型随机变量所服从的概率分布，对于连续随机变量$X$，其概率密度函数$p(x)$是通过落于$x$附近的区间$[x,x+dx]$内的随机数的概率$p(x)dx$来定义的，然而这种定义方式并不直观，所以连续随机变量的概率分布一般通过更直观的称为概率分布函数或者累积分布函数（cumulative distribution function, 简称CDF）来定义，连续随机变量$X$的累积分布函数用大写字母$P$表示，其定义如下： P(y)=Pr\\{x\\leq y\\}=\\int_{-\\infty}^yp(x)dx \\tag {15}&emsp;&emsp;可以看到，概率分布函数$P(y)$定义的是所有随机数的值中小于或等于$y$的随机变量的概率的积分，即理解成对于一个随机数$x$，其小于等于$y$的概率。因此，概率分布函数是一个递增函数。连续随机变量的概率密度函数$p(x)$具有以下的属性： \\forall x:p(x) \\geq 0 \\tag {16} \\int _{-\\infty}^{+\\infty}p(x)dx = 1 \\tag {17} p(x)=\\frac{dP(x)}{dx} \\tag {18}&emsp;&emsp;其中，式$(8)$说明了$p(x)$和$P(x)$的关系，前者是后者的导数。那么给定一个随机变量的区间范围$[a,b]$，随机变量的值$x$落在这个区间的概率计算如下： Pr\\{a\\leq x\\leq b\\}=Pr(x\\leq b)-Pr(x\\leq a)\\\\ =P(b)-P(a)=\\int_a^b p(z)dz \\tag {19}&emsp;&emsp;注意，这里的$Pr$函数是概率函数，而不是概率分布函数。直观来讲，概率密度函数$p(x)$给定的并不是随机变量取值$x$的概率，概率密度函数与轴围成的面积才是给定的概率。如下所示，图(a)是概率分布函数，而图$(b)$则是概率密度函数，给定区间的$[a,b]$的概率就是下图(b)中的面积，这也对应了公式$(19)$中的积分形式（积分的几何意义就是面积）。 &emsp;&emsp;在这里，我们要特别关注的一个分布，那就是均匀分布！对于$[a,b]$区间上的均匀分，其概率密度函数为常数$\\frac{1}{b-a}$，它表示随机抽样结果落于区间$[x,x+dx]$的概率在每个$x$处都相同。均匀分布的随机变量是整个蒙特卡罗方法的基础，在计算机模拟中，通过都是由系统提供的random()函数生成某个区间内的均匀分布，然后通过一些方法将均匀分布的随机变量转换为具有任意概率密度分布的随机变量。 2、数学期望&emsp;&emsp;对于离散随机变量$X$，假设其值$x_i$对应的抽样概率为$p_i$，则该随机变量$X$的数学期望，或称为均值，为： E[X]=\\Sigma_{i=1}^np_ix_i \\tag {20}&emsp;&emsp;数学期望代表的是对一个随机变量$X$进行抽样的平均结果。例如，对于骰子的例子，它的数学期望为： E[X_{die}]=\\Sigma_{i=1}^6p_i x_i\\\\ =\\Sigma_{i=1}^6\\frac16x_i=\\frac16(1+2+3+4+5+6)=3.5 \\tag {21}&emsp;&emsp;相应地，对于连续随机变量$X$，其期望值为随机变量值$x$与其概率密度函数$p(x)$的乘积在全定义域上的积分： E[X]=\\int_{-\\infty}^{+\\infty}xp(x)dx \\tag {22}&emsp;&emsp;连续随机变量$X$的数学期望为什么上面的公式$(22)$形式呢？这其实可以通过离散划分连续随便变量的定义域，然后按照离散数学期望得到一个近似的公式，当划分数趋向于无穷大且划分区间趋向于无穷小时，就是公式$(22)$的积分定义。如下所示： E[X]\\approx\\frac{b-a}{n}\\Sigma_{i=1}^{n}x_ip(x_i) \\\\ n\\to+\\infty,\\ \\frac{b-a}{n}\\Sigma_{i=1}^{n}x_ip(x_i)=\\int_a^bxp(x)dx=E[X]&emsp;&emsp;通常我们对随机变量的函数更感兴趣。考虑以随机变量$X$为自变量的函数$Y=g(X)$，我们只知道随机变量$X$的概率分布，怎样求出随机变量$Y$的数学期望值呢？我们可以通过无意识的统计规律（law of the unconsicious statistician）来求随机变量函数的数学期望：设$Y$是随机变量$X$的函数$Y=g(X)$，且函数$g$是连续函数。若$X$是离散型随机变量，它的概率函数为$P\\{X=x_i\\}=p_i,i=1,2,…$，则有： E[Y]=E[g(X)]=\\Sigma_{i=1}^{\\infty}g(x_i)p_i \\tag {23}&emsp;&emsp;若$X$是连续型随机变量，它的概率密度函数为$p(x)$，则有： E[Y]=E[g(X)]=\\int_{-\\infty}^{+\\infty}g(x)p(x)dx \\tag {24}&emsp;&emsp;该方法的重要意义在于：当求$E[Y]$时，我们不必求出$Y$的分布律或概率密度函数，只需利用$X$的分布律或概率密度即可。 3、大数定律&emsp;&emsp;在统计学中，很多问题涉及对大量独立的随机变量抽样$x_i$的和进行处理，这些随机变量拥有相同的概率密度函数$p(x)$，这样的随机变量称为独立同分布的随机变量。当这些随机变量抽样的和被除以这些随机变量抽样的数量$N$时，我们就得到该随机变量的期望值的一个估计： E[X]\\approx\\overline X=\\frac1N\\Sigma_{i=1}^Nx_i \\tag {25}&emsp;&emsp;随着抽象数量$N$的增大，该估计的方差逐渐减小。当$N$的值足够大时，该估计的值就能够充分接近实际数学期望的值，这样我们就能够将统计方法用于解决确定性问题。大数定律（law of large numbers）告诉我们，当$N\\to\\infty$时，我们可以确定随机变量的统计平均值趋近于数学期望的值，即： P\\{E[X]=lim_{N\\to \\infty}\\frac1N\\Sigma_{i=1}^Nx_i\\} = 1 \\tag {26}&emsp;&emsp;因此，随机变量的数学期望可以通过对随机变量执行大量的重复抽样来近似计算得到。 4、蒙特卡罗积分&emsp;&emsp;假设我们要计算一个一维函数的积分，如$\\int_a^bf(x)dx$，数值分析方法通常采用一些近似方法来计算积分。一种最简单的求积分的方法就是采用梯形法，它通过将被积函数沿作用域上划分成多个区域，然后计算这些区域面积的和。这种方法不适用于多维积分的计算，计算机图形学领域用的最多的还是蒙特卡罗方法。大数定律用于对数学期望的积分公式进行估计，即对积分$\\int_{-\\infty}^{+\\infty}xf(x)dx$进行估计。但是通常情况下我们要求的积分公式是对任意的一个函数积分，假设函数$g(x)$的定义域为$x\\in S$（可以是一个多维空间），我们希望计算如下的积分： I=\\int_{x\\in S}g(x)dx \\tag {27}&emsp;&emsp;现在先不管公式$(27)$。由前面我们知道，给定任意一个关于随机变量的实数函数$f$以及服从$p(x)$概率密度函数的随机变量$x$，我们可以采用如下的公式来近似计算随机变量函数$f(x)$的数学期望： E[f(x)]=\\int_{x\\in S}f(x)p(x)dx\\approx\\frac1N\\Sigma_{i=1}^Nf(x_i) \\tag {28}&emsp;&emsp;现在我们令公式$(27)$的被积函数$g(x)=f(x)p(x)$，则$f(x)=\\frac{g(x)}{p(x)}$，那么公式$(28)$即可转变对公式$(27)$的形式，如下所示： \\int_{x\\in S}f(x)p(x)dx=\\int_{x\\in S}g(x)dx\\approx\\frac1N\\Sigma_{i=1}^N\\frac{g(x_i)}{p(x_i)} \\tag {29}&emsp;&emsp;可以看到通过这个变换，我们巧妙地转换成我们要求的积分公式，这就是蒙特卡洛方法求积分的核心思想。公式$(29)$的期望值为： E[\\frac1N\\Sigma_{i=1}^N\\frac{g(x_i)}{p(x_i)}]=\\frac1N\\Sigma_{i=1}^NE[\\frac{g(x_i)}{p(x_i)}]\\\\ =\\frac1NN\\int\\frac{g(x)}{p(x)}p(x)dx=\\int g(x)dx \\tag {30}&emsp;&emsp;而公式$(29)$的估计方差为： \\sigma^2=\\frac1N\\int(\\frac{g(x)}{p(x)}-I)^2p(x)dx \\tag {31}&emsp;&emsp;可以看到，随着$N$的增大，公式$(31)$的方差随之降低（成反比），这就是一般蒙特卡罗方法的特点。实际上蒙特卡罗方法最大的问题就是估计逼近正确结果的速度非常慢。理论上，公式$(29)$的$p(x)$函数的选择可以是任意的，这也是蒙特卡罗方法的优点，因为通常很难生成与被积函数具有一致分布的随机数。从公式$(31)$也可以看出，通过使$g(x_i)$和$p(x_i)$的比值尽可能地小也可以减少估计误差，在实践上通常我们尽可能地使$p(x)$的分布接近于$g(x)$。综上，蒙特卡洛积分方法计算任意函数的积分步骤如下： 首先对一个满足某种概率分布的随机数进行抽样； 使用该抽样值计算$\\frac{g(x_i)}{p(x_i)}$的值，这称为该样本的贡献值； 最后对所有抽样点计算的结果求平均值。 &emsp;&emsp;上面的步骤中，最困难的就是怎么样对一个具有任意分布函数的随机变量进行抽样。 5、随机抽样&emsp;&emsp;首先定义什么是抽样。给定一个定于域空间$\\Omega_0$及其概率密度函数$p(x)$，其中$x\\in \\Omega_0$，则应有： \\int_{\\Omega_0}p(x)dx=1 \\tag {32}&emsp;&emsp;抽样是这样的一个算法，它能够从$p(x)$对应的随机变量$X$中产生一系列随机数$X1,X2,…$，使得对任意的$\\Omega \\in \\Omega_0$满足如下： P\\{X_k\\in\\Omega\\}=\\int_{\\Omega}p(x)dx\\leq 1 \\tag {33}&emsp;&emsp;在实现中我们并不能直接从$p(x)$产生随机数，在计算机程序中这个过程必须要求首先具有某些基础随机数的一个序列。我们通常采用均匀随机数random来产生一个均匀分布的随机数，然后用来作为抽象所需的基础随机数。目前抽象方法根据不同情况有不同的方法，这里目前只介绍逆变换算法。 &emsp;&emsp;逆变换算法的定义为：设$X$是连续随机变量，其概率分布函数为$P_X$，若随机变量$Y$是一个$[0,1]$上的均匀分布，则随机变量$P_X^{-1}(Y)$具有和$X$一样的概率分布。即我们通过概率分布函数的反函数来获取服从$p(x)$概率密度函数的随机变量，注意是概率分布函数$P(x)$的反函数，而不是概率密度函数$p(x)$的反函数。有时我们不知道概率分布函数，这时我们可以通过概率密度函数来求它的概率分布函数。 &emsp;&emsp;逆变换算法从一个概率密度函数$p(x)$产生随机数$X_i$的步骤如下： 首先计算$p(x)$的概率分布函数：$P(x)=\\int_0^xp(t)dt$； 其次计算累计分布函数的反函数：$P^{-1}(x)$； 然后从一个$[0,1]$上的均匀分布产生一个随机数$\\phi$； 最后将随机数$\\phi$代入$P^{-1}(x)$求出服从$p(x)$分布的随机数：$X_i=P^{-1}(\\phi)$。 八、重要性采样&emsp;&emsp;重要性采样（importance sampling）是蒙特卡罗方法中最重要的方差缩减方法，它通过选择对一个与目标概率分布具有相似形状的分布函数进行抽样来减少方差。重要性采样试图在被积函数中贡献较多的区域放置更多的采样点，以体现这部分区域的重要性。给定一个概率密度函数$p(x)$以及根据该概率密度函数抽样得到的$N$个随机数$x_i$，根据蒙特卡洛方法，被积函数$f(x)$的积分$I$（即前面的公式（27），被积函数换成$f(x)$）可以通过以下公式来近似估计： I_{N}=\\frac1N\\Sigma_{i=1}^N\\frac{f(x_i)}{p(x_i)} \\tag {34}&emsp;&emsp;一个理想的估计的方差应该为$0$，即： \\sigma^2=\\frac1N\\int(\\frac{f(x)}{p(x)}-I)^2p(x)dx=0 \\tag {35}&emsp;&emsp;注意到公式$(35)$中，被积函数部分的$p(x)&gt;0$，故应有$(\\frac{f(x)}{p(x)}-I)^2=0$，从而有如下的推导： p(x)=\\frac{|f(x)|}{I} \\tag {36}&emsp;&emsp;若我们采用公式$(36)$得到的概率密度函数进行采样，那么方差就会被完全消除。但是公式$(36)$要求我们首先计算$I$的值，而这正是我们尝试去求解的，因而行不通。但是我们可以通过选取与被积函数$f(x)$具有相似形状的概率密度函数来减少方差。选择用于抽样的概率密度函数非常重要，尽管蒙特卡罗方法本身没有限制对概率密度函数的选择，但是选择不好的概率密度函数会大大增加蒙特卡罗估计的方差。 &emsp;&emsp;直观来讲，重要性采样就是根据被积函数$f(x)$的值来调整$p(x)$的概率分布。$f(x)$值大的地方，就多采样几个点；$f(x)$值小的地方，就少采样一些点。$p(x)$概率密度函数越是接近$f(x)$，蒙特卡罗方法估算的结果就越精确。 1、复合重要性采样&emsp;&emsp;在实际的情景中，计算机图形学中的被积函数通常非常复杂，它们可能是不连续的，通常在少数区间拥有奇点或者一些较大的值，所以很难找到一个简单的与被积函数相似的分布来做重要性采样。例如，我们考虑渲染中最普通的直接光源的计算公式，如下所示： L_o(p,v)=\\int_{\\Omega}f_r(l,v)\\times L_i(p,l)cos\\theta_id\\omega_i \\tag {37}&emsp;&emsp;我们可以选取$L_i$或者$f_r$来做重要性采样，但是这种方式表现效果并不佳。考虑一个接近镜面的BRDF表面被一个球形面积光照亮的例子。如下所示： &emsp;&emsp;若将面积光源的分布$L_i$作为重要性采样概率密度函数，因为物体表面几乎是镜面的，所以除了沿镜面反射光方向$\\omega_i$，大部分光源上的采样对在最终的光照贡献都为0，因此估计的方差会非常大；而若采用BRDF分布作为重要性采样分布，那么对于小面积光源，依然会导致很大的方差。 &emsp;&emsp;因此，我们通常使用更复杂的采样方式，从而降低估算的方差。通常是根据被积函数的分布特征对其进行区域划分，然后在不同特征的区域上使用不同的分布函数进行采样，最后将这些结果以某种方式进行混合。复合重要性采样就是这一类的采样方法，它提供了一个策略使得可以从多个不同的分布中采样，然后对这些不同的采样结果进行加权组合。复合重要性采样可以简单地分成以下几步： 首先，选取一系列的重要性分布$p1,…,p_n$，使得对于被积函数$f$的每一个函数值比较大的区域$\\Omega_i$，在这个区域$\\Omega_i$，分布函数$p_i$近似为被积函数$f$。通常一个复杂的被积函数是多个不相关的简单分布的乘积形式，所以这些重要性分布来源于这些简单分布。 然后，从每个分布$p_i$产生$n_i$个随机数$X_{i,1},…,X_{i,n_i}$； 最后，将所有的分布估算结果通过加权组合起来。 &emsp;&emsp;复合重要性采样加权组合公式如下所示： I_N=\\Sigma_{i=1}^n\\frac1n_i\\Sigma_{j=1}^{n_i}\\omega_i(X_{i,j})\\frac{f(X_{i,j})}{p_i(X_{i,j})} \\tag {38}&emsp;&emsp;公式$(38)$中的$\\Sigma_{i=1}^n$表明结果是由$n$个采样技术的叠加，$\\frac1n_i\\Sigma_{j=1}^{n_i}\\omega_i(X_{i,j})\\frac{f(X_{i,j})}{p_i(X_{i,j})}$即表示一种特定的采样分布$p_i$的蒙特卡罗估算结果。可以看到，这里还乘上了一个权重系数$\\omega_i(X_{i,j})$，$w_i(x)$可以在每个$x$处的值不一样，只要保证对于给定的$x$值，满足$\\Sigma_{i=1}^n\\omega_i(x)=1$即可。 2、平衡启发式&emsp;&emsp;现在我们需要确定公式$(38)$中的权重系数计算方法。假设我们采用两个采样分布$p_1(x)$和$p_2(x)$，两个采样分布单独的采样估算结果分别为$\\frac1{n_1}\\Sigma\\frac{f(x)}{p_1(x)}$和$\\frac{1}{n_2}\\Sigma\\frac{f(x)}{p_2(x)}$，它们各自的方差都很大，所以我们给它们各自乘上一个系数进行加权组合。为了尽可能地发挥每个采样分布的优势，我们往往尽可能地保证在每个区域贡献较大的采样分布拥有更大的权值系数。考虑如下的权重系数函数： \\omega_i(x)=\\frac{c_ip_i(x)}{\\Sigma_j^nc_jp_j(x)} \\tag {39}&emsp;&emsp;其中$c_i$是每个采样分布$p_i$对应的采样数量占比，即$c_i=\\frac{n_i}{N}$，故$\\Sigma_ic_i=1$，$c_i$在采样之前我们就可以确定得到。公式$(39)$被称为平衡启发式，将$c_i=\\frac{n_i}{N}$和公式$(39)$代入到公式$(38)$，可以推导出如下的标准的蒙特卡洛估算方法（做一些消去）： I_N= \\Sigma_{i=1}^n\\frac1n_i\\Sigma_{j=1}^{n_i}\\omega_i(X_{i,j})\\frac{f(X_{i,j})}{p_i(X_{i,j})}\\\\ =\\Sigma_{i=1}^n\\Sigma_{j=1}^{n_i}\\frac1n_i\\frac{c_ip_i(X_{i,j})}{\\Sigma_j^nc_jp_j(X_{i,j})}\\frac{f(X_{i,j})}{p_i(X_{i,j})}\\\\ =\\Sigma_{i=1}^n\\Sigma_{j=1}^{n_i}\\frac1n_i\\frac{\\frac{n_i}{N}p_i(X_{i,j})}{\\Sigma_j^nc_jp_j(X_{i,j})}\\frac{f(X_{i,j})}{p_i(X_{i,j})}\\\\ =\\frac{1}{N}\\Sigma_{i=1}^n\\Sigma_{j=1}^{n_i}\\frac{f(X_{i,j})}{\\Sigma_j^nc_jp_j(X_{i,j})} \\\\ =\\frac{1}{N}\\Sigma_{i=1}^n\\Sigma_{j=1}^{n_i}\\frac{f(X_{i,j})}{\\overline p(X_{i,j})} \\tag {40}&emsp;&emsp;其中，$\\overline p(x)$又被称为联合抽样分布，其数学公式如下所示。总的采样数$N$，每个分布$p_i$采集$n_i$个随机数$X_{i,j}$。以上就是平衡启发式的核心思想，一种很自然地组合多种采样分布的方式。我们采用一个单一的与$i$无关的分布$\\overline p(x)$来表述这种组合方式。 \\overline p(x)=\\Sigma_{i=1}^nc_ip_i(x) \\tag {41}九、MC光线追踪&emsp;&emsp;了解了相关的原理，接下来我们就实现一个MC（Monte Carlo，蒙特卡罗）光线追踪，主要的参考资料是Peter Shirley的《Ray Tracing_ the Rest of Your Life.pdf》。采样方法是复合重要性采样，复合的采样分布为Lambertian材质BRDF采样分布加上光源采样分布。 1、立体角&emsp;&emsp;在球面坐标中，一个方向向量我们通常采用$(\\theta, \\phi)$来唯一地表示，分别是天顶角和方位角。在衡量发光强度和辐射辐射度量学中，立体角有着广泛的应用。立体角描述了站在某一点的观察者观测到的物体大小的尺度，它被定义为球表面截取的面积微分与球半径平方之比，单位为球面度，写作$sr$。显然，立体角是二维圆心角的三维扩展： d\\omega=\\frac{dA}{r^2} \\tag {42}&emsp;&emsp;更一般的情况，立体角通常转换为$(\\theta, \\phi)$来表示，在单位球体上，$d\\omega=dA$，我们转换成用$(\\theta,\\phi)$求微分面积$dA$。我们知道二维的弧长公式为：圆心角弧度数*半径（注意圆心角要换成弧度制）。如下所示，$\\theta$和$\\phi$对应的弧长为： s_{\\theta}=\\theta * r_{\\theta} \\\\ s_{\\phi}=\\phi * r_{\\phi} \\tag {43} 图4 求弧长 &emsp;&emsp;公式$(43)$中的$r_\\theta$其实就是球体半径，$r_\\phi$与$r_\\theta$的关系为：$r_\\phi=r_\\theta * sin\\theta$。微分面积$dA$可以看成是一个矩形，宽和高分别为对应的弧长$d_{r_\\phi}$和$d_{r_\\theta}$，根据公式$(43)$我们可知$d_{r_\\phi}$、$d_{r_\\theta}$计算方法如下： d_{s_\\theta}=r_{\\theta}d\\theta\\\\ d_{s_\\phi}=r_{\\phi}d\\phi \\tag {44}&emsp;&emsp;对于单位球体，$r_\\theta=r=1,r_\\phi=r_\\theta*sin\\theta=sin\\theta$，从而立体角微分可转换成如下表示： d\\omega=dA=d_{s_\\theta}*d_{s_\\phi}=sin\\theta d\\theta d\\phi \\tag {45}2、Lambertian材质BRDF采样&emsp;&emsp;对于Lambertian材质我们假定其光线的散射分布与$cos\\theta$成正比，这里的$\\theta$是光线与表面法向量的夹角，也就是说在靠近法线的方向光线散射得比较多。当光线与表面法线夹角大于90度时，不发生光线散射。我们记得光线得散射概率密度函数pdf为$C*cos\\theta$，其中$C$为某个常数。对于概率密度函数，我们必须保证其在全定义域上的（这里就是整个半球方向）积分为1，即有（涉及到了立体角转球面坐标表示形式和求定积分）： \\int_{\\Omega}C*cos\\theta d\\omega=C\\int_0^{2\\pi}d\\phi\\int_0^{\\frac\\pi2}cos\\theta sin\\theta d\\theta\\\\ =C*2\\pi\\int_0^{\\frac\\pi2}sin\\theta d(\\sin\\theta)=C*2\\pi * \\frac12=1\\\\ \\to C=\\frac{1}{\\pi} \\tag {46}&emsp;&emsp;从而，Lambertian材质的光线散射概率密度函数PDF，记为$pS(direction)$，如下所示： pS(direction)=\\frac{cos\\theta}{\\pi} \\tag {47}&emsp;&emsp;现在我们要根据这个概率密度函数生成服从该分布的随机半球向量，根据前面随机抽样部分，我们首先要求出它的概率分布函数。根据定义，概率分布函数就是对概率密度函数积分： P=\\int_{\\Omega}\\frac{cos\\theta}{\\pi}d\\omega =\\int_0^{2\\pi}d\\phi\\int_0^\\theta \\frac{cos\\ t}{\\pi}sin\\ t\\ dt\\\\ =2\\pi * \\frac1\\pi \\int_0^{\\theta}sin\\ td(sin\\ t)=sin^2\\theta=1-cos^2\\theta \\tag {48}&emsp;&emsp;根据逆变换算法，我们要取概率分布函数的反函数。这里有个小技巧，我们不需要调用反三角函数得到反函数，我们只需得到$cos\\theta$即可。因为即便调用反三角函数得到$\\theta$，后面我们将$(\\theta,\\phi)$转换成笛卡尔坐标向量的时候还是要调用三角函数$cos$，我们直接避免这个比较费时的过程。所以，任取一个$[0,1]$上均匀随机数$r_2$： cos\\theta =\\sqrt{1-r_2} \\tag {49}&emsp;&emsp;公式$(49)$只得到随机方向向量的$\\theta$，我们还需要$\\phi$。对于Lamberatian材质，光线在方向角上是均匀分布的，故其概率密度函数为$\\frac{1}{2\\pi}$，概率分布函数为$\\frac{\\phi}{2\\pi}$。故对$\\phi$的随机采样如下，任取一个$[0,1]$上的均匀随机数$r_1$： \\frac{\\phi}{2\\pi}=r_1 \\to \\phi=r_1*2\\pi \\tag {50}&emsp;&emsp;采样得方向向量的$(\\theta,\\phi)$，我们还要将其转换到笛卡尔坐标系的形式，这个过程不难理解，仔细观察图4，不再赘述。从而，服从公式$(47)$采样方向向量的代码如下所示： 12345678910 static Vector3D randomCosineDir()&#123; float r1 = drand48(); float r2 = drand48(); float z = sqrt(1-r2); float phi = 2 * M_PI * r1; float x = cos(phi) * 2 * sqrt(r2); float y = sin(phi) * 2 * sqrt(r2); return Vector3D(x,y,z);&#125; &emsp;&emsp;值得注意的是，我们的采样是以物体表面的切线和法线构成的坐标轴为参考系的，其中z轴方向是表面的法线向量。因此，通过上面的代码采样的得到方向向量还要转到该局部坐标系下。这个过程可以构建矩阵，也可以直接将方向向量三个分量与轴向量相乘，最后相加得到。我们采用了后者，首先构建一个局部坐标的正交基（Ortho-normal Bases）： 123456789101112131415161718192021222324252627282930313233343536373839class ONB&#123;private: Vector3D m_axis[3];public: ONB() = default; Vector3D u() const &#123; return m_axis[0]; &#125; Vector3D v() const &#123; return m_axis[1]; &#125; Vector3D w() const &#123; return m_axis[2]; &#125; Vector3D operator[](int i) const &#123; return m_axis[i]; &#125; Vector3D local(float a, float b, float c) const &#123; return u() * a + v() * b + w() * c; &#125; Vector3D local(const Vector3D &amp;a) const &#123; return u() * a.x + v() * a.y + w() * a.z; &#125; void buildFromW(const Vector3D &amp;n);&#125;;void ONB::buildFromW(const Vector3D &amp;n)&#123; m_axis[2] = n; m_axis[2].normalize(); Vector3D a; if(fabs(w().x) &gt; 0.9f) a = Vector3D(0,1,0); else a = Vector3D(1,0,0); m_axis[1] = w().crossProduct(a); m_axis[1].normalize(); m_axis[0] = w().crossProduct(v()); m_axis[0].normalize();&#125; &emsp;&emsp;再构建一个PDF虚类，将PDF函数的函数值和采样抽线为$value$接口和$generate$接口。并继承它创建CosinePDF类，可以看到CosinePDF的$value$是按照公式$(47)$计算的： 123456789101112131415161718192021222324252627282930313233343536class PDF&#123;public: virtual float value(const Vector3D &amp;driection) const = 0; virtual Vector3D generate() const = 0;&#125;;class CosinePDF : public PDF&#123;private: ONB uvw;public: CosinePDF(const Vector3D &amp;w) &#123; uvw.buildFromW(w); &#125; virtual float value(const Vector3D &amp;driection) const; virtual Vector3D generate() const;&#125;;float CosinePDF::value(const Vector3D &amp;direction) const&#123; Vector3D dir = direction; dir.normalize(); float cosine = dir.dotProduct(uvw.w()); if(cosine &gt; 0.0f) return cosine / M_PI; else return 0.0f;&#125;Vector3D CosinePDF::generate() const&#123; return uvw.local(Vector3D::randomCosineDir());&#125; 3、直接光源采样&emsp;&emsp;显然在靠近光源的方向上，光照值对物体表面的颜色贡献更大，因此直接对光源采样对减少蒙特卡洛积分的方差有非常重要的作用。直接光源采样需要我们首先求采样分布的概率密度函数，目前我们先讨论一个最简单的光源例子，即矩形光源。假设矩形光源的面积为A，那么这个矩形光源的直接均匀采样的概率密度函数PDF为$\\frac1A$，但是通常我们采样的单位是立体角微分，如下所示， 图5 直接光源采样 &emsp;&emsp;$d\\omega$与$dA$存在着一个对应关系，实际上我们可以通过前面提到的立体角定义（即公式$(42)$）得到$d\\omega$与$dA$的关系如下所示，这个公式不难理解。其中$\\alpha$夹角是采样方向向量与矩形表面的法线向量的夹角，$dAcos\\alpha$实际上是将矩形的微分面积$dA$投影到采样方向$pq$上，这是因为从$pq$方向看去只能看到$dAcos\\alpha$这个大小的面积，然后再比上半径长度的平方$||pq||^2$，这是立体角的定义。 d\\omega=\\frac{dA*cos\\alpha}{||pq||^2} \\tag {51}&emsp;&emsp;现在对$dA$的采样概率为$\\frac{dA}{A}$，在球体方向上对立体角$d\\omega$采样的概率为$p(direction)d\\omega$，其中$p(direction)$是我们假定的对光源直接采样的概率密度函数。理论上来说，$\\frac{dA}{A}$应该等于$p(direction)d\\omega$，即有： p(direction)*\\frac{dA*cos\\alpha}{||pq||^2}=\\frac{dA}{A}\\\\ \\to p(direction)=\\frac{||pq||^2}{Acos\\alpha} \\tag {52}&emsp;&emsp;公式$(52)$推导出了我们要找的直接光源采样的概率密度函数。根据逆变换算法，我们还要求它的概率分布函数从而生成服从公式$(52)$概率密度函数的随机采样方向，但是这里其实没有必要。我们直接在矩形光源上随机采样一个点，然后将这个采样点与物体表面上的点连接起来就是我们的直接光源采样方向，通过这个方法省去了比较复杂的高数推导过程。 &emsp;&emsp;有了以上的理论基础，我们接下来就实现矩形的直接光源采样。我这里的定义的一个矩形平面是由两个三角形组成，默认是在xz平面上的边长为2的正方形。 1234567891011121314151617181920212223242526272829Vector3D Plane::random(const Vector3D &amp;o) const&#123; Vector3D center = m_transformation.translation(); Vector3D leftCorner; float width = m_transformation.scale().x * 2.0f; float height = m_transformation.scale().z * 2.0f; leftCorner.x = center.x - m_transformation.scale().x; leftCorner.z = center.z - m_transformation.scale().z; leftCorner.y = center.y; Vector3D random_point(leftCorner.x + drand48() * width, leftCorner.y, leftCorner.z + drand48() * height); return random_point - o;&#125;float Plane::pdfValue(const Vector3D &amp;o, const Vector3D &amp;v) const&#123; HitRecord rec; if(this-&gt;hit(Ray(o,v), 0.001f, FLT_MAX, rec)) &#123; float area = m_transformation.scale().x * 2.0f * m_transformation.scale().z * 2.0f; float distance_squared = v.getSquaredLength(); float cosine = fabs(v.dotProduct(rec.m_normal) / v.getLength()); float ret = distance_squared / (cosine * area); return ret; &#125; else return 0.0f;&#125; &emsp;&emsp;除了矩形区域光源，我们接下来还添加一个对球形区域光源的重要性采样。我们采取的坐标系依然是球形光源的局部坐标，而且依然是对光源区域做均匀采样。设想我们从物体表面上的一点望向一个球形区域光源，我们能够看到的区域就是我们要做均匀采样的区域，采样方法依然是围绕$(\\theta,\\phi)$展开，其中$\\theta$是采样方向向量与物体表面的点与球心构成的方向向量的夹角。 &emsp;&emsp;显然方位角$\\phi$依然是$[0,2\\pi]$的范围，不然我们不可能看到一个圆形。而$\\theta$则需要做一些限制，它现在有个上界，如下图6所示，P是物体表面上的一点，C为球形光源的球心，R是球形光源的半径。 图6 球形区域光源采样 &emsp;&emsp;由图6可知，$sin(\\theta_{max})=\\frac{R}{||C-P||}$，相应的$\\theta_{max}$的余弦值如下所示： cos(\\theta_{max})=\\sqrt{1-\\frac{R^2}{||C-P||^2}} \\tag {53}&emsp;&emsp;然后我们是对$\\theta$和$\\phi$做均匀采样，$\\phi$的采样与前面Lambertian采样一样，这里不再赘述。对于$\\theta$，因为是均匀采样，那么它的概率密度函数必然也是一个常数，我们设为$C$，那么其概率分布函数计算如下： P=\\int_{\\Omega}Cd\\omega =\\int_0^{2\\pi}d\\phi \\int_0^{\\theta}Csint\\ dt\\\\ =2\\pi C(1-cos\\theta) \\tag {54}&emsp;&emsp;根据逆变换算法，取$[0,1]$上的均匀随机数$r_2$，并结合公式$(54)$的反函数，可得采样的$cos\\theta$如下： cos(\\theta)=1-\\frac{r_2}{2\\pi C} \\tag {55}&emsp;&emsp;现在有个问题就是$C$这个具体是多少？我们已经知道$\\theta$的上界$\\theta_{max}$，当$\\theta=\\theta_{max}$时，应该取概率分布函数值$P(\\theta_{max})$为1，也就是$r_2=1$。故将其代入公式$(55)$我们可以得到$C$的具体表达式： C=\\frac{1}{2\\pi (1-cos\\theta_{max})} \\tag {56}&emsp;&emsp;然后再将公式$(56)$和公式$(53)$代入公式$(55)$，可得： cos(\\theta)=1+r_2(cos(\\theta_{max})-1)\\\\ =1+r_2(\\sqrt{1-\\frac{R^2}{||C-P||^2}}-1) \\tag {57}&emsp;&emsp;公式$(56)$j就是我们所需的概率密度函数，可以看起来不是很直观，这里我稍微解释一下。公式$(56)$其实就是我们从物体表面上的一点观测到的球形光源所占的立体角的倒数（注意，这里的立体角是以物体表面上的一点为球心而不是球形光源的球面上的立体角）！立体角的几何意义是就是单位球体上的面积，然后做一个倒数是因为我们是做均匀随机采样。立体角的求法如下所示： SolidAngle =\\int_0^{2\\pi}d\\phi\\int_0^{\\theta_{max}}sin\\theta d\\theta=2\\pi(1-cos(\\theta_{max})) \\tag {58}&emsp;&emsp;可以看到公式$(58)$求得的结果就是公式$(56)$中的分母。取球形光源上的随机一点采样算法如下，就是公式$(57)$的实现。 12345678910static Vector3D randomToSphere(float radius, float distance_squared)&#123; float r1 = drand48(); float r2 = drand48(); float z = 1 + r2 * (sqrt(1 - radius * radius/distance_squared) - 1); float phi = 2 * M_PI * r1; float x = cos(phi) * sqrt(1 - z * z); float y = sin(phi) * sqrt(1 - z * z); return Vector3D(x, y, z);&#125; &emsp;&emsp;对球形光源的随机采样以及求取概率密度函数的值如下所示： 123456789101112131415161718192021float Sphere::pdfValue(const Vector3D &amp;o, const Vector3D &amp;v) const&#123; HitRecord rec; if(this-&gt;hit(Ray(o,v), 0.001f, FLT_MAX, rec)) &#123; float cos_theta_max = sqrt(1- m_radius * m_radius/(m_center - o).getSquaredLength()); float solid_angle = 2 * M_PI * (1 - cos_theta_max); return 1.0f / solid_angle; &#125; else return 0.0f;&#125;Vector3D Sphere::random(const Vector3D &amp;o) const&#123; Vector3D dir = m_center - o; float distance_squared = dir.getSquaredLength(); ONB uvw; uvw.buildFromW(dir); return uvw.local(Vector3D::randomToSphere(m_radius, distance_squared));&#125; 2、复合重要性采样&emsp;&emsp;上面我们分别讨论了Lambertian采样和直接光源采样，然后我们要把它复合到一起。场景中通常有多个光源，所以直接光源采样应该对多个光源进行采样，我们采取均匀随机的策略，对于一束光线，它采样哪个光源由均匀的随机数决定，这样就能雨露均沾。复合的权重套用前面提到的平衡启发式，Lambertian采样和直接光源采样的权值各0.5，也就是各占一半。 123456789101112131415161718192021class MixturePDF : public PDF&#123;private: PDF* m_pdf[2];public: MixturePDF(PDF *p0, PDF *p1) &#123; m_pdf[0] = p0;m_pdf[1] = p1; &#125; virtual float value(const Vector3D &amp;direction) const &#123; return 0.5f * m_pdf[0]-&gt;value(direction) + 0.5f * m_pdf[1]-&gt;value(direction); &#125; virtual Vector3D generate() const &#123; if(drand48() &lt; 0.5f) return m_pdf[0]-&gt;generate(); else return m_pdf[1]-&gt;generate(); &#125;&#125;; &emsp;&emsp;对于多个光源的直接采样，我们采取均匀随机的策略，那么PDF值也应该是这些直接光源采样概率密度函数的平均值。 1234567891011121314float HitableList::pdfValue(const Vector3D &amp;o, const Vector3D &amp;v) const&#123; float weight = 1.0f / m_list.size(); float sum = 0; for(int x = 0;x &lt; m_list.size();++ x) sum += m_list[x]-&gt;pdfValue(o, v); return sum * weight;&#125;Vector3D HitableList::random(const Vector3D &amp;o) const&#123; int index = static_cast&lt;int&gt;(drand48() * m_list.size()); return m_list[index]-&gt;random(o);&#125; &emsp;&emsp;最后在光线追踪递归函数中加上我们的复合重要性采样。 12345678910111213141516171819202122232425262728293031323334353637383940414243Vector4D Tracer::tracing(const Ray &amp;r, Hitable *world, Hitable *light, int depth)&#123; HitRecord rec; if (world-&gt;hit(r, 0.001f, FLT_MAX, rec)) &#123; ...... if (depth &lt; m_config.m_maxDepth &amp;&amp; material-&gt;scatter(r, rec, srec)) &#123; if(srec.m_isSpecular) &#123; return srec.m_attenuation * tracing(srec.m_scatterRay, world, light, depth + 1); &#125; else &#123; Vector3D dir; float pdf_val; if(!m_samplingList.isEmpty()) &#123; HitablePDF light_pdf(light, rec.m_position); MixturePDF mix_pdf(&amp;light_pdf, srec.m_pdf.get()); dir = mix_pdf.generate(); pdf_val = mix_pdf.value(dir); &#125; else &#123; dir = srec.m_pdf-&gt;generate(); pdf_val = srec.m_pdf-&gt;value(dir); &#125; Ray scattered = Ray(rec.m_position, dir); return emitted + srec.m_attenuation * material-&gt;scattering_pdf(r, rec, scattered) * tracing(scattered, world, light, depth + 1) / pdf_val; &#125; &#125; else return emitted; &#125; else &#123; // background color. ...... &#125;&#125; 程序效果 参考资料$[1$ http://www.thegibook.com/ $[2]$ Peter Shirley. Ray Tracing in One Weekend. Amazon Digital Services LLC, January 26, 2016. $[3]$ https://software.intel.com/en-us/node/506045?_ga=2.114625223.1582767698.1558613799-2057498546.1558613799 $[4]$ https://blog.csdn.net/zoufeiyy/article/details/1887579 $[5]$ https://www.jianshu.com/p/b570b1ba92bb $[6]$ https://blog.csdn.net/libing_zeng/article/details/74989755 $[7]$ https://www.qiujiawei.com/solid-angle/","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/about/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/about/tags/Ray-Tracer/"}]},{"title":"光线追踪器Ray Tracer：入门篇","slug":"RayTracer-Basis","date":"2019-05-08T14:12:14.121Z","updated":"2019-07-17T03:40:11.410Z","comments":true,"path":"2019/05/08/RayTracer-Basis/","link":"","permalink":"https://yangwc.com/about/2019/05/08/RayTracer-Basis/","excerpt":"光线追踪技术是计算机图形学的一类全局光照算法，目前的影视行业大多都采用光线追踪做离线渲染。本章开始构建一个光线追踪离线渲染器（路径追踪），深入理解光线追踪的技术原理。主要参考资料为Peter Shirley的《Ray Tracing in One Weekend》。数学库沿用之前自己写的3D数学库，这方面的东西不再赘述。相关的完全代码在这里。","text":"光线追踪技术是计算机图形学的一类全局光照算法，目前的影视行业大多都采用光线追踪做离线渲染。本章开始构建一个光线追踪离线渲染器（路径追踪），深入理解光线追踪的技术原理。主要参考资料为Peter Shirley的《Ray Tracing in One Weekend》。数学库沿用之前自己写的3D数学库，这方面的东西不再赘述。相关的完全代码在这里。 光线追踪纵览 实现光线追踪渲染器 程序结果 一、光线追踪纵览&emsp;&emsp;光线追踪 (Ray Tracing) 算法是一种基于真实光路模拟的计算机三维图形渲染算法，相比其它大部分渲染算法，光线追踪算法可以提供更为真实的光影效果。此算法由 Appel 在 1968 年初步提出，1980 年由Whitted 改良为递归算法并提出全局光照模型。直到今天，光线追踪算法仍是图形学的热点，大量的改进在不断涌现。基于对自然界光路的研究, 光线追踪采取逆向计算光路来还原真实颜色。追踪的过程中涵盖了光的反射、折射、吸收等特性 (精确计算)， 并辅以其它重要渲染思想 (进一步模拟)。 其中包含了重要方法，诸如冯氏光照模型 (Phong Shading)、辐射度(Radiosity)、光子映射 (Photon Mapping)、蒙特卡罗方法 (Monte Carlo) 等等。鉴于光线追踪算法对场景仿真程度之高，其被普遍认为是计算机图形学的核心内容， 以及游戏设计、电影特效等相关领域的未来方向。 近年来由于硬件系统的迅速改良， 基于分布式、GPU， 甚至实时渲染的光线追踪显卡也纷纷出现（本人就是入手了一块实时光追显卡rtx2070）。 &emsp;&emsp;光线追踪算法是一种非常自然的技术，相比于光栅化的方法，它更加简单、暴力、真实。与光栅化根据物体计算所在的像素的方式不同，光线路径追踪的方法是一个相反的过程，它在于用眼睛去看世界而不是世界如何到达眼中。如下图所示，从视点出发向屏幕上每一个像素发出一条光线View Ray，追踪此光路并计算其逆向光线的方向，映射到对应的像素上。通过计算光路上颜色衰减和叠加，即可基本确定每一个像素的颜色。 图1 光线追踪示意图 &emsp;&emsp;可以看到光线追踪是一个递归的过程。发射一束光线到场景，求出光线和几何图形间最近的交点，如果该交点的材质是反射性或折射性的，可以在该交点向反射方向或折射方向继续追踪，如此递归下去，直到设定的最大递归深度或者射线追踪到光源处（或者背景色），如此便计算处一个像素的着色值。 &emsp;&emsp;基本的光线追踪tracing()递归算法如下所示： &emsp;&emsp;Algorithm 1: 光线追踪递归算法 &emsp;&emsp;Input: 射线ray &emsp;&emsp;Output: 反向光颜色 &emsp;&emsp;Function tracing(): &emsp;&emsp;if no intersection with any object then&emsp;&emsp;&emsp;&emsp;return background color&emsp;&emsp;else&emsp;&emsp;&emsp;&emsp;obj $\\leftarrow$ find nearest object from the ray;&emsp;&emsp;&emsp;&emsp;reflect ray $\\leftarrow$getReflectRay(obj);&emsp;&emsp;&emsp;&emsp;refract ray $\\leftarrow$ getRefractRay(obj);&emsp;&emsp;&emsp;&emsp;main color $\\leftarrow$ the radiance of obj;&emsp;&emsp;&emsp;&emsp;reflect color $\\leftarrow$ tracing(reflect ray);&emsp;&emsp;&emsp;&emsp;refract color $\\leftarrow$ tracing(refract ray); &emsp;&emsp;&emsp;&emsp;return mix(main color, reflect color, refract color); 二、实现光线追踪渲染器&emsp;&emsp;采用C++语言不借助第三方图形渲染API实现一个简易的光线追踪器，为了将最后的结果显示出来，我采用stb_image将计算得到的像素矩阵保存为png图片。本篇实现的光线追踪只包含求交运行、计算光线反射和折射向量、反走样、景深等较为初级的方面，而实现的材质包含磨砂材质、玻璃材质和金属材质。 1、摄像机&emsp;&emsp;与光栅化的空间变换过程相反，光线追踪大部分操作都是在世界空间中进行，因而需要将屏幕空间的像素坐标变换到世界空间中，并相应地发射出一条射线。在这里我们不再构建矩阵，直接求解出摄像机的三个坐标轴，然后根据视锥体的视域fov和屏幕的宽高比aspect得到每个像素发射出来的射线。 &emsp;&emsp;首先我们创建一个射线类$Ray$，射线通常用一个射线原点$m_origin$和射线方向$m_direction$表示，射线上的每个点则表示为$p(t)=m_origin+t*m_direction$，射线上每一个独立的点都有一个自己唯一的$t$值。因而创建的$Ray$类如下所示，其中$pointAt$函数根据给定的$t$值返回相应的射线上的点： 12345678910111213141516171819202122class Ray&#123;private: Vector3D m_origin; Vector3D m_direction;public: // ctor/dtor. Ray() = default; ~Ray() = default; Ray(const Vector3D &amp;org, const Vector3D &amp;dir) :m_origin(org), m_direction(dir) &#123; m_direction.normalize(); &#125; // Getter. Vector3D getOrigin() const &#123; return m_origin; &#125; Vector3D getDirection() const &#123; return m_direction; &#125; // p(t) = origin + t*dir; Vector3D pointAt(const float &amp;t)const &#123; return m_origin + m_direction * t; &#125;&#125;; &emsp;&emsp;我们实现的基于cpu的光线追踪核心渲染流程是对给定分辨率的像素矩阵，逐行逐列地遍历每个像素坐标，如下所示： 1234567891011unsigned char *RayTracing::render()&#123; for(int row = 0;row &lt; m_height;++ row) &#123; for(int col = 0;col &lt; m_width;++ col) &#123; ...... &#125; &#125; return m_image;&#125; &emsp;&emsp;因而对于每个给定的像素坐标$(x,y)$，我们需要获取这个像素坐标对应的发射出去的射线，首先我们把值域为$[0,m_width]$和$[0,m_height]$的像素坐标映射到$[0,1]$，正如如下所示： 12float u = static_cast&lt;float&gt;(col) / static_cast&lt;float&gt;(m_config.m_width);float v = static_cast&lt;float&gt;(row) / static_cast&lt;float&gt;(m_config.m_height); &emsp;&emsp;接下来我们根据$u$和$v$获取射线方向向量，这涉及到两个方面，一个摄像机的坐标系统，另一个是关于视锥的大小设置。摄像机的坐标轴决定了当前的朝向，视锥的大小设定决定了当前视域的大小。为此，我把摄像机与视锥合并一起，坐标系类型依然是右手坐标系。创建的摄像机类如下所示： 12345678910111213141516171819202122232425262728293031class Camera&#123;public: Vector3D m_pos; Vector3D m_target; Vector3D m_lowerLeftCorner; Vector3D m_horizontal; Vector3D m_vertical; float m_fovy, m_aspect; Vector3D m_axisX, m_axisY, m_axisZ; Camera(const Vector3D &amp;cameraPos, const Vector3D &amp;target,float vfov, float aspect); // Getter. Ray getRay(const float &amp;s, const float &amp;t) const; Vector3D getPosition() const &#123; return m_pos; &#125; Vector3D getTarget() const &#123; return m_target; &#125; Vector3D getAxisX() const &#123; return m_axisX; &#125; Vector3D getAxisY() const &#123; return m_axisY; &#125; Vector3D getAxisZ() const &#123; return m_axisZ; &#125; // Setter. void setPosition(const Vector3D &amp;pos) &#123; m_pos = pos; update(); &#125; void setTarget(const Vector3D &amp;_tar) &#123; m_target = _tar; update(); &#125; void setFovy(const float &amp;fov) &#123; m_fovy = fov; update(); &#125; void setAspect(const float &amp;asp) &#123; m_aspect = asp; update(); &#125;private: void update();&#125;; &emsp;&emsp;其中$m_pos$即摄像机的世界坐标位置，$m_target$即目标位置，而$m_lowerLeftCorner$表示视锥近平面的左下角位置，$m_horizontal$表示近平面在摄像机坐标系下水平方向的跨度，$m_vertical$则是近平面在摄像机坐标系下垂直方向的跨度。$m_fovy$和$m_aspect$分别是视锥的垂直视域和屏幕的宽高比。初始时我们传入摄像机坐标、目标点以及垂直视域和视口宽高比，然后我们根据这些计算摄像机的三个坐标轴，以及近平面的位置： 123456789101112131415161718192021void Camera::update()&#123; const Vector3D worldUp(0.0f, 1.0f, 0.0f); // frustum. float theta = radians(m_fovy); float half_height = static_cast&lt;float&gt;(tan(theta * 0.5f)); float half_width = m_aspect * half_height; // camera coordinate system. m_axisZ = m_pos - m_target; m_axisZ.normalize(); m_axisX = worldUp.crossProduct(m_axisZ); m_axisX.normalize(); m_axisY = m_axisZ.crossProduct(m_axisX); m_axisY.normalize(); // view port. m_lowerLeftCorner = m_pos - m_axisX * half_width - m_axisY * half_height - m_axisZ; m_horizontal = m_axisX * 2.0f * half_width; m_vertical = m_axisY * 2.0f * half_height;&#125; &emsp;&emsp;然后我们对于给定在$[0,1]$的$u$和$v$，就可以计算出一条对应的射线向量了。 1234Ray Camera::getRay(const float &amp;s, const float &amp;t) const&#123; return Ray(m_pos , m_lowerLeftCorner + m_horizontal * s + m_vertical * t - m_pos );&#125; 12345678910for (int row = m_config.m_height - 1; row &gt;= 0; --row)&#123; for (int col = 0; col &lt; m_config.m_width; ++col) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = camera.getRay(u, v); ...... &#125;&#125; 2、物体求交&emsp;&emsp;射线发射出去之后要与物体进行求交运行，对于这类能够被射线碰撞到的物体我们把它抽象为$Hitable$，并用一个虚函数$Hit$作为所有的碰撞求交的接口，创建$Hitable$虚类如下： 12345678910111213141516class Material;struct HitRecord&#123; float m_t; Vector3D m_position; Vector3D m_normal; Material *m_material;&#125;;class Hitable&#123;public: Hitable() = default; virtual ~Hitable() &#123;&#125; virtual bool hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const = 0;&#125;; &emsp;&emsp;可以看到我们还创建了一个$HitRecord$结构体，它包含一次射线碰撞求交的结果记录，其中$m_t$是射线方程的参数$t$，$m_position$是交点的位置，$m_normal$是交点的法向量，而$m_material$则是交点所在物体的材质，求交之后我们需要根据这些记录来计算物体的折射、反射。 &emsp;&emsp;$Hitable$中的$hit$接口以一条射线$ray$作为输入参数，以一个$Hitable$的引用$ret$作为求交的结果记录，函数返回布尔值以表示是否发生了射线碰撞。此外，值得一提的是我们还输入了两个参数，分别是$t_min$和$t_max$，这个是我们自己对射线线段长度做的一个限制，可以分别去掉太近和太远的物体。 &emsp;&emsp;然后我们需要向场景中添加物体，光线追踪器的一个”Hello, world!”是球体。我们知道，一个球体的数学表达式为如下所示： (x-cx)^2+(y-cy)^2+(z-cz)^2=R^2 \\tag {1}&emsp;&emsp;其中$c=(cx,cy,cz)$是球体的球心，$R$为球体半径。我们现在要求的就是，对于射线上的一点$P(t)=S+tV$，使得$(x,y,z)=P(t)=S+tV$带入公式$(1)$成立，公式$(1)$可以写成点乘的形式如下： (P-c)\\cdot (P-c) = R^2 \\tag {2}&emsp;&emsp;将$P=P(t)=S+tV$带入公式$(2)$可得： (V\\cdot V)t^2+2(V\\cdot(S-c))t+(S-c)\\cdot(S-c)-R^2=0 \\tag {3}&emsp;&emsp;可以看到公式$(3)$中只有$t$未知，它是一个一元二次方程。对于任意的一元二次方程$at^2+bt+c=0$，其解有如下形式： t=\\frac{-b\\pm \\sqrt{b^2-4ac}}{2a} \\tag {4}&emsp;&emsp;其中根号内的$D=b^2-4ac$称为根的判别式，它可以反应多项式根的数量。若$D&gt;0$则有两个实数根，若$D=0$则只有一个实数根，若$D&lt;0$则没有实数根。我们首先可以根据判别式判断是否存在交点，然后再求出具体的交点坐标。下面的$Hit$函数，我们首先求出多项式方程的常数项$a$、$b$和$c$，然后求判别式，最后再有解的情况下求取交点。注意，在有两个交点的情况下，我们首先取较近的点，不符合再取较远的那个点。只有一个交点的情况下（如下图2所示），我们不当作射线发生了碰撞（擦边而过）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Sphere : public Hitable &#123; public: float m_radius; Vector3D m_center; Material *m_material; Sphere(const Vector3D &amp;cen, const float r, Material *mat) :m_center(cen), m_radius(r), m_material(mat) &#123;&#125; ~Sphere() &#123; if (m_material)delete m_material; m_material = nullptr; &#125;; virtual bool hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const; &#125;; bool Sphere::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const &#123; Vector3D oc = ray.getOrigin() - m_center; float a = ray.getDirection().dotProduct(ray.getDirection()); float b = oc.dotProduct(ray.getDirection()); float c = oc.dotProduct(oc) - m_radius * m_radius; // discriminant float discriminant = b * b - a * c; if (discriminant &gt; 0) &#123; float temp = (-b - sqrt(b * b - a * c)) / a; if (temp &gt; t_min &amp;&amp; temp &lt; t_max) &#123; ret.m_t = temp; ret.m_position = ray.pointAt(ret.m_t); ret.m_normal = (ret.m_position - m_center) / m_radius; ret.m_material = m_material; return true; &#125; temp = (-b + sqrt(b * b - a * c)) / a; if (temp &gt; t_min &amp;&amp; temp &lt; t_max) &#123; ret.m_t = temp; ret.m_position = ray.pointAt(ret.m_t); ret.m_normal = (ret.m_position - m_center) / m_radius; ret.m_material = m_material; return true; &#125; &#125; return false; &#125; 图2 射线与球体的相交情况 &emsp;&emsp;当场景中有多个物体时，当前的做法是在每次求交时遍历所有的物体，我们需要一个$HitableList$来存储这些物体。我们令$HitableList$继承自$Hitable$，这样$HitableList$就表现得好像只有一个很大的物体一样，并在实现$hit$函数中对场景得所有物体遍历调用他们的$Hit$方法： 1234567891011121314151617181920212223242526272829303132333435363738class HitableList : public Hitable&#123;public: std::vector&lt;Hitable*&gt; m_list; HitableList() = default; ~HitableList() = default; void addHitable(Hitable *target) &#123; m_list.push_back(target); &#125; void clearHitable() &#123; for (int x = 0; x &lt; m_list.size(); ++x) &#123; delete m_list[x]; m_list[x] = nullptr; &#125; &#125; virtual bool hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const;&#125;;bool HitableList::hit(const Ray &amp;ray, const float &amp;t_min, const float &amp;t_max, HitRecord &amp;ret) const&#123; HitRecord tempRec; bool hitAny = false; double closestSoFar = t_max; for (unsigned int x = 0; x &lt; m_list.size(); ++x) &#123; if (m_list[x]-&gt;hit(ray, t_min, closestSoFar, tempRec)) &#123; hitAny = true; closestSoFar = tempRec.m_t; ret = tempRec; &#125; &#125; return hitAny;&#125; &emsp;&emsp;此外，值得一提的是，在$HitableList$的$hit$函数中我们需要做一个类似于深度测试的步骤，我们从摄像机发射的射线只能跟最靠近摄像机的那个交点做反射、折射，一条射线发射出去可能会与多个物体相交，我们必须取最近的交点。这个距离我们用射线方程中的$t$来描述，显然$t$越大则交点越远，因此用$closestSoFar$来记录当前获取的交点的最小$t$，以此作为$t$的上限，这样最终求出来的必然就是最近的交点。 3、物体材质&emsp;&emsp;现在我们的一个问题就是求出交点之后，光线在交点上做什么样的反射和折射？这取决于物体的材质。若物体的材质是透明的玻璃，那么光线一般做折射；而若物体是光滑的镜面，则光线做完美的反射。针对不同物体的材质，光线的散射情况各不相同，为此我们创建一个虚类$Material$，并把光线散射的这一过程抽象为$sactter$函数接口。 123456789class Material&#123;public: Material() = default; virtual ~Material() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const = 0;&#125;; &emsp;&emsp;可以看到，$scatter$函数接收入射射线$Ray$以及求交获得的$HitRecord$，计算散射光线的向量，返回的结果表示是否发生了散射。其中的$attenuation$本质上是物体自身的反射颜色，之所以叫$attenuation$是因为光线照射到物体上，物体一般会吸收光线中的大部分颜色，然后仅反射自身颜色的部分，这个过程使得光线在反射过程中不断衰减。 3.1 Lambertian反射材质&emsp;&emsp;首先我们要实现的是Lambertian反射的材质，Lambertian反射也叫理想散射。Lambertian表面是指在一个固定的照明分布下从所有的视场方向上观测都具有相同亮度的表面，Lambertian表面不吸收任何入射光。Lambertian反射也叫散光反射，不管照明分布如何，Lambertian表面在所有的表面方向上接收并发散所有的入射照明，结果是每一个方向上都能看到相同数量的能量。这是一种理想情况，现实中不存在完全漫反射，但Lambertian可以用来近似的模拟一些粗糙表面的效果，比如纸张。 图3 Lambertian反射 &emsp;&emsp;为了实现Lambertian表面的均匀反射现象，我们令射线碰撞到表面之后，在交点的半球方向上随机地反射，只要随机性够均匀，我们就能模拟出理想散射的情况。为此，我们取一个正切于交点$P$表面的单位球体，在这个球体内随机取一个点$S$，则反射的向量就为$S-P$。这个正切于交点$P$表面的单位球体不难求得，设交点$P$的单位法向量为$N$，那么该正切球体的球心为$P+N$。我们首先在球心为原点的单位球内随机求得一个方向向量，然后将这个方向向量加上正切球体的球心即可得出反射的方向向量。（$drand48$是生成$[0,1)$之间的均匀随机数函数，一般linux下才有这个内建函数，windows下没有，所以我们就自己写了。） 123456789101112131415161718192021222324252627282930313233343536373839404142434445#define rndm 0x100000000LL#define rndc 0xB16#define rnda 0x5DEECE66DLL static unsigned long long seed = 1; inline double drand48(void) &#123; seed = (rnda * seed + rndc) &amp; 0xFFFFFFFFFFFFLL; unsigned int x = seed &gt;&gt; 16; return ((double)x / (double)rndm); &#125;=============================================================== static Vector3D randomInUnitSphere() &#123; Vector3D pos; do &#123; pos = Vector3D(drand48(), drand48(), drand48()) * 2.0f - Vector3D(1.0, 1.0, 1.0); &#125; while (pos.getSquaredLength() &gt;= 1.0); return pos; &#125;=============================================================== class Lambertian : public Material &#123; private: Vector3D m_albedo; public: Lambertian(const Vector3D &amp;a) : m_albedo(a) &#123;&#125; virtual ~Lambertian() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const; &#125;; bool Lambertian::scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const &#123; Vector3D target = rec.m_position + rec.m_normal + Vector3D::randomInUnitSphere(); scattered = Ray(rec.m_position, target - rec.m_position); attenuation = m_albedo; return true; &#125; &emsp;&emsp;其中的$m_albedo$为物体自身的反射颜色。 3.2 金属镜面反射材质&emsp;&emsp;金属的表面比较光滑，因而不会呈现出光线随机散射的情况。对于一个完美镜面的材质来说，入射光线和反射光线遵循反射定律，即光射到镜面上时，反射线跟入射线和法线在同一平面内，反射线和入射线分居法线两侧，并且与界面法线的夹角（分别叫做入射角和反射角）相等。反射角等于入射角。 &emsp;&emsp;求反射向量如下图4所示，比较简单，不再赘述。 图4 反射向量 R = I-2(N\\cdot I)N \\tag {5}1234static Vector3D reflect(const Vector3D &amp;ray, const Vector3D &amp;normal)&#123; return ray - normal * (ray.dotProduct(normal)) * 2.0f;&#125; &emsp;&emsp;对于一个完美镜面的金属材质来说，我们只需求出反射向量，然后按照这个反射向量递归下去就行了。但是有些金属并没有那么光滑，它的高光反射并没有那么锐利，为此我们对求出的反射向量做一定的扰动，使反射向量在一定的波瓣内随机，这个波瓣有多大由用户决定（波瓣越大则金属越粗糙）。废话不多说直接上图就明白了。 &emsp;&emsp;我们在反射向量的终点上取一个给定半径的球体，在这个球体内随机选一个点作为新的反射向量的终点即可。这个半径的大小我们用$m_fuzz$变量存储，交给用户决定。 12345678910111213141516171819202122class Metal : public Material&#123;private: float m_fuzz; Vector3D m_albedo;public: Metal(const Vector3D &amp;a, const float &amp;f) : m_albedo(a), m_fuzz(f) &#123; if (f &gt; 1.0f)m_fuzz = 1.0f; &#125; virtual ~Metal() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const;&#125;;bool Metal::scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const&#123; Vector3D reflectedDir = Vector3D::reflect(in.getDirection(), rec.m_normal); scattered = Ray(rec.m_position, reflectedDir + Vector3D::randomInUnitSphere() * m_fuzz); attenuation = m_albedo; return (scattered.getDirection().dotProduct(rec.m_normal) &gt; 0.0f);&#125; 3.3 透明玻璃折射材质&emsp;&emsp;对于水、玻璃和钻石等等物体的材质，光线照射到它们的表面时，它会把光线分成折射（也叫透射）光线和反射光线两部分。我们实现的材质采用随机的策略， 就是在折射和反射两个部分中随机选取一种。首先我们要根据入射向量、法线以及入射介质系数和折射介质系数计算折射方向向量，相比反射向量，推导计算的过程稍微有点复杂。折射表面有折射系数属性，根据Snell定律，如图5所示，入射角$\\theta _L$和折射角$\\theta _T$之间的关系有： \\eta _Lsin\\theta _L=\\eta _rsin\\theta _r \\tag {6} 图5 折射向量的计算 &emsp;&emsp;其中，$\\eta _L$时光线离开的介质的折射系数，$\\eta _r$是光线进入的介质的折射系数。空气的折射系数通常定位$1.00$，折射系数越大，则在两种不同介质之间光线弯曲效果越明显。$N$和$L$都是单位方向向量。折射向量$T$可为与法向量平行的向量$-Ncos\\theta_T$和垂直的向量$-Gsin\\theta _T$，$G$是上图所示的单位向量。而$perp_NL$与$G$向量平行，且$||perp_NL=sin\\theta_L||$，故有： G=\\frac{perp_NL}{sin\\theta_L}=\\frac{L-(N\\cdot L)N}{sin\\theta_L} \\tag {7}&emsp;&emsp;折射向量$T$可以表示为： T=-Ncos\\theta_T-Gsin\\theta_T\\\\ =-Ncos\\theta_T-\\frac{sin\\theta_T}{sin\\theta_L}[L-(N\\cdot L)N] \\tag {8}&emsp;&emsp;利用公式$(6)$，我们可以将上式中的正弦商替换为$\\eta _L/\\eta _T$，可得： T=-Ncos\\theta_T-\\frac{\\eta _L}{\\eta _T}[L-(N\\cdot L)N] \\tag {9}&emsp;&emsp;注意到公式$(9)$中的$cos\\theta_T$未知，用$\\sqrt{1-sin^2\\theta_T}$替换$cos\\theta_T$，再用$(\\eta_L/\\eta_r)sin\\theta_L$代替$sin\\theta_T$，可得： T=-N\\sqrt{1-\\frac{\\eta^2_L}{\\eta^2_T}sin^2\\theta_L}-\\frac{\\eta_L}{\\eta_T}[L-(N\\cdot L)N] \\tag {10}&emsp;&emsp;最后再用$1-cos^2\\theta_L=1-(N\\cdot L)^2$代替$sin^2\\theta_L$，得到最终的表达式为： T=(\\frac{\\eta_L}{\\eta_T}N\\cdot L-\\sqrt{1-\\frac{\\eta^2_L}{\\eta^2_T}[1-(N\\cdot L)^2]}\\ )N-\\frac{\\eta_L}{\\eta_T}L \\tag {11}&emsp;&emsp;如果$\\eta_L&gt;\\eta_T$，则上式平方根里的数值可能为负，这种情况发生在当光线从一个大折射率的介质进入一个小折射率的介质时，此时光线与表面之间的入射角较大。特别的，若仅当$sin\\theta_L\\leq \\eta_r/\\eta_L$时，公式$(11)$有效，如果平方根里的数值为负，则会出现所谓的全内反射现象，也就是光线不被折射，仅在介质内部反射。此外，需要注意的是，我们在程序实现中的入射向量与图5中$L$是相反的，所以需要将公式中的$(11)$的入射向量取反，如下所示： T=\\frac{\\eta_L}{\\eta_T}(L-(N\\cdot L)N)-N\\sqrt{1-\\frac{\\eta^2_L}{\\eta^2_T}[1-(N\\cdot L)^2]}\\ \\tag {12}123456789101112131415static bool refract(const Vector3D &amp;ray, const Vector3D &amp;normal, float niOvernt, Vector3D &amp;refracted)&#123; Vector3D uv = ray; uv.normalize(); float dt = uv.dotProduct(normal); float discriminant = 1.0f - niOvernt * niOvernt * (1.0f - dt * dt); if (discriminant &gt; 0.0f) &#123; refracted = (uv - normal * dt) * niOvernt - normal * sqrt(discriminant); return true; &#125; else return false;&#125; &emsp;&emsp;然后创建一个$Dielectric$类，它有一个私有变量$refIdx$，它表面该物体的材质折射系数。在实现玻璃材质物体的散射函数$scatter$时，我们需要判断当前射线是从外部折射到内部还是从内部折射到外部，这可以通过计算入射向量与法向量的夹角余弦值来判断（通常法向量朝外），然后相应地将法向量的方向扭正。这里用$ni-over-nt$变量来记录$\\frac{\\eta_L}{\\eta_T}$，我们知道空气的折射系数为$1.00$，所以从外面折射入物体内部时其取值等于$1.0/refIdx$，从内部折射到外部时取值为$refIdx$。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354 class Dielectric : public Material &#123; private: float refIdx; public: Dielectric(float ri) : refIdx(ri) &#123;&#125; virtual ~Dielectric() = default; virtual bool scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const; &#125;; bool Dielectric::scatter(const Ray &amp;in, const HitRecord &amp;rec, Vector3D &amp;attenuation, Ray &amp;scattered) const &#123; Vector3D outward_normal; Vector3D reflected = Vector3D::reflect(in.getDirection(), rec.m_normal); float ni_over_nt; attenuation = Vector3D(1.0f, 1.0f, 1.0f); Vector3D refracted; float reflect_prob; float cosine; // from inside to outside. if (in.getDirection().dotProduct(rec.m_normal) &gt; 0.0f) &#123; outward_normal = -rec.m_normal; ni_over_nt = refIdx; cosine = refIdx * in.getDirection().dotProduct(rec.m_normal) / in.getDirection().getLength(); &#125; // from outside to inside. else &#123; outward_normal = rec.m_normal; ni_over_nt = 1.0 / refIdx; cosine = -in.getDirection().dotProduct(rec.m_normal) / in.getDirection().getLength(); &#125; if (Vector3D::refract(in.getDirection(), outward_normal, ni_over_nt, refracted)) &#123; reflect_prob = schlick(cosine, refIdx); &#125; else &#123; scattered = Ray(rec.m_position, reflected); reflect_prob = 1.0f; &#125; if (drand48() &lt; reflect_prob) scattered = Ray(rec.m_position, reflected); else scattered = Ray(rec.m_position, refracted); return true; &#125;&#125; &emsp;&emsp;这里还要引入一个菲涅尔反射现象（仅对电介质和非金属表面有定义）。生活中，当我们以垂直的视角观察时，任何物体或者材质表面都有一个基础反射率(Base Reflectivity)，但是如果以一定的角度往平面上看的时候所有反光都会变得明显起来。你可以自己尝试一下，用垂直的视角观察你自己的木制桌面，此时一定只有最基本的反射性。但是如果你从近乎与法线成90度的角度观察的话反光就会变得明显的多。如果从理想的90度的视角观察，所有的平面理论上来说都能完全的反射光线。这种现象因菲涅尔而闻名，并体现在了菲涅尔方程之中。菲涅尔方程是一个相当复杂的方程式，不过幸运的是菲涅尔方程可以用Fresnel-Schlick近似法求得近似解： F_{schlick(h,v,F_0)}=F_0+(1-F_0)(1-(h\\cdot v))^5 \\tag {13}&emsp;&emsp;这里的$F_0$y由物体的折射系数得到，$h$是入射向量的负向量（因为我们定义的入射向量方向朝向交点），$v$则是交点处的法向量$v$，我们实现一个$schlick$函数如下： 123456float schlick(float cosine, float ref_idx) const&#123; float r0 = (1.0f - ref_idx) / (1.0f + ref_idx); r0 = r0 * r0; return r0 + (1.0f - r0) * pow((1.0f - cosine), 5.0f);&#125; &emsp;&emsp;我们还定义了一个reflect_prob变量，它介于0~1之间。我们根据reflect_prob与介于$[0,1)$的随机数做比较确定选择反射还是折射，这个还是很合理的，为什么呢？因为我们做了100次采样！那么我们可以理直气壮的说，我们的透明电介质真正做到了反射和折射的混合（除了全反射现象），而且这样符合光线照射透明电介质时，它会分裂为反射光线和折射光线的物理现象。（在程序中，教程作者在从内部折射到外部的时候将$cosine$值还乘上了个$refIdx$，这个操作没明白作者的意图，不乘上$refIdx$好像也没有发现渲染结果有明显的错误）。 &emsp;&emsp;最后，我们实现的玻璃球球内图像是颠倒的，这属于正常现象，原因如下图所示。光线经过两次折射最终导致了图像的颠倒。 4、抗锯齿&emsp;&emsp;为了减少光线追踪方法的噪声点和锯齿，我们需要做一些抗锯齿处理。方法就是在计算一个像素坐标的像素值时，发射很多条射线，射线的取值范围在一个像素之内，然后将所有光线获取的像素值累加起来，最后除以总的采样数。代码如下： 123456789101112131415161718192021int samples = 100;for (int row = m_config.m_height - 1; row &gt;= 0; --row)&#123; for (int col = 0; col &lt; m_config.m_width; ++col) &#123; Vector4D color; for (int sps = 0; sps &lt; samples; ++sps) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = camera.getRay(u, v); color += tracing(ray, world, 0); &#125; color /= static_cast&lt;float&gt;(samples); color.w = 1.0f; // gamma correction. color = Vector4D(sqrt(color.x), sqrt(color.y), sqrt(color.z), color.w); drawPixel(col, row, color); &#125;&#125; &emsp;&emsp;这里还提到了gamma矫正，关于gamma矫正请看这里)。我们对计算得到的像素做了一个简单的gamma矫正，gamma矫正系数取为$2.0$。不进行gamma矫正的话，渲染出来的图片明显偏暗。 5、景深&emsp;&emsp;关于现实生活中摄像机的景深原理，我不再详细说明。在光线追踪中实现景深并不复杂。实现的方法：首先是射线的出发点视点，我们的眼睛（或者相机）不再是一个点而是眼睛所在的周围圆盘上的随机点，因为实际的相机是有摄像镜头的，摄像镜头是一个大光圈（很大一个镜片），并不是针孔类的东东，所以，我们要模拟镜头，就要随机采针孔周围的光圈点。 &emsp;&emsp;此外还有一个焦距的问题，我们一开始假设成像平面在摄像机坐标系的$z=-1$上，为了实现摄像机的景深效果，现在我们要引入现实摄像机的焦距概念。简单的说焦距是焦点到面镜的中心点之间的距离。因此我们提供了一个焦距的参数给用户调整，以确定所需的景深效果。通常情况下焦距$focusDist$等于$length(target-cameraPos)$。这个时候我们将成像平面挪到了摄像机坐标系的$z=-focusDist$上，相应地需要调整计算成像平面的$halfHeight$（在前面的基础上再乘上个$focusDist$）。 12345678910111213141516171819202122232425262728293031323334Camera::Camera(const Vector3D &amp;cameraPos, const Vector3D &amp;target, float vfov, float aspect, float aperture, float focus_dist)&#123; m_pos = cameraPos; m_target = target; m_fovy = vfov; m_aspect = aspect; m_lensRadius = aperture * 0.5f; m_focusDist = focus_dist; update();&#125;void Camera::update()&#123; const Vector3D worldUp(0.0f, 1.0f, 0.0f); // frustum. float theta = radians(m_fovy); float half_height = static_cast&lt;float&gt;(tan(theta * 0.5f)) * m_focusDist; float half_width = m_aspect * half_height; // camera coordinate system. m_axisZ = m_pos - m_target; m_axisZ.normalize(); m_axisX = worldUp.crossProduct(m_axisZ); m_axisX.normalize(); m_axisY = m_axisZ.crossProduct(m_axisX); m_axisY.normalize(); // view port. m_lowerLeftCorner = m_pos - m_axisX * half_width - m_axisY * half_height - m_axisZ * m_focusDist; m_horizontal = m_axisX * 2.0f * half_width; m_vertical = m_axisY * 2.0f * half_height;&#125; 6、递归光线追踪&emsp;&emsp;最后，我们实现的光线追踪器$Tracer$如下，追踪器的核心实现主要在$tracing$函数和$render$函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168class Hitable;class Vector3D;class Vector4D;class Tracer&#123;private: class Setting &#123; public: int m_maxDepth; int m_width, m_height, m_channel; Setting():m_maxDepth(50), m_channel(4) &#123;&#125; &#125;; Setting m_config; unsigned char *m_image;public: Tracer(); ~Tracer(); void initialize(int w, int h, int c = 4); unsigned char *render(); int getWidth() const &#123; return m_config.m_width; &#125; int getHeight() const &#123; return m_config.m_height; &#125; int getChannel() const &#123; return m_config.m_channel; &#125; int getRecursionDepth() const &#123; return m_config.m_maxDepth; &#125; unsigned char *getImage() const &#123; return m_image; &#125; void setRecursionDepth(int depth); void setCamera(const Vector3D &amp;cameraPos, const Vector3D &amp;target, const Vector3D &amp;worldUp, float fovy, float aspect, float aperture, float focus_dist);private: Hitable *randomScene(); Vector4D tracing(const Ray &amp;r, Hitable *world, int depth); float hitSphere(const Vector3D &amp;center, const float &amp;radius, const Ray &amp;ray); void drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color);&#125;;void Tracer::initialize(int w, int h, int c)&#123; m_config.m_width = w; m_config.m_height = h; if (m_image != nullptr) delete m_image; m_image = new unsigned char[m_config.m_width * m_config.m_height * m_config.m_channel];&#125;unsigned char *Tracer::render()&#123; // viewport Vector3D lower_left_corner(-2.0, -1.0, -1.0); Vector3D horizontal(4.0, 0.0, 0.0); Vector3D vertical(0.0, 2.0, 0.0); Vector3D origin(0.0, 0.0, 0.0); // scene Hitable* world = randomScene(); // camera Vector3D lookfrom(3, 4, 10); Vector3D lookat(0, 0, 0); float dist_to_focus = 10.0f; float aperture = 0.0f; Camera camera(lookfrom, lookat, 45, static_cast&lt;float&gt;(m_config.m_width) / m_config.m_height, aperture, dist_to_focus); int samples = 100; for (int row = m_config.m_height - 1; row &gt;= 0; --row) &#123; for (int col = 0; col &lt; m_config.m_width; ++col) &#123; Vector4D color; for (int sps = 0; sps &lt; samples; ++sps) &#123; float u = static_cast&lt;float&gt;(col + drand48()) / static_cast&lt;float&gt;(m_config.m_width); float v = static_cast&lt;float&gt;(row + drand48()) / static_cast&lt;float&gt;(m_config.m_height); Ray ray = camera.getRay(u, v); color += tracing(ray, world, 0); &#125; color /= static_cast&lt;float&gt;(samples); color.w = 1.0f; // gamma correction. color = Vector4D(sqrt(color.x), sqrt(color.y), sqrt(color.z), color.w); drawPixel(col, row, color); &#125; &#125; reinterpret_cast&lt;HitableList*&gt;(world)-&gt;clearHitable(); delete world; return m_image;&#125;void Tracer::drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color)&#123; if (x &lt; 0 || x &gt;= m_config.m_width || y &lt; 0 || y &gt;= m_config.m_height) return; unsigned int index = (y * m_config.m_width + x) * m_config.m_channel; m_image[index + 0] = static_cast&lt;unsigned char&gt;(255 * color.x); m_image[index + 1] = static_cast&lt;unsigned char&gt;(255 * color.y); m_image[index + 2] = static_cast&lt;unsigned char&gt;(255 * color.z); m_image[index + 3] = static_cast&lt;unsigned char&gt;(255 * color.w);&#125;Hitable *Tracer::randomScene()&#123; int n = 500; HitableList *list = new HitableList(); list-&gt;addHitable(new Sphere(Vector3D(0, -1000.0, 0), 1000, new Lambertian(Vector3D(0.5, 0.5, 0.5)))); for (int a = -11; a &lt; 11; ++a) &#123; for (int b = -11; b &lt; 11; ++b) &#123; float choose_mat = drand48(); Vector3D center(a + 0.9*drand48(), 0.2, b + 0.9*drand48()); if ((center - Vector3D(4, 0.2, 0)).getLength() &gt; 0.9) &#123; // diffuse. if (choose_mat &lt; 0.4f) list-&gt;addHitable(new Sphere(center, 0.2, new Lambertian (Vector3D(drand48()*drand48(), drand48()*drand48(), drand48()*drand48())))); // metal else if (choose_mat &lt; 0.6f) list-&gt;addHitable(new Sphere(center, 0.2, new Metal (Vector3D(0.5f*(1.0f + drand48()), 0.5f*(1.0f + drand48()), 0.5f*(1.0f + drand48())), 0.5f*drand48()))); // glass else list-&gt;addHitable(new Sphere(center, 0.2, new Dielectric (1.5f))); &#125; &#125; &#125; list-&gt;addHitable(new Sphere(Vector3D(0, 1, 0), 1.0, new Dielectric(1.5f))); list-&gt;addHitable(new Sphere(Vector3D(-4, 1, 0), 1.0, new Lambertian(Vector3D(0.4, 0.2, 0.1)))); list-&gt;addHitable(new Sphere(Vector3D(4, 1, 0), 1.0, new Metal(Vector3D(0.7, 0.6, 0.5), 0.0f))); return list;&#125;Vector4D Tracer::tracing(const Ray &amp;r, Hitable *world, int depth)&#123; HitRecord rec; if (world-&gt;hit(r, 0.001f, FLT_MAX, rec)) &#123; Ray scattered; Vector3D attenuation; if (depth &lt; m_config.m_maxDepth &amp;&amp; rec.m_material-&gt;scatter(r, rec, attenuation, scattered)) return attenuation * tracing(scattered, world, depth + 1); else return Vector4D(0.0f, 0.0f, 0.0f, 1.0f); //return backgroundColor(Ray(rec.m_position, target - rec.m_position), world) * 0.5f; //return rec.normal * 0.5f + Vector3D(0.5f, 0.5f, 0.5f); &#125; else &#123; float t = 0.5f * (r.getDirection().y + 1.0f); Vector4D ret = Vector3D(1.0f, 1.0f, 1.0f) * (1.0f - t) + Vector3D(0.5f, 0.7f, 1.0f) * t; ret.w = 1.0f; return ret; &#125;&#125; 三、程序结果 参考资料$[1]$ https://www.cnblogs.com/jerrycg/p/4941359.html $[2]$ https://blog.csdn.net/baishuo8/article/details/81476422 $[3]$ https://blog.csdn.net/silangquan/article/details/8176855 $[4]$ Peter Shirley. Ray Tracing in One Weekend. Amazon Digital Services LLC, January 26, 2016. $[5]$ https://learnopengl-cn.github.io/07%20PBR/01%20Theory/ $[6]$ https://www.cnblogs.com/lv-anchoret/p/10223222.html","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/about/categories/Ray-Tracer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Ray Tracer","slug":"Ray-Tracer","permalink":"https://yangwc.com/about/tags/Ray-Tracer/"}]},{"title":"软渲染器Soft Renderer：光照着色篇（完结）","slug":"SoftRenderer-Shading","date":"2019-05-05T12:39:50.871Z","updated":"2019-05-25T07:03:43.119Z","comments":true,"path":"2019/05/05/SoftRenderer-Shading/","link":"","permalink":"https://yangwc.com/about/2019/05/05/SoftRenderer-Shading/","excerpt":"在前面的博客我们已经实现了基本的三维渲染管线流程，这一章主要是在此基础上进行润色，不借助任何库实现obj模型导入、Blin-Phong光照模型、摄像机漫游（第一人称摄像机、第三人称摄像机）。注意：初学者慎入","text":"在前面的博客我们已经实现了基本的三维渲染管线流程，这一章主要是在此基础上进行润色，不借助任何库实现obj模型导入、Blin-Phong光照模型、摄像机漫游（第一人称摄像机、第三人称摄像机）。注意：初学者慎入 obj模型导入 Blinn-Phong光照着色 虚拟场景漫游 程序结果 结语 一、obj模型导入&emsp;&emsp;obj模型文件（这里不是指c++编译得到的.o中间文件）是一种格式简单、清晰的模型文件，这种模型的格式非常容易解析。目前有一个非常流行的开源的模型导入库Assimp，封装了各种各样模型文件的加载，省去很多麻烦。而我因为一方面为了尽量避免引入第三方库，另一方面obj模型的导入不难，所以自己实现了一个obj加载类$ObjModel$。实现obj模型加载并不难，只需简单了解一下obj文件的格式即可。 &emsp;&emsp;obj文件格式有类数据，一类一行，分别以v、vt、vn和f开头。用记事本打开一个简单的obj文件，如下所示： &emsp;&emsp;以v（即vertex的缩写）开头的一行分别为模型顶点的$x$、$y$、$z$坐标，以vt（即vertex texcoord的缩写）开头的一行分别为纹理坐标的$u$、$v$值，以vn（即vertex normal的缩写）开头的一行分别是法向量的$x$、$y$、$z$值。而f（即face的缩写）格式为v/vt/vn，其中对应的是各自的索引值，一个v/vt/vn描述了一个三角形顶点的顶点坐标、纹理坐标、法线向量，通常以f的一行有三列v/vt/vn，组成一个三角形面片。所以我们读取的时候按照这些开头标记读取即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394class ObjModel : public Mesh&#123;private: Vector3D minPoint, maxPoint; // Bounding box.public: // ctor/dtor. ObjModel(const std::string &amp;path); virtual ~ObjModel(); // Size setting. Vector3D setSizeToVector(float sx, float sy, float sz) const; Matrix4x4 setSizeToMatrix(float sx, float sy, float sz) const;private: // Obj file loader. void loadObjFile(const std::string &amp;path);&#125;;void ObjModel::loadObjFile(const std::string &amp;path)&#123; // obj loader. ifstream in; in.open(path, ifstream::in); if(in.fail()) &#123; std::cout &lt;&lt; \"Fail to load obj-&gt;\" &lt;&lt; path &lt;&lt; endl; &#125; string line; minPoint = Vector3D(+10000000000,+10000000000,+10000000000); maxPoint = Vector3D(-10000000000,-10000000000,-10000000000); vector&lt;Vector3D&gt; vertices; vector&lt;Vector3D&gt; normals; vector&lt;Vector2D&gt; texcoords; while(!in.eof()) &#123; getline(in, line); istringstream iss(line.c_str()); char trash; //vertex if(!line.compare(0, 2, \"v \")) &#123; iss &gt;&gt; trash; Vector3D vertex; iss &gt;&gt; vertex.x; iss &gt;&gt; vertex.y; iss &gt;&gt; vertex.z; vertices.push_back(vertex); if(minPoint.x &gt; vertex.x)minPoint.x = vertex.x; if(minPoint.y &gt; vertex.y)minPoint.y = vertex.y; if(minPoint.z &gt; vertex.z)minPoint.z = vertex.z; if(maxPoint.x &lt; vertex.x)maxPoint.x = vertex.x; if(maxPoint.y &lt; vertex.y)maxPoint.y = vertex.y; if(maxPoint.z &lt; vertex.z)maxPoint.z = vertex.z; &#125; // normal else if(!line.compare(0, 3, \"vn \")) &#123; iss &gt;&gt; trash &gt;&gt; trash; Vector3D normal; iss &gt;&gt; normal.x; iss &gt;&gt; normal.y; iss &gt;&gt; normal.z; normal.normalize(); normals.push_back(normal); &#125; // texcoord else if(!line.compare(0, 3, \"vt \")) &#123; iss &gt;&gt; trash &gt;&gt; trash; Vector2D texcoord; iss &gt;&gt; texcoord.x; iss &gt;&gt; texcoord.y; texcoords.push_back(texcoord); &#125; // face else if(!line.compare(0, 2, \"f \")) &#123; iss &gt;&gt; trash; int index[3]; while(iss &gt;&gt; index[0] &gt;&gt; trash &gt;&gt; index[1] &gt;&gt; trash &gt;&gt; index[2]) &#123; Vertex data; data.position = vertices[index[0] - 1]; data.texcoord = texcoords[index[1] - 1]; data.normal = normals[index[2] - 1]; data.color = Vector4D(1.0,1.0,1.0,1.0); m_indices.push_back(m_vertices.size()); m_vertices.push_back(data); &#125; &#125; &#125; in.close();&#125; &emsp;&emsp;可以看到这里继承了父类$Mesh$，这样读进来就作为一个网格类，能够传进渲染管线中渲染。测试读取了几个模型文件，效果如下： 二、Blin-Phong光照着色&emsp;&emsp;之前我们的着色器一直都是直接传输数据，没有做一些着色器计算，这里我们给渲染出来的模型加上光照着色。采用的光照模型是Blinn-Phong光照模型，并实现了两种着色器方法，分别是Gouraud着色、Phong着色。注意别混淆了光照模型和着色模型，光照模型是一种理论模型，着色模型则是具体的实现方式。Gouraud着色和Phong着色都是采用Blinn-Phong光照模型，差别在于两者在何处实现光照计算。 &emsp;&emsp;网上的LearnOpenGL教程很详细地介绍了Phong光照模型以及Blinn-Phong光照（Phong和Blinnn的差别只在于高光计算的一小部分），我就不再说太多这些方面的东西了，想具体了解的朋友请看这里)和这里)。概括起来，Phong光照模型包含环境光、漫反射光和镜面高光，其计算方式如下： I=K_aI_a+k_dI_ecos\\alpha+k_sI_scos^n\\lambda \\tag {1}&emsp;&emsp;其中的$k_a$、$k_d$和$k_s$分别为物体的环境光颜色、漫反射颜色和镜面高光颜色数，$n$是物体的高光读，而$I_a$、$I_e$和$I_s$是光源的环境光颜色、漫反射照亮的颜色和镜面反射的颜色。针对物体材质和光照的种类，我们创建一个$Material$和虚类$Light$，并把光照的计算过程抽象为一个函数$lighting$： 123456789101112131415161718192021222324252627282930313233class Material&#123;public: Material() = default; ~Material() = default; double m_shininess; Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_reflect; void setMaterial(Vector3D _amb, Vector3D _diff, Vector3D _spec, double _shin) &#123; m_shininess = _shin; m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; &#125;&#125;;class Light&#123;public: Light() = default; virtual ~Light() = default; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const = 0;&#125;; &emsp;&emsp;根据光源的种类不同，通常有平行光、点光源和聚束光三类（关于这类光，请看LearnOpenGL的这篇)）。平行光的特点就是光线束都是平行的，因而只需记录平行光的方向即可： 12345678910111213141516171819202122232425262728293031323334353637383940414243class DirectionalLight : public Light&#123;public: Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_direction; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const; void setDirectionalLight(Vector3D _amb, Vector3D _diff, Vector3D _spec, Vector3D _dir) &#123; m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; m_direction = _dir; m_direction.normalize(); &#125;&#125;;void DirectionalLight::lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D &amp;ambient, Vector3D &amp;diffuse, Vector3D &amp;specular) const&#123; float diff = max(normal.dotProduct(-this-&gt;m_direction), 0.0f); Vector3D halfwayDir = eyeDir + this-&gt;m_direction; halfwayDir.normalize(); float spec = pow(max(eyeDir.dotProduct(halfwayDir), 0.0f), material.m_shininess); ambient = m_ambient; diffuse = m_diffuse * diff; specular = m_specular * spec;&#125; &emsp;&emsp;点光源则需要记录光源的位置，用以计算光照的方向。与平行光还有一点不同的是，点光源通常有个照明区域范围，光照的强度随着距离的增加而削弱，且这类减弱不是线性的。因此我们还要衰减因子，把计算得到的光照颜色再乘上这个衰减因子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class PointLight : public Light&#123;public: Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_position; Vector3D m_attenuation; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const; void setPointLight(Vector3D _amb, Vector3D _diff, Vector3D _spec, Vector3D _pos, Vector3D _atte) &#123; m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; m_position = _pos; m_attenuation = _atte; &#125;&#125;;void PointLight::lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D &amp;ambient, Vector3D &amp;diffuse, Vector3D &amp;specular) const&#123; // ambient ambient = this-&gt;m_ambient; // diffuse Vector3D lightDir = (this-&gt;m_position - position); lightDir.normalize(); float diff = max(normal.dotProduct(lightDir), 0.0f); diffuse = this-&gt;m_diffuse * diff; // specular Vector3D halfwayDir = eyeDir + lightDir; halfwayDir.normalize(); float spec = pow(max(eyeDir.dotProduct(halfwayDir), 0.0f), material.m_shininess); specular = this-&gt;m_specular * spec; // attenuation float distance = (this-&gt;m_position - position).getLength(); float attenuation = 1.0 / (m_attenuation.x + m_attenuation.y * distance + m_attenuation.z * (distance * distance)); ambient *= attenuation; diffuse *= attenuation; specular *= attenuation;&#125; &emsp;&emsp;聚束光是一种比较特殊的光源（例如手电筒光、舞台灯光），它的特点就是只有在聚光方向的特定半径内的物体才会被照亮，其它的物体都会保持黑暗。我们采用一个光源位置、照明方向和切光角来描述一个聚光灯： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class SpotLight : public Light&#123;public: double m_cutoff, m_outcutoff; Vector3D m_ambient; Vector3D m_diffuse; Vector3D m_specular; Vector3D m_position; Vector3D m_direction; Vector3D m_attenuation; virtual void lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D&amp; ambient, Vector3D&amp; diffuse, Vector3D&amp; specular) const; void setSpotLight(Vector3D _amb, Vector3D _diff, Vector3D _spec, double _cut, Vector3D _pos, Vector3D _dir, Vector3D _atte) &#123; m_cutoff = cos(_cut * M_PI/180.0); m_outcutoff = cos((_cut + 10.0) * M_PI/180.0); m_ambient = _amb; m_diffuse = _diff; m_specular = _spec; m_position = _pos; m_direction = _dir; m_attenuation = _atte; m_direction.normalize(); &#125;&#125;;void SpotLight::lighting(const Material &amp;material, const Vector3D &amp;position, const Vector3D &amp;normal, const Vector3D &amp;eyeDir, Vector3D &amp;ambient, Vector3D &amp;diffuse, Vector3D &amp;specular) const&#123; // ambient ambient = this-&gt;m_ambient; // diffuse Vector3D lightDir = this-&gt;m_position - position; lightDir.normalize(); float diff = max(normal.dotProduct(lightDir), 0.0f); diffuse = this-&gt;m_diffuse * diff ; // specular Vector3D halfwayDir = eyeDir + lightDir; halfwayDir.normalize(); float spec = pow(max(eyeDir.dotProduct(halfwayDir), 0.0f), material.m_shininess); specular = this-&gt;m_specular * spec; // spotlight (soft edges) float theta = lightDir.dotProduct(-this-&gt;m_direction); float epsilon = (this-&gt;m_cutoff - this-&gt;m_outcutoff); float intensity = (theta - this-&gt;m_outcutoff) / epsilon; if(intensity &lt; 0.0f)intensity = 0.0f; if(intensity &gt; 1.0f)intensity = 1.0f; diffuse *= intensity; specular *= intensity; // attenuation float distance = (this-&gt;m_position - position).getLength(); float attenuation = 1.0 / (m_attenuation.x + m_attenuation.y * distance + m_attenuation.z * (distance * distance)); ambient *= attenuation; diffuse *= attenuation; specular *= attenuation;&#125; &emsp;&emsp;然后我们就需要把光照计算集成到着色器中，这里提供了两种方式：光照计算集成到顶点着色器，即Gouraud着色方法，逐顶点光照，然后靠线性插值得到每个像素的光照颜色；光照计算集成到片元着色器，即Phong着色法，逐像素光照，根据插值得到的法向量做相应的计算。显然前者计算量少了很多，但是后者更为真实。我们建立一个$Gouraud$着色类如下： 123456789101112131415161718192021222324252627282930313233class GouraudShader : public BaseShader&#123;private: // Those are not created by shader. const Light *m_light; // Light.(just only one) const Material *m_material; // Mesh material. const Texture2D *m_unit; // Texture unit. Vector3D m_eyePos; // Observer's position. Matrix4x4 m_modelMatrix; // Model matrix. Matrix4x4 m_viewMatrix; // View matrix. Matrix4x4 m_projectMatrix; // Projection matrix. Matrix4x4 m_invModelMatrix; // Inverse of model matrix for normal.public: // ctor/dtor. GouraudShader(); virtual ~GouraudShader() = default; // Shader stage. virtual VertexOut vertexShader(const Vertex &amp;in); virtual Vector4D fragmentShader(const VertexOut &amp;in); // Shader setting. virtual void bindShaderUnit(Texture2D *unit)&#123;m_unit = unit;&#125; virtual void setModelMatrix(const Matrix4x4 &amp;world) &#123;m_modelMatrix = world;m_invModelMatrix = m_modelMatrix.getInverseTranspose();&#125; virtual void setViewMatrix(const Matrix4x4 &amp;view)&#123;m_viewMatrix = view;&#125; virtual void setProjectMatrix(const Matrix4x4 &amp;project)&#123;m_projectMatrix = project;&#125; virtual void setMaterial(const Material *material)&#123;m_material = material;&#125; virtual void setLight(const Light *light)&#123;m_light = light;&#125; virtual void setEyePos(const Vector3D eye)&#123;m_eyePos = eye;&#125;&#125;; &emsp;&emsp;这里提一下关于顶点法向量的变换矩阵。我们目前已经有顶点的model矩阵，但是顶点做变换之后的法向量却不能直接乘上model矩阵获得。我们知道顶点的切线与法线相互垂直，因而它们的点乘为$0$，即有： N\\cdot T = N^T*T = 0 \\tag {2}&emsp;&emsp;顶点切线必然随着模型矩阵的变换而变换，即模型矩阵为$M$，因而变换后的切线$T’=M\\cdot T$。我们记变换后的法向量为$N’$，其正确的法线变换为$Q$，则$N’=Q\\cdot N$，那么变换后$N’$和$T’$应该依旧保持垂直关系，依旧有$N’\\cdot T’=(Q\\cdot N)\\cdot (M\\cdot T)=(Q\\cdot N)^T\\cdot (M\\cdot T)=N^T\\cdot (Q^T\\cdot M)\\cdot T$，与公式$(2)$对比，我们只要令$Q^T\\cdot M = I$结果为单位矩阵，则有$N’\\cdot T’=N\\cdot T = 0$。从而可得法线的变换矩阵为： Q= (N^{-1})^T \\tag {3}123456789101112131415161718192021222324252627282930313233343536373839404142434445VertexOut GouraudShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = m_modelMatrix * in.position; result.posH = m_projectMatrix * m_viewMatrix * result.posTrans; result.color = in.color; result.texcoord = in.texcoord; result.normal = m_invModelMatrix * Vector4D(in.normal); // Gouraud shading. if(m_unit) result.color = m_unit-&gt;sample(result.texcoord); Vector3D _amb, _diff, _spec; if(m_light) &#123; Vector3D eyeDir = m_eyePos - result.posTrans; eyeDir.normalize(); m_light-&gt;lighting(*m_material, result.posTrans, result.normal, eyeDir, _amb, _diff, _spec); result.color.x *= (_amb.x + _diff.x + _spec.x); result.color.y *= (_amb.y + _diff.y + _spec.y); result.color.z *= (_amb.z + _diff.z + _spec.z); result.color.w = 1.0f; &#125; // oneDivZ to correct lerp. result.oneDivZ = 1.0 / result.posH.w; result.posTrans *= result.oneDivZ; result.texcoord *= result.oneDivZ; result.color *= result.oneDivZ; return result;&#125;Vector4D GouraudShader::fragmentShader(const VertexOut &amp;in)&#123; Vector4D litColor = in.color; return litColor;&#125; &emsp;&emsp;Phong着色方式则在$fragmentShader$中实现光照计算，原理简单，不再赘述。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class PhongShader : public BaseShader&#123;private: // Those are not created by shader. const Light *m_light; // Light.(just only one) const Material *m_material; // Mesh material. const Texture2D *m_unit; // Texture unit. Vector3D m_eyePos; // Observer's position. Matrix4x4 m_modelMatrix; // Model matrix. Matrix4x4 m_viewMatrix; // View matrix. Matrix4x4 m_projectMatrix; // Projection matrix. Matrix4x4 m_invModelMatrix; // Inverse of model matrix for normal.public: // ctor/dtor PhongShader(); virtual ~PhongShader() = default; // Shader stage. virtual VertexOut vertexShader(const Vertex &amp;in); virtual Vector4D fragmentShader(const VertexOut &amp;in); // Shader setting. virtual void bindShaderUnit(Texture2D *unit)&#123;m_unit = unit;&#125; virtual void setModelMatrix(const Matrix4x4 &amp;world) &#123;m_modelMatrix = world;m_invModelMatrix = m_modelMatrix.getInverseTranspose();&#125; virtual void setViewMatrix(const Matrix4x4 &amp;view)&#123;m_viewMatrix = view;&#125; virtual void setProjectMatrix(const Matrix4x4 &amp;project)&#123;m_projectMatrix = project;&#125; virtual void setMaterial(const Material *material)&#123;m_material = material;&#125; virtual void setLight(const Light *light)&#123;m_light = light;&#125; virtual void setEyePos(const Vector3D eye)&#123;m_eyePos = eye;&#125;&#125;;VertexOut PhongShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = m_modelMatrix * in.position; result.posH = m_projectMatrix * m_viewMatrix * result.posTrans; result.color = in.color; result.texcoord = in.texcoord; result.normal = m_invModelMatrix * Vector4D(in.normal); // oneDivZ to correct lerp. result.oneDivZ = 1.0 / result.posH.w; result.posTrans *= result.oneDivZ; result.texcoord *= result.oneDivZ; result.color *= result.oneDivZ; return result;&#125;Vector4D PhongShader::fragmentShader(const VertexOut &amp;in)&#123; Vector4D litColor = in.color; // Gouraud shading. if(m_unit) litColor = m_unit-&gt;sample(in.texcoord); Vector3D _amb, _diff, _spec; if(m_light) &#123; Vector3D eyeDir = m_eyePos - in.posTrans; eyeDir.normalize(); m_light-&gt;lighting(*m_material, in.posTrans, in.normal, eyeDir, _amb, _diff, _spec); litColor.x *= (_amb.x + _diff.x + _spec.x); litColor.y *= (_amb.y + _diff.y + _spec.y); litColor.z *= (_amb.z + _diff.z + _spec.z); litColor.w = 1.0f; &#125; return litColor;&#125; &emsp;&emsp;下图分别为Phong着色方式的平行光、点光源、聚束光效果： 三、虚拟场景漫游&emsp;&emsp;虚拟场景漫游是一个三维程序必不可少的，我们比较常用的虚拟摄像机有两类：第一人称摄像机、第三人生摄像机。第三人称摄像机又称为半上帝视角，一般的rpg游戏都是采用的第三人称视角。摄像机一般都是相应键盘按键、鼠标移动、鼠标滚轮事件，为了方便描述，我们创建一个$Camera3D$虚类如下： 12345678910111213141516171819202122class Camera3D&#123;public: // Local axis. // Here LocalForward should (0,0,-1). static const Vector3D LocalForward; static const Vector3D LocalUp; static const Vector3D LocalRight; // ctor/dtor. Camera3D() = default; virtual ~Camera3D()&#123;&#125; // Getter. virtual Matrix4x4 getViewMatrix() = 0; virtual Vector3D getPosition() = 0; // Key/Mouse reaction. virtual void onKeyPress(char key) = 0; virtual void onWheelMove(double delta) = 0; virtual void onMouseMove(double deltaX, double deltaY, std::string button) = 0;&#125;; 1、第一人称相机&emsp;&emsp;LearnOpenGl的这篇)对第一人称相机的构建做了的很详细的描述。不同的是，我不再采用欧拉角来描述渲染，而是采用了四元数（关于四元数，请看知乎的这篇）。理解了四元数，采用欧拉角反而比较繁琐。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class FPSCamera : public Camera3D&#123;private: mutable bool m_dirty; // Should update or not. Vector3D m_translation; // Camera's translation. Quaternion m_rotation; // Camera's rotation. Matrix4x4 m_viewMatrix; // View Matrix.public: // ctor/dtor FPSCamera(Vector3D _pos); virtual ~FPSCamera() = default; // Getter. virtual Vector3D getPosition() &#123;return m_translation;&#125; virtual Matrix4x4 getViewMatrix(); // Key/Mouse reaction. virtual void onKeyPress(char key); virtual void onWheelMove(double delta); virtual void onMouseMove(double deltaX, double deltaY, std::string button); // Transform camera's axis. void translate(const Vector3D &amp;dt); void rotate(const Vector3D &amp;axis, float angle); void setTranslation(const Vector3D &amp;t); void setRotation(const Quaternion &amp;r); // Query for camera's axis. Vector3D forward() const; Vector3D up() const; Vector3D right() const;&#125;;void FPSCamera::onKeyPress(char key)&#123; switch(key) &#123; case 'W': this-&gt;translate(forward() * 0.2f); break; case 'S': this-&gt;translate(-forward() * 0.2f); break; case 'A': this-&gt;translate(-right() * 0.2f); break; case 'D': this-&gt;translate(+right() * 0.2f); break; case 'Q': this-&gt;translate(up() * 0.2f); break; case 'E': this-&gt;translate(-up() * 0.2f); break; default: break; &#125;&#125;void FPSCamera::onWheelMove(double delta)&#123; // nothing now.&#125;void FPSCamera::onMouseMove(double deltaX, double deltaY, std::string button)&#123; double speed = 0.1f; deltaX *= speed; deltaY *= speed; this-&gt;rotate(LocalUp, -deltaX); this-&gt;rotate(right(), -deltaY);&#125; 2、第三人称摄像机&emsp;&emsp;第三人称有一个固定的目标，这个目标通常就是玩家操控的物体。摄像机可以拉远拉近、围绕目标在$xz$平面旋转、绕$x$轴上下旋转，而且摄像机永远在玩家的上方（即俯视）。为此，我们用$distance$（摄像机到玩家的距离）、$pitch$（绕$x$轴的旋转角）、$yaw$（绕$y$轴的旋转角）来获取摄像机的位置，最后获取了摄像机的位置后我们就可以直接用$LookAt$矩阵获得视图矩阵。更多关于第三人称摄像机方面的细节请看youtube上的这个视频。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596class TPSCamera : public Camera3D&#123;private: mutable bool m_dirty; // Should update or not. Vector3D m_cameraPos; // Camera's position. Transform3D m_player; // Player's transformation. Matrix4x4 m_viewMatrix; // View matrix. double m_yaw, m_pitch, m_distance; // yaw, pitch and distance to player's space.public: // ctor/dtor. TPSCamera(Vector3D target); virtual ~TPSCamera() = default; // Getter. Matrix4x4 getPlayerMatrix(); virtual Matrix4x4 getViewMatrix(); virtual Vector3D getPosition() &#123;update();return m_cameraPos;&#125; // Key/Mouse reaction. virtual void onKeyPress(char key); virtual void onWheelMove(double delta); virtual void onMouseMove(double deltaX, double deltaY, std::string button);private: // Update view matrix. void update();&#125;;void TPSCamera::onKeyPress(char key)&#123; double speed = 2.0f; switch(key) &#123; case 'W': m_dirty = true; m_player.translate(-m_player.forward() * 0.1f); break; case 'S': m_dirty = true; m_player.translate(+m_player.forward() * 0.1f); break; case 'A': m_dirty = true; m_player.rotate(m_player.up(), +speed); break; case 'D': m_dirty = true; m_player.rotate(m_player.up(), -speed); break; &#125;&#125;void TPSCamera::onWheelMove(double delta)&#123; m_dirty = true; double speed = 0.01; m_distance += -speed * delta; if(m_distance &gt; 35.0)m_distance = 35.0; if(m_distance &lt; 5.00)m_distance = 5.0;&#125;void TPSCamera::onMouseMove(double deltaX, double deltaY, std::string button)&#123; double speed = 0.2; if(button == \"RIGHT\") &#123; m_dirty = true; m_pitch += speed * deltaY; if(m_pitch &lt; 0.0)m_pitch = 0.0; if(m_pitch &gt; 89.9)m_pitch = 89.9; &#125; else if(button == \"LEFT\") &#123; m_dirty = true; m_yaw += -speed * deltaX; fmod(m_yaw, 360.0); &#125;&#125;void TPSCamera::update()&#123; if(m_dirty) &#123; m_dirty = false; Vector3D target = m_player.translation(); float height = m_distance * sin(radians(m_pitch)); float horizon = m_distance * cos(radians(m_pitch)); Vector3D _playerRot = m_player.rotation().eulerAngle(); _playerRot.y = fmod(_playerRot.y, 360); m_cameraPos.y = target.y + height; m_cameraPos.x = target.x + horizon * sin(radians(m_yaw)); m_cameraPos.z = target.z + horizon * cos(radians(m_yaw)); m_viewMatrix.setLookAt(m_cameraPos, m_player.translation(), LocalUp); &#125;&#125; 四、程序结果 五、结语&emsp;&emsp;软渲染器的搭建就此告一段落，不借助任何图形库从零开始搭建这么一个渲染管线的初衷是为了更加深入地了解当前三维渲染的整个流程，很多理论东西需要实践才能彻底地理解。这么几天关于搭建软渲染器的折腾让我收获不少，这为以后的图形学道路打下了深厚的基础。目前我实现的软渲染管线已经包含了一个传统固定管线的基本功能，我借助一些工具统计得软渲染管线的核心代码（不包括空行、注释）共2838行。不再打算加入更多的功能特性如透明融合、阴影等等，因为没必要了。相关的全部源代码已经提交到github上，请点这里。 &emsp;&emsp;由于本人的知识水平有限，若网友发现任何bug或者博文叙述错误，欢迎指正，感谢！ 参考资料$[1]$ https://learnopengl-cn.github.io/02%20Lighting/02%20Basic%20Lighting/ $[2]$ https://learnopengl-cn.github.io/01%20Getting%20started/09%20Camera/ $[3]$ https://www.youtube.com/watch?v=PoxDDZmctnU&amp;list=PLRIWtICgwaX0u7Rf9zkZhLoLuZVfUksDP&amp;index=19 $[4]$ https://github.com/ssloy/tinyrenderer/wiki","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/about/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/about/tags/Soft-Renderer/"},{"name":"3D pipeline","slug":"3D-pipeline","permalink":"https://yangwc.com/about/tags/3D-pipeline/"}]},{"title":"软渲染器Soft Renderer：进击三维篇","slug":"SoftRenderer-3DPipeline","date":"2019-05-02T06:26:22.186Z","updated":"2019-05-25T07:09:49.817Z","comments":true,"path":"2019/05/02/SoftRenderer-3DPipeline/","link":"","permalink":"https://yangwc.com/about/2019/05/02/SoftRenderer-3DPipeline/","excerpt":"有了自己实现好的的3D数学库和一个基本的光栅化渲染框架，就可以开始向这个渲染框架填充内容了。本章内容主要关于3维渲染管线的实现、深度测试、背面剔除、几何裁剪、透视纹理映射，这些内容早已被渲染API集成。学习和实现这些算法，是为了彻底了解三维物体的整个渲染流程。注意：初学者慎入","text":"有了自己实现好的的3D数学库和一个基本的光栅化渲染框架，就可以开始向这个渲染框架填充内容了。本章内容主要关于3维渲染管线的实现、深度测试、背面剔除、几何裁剪、透视纹理映射，这些内容早已被渲染API集成。学习和实现这些算法，是为了彻底了解三维物体的整个渲染流程。注意：初学者慎入 进入三维世界 裁剪、剔除优化 透视纹理映射、采样 程序结果 一、进入三维世界&emsp;&emsp;尽管二维的屏幕只能显示二维的像素，但是我们可以通过将三维的物体变换到二维的屏幕上，从而渲染出三维空间的一个投影面。这与我们人类的视觉系统类似，视网膜上最终获取的也只是三维空间某个角度下的投影。为了让三维物体正确地显示到屏幕上，我们需要借助一系列的坐标空间变换。 1、坐标系统&emsp;&emsp;在渲染管线中，三维物体的顶点在最终转换为屏幕坐标之前会被变换到多个坐标系统，这其中有几个过渡性的坐标系，使得整个变换流程逻辑清晰、便于理解。此外在某些特定情况下在这些特定的坐标系中，一些操作更加容易、方便和灵活。通常，渲染管线有$5$个不同的坐标系统，分别是局部空间、世界空间、视觉空间、裁剪空间和屏幕空间，以下是LearnOpenGL CN)的原话： 局部坐标是对象相对于局部原点的坐标，也是物体起始的坐标。 下一步是将局部坐标变换为世界空间坐标，世界空间坐标是处于一个更大的空间范围的。这些坐标相对于世界的全局原点，它们会和其它物体一起相对于世界的原点进行摆放。 接下来我们将世界坐标变换为观察空间坐标，使得每个坐标都是从摄像机或者说观察者的角度进行观察的。 坐标到达观察空间之后，我们需要将其投影到裁剪坐标。裁剪坐标会被处理至-1.0到1.0的范围内，并判断哪些顶点将会出现在屏幕上。 最后，我们将裁剪坐标变换为屏幕坐标，我们将使用一个叫做视口变换(Viewport Transform)的过程。视口变换将位于-1.0到1.0范围的坐标变换到由glViewport函数所定义的坐标范围内。最后变换出来的坐标将会送到光栅器，将其转化为片段 &emsp;&emsp;通过以上的几个步骤，三维的物体坐标最终变换到了屏幕的坐标上，其中视图矩阵和投影矩阵的构建较为复杂一点，前面我的博文软渲染器Soft Renderer：3D数学篇已经推导过这两个矩阵，这里就不再赘述了。若想查看更多关于坐标系统的内容，请查看LearnOpenGL CN的这篇文章：坐标系统)。坐标变换是一般发生在顶点着色器以及顶点着色器输出到光栅化这一阶段，视口变换在顶点着色器输出之后，不在着色器中进行（视口变换已经在前面的光栅化篇提到过了）。所以为了实现坐标变换，我们的着色器要存储$model$、$view$、$project$这三个矩阵，在$SimpleShader$中添加相关的成员变量及方法： 1234567891011121314151617181920212223242526272829class SimpleShader : public BaseShader&#123;private: Matrix4x4 m_modelMatrix; Matrix4x4 m_viewMatrix; Matrix4x4 m_projectMatrix;public: ...... virtual void setModelMatrix(const Matrix4x4 &amp;world); virtual void setViewMatrix(const Matrix4x4 &amp;view); virtual void setProjectMatrix(const Matrix4x4 &amp;project);&#125;;void SimpleShader::setModelMatrix(const Matrix4x4 &amp;world)&#123; m_modelMatrix = world;&#125;void SimpleShader::setViewMatrix(const Matrix4x4 &amp;view)&#123; m_viewMatrix = view;&#125;void SimpleShader::setProjectMatrix(const Matrix4x4 &amp;project)&#123; m_projectMatrix = project;&#125; &emsp;&emsp;这样外部要渲染时，应该向着色器输入这三个矩阵。然后在我们的顶点着色器中填入相关的逻辑： 12345678910VertexOut SimpleShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = m_modelMatrix * in.position; result.posH = m_projectMatrix * m_viewMatrix * result.posTrans; result.color = in.color; result.normal = in.normal; result.texcoord = in.texcoord; return result;&#125; &emsp;&emsp;$VertexOut$是前面文章定义的顶点着色器输出的类，它存储投影后的顶点$posH$、世界空间中的顶点$posTrans$、物体的颜色、顶点法线以及纹理坐标。接着在视口变换并送入光栅化部件之前执行透视除法，即直接将裁剪空间的顶点坐标除以它的第四个分量$w$即可。然后我们在外部的渲染循环中设置模型矩阵、视图矩阵已经投影矩阵，就能显示出三维的立体感了，以我们前一章画的三角形为例（gif录制的好像有bug，出现绿色它就给我录制成这个模糊的鬼样，实际上是非常清晰，不是渲染的锅）。 &emsp;&emsp;进入3D世界，怎么能少了3D渲染的”hello world!”——立方体呢？在$Mesh.h$手动创建一个立方体的网格数据，然后用立方体替换掉上面丑陋的三角形： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154void Mesh::asBox(double width, double height, double depth)&#123; vertices.resize(24); indices.resize(36); float halfW = width * 0.5f; float halfH = height * 0.5f; float halfD = depth * 0.5f; //front vertices[0].position = Vector3D(halfW, halfH, halfD); vertices[0].normal = Vector3D(0.f, 0.f, 1.f); vertices[0].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[0].texcoord = Vector2D(1.f, 1.f); vertices[1].position = Vector3D(-halfW, halfH, halfD); vertices[1].normal = Vector3D(0.f, 0.f, 1.f); vertices[1].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[1].texcoord = Vector2D(0.f, 1.f); vertices[2].position = Vector3D(-halfW,-halfH, halfD); vertices[2].normal = Vector3D(0.f, 0.f, 1.f); vertices[2].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[2].texcoord = Vector2D(0.f, 0.f); vertices[3].position = Vector3D(halfW, -halfH, halfD); vertices[3].normal = Vector3D(0.f, 0.f, 1.f); vertices[3].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[3].texcoord = Vector2D(1.f, 0.f); //left vertices[4].position = Vector3D(-halfW, +halfH, halfD); vertices[4].normal = Vector3D(-1.f, 0.f, 0.f); vertices[4].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[4].texcoord = Vector2D(1.f, 1.f); vertices[5].position = Vector3D(-halfW, +halfH, -halfD); vertices[5].normal = Vector3D(-1.f, 0.f, 0.f); vertices[5].color = Vector4D(1.f, 1.f, 0.f, 1.f); vertices[5].texcoord = Vector2D(0.f, 1.f); vertices[6].position = Vector3D(-halfW, -halfH, -halfD); vertices[6].normal = Vector3D(-1.f, 0.f, 0.f); vertices[6].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[6].texcoord = Vector2D(0.f, 0.f); vertices[7].position = Vector3D(-halfW, -halfH, halfD); vertices[7].normal = Vector3D(-1.f, 0.f, 0.f); vertices[7].color = Vector4D(1.f, 1.f, 1.f, 1.f); vertices[7].texcoord = Vector2D(1.f, 0.f); //back vertices[8].position = Vector3D(-halfW, +halfH, -halfD); vertices[8].normal = Vector3D(0.f, 0.f, -1.f); vertices[8].color = Vector4D(1.f, 0.f, 1.f, 1.f); vertices[8].texcoord = Vector2D(0.f, 0.f); vertices[9].position = Vector3D(+halfW, +halfH, -halfD); vertices[9].normal = Vector3D(0.f, 0.f, -1.f); vertices[9].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[9].texcoord = Vector2D(1.f, 0.f); vertices[10].position = Vector3D(+halfW, -halfH, -halfD); vertices[10].normal = Vector3D(0.f, 0.f, -1.f); vertices[10].color = Vector4D(1.f, 1.f, 0.f, 1.f); vertices[10].texcoord = Vector2D(1.f, 1.f); vertices[11].position = Vector3D(-halfW, -halfH, -halfD); vertices[11].normal = Vector3D(0.f, 0.f, -1.f); vertices[11].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[11].texcoord = Vector2D(0.f, 1.f); //right vertices[12].position = Vector3D(halfW, +halfH, -halfD); vertices[12].normal = Vector3D(1.f, 0.f, 0.f); vertices[12].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[12].texcoord = Vector2D(0.f, 0.f); vertices[13].position = Vector3D(halfW, +halfH, +halfD); vertices[13].normal = Vector3D(1.f, 0.f, 0.f); vertices[13].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[13].texcoord = Vector2D(1.f, 0.f); vertices[14].position = Vector3D(halfW, -halfH, +halfD); vertices[14].normal = Vector3D(1.f, 0.f, 0.f); vertices[14].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[14].texcoord = Vector2D(1.f, 1.f); vertices[15].position = Vector3D(halfW, -halfH, -halfD); vertices[15].normal = Vector3D(1.f, 0.f, 0.f); vertices[15].color = Vector4D(1.f, 0.f, 1.f, 1.f); vertices[15].texcoord = Vector2D(0.f, 1.f); //top vertices[16].position = Vector3D(+halfW, halfH, -halfD); vertices[16].normal = Vector3D(0.f, 1.f, 0.f); vertices[16].color = Vector4D(0.f, 0.f, 0.f, 1.f); vertices[16].texcoord = Vector2D(0.f, 0.f); vertices[17].position = Vector3D(-halfW, halfH, -halfD); vertices[17].normal = Vector3D(0.f, 1.f, 0.f); vertices[17].color = Vector4D(1.f, 1.f, 0.f, 1.f); vertices[17].texcoord = Vector2D(1.f, 0.f); vertices[18].position = Vector3D(-halfW, halfH, halfD); vertices[18].normal = Vector3D(0.f, 1.f, 0.f); vertices[18].color = Vector4D(0.f, 1.f, 1.f, 1.f); vertices[18].texcoord = Vector2D(1.f, 1.f); vertices[19].position = Vector3D(+halfW, halfH, halfD); vertices[19].normal = Vector3D(0.f, 1.f, 0.f); vertices[19].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[19].texcoord = Vector2D(0.f, 1.f); //down vertices[20].position = Vector3D(+halfW, -halfH, -halfD); vertices[20].normal = Vector3D(0.f, -1.f, 0.f); vertices[20].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[20].texcoord = Vector2D(0.f, 0.f); vertices[21].position = Vector3D(+halfW, -halfH, +halfD); vertices[21].normal = Vector3D(0.f, -1.f, 0.f); vertices[21].color = Vector4D(1.f, 1.f, 1.f, 1.f); vertices[21].texcoord = Vector2D(1.f, 0.f); vertices[22].position = Vector3D(-halfW, -halfH, +halfD); vertices[22].normal = Vector3D(0.f, -1.f, 0.f); vertices[22].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[22].texcoord = Vector2D(1.f, 1.f); vertices[23].position = Vector3D(-halfW, -halfH, -halfD); vertices[23].normal = Vector3D(0.f, -1.f, 0.f); vertices[23].color = Vector4D(1.f, 0.f, 1.f, 1.f); vertices[23].texcoord = Vector2D(0.f, 1.f); //front indices[0] = 0; indices[1] = 1; indices[2] = 2; indices[3] = 0; indices[4] = 2; indices[5] = 3; //left indices[6] = 4; indices[7] = 5; indices[8] = 6; indices[9] = 4; indices[10] = 6; indices[11] = 7; //back indices[12] = 8; indices[13] = 9; indices[14] = 10; indices[15] = 8; indices[16] = 10; indices[17] = 11; //right indices[18] = 12; indices[19] = 13; indices[20] = 14; indices[21] = 12; indices[22] = 14; indices[23] = 15; //top indices[24] = 16; indices[25] = 17; indices[26] = 18; indices[27] = 16; indices[28] = 18; indices[29] = 19; //down indices[30] = 20; indices[31] = 21; indices[32] = 22; indices[33] = 20; indices[34] = 22; indices[35] = 23;&#125; &emsp;&emsp;结果我们就得到一个如下面所示的奇怪的立方体： &emsp;&emsp;下面是动图gif（再重复一遍，模糊不是渲染的锅）： &emsp;&emsp;这的确有点像是一个立方体，但又有种说不出的奇怪。立方体的某些本应被遮挡住的面被绘制在了这个立方体其他面之上。出现这样结果的原因是因为我们的软渲染器是对一个一个三角形进行绘制的，而且计算像素时时直接覆盖而不管这个像素是否已经有其他值了，所以一个像素的值完全取决于最后赋予它的$RGBA$。除非渲染管线自动按照从远到近的顺序（这类算法有画家算法、空间分割BSP树算法）绘制三角形，否则直接覆盖的方法获取不了正确的像素值。正确渲染结果应该是像素的$RGBA$值为最靠近视点的片元值，一种常用的技术是借助第三维信息——深度来对每个相同位置的不同片元做深度的比较，并且取深度较低的那一个。 2、深度测试&emsp;&emsp;为了获取正确的三维渲染结果，我们采用一种深度缓冲的技术。深度缓冲存储深度信息，它的分辨率应该与颜色缓冲一致，深度值存储在每个片段里面（作为片段的z值），当片段想要输出它的颜色时，我们将它的深度值和z缓冲进行比较，如果当前的片段在其它片段之后，它将会被丢弃，否则将会覆盖。这个过程称为深度测试。在OpenGL和DirectX这些渲染API中，深度缓冲会自动执行而无需用户操作。在我们的软渲染器中，我们自己实现一个这样的深度测试，算法原理很简单，但是效果非常不错！ &emsp;&emsp;深度缓冲通常和颜色缓冲一起，作为帧缓冲的附件，我们在帧缓冲类中增加深度缓冲相关的变量、方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class FrameBuffer&#123;private: ...... std::vector&lt;double&gt; m_depthBuffer;public: ...... void clearColorAndDepthBuffer(const Vector4D &amp;color); double getDepth(const unsigned int &amp;x, const unsigned int &amp;y)const; void drawDepth(const unsigned int &amp;x, const unsigned int &amp;y, const double &amp;value);&#125;;void FrameBuffer::clearColorAndDepthBuffer(const Vector4D &amp;color)&#123; // fill the color buffer and depth buffer. unsigned char red = static_cast&lt;unsigned char&gt;(255*color.x); unsigned char green = static_cast&lt;unsigned char&gt;(255*color.y); unsigned char blue = static_cast&lt;unsigned char&gt;(255*color.z); unsigned char alpha = static_cast&lt;unsigned char&gt;(255*color.w); for(unsigned int row = 0;row &lt; m_height;++ row) &#123; for(unsigned int col = 0;col &lt; m_width;++ col) &#123; m_depthBuffer[row*m_width+col] = 1.0f; ...... &#125; &#125;&#125;double FrameBuffer::getDepth(const unsigned int &amp;x, const unsigned int &amp;y) const&#123; if(x &lt; 0 || x &gt;= m_width || y &lt; 0 || y &gt;= m_height) return 0.0f; return m_depthBuffer[y*m_width+x];&#125;void FrameBuffer::drawDepth(const unsigned int &amp;x, const unsigned int &amp;y, const double &amp;value)&#123; if(x &lt; 0 || x &gt;= m_width || y &lt; 0 || y &gt;= m_height) return; unsigned int index = y*m_width + x; m_depthBuffer[index] = value;&#125; &emsp;&emsp;然后我们对于每一个片元，我们获取深度缓冲中相应的数值并进行比较。在这之前，我们还要简单回顾一下在透视投影矩阵中深度值的非线性映射，在前面的数学篇中我们知道透视投影矩阵有如下形式： M_{projection}= \\left( \\begin{matrix} \\frac{1}{aspect*tan(fovy/2)}&0&0&0\\\\ 0&\\frac{1}{tan(fovy/2)}&0&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right)&emsp;&emsp;因而视图空间中的深度信息$z_e$和标准化设备空间中的深度信息$z_n$关系为： z_n=(-\\frac{f+n}{f-n}z_e-\\frac{2fn}{f-n})/{-z_e} =\\frac{2fn}{z_e(f-n)}+\\frac{f+n}{f-n} \\tag {1}&emsp;&emsp;可以看到$z_e$d到$z_n$是一种从$[-f, -n]$到$[-1,1]$的非线性映射。当$z_e$比较小的时候，公式$(1)$有很高的精度；当$z_e$比较大的时候，公式$(1)$应为取值精度降低。这个关系可以直观地从下图的函数曲线看出来： &emsp;&emsp;可以看到，深度值很大一部分是由很小的z值所决定的，这给了近处的物体很大的深度精度。$z_n$取值为$[-1,1]$，我们最后将其简单地映射到$[0,1]$，这一步我放在透视除法后。 123456789void Pipeline::perspectiveDivision(VertexOut &amp;target)&#123; target.posH.x /= target.posH.w; target.posH.y /= target.posH.w; target.posH.z /= target.posH.w; target.posH.w = 1.0f; // map from [-1,1] to [0,1] target.posH.z = (target.posH.z+1.0f) * 0.5f;&#125; &emsp;&emsp;在写入深度缓冲之前应该要清除上一帧的深度缓冲，全部置$1.0f$即可，我把这一步和清除颜色缓冲放一起了，即前面的帧缓冲类的$clearColorAndDepthBuffer$方法。在光栅化步骤，获取每个片元的屏幕位置，查找深度缓并比较，若小于当前深度缓冲中获取的值，则通过深度测试并写入深度缓冲。 12345678910111213141516171819202122232425262728void Pipeline::scanLinePerRow(const VertexOut &amp;left, const VertexOut &amp;right)&#123; // scan the line from left to right. VertexOut current; int length = right.posH.x - left.posH.x + 1; for(int i = 0;i &lt;= length;++i) &#123; // linear interpolation double weight = static_cast&lt;double&gt;(i)/length; current = lerp(left, right, weight); current.posH.x = left.posH.x + i; current.posH.y = left.posH.y; // depth testing. double depth = m_backBuffer-&gt;getDepth(current.posH.x, current.posH.y); if(current.posH.z &gt; depth) continue;// fail to pass the depth testing. m_backBuffer-&gt;drawDepth(current.posH.x,current.posH.y,current.posH.z); double w = 1.0/current.oneDivZ; current.posTrans *= w; current.color *= w; current.texcoord *= w; // fragment shader m_backBuffer-&gt;drawPixel(current.posH.x, current.posH.y, m_shader-&gt;fragmentShader(current)); &#125;&#125; &emsp;&emsp;然后就可以根据深度信息正确地渲染出三维的立体感了。 3、裁剪、剔除优化&emsp;&emsp;目前目前我们已经构建出三维的渲染管线，但是这还不够，因为图形渲染计算量很大，通常我们需要做一些优化。常见的嵌入在渲染管线中的优化算法有几何裁剪、背面剔除。 几何裁剪&emsp;&emsp;注意在坐标系统的变换过程中，位于视锥体内的顶点坐标各分量都会被映射到$[-1,1]$的范围内，超出视锥体的顶点则被映射到超出$[-1,1]$的范围。我们在这个基础上的做相关的裁剪，注意在透视除法之前各分量实际上是处于$[-w,w]$的范围内的，这里的$w$就是该顶点坐标的第四个分量$w$。针对线框模式渲染和填充模式渲染，我们有两种不同的裁剪算法。 Cohen-Sutherland线条裁剪算法&emsp;&emsp;一条线段在视口内的情况有如下所示的四种。其中端点完全在视口内和一端在视口内而另一端是在视口外的情况很好判断，但是线段完全在视口外就没那么简单了。可以看到线段$GH$的端点都在视口外部，但是线段的一部分却在视口的内部，这是如果直接根据两个端点是否在视口外做剔除的话会导致在边缘部分的线段直接消失，得到错误的结果。一种暴力的解法就是计算线段与视口窗口的交点，但是这并不高效。 &emsp;&emsp;Cohen-Sutherland提出了一种基于编码的判断算法，通过简单的移位、与或逻辑运算就可以判断一条线段处于哪种情况。对于每一个端点$(x,y)$，我们定义一个outcode——$b_0b_1b_2b_3$，视口所处的范围用$x_{min}$、$x_{max}$、$y_{min}$、$y_{max}$表示。每个端点$(x,y)$的outcode的计算方法如下： &emsp;&emsp;$b_0 = 1\\ if \\ y &gt; y_{max},\\ 0\\ otherwiose$ &emsp;&emsp;$b_1 = 1\\ if \\ y &lt; y_{min},\\ 0\\ otherwiose$ &emsp;&emsp;$b_2 = 1\\ if \\ x &gt; x_{min},\\ 0\\ otherwiose$ &emsp;&emsp;$b_3 = 1\\ if \\ x &lt; x_{max},\\ 0\\ otherwiose$ &emsp;&emsp;可以看出outcode将屏幕空间分成了$9$个部分： &emsp;&emsp;观察上面的$9$个区域，对于两个端点outcode1和outcode2，做如下的判断策略，其中的$OR$和$AND$是逻辑按位运算： &emsp;&emsp;若$(outcode1\\ OR\\ outcode2)==0$，那么线段就完全在视口内部； &emsp;&emsp;若$(outcode1\\ AND\\ outcode2)!=0$，那么线段就完全在视口外部； &emsp;&emsp;若$(outcode1\\ AND\\ outcode2)==0$，那么线段就可能部分在视口外部，部分在内部，还需要做进一步的判断（这里我进一步判断用了包围盒，因为比较常见和简单，就不过多描述了）。 &emsp;&emsp;这里我的实现就是只裁剪掉肯定完全在视口外部的线段，若还想裁剪掉部分外视口外部的线段则需要进一步的求交运算。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950bool Pipeline::lineCliping(const VertexOut &amp;from, const VertexOut &amp;to)&#123; // return whether the line is totally outside or not. float vMin = -from.posH.w, vMax = from.posH.w; float x1 = from.posH.x, y1 = from.posH.y; float x2 = to.posH.x, y2 = to.posH.y; int tmp = 0; int outcode1 = 0, outcode2 = 0; // outcode1 calculation. tmp = (y1&gt;vMax)?1:0; tmp &lt;&lt;= 3; outcode1 |= tmp; tmp = (y1&lt;vMin)?1:0; tmp &lt;&lt;= 2; outcode1 |= tmp; tmp = (x1&gt;vMax)?1:0; tmp &lt;&lt;= 1; outcode1 |= tmp; tmp = (x1&lt;vMin)?1:0; outcode1 |= tmp; // outcode2 calculation. tmp = (y2&gt;vMax)?1:0; tmp &lt;&lt;= 3; outcode2 |= tmp; tmp = (y2&lt;vMin)?1:0; tmp &lt;&lt;= 2; outcode2 |= tmp; tmp = (x2&gt;vMax)?1:0; tmp &lt;&lt;= 1; outcode2 |= tmp; tmp = (x2&lt;vMin)?1:0; outcode2 |= tmp; if((outcode1 &amp; outcode2) != 0) return true; // bounding box judge. Vector2D minPoint,maxPoint; minPoint.x = min(from.posH.x, to.posH.x); minPoint.y = min(from.posH.y, to.posH.y); maxPoint.x = max(from.posH.x, to.posH.x); maxPoint.y = max(from.posH.y, to.posH.y); if(minPoint.x &gt; vMax || maxPoint.x &lt; vMin || minPoint.y &gt; vMax || maxPoint.y &lt; vMin) return true; return false;&#125; 三角形裁剪&emsp;&emsp;判断三角形是否完全在外面也不能直接根据三个端点是否完全在视口外部来判断（我看有些软渲染的博主就用了这个错误的策略），因为还要考略以下的特殊情况。 &emsp;&emsp;为此，我直接计算三角形的轴向包围盒，然后这个包围盒判断三角形是否完全是视口外部。更进一步的裁剪是将部分在视口内部的三角形做求交，然后重新分割成完全在视口内部的三角形，这里我没有做进一步的裁剪。 1234567891011121314151617181920212223242526bool Pipeline::triangleCliping(const VertexOut &amp;v1, const VertexOut &amp;v2, const VertexOut &amp;v3)&#123; // true:not clip; // false: clip. float vMin = -1.0; float vMax = +1.0; // if the triangle is too far to see it, just return false. if(v1.posH.z &gt; vMax &amp;&amp; v2.posH.z &gt; vMax &amp;&amp; v3.posH.z &gt; vMax) return false; // if the triangle is behind the camera, just return false. if(v1.posH.z &lt; vMin &amp;&amp; v2.posH.z &lt; vMin &amp;&amp; v3.posH.z &lt; vMin) return false; // calculate the bounding box and check if clip or not. Vector2D minPoint,maxPoint; minPoint.x = min(v1.posH.x, min(v2.posH.x, v3.posH.x)); minPoint.y = min(v1.posH.y, min(v2.posH.y, v3.posH.y)); maxPoint.x = max(v1.posH.x, max(v2.posH.x, v3.posH.x)); maxPoint.y = max(v1.posH.y, max(v2.posH.y, v3.posH.y)); if(minPoint.x &gt; vMax || maxPoint.x &lt; vMin || minPoint.y &gt; vMax || maxPoint.y &lt; vMin) return false; return true;&#125; &emsp;&emsp;然后我们把几何裁剪放到渲染管线中，几何裁剪一般是在顶点着色器之后、光栅化之前。这里我把它放到了透视除法和视口变换之前。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667void Pipeline::drawIndex(RenderMode mode)&#123; // renderer pipeline. bool line1 = false, line2 = false, line3 = false; m_mode = mode; if(m_indices.empty()) return; for(unsigned int i = 0;i &lt; m_indices.size();i += 3) &#123; //! assembly to triangle primitive. Vertex p1,p2,p3; &#123; ...... &#125; //! vertex shader stage. VertexOut v1,v2,v3; &#123; ...... &#125; //! perspective division. &#123; ...... &#125; //! geometry cliping. &#123; if(m_config.m_geometryCliping) &#123; if(m_config.m_polygonMode == PolygonMode::Wire) &#123; line1 = lineCliping(v1,v2); line2 = lineCliping(v2,v3); line3 = lineCliping(v3,v1); &#125; if(m_config.m_polygonMode == PolygonMode::Fill &amp;&amp; !triangleCliping(v1,v2,v3)) continue; &#125; &#125; //! view port transformation. &#123; ...... &#125; //! rasterization and fragment shader stage. &#123; if(mode == RenderMode::wire) &#123; if(!line1) bresenhamLineRasterization(v1,v2); if(!line2) bresenhamLineRasterization(v2,v3); if(!line3) bresenhamLineRasterization(v3,v1); &#125; else if(mode == RenderMode::fill) &#123; edgeWalkingFillRasterization(v1,v2,v3); &#125; &#125; ...... &#125;&#125; 背面剔除&emsp;&emsp;背面剔除网上的这篇博客已经讲得非常详细了，原理也很简单，我就不过多描述。我们定义顶点逆时针的环绕顺序正面，然后通过三角形的三个顶点计算出法线，将顶点与视线做点乘并判断其符号即可。 123456789101112131415bool Pipeline::backFaceCulling(const Vector4D &amp;v1, const Vector4D &amp;v2, const Vector4D &amp;v3)&#123; // back face culling. if(m_mode == RenderMode::wire) return true; Vector4D tmp1 = v2 - v1; Vector4D tmp2 = v3 - v1; Vector3D edge1(tmp1.x, tmp1.y, tmp1.z); Vector3D edge2(tmp2.x, tmp2.y, tmp2.z); Vector3D viewRay(m_eyePos.x - v1.x, m_eyePos.y - v1.y, m_eyePos.z - v1.z); Vector3D normal = edge1.crossProduct(edge2); return normal.dotProduct(viewRay) &gt; 0;&#125; &emsp;&emsp;然后背面剔除应该放在渲染管线的顶点着色器输出之后，如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849void Pipeline::drawIndex(RenderMode mode)&#123; // renderer pipeline. bool line1 = false, line2 = false, line3 = false; m_mode = mode; if(m_indices.empty())return; for(unsigned int i = 0;i &lt; m_indices.size();i += 3) &#123; //! assembly to triangle primitive. Vertex p1,p2,p3; &#123; ...... &#125; //! vertex shader stage. VertexOut v1,v2,v3; &#123; ...... &#125; //! back face culling. &#123; if(!backFaceCulling(v1.posTrans, v2.posTrans, v3.posTrans)) continue; &#125; //! geometry cliping. &#123; ...... &#125; //! perspective division. &#123; ...... &#125; //! view port transformation. &#123; ...... &#125; //! rasterization and fragment shader stage. &#123; ...... &#125; &#125;&#125; 二、透视纹理映射、采样&emsp;&emsp;纹理映射是丰富三维物体细节的一个非常重要的方法，简单、廉价、快速，只需计算好的纹理坐标、纹理图片即可实现物体的多姿多彩。通常纹理图片的制作（除了过程式纹理的生成）由设计师完成，无需我们关心。而纹理坐标的计算则需要非常注意，送入渲染管线的纹理坐标只是逐顶点的纹理坐标，在光栅化阶段我们还要将纹理坐标做插值操作，最后根据插值后得到的纹理坐标对纹理图片采样获取片元的像素值。 1、透视纹理映射&emsp;&emsp;在光栅化阶段，我们是根据屏幕空间的$x$值和$y$值做线性插值操作获取片元的位置，而片元的纹理坐标如果也这么获得的话（这种方法叫做仿射纹理映射），将会导致严重的纹理扭曲。这是因为仿射纹理映射是基于这样的一个假设：物体空间的纹理坐标与屏幕空间的顶点坐标呈线性管线。 &emsp;&emsp;我们知道纹理坐标是定义在物体的顶点上面的，当我们根据屏幕空间的顶点坐标插值时，就默认了纹理坐标的变化与屏幕空间顶点坐标的变化是呈线性、均匀的关系的。但是问题在于：默认的屏幕空间上的线性关系，还原到世界空间中，就不是那么回事了，如下图所示。这张图是相机空间的一张俯视图。我们把一个多边形通过透视投影的方式变换到了投影平面上，图中红色的是世界空间中的多边形，蓝色的是变换到投影平面之后的多边形。可以看到，在投影平面上的蓝色线段被表示成若干个相等的单位步长线段。与此同时，投影面上单位步长的线段所对应的投影之前的红色线段的长度却不是相等的，从左到右所对应的长度依次递增。我们的纹理坐标是定义在红色的多边形上的，因此纹理坐标的增量应该是和红色线段的步长对应的。我们的线性插值却把纹理坐标增量根据蓝色线段的步长平均分配了。 &emsp;&emsp;这就导致了仿射纹理映射的错误的结果，如下图所示，仿射纹理映射产生了严重的扭曲。 &emsp;&emsp;而如果你不信，大可以试一试，然后你就会得到和我下面这张图一样奇怪的结果。 &emsp;&emsp;那么如何进行矫正了？网上的这篇博客已经非常详细地说明了相关的矫正方法，核心思想就是想办法让纹理坐标变得与屏幕空间的坐标线性相关，这一点可以看成纹理坐标的透视投影（与世界空间的顶点坐标投影到屏幕空间，从而通过插值获得其他的屏幕空间坐标进行光栅化有异曲同工之妙）。 &emsp;&emsp;纹理透视投影的详细过程请看这篇博客，其中借助的关系就是纹理坐标与世界空间顶点坐标是相关的（我们定义纹理坐标就是逐个顶点定义的），然后世界空间顶点坐标（为了便于讨论，这里世界空间就是视图空间）通过投影矩阵变成屏幕空间顶点坐标。在世界空间中，顶点的$x$和$y$值与$z$值呈线性关系（因为我们定义基本图元是三角形，在三角形平面上，必然是线性的，否则就是非线性的曲面了），即存在$A$和$B$有： x_e = Az_e+B\\\\ y_e = Az_e+B \\tag {2}&emsp;&emsp;$(x_e,y_e,z_e)$是视图空间的顶点坐标，即$(x’,y’)$是投影到近平面的顶点坐标。根据透视投影矩阵可知（其实就是相似三角形），$(x’,y’)$与视图空间的顶点坐标关系如下： \\begin{cases} x'=-N\\frac {x_e}{z_e}\\ \\to x_e= -\\frac{x'z_e}{N} \\\\ y'=-N\\frac {y_e}{z_e}\\ \\to y_e= -\\frac{y'z_e}{N} \\end{cases} \\tag {3}&emsp;&emsp;将公式$(3)$带入公式$(2)$，则有： \\begin{cases} x'=-N\\frac{B}{z_e}-AN\\\\ y'=-N\\frac{B}{z_e}-AN \\end{cases} \\tag {4}&emsp;&emsp;其中的$A$、$B$、$N$都是常量，把$\\frac 1{z_e}$看成一个整体，则通过透视投影矩阵的变换之后$x’$、$y’$均与$\\frac{1}{z_e}$成线性关系，这也就是透视投影的效果是近大远小的根本原因。然后注意到在三维空间中，纹理坐标$(s,t)$和$(x_e,y_e)$成线性关系。即有（这里只是定性分析，$A$和$B$具体多少我们不用关心）： \\begin{cases} x_e=As+B\\\\ x_e=At+B\\\\ y_e=As+B\\\\ y_e=At+B \\end{cases} \\tag {5}&emsp;&emsp;把公式$(5)$带入$(3)$则有（以公式$(5)$的第一个为例，其他类似）： As+B=-\\frac{x'z_e}{N}\\ \\to\\ A\\frac{s}{z_e}+B\\frac{1}{z_e}=-\\frac{x'}{N} \\tag {6}&emsp;&emsp;公式$(6)$彻底说明了纹理坐标与屏幕空间的顶点坐标的关系！$s$和$x’$并不是简单的线性关系，因为还出现了$\\frac{1}{z_e}$这个项，如果$\\frac{1}{z_e}$具体值已知，那么$\\frac{s}{z_e}$就与 $x’$成线性关系！那么我们在线性插值之前给纹理坐标$s$乘上一个$\\frac{1}{z_e}$，就可以根据屏幕空间的顶点坐标做线性插值了，然后对插值得到的纹理坐标$s’$乘上$z_e$就能还原出正确的纹理坐标！！！！ &emsp;&emsp;说了这么多都是在捋清函数关系，实现其实很简单的，上面已经说的很清楚了。我们在$VertexOut$中定义的变量$oneDivZ$就用于的透射投影映射的。除开纹理坐标，其他的世界空间坐标、顶点颜色、法线都是定义在世界空间的坐标顶点上的，为了得到正确的插值，都需要做与纹理坐标一样的处理。乘上$\\frac{1}{z_e}$这一步我放在了顶点着色器的最后一步，只要放在插值之前都行。 1234567891011VertexOut SimpleShader::vertexShader(const Vertex &amp;in)&#123; ..... // oneDivZ to correct mapping. result.oneDivZ = 1.0 / result.posH.w; result.posTrans *= result.oneDivZ; result.texcoord *= result.oneDivZ; result.color *= result.oneDivZ; return result;&#125; &emsp;&emsp;然后再光栅化插值之后各自乘上相应的倒数即可恢复出正确的插值结果。 123456789101112131415161718192021222324252627void Pipeline::scanLinePerRow(const VertexOut &amp;left, const VertexOut &amp;right)&#123; // scan the line from left to right. VertexOut current; int length = right.posH.x - left.posH.x + 1; for(int i = 0;i &lt;= length;++i) &#123; // linear interpolation double weight = static_cast&lt;double&gt;(i)/length; current = lerp(left, right, weight); current.posH.x = left.posH.x + i; current.posH.y = left.posH.y; // depth testing. ...... // restore. double w = 1.0/current.oneDivZ; current.posTrans *= w; current.color *= w; current.texcoord *= w; // fragment shader m_backBuffer-&gt;drawPixel(current.posH.x, current.posH.y, m_shader-&gt;fragmentShader(current)); &#125;&#125; 2、双线性纹理采样&emsp;&emsp;定义的纹理坐标都是$[0.0f,1.0f]$的浮点数，为了采样纹理我们需要把它乘上纹理的宽高转成整数的下标取访问纹理的像素矩阵。乘上纹理的宽高之后我们得到的依然应该是一个浮点数，为了获取像素下标，一个简单的方法就是向下取整（这种采样方法对应于OpenGL的GL_NEAREST纹理过滤方法）。如下所示： 12345678910double trueU = texcoord.x * (m_width - 1);double trueV = texcoord.y * (m_height - 1);x = static_cast&lt;unsigned int&gt;(trueU);y = static_cast&lt;unsigned int&gt;(trueV);int index[0] = (x * m_width + y) * m_channel;Vector3D texels;// INV_SCALE is 1.0/255texels.x = static_cast&lt;float&gt;(m_pixelBuffer[index + 0]) * INV_SCALE;texels.y = static_cast&lt;float&gt;(m_pixelBuffer[index + 1]) * INV_SCALE;texels.z = static_cast&lt;float&gt;(m_pixelBuffer[index + 2]) * INV_SCALE; &emsp;&emsp;问题就出在这里，这样直接抛弃小数点以后的值导致采样出的相邻纹理并不连续，那么用float采样行吗？答案是：不行！这边实现的采样函数是从数组取值，纹理坐标转为数组下标，数组下标不能用float只能用int，那么就没办法了吗？并不是，可以对周围纹理进行采样然后按照各自比例进行混合，这样能够提高显示效果。混合的方法就是双线性插值。所谓双线性插值，就是先后线性插值一次，共两次。即横向线性插值一次，然后根据前面一次的插值结果竖向插值一次，二维纹理是有两个维度，所以做双线性插值。 &emsp;&emsp;除了采样之外，还有一个纹理坐标溢出的问题。纹理坐标超过的$[0,1]$通常由两种处理方式，一种是$clamp$，超过$[0,1]$的地方的像素都获取边上的像素，这样效果就是拉伸。一种是$repeat$，故名思议，即重复平铺。这里我实现的是重复平铺，在计算真正的纹理下标之前做相应的判断和处理即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108class Texture2D&#123;private: int m_width; int m_height; int m_channel; unsigned char *m_pixelBuffer;public: Texture2D():m_width(0), m_height(0), m_channel(0), m_pixelBuffer(nullptr)&#123;&#125; ~Texture2D(); bool loadImage(const std::string &amp;path); Vector4D sample(const Vector2D &amp;texcoord) const;&#125;;bool Texture2D::loadImage(const std::string &amp;path)&#123; if(m_pixelBuffer)delete m_pixelBuffer; m_pixelBuffer = nullptr; m_pixelBuffer = stbi_load(path.c_str(), &amp;m_width, &amp;m_height, &amp;m_channel, 0); if(m_pixelBuffer == nullptr) &#123; qDebug() &lt;&lt; \"Failed to load image-&gt;\" &lt;&lt; QString::fromStdString(path); &#125; return m_pixelBuffer != nullptr;&#125;Vector4D Texture2D::sample(const Vector2D &amp;texcoord) const&#123; // just for rgb and rgba format. Vector4D result(0.0,0.0,0.0,1.0); if(m_pixelBuffer == nullptr) return result; unsigned int x = 0, y = 0; // for bilinear interpolation. double factorU = 0, factorV = 0; // calculate the corresponding coordinate. if(texcoord.x &gt;= 0.0f &amp;&amp; texcoord.x &lt;= 1.0f &amp;&amp; texcoord.y &gt;= 0.0f &amp;&amp; texcoord.y &lt;= 1.0f) &#123; double trueU = texcoord.x * (m_width - 1); double trueV = texcoord.y * (m_height - 1); x = static_cast&lt;unsigned int&gt;(trueU); y = static_cast&lt;unsigned int&gt;(trueV); factorU = trueU - x; factorV = trueV - y; &#125; else &#123; // repeating way. float u = texcoord.x,v = texcoord.y; if(texcoord.x &gt; 1.0f) u = texcoord.x - static_cast&lt;int&gt;(texcoord.x); else if(texcoord.x &lt; 0.0f) u = 1.0f - (static_cast&lt;int&gt;(texcoord.x) - texcoord.x); if(texcoord.y &gt; 1.0f) v = texcoord.y - static_cast&lt;int&gt;(texcoord.y); else if(texcoord.y &lt; 0.0f) v = 1.0f - (static_cast&lt;int&gt;(texcoord.y) - texcoord.y); double trueU = u * (m_width - 1); double trueV = v * (m_height - 1); x = static_cast&lt;unsigned int&gt;(trueU); y = static_cast&lt;unsigned int&gt;(trueV); factorU = trueU - x; factorV = trueV - y; &#125; // texel fetching. Vector3D texels[4]; int index[4]; index[0] = (x * m_width + y) * m_channel; index[1] = (x * m_width + y + 1) * m_channel; index[2] = ((x + 1) * m_width + y + 1) * m_channel; index[3] = ((x + 1) * m_width + y) * m_channel; // left bottom texels[0].x = static_cast&lt;float&gt;(m_pixelBuffer[index[0] + 0]) * INV_SCALE; texels[0].y = static_cast&lt;float&gt;(m_pixelBuffer[index[0] + 1]) * INV_SCALE; texels[0].z = static_cast&lt;float&gt;(m_pixelBuffer[index[0] + 2]) * INV_SCALE; //return texels[0]; // left top texels[1].x = static_cast&lt;float&gt;(m_pixelBuffer[index[1] + 0]) * INV_SCALE; texels[1].y = static_cast&lt;float&gt;(m_pixelBuffer[index[1] + 1]) * INV_SCALE; texels[1].z = static_cast&lt;float&gt;(m_pixelBuffer[index[1] + 2]) * INV_SCALE; // right top texels[2].x = static_cast&lt;float&gt;(m_pixelBuffer[index[2] + 0]) * INV_SCALE; texels[2].y = static_cast&lt;float&gt;(m_pixelBuffer[index[2] + 1]) * INV_SCALE; texels[2].z = static_cast&lt;float&gt;(m_pixelBuffer[index[2] + 2]) * INV_SCALE; // right bottom texels[3].x = static_cast&lt;float&gt;(m_pixelBuffer[index[3] + 0]) * INV_SCALE; texels[3].y = static_cast&lt;float&gt;(m_pixelBuffer[index[3] + 1]) * INV_SCALE; texels[3].z = static_cast&lt;float&gt;(m_pixelBuffer[index[3] + 2]) * INV_SCALE; // bilinear interpolation. // horizational texels[0] = texels[0] * (1.0 - factorU) + texels[3] * factorU; texels[1] = texels[1] * (1.0 - factorU) + texels[2] * factorU; //vertical result = texels[0] * (1.0 - factorV) + texels[1] *factorV; return result;&#125; &emsp;&emsp;加载图片我的用的stb_image，一个简单使用的头文件，因为加载图片不是我们的重点，所以就不造这方面的轮子了。 三、程序结果&emsp;&emsp;目前的帧率还不错hhh。 参考资料$[1]$ https://learnopengl.com/Advanced-OpenGL/Depth-testing $[2]$ https://www.cnblogs.com/pbblog/p/3484193.html $[3]$ https://learnopengl.com/Getting-started/Coordinate-Systems $[4]$ http://www.songho.ca/opengl/gl_projectionmatrix.html $[5]$ https://blog.csdn.net/popy007/article/details/5570803 $[6]$ https://learnopengl-cn.github.io/01%20Getting%20started/06%20Textures/","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/about/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/about/tags/Soft-Renderer/"},{"name":"3D pipeline","slug":"3D-pipeline","permalink":"https://yangwc.com/about/tags/3D-pipeline/"}]},{"title":"软渲染器Soft Renderer：光栅化篇","slug":"SoftRenderer-Rasterization","date":"2019-05-01T02:37:54.047Z","updated":"2019-05-23T12:43:31.730Z","comments":true,"path":"2019/05/01/SoftRenderer-Rasterization/","link":"","permalink":"https://yangwc.com/about/2019/05/01/SoftRenderer-Rasterization/","excerpt":"本章开始构建基于Qt平台软渲染器的初步框架，当然Qt相关的内容并不是软渲染器的重点，我只是借助Qt平台将渲染出来的像素矩阵用Qt的控件显示出来。光栅化是当今图形学渲染的一种方式，与之对应的是光线追踪渲染方式，本章我根据自己的理解着重讲述线框光栅化的Bresenham画线算法以及三角形填充光栅化的Edge-Walking算法。注意：初学者慎入。本篇相关的完整代码请看这里。","text":"本章开始构建基于Qt平台软渲染器的初步框架，当然Qt相关的内容并不是软渲染器的重点，我只是借助Qt平台将渲染出来的像素矩阵用Qt的控件显示出来。光栅化是当今图形学渲染的一种方式，与之对应的是光线追踪渲染方式，本章我根据自己的理解着重讲述线框光栅化的Bresenham画线算法以及三角形填充光栅化的Edge-Walking算法。注意：初学者慎入。本篇相关的完整代码请看这里。 渲染管线框架 光栅化算法 一、渲染管线框架&emsp;&emsp;渲染管线的搭建主要包含像素显示、网格数据封装、渲染循环、帧率fps计算、帧缓冲、着色器、渲染逻辑、光栅化等等，其中光栅化作为重点对象抽出来放在后面。当然我们不会一下子就完成渲染管线的基本功能，我们现在是要搭建一个框架，大部分的内容不用写入或者仅仅是做简单的处理，这样后面完善软渲染器的时候只需在相应的位置填写相应的代码逻辑即可。本章目标就是搭建一个渲染管线，用光栅化算法画三角形。当然，如果仅仅是画一个三角形，当然不用这么麻烦，但是我的目标是实现三维的软渲染器，深入理解三维渲染的整个流程，得从基础一步一步慢慢来。 1、像素显示的画布&emsp;&emsp;渲染器最终渲染出来的是一个像素矩阵，我们要把这个像素矩阵显示出来。显示的方法有很多，因人而异，这里我采用自己最熟悉的$Qt$来实现。显示的窗口继承一个普通的$QWidget$父类，然后我们通过重写它的$paintEvent$函数，将渲染出来的像素画到$QWidget$上。但是采用$QPainter$直接画上去的方式效率非常低，我通过查询资料得知，若想要快速地绘制给定的像素矩阵，可以利用$QImage$来实现。话不多说，上代码： 123456789101112131415class Window : public QWidget&#123; Q_OBJECTpublic: explicit Window(QWidget *parent = nullptr); ~Window();private: void paintEvent(QPaintEvent *) override;private: Ui::Window *ui; QImage *canvas;&#125;; &emsp;&emsp;接收到一帧的像素之后，在重绘事件里面利用$QImage$绘制给定的像素数组（记得调用$update$触发重绘事件）。由于篇幅原因，我不会讲太多细节方面的东西，代码也不会全部放出来，那样没意义。想看完整源代码的朋友直接去本人的github上看。 12345678910111213141516void Window::receiveFrame(unsigned char *image)&#123; if(canvas) delete canvas; canvas = new QImage(image, width(), height(), QImage::Format_RGBA8888); update();&#125;void Window::paintEvent(QPaintEvent *event)&#123; if(canvas) &#123; QPainter painter(this); painter.drawImage(0, 0, *canvas); &#125; QWidget::paintEvent(event);&#125; 2、帧缓冲类&emsp;&emsp;帧缓冲通常包含基本的颜色缓冲附件、深度缓冲附件等，这里我们暂且只实现颜色缓冲附件（四通道，格式为$RGBA$，各占一个字节），深度缓冲附件后面再加上。渲染管线最终的渲染结果是写入帧缓冲的，我们采用一个一维的单字节数组作为帧缓冲的颜色缓冲。帧缓冲的最基本的功能就是清楚缓冲区、写入像素： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162class FrameBuffer&#123;private: int m_width, m_height, m_channel; std::vector&lt;unsigned char&gt; m_colorBuffer;public: FrameBuffer(int width, int height); ~FrameBuffer() = default; int getWidth()&#123;return m_width;&#125; int getHeight()&#123;return m_height;&#125; unsigned char *getColorBuffer() &#123;return m_colorBuffer.data();&#125; void clearColorBuffer(const Vector4D &amp;color); void drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color);&#125;;FrameBuffer::FrameBuffer(int width, int height) :m_channel(4), m_width(width), m_height(height)&#123; m_colorBuffer.resize(m_width*m_height*m_channel, 255);&#125;void FrameBuffer::clearColorBuffer(const Vector4D &amp;color)&#123; // fill the color buffer. unsigned char red = static_cast&lt;unsigned char&gt;(255*color.x); unsigned char green = static_cast&lt;unsigned char&gt;(255*color.y); unsigned char blue = static_cast&lt;unsigned char&gt;(255*color.z); unsigned char alpha = static_cast&lt;unsigned char&gt;(255*color.w); for(int row = 0;row &lt; m_height;++ row) &#123; for(int col = 0;col &lt; m_width;++ col) &#123; m_colorBuffer[row*m_width*m_channel+col*m_channel + 0] = red; m_colorBuffer[row*m_width*m_channel+col*m_channel + 1] = green; m_colorBuffer[row*m_width*m_channel+col*m_channel + 2] = blue; m_colorBuffer[row*m_width*m_channel+col*m_channel + 3] = alpha; &#125; &#125;&#125;void FrameBuffer::drawPixel(unsigned int x, unsigned int y, const Vector4D &amp;color)&#123; if(x &lt; 0 || x &gt;= m_width || y &lt; 0 || y &gt;= m_height) return; unsigned char red = static_cast&lt;unsigned char&gt;(255*color.x); unsigned char green = static_cast&lt;unsigned char&gt;(255*color.y); unsigned char blue = static_cast&lt;unsigned char&gt;(255*color.z); unsigned char alpha = static_cast&lt;unsigned char&gt;(255*color.w); unsigned int index = y*m_width*m_channel + x*m_channel; m_colorBuffer[index + 0] = red; m_colorBuffer[index + 1] = green; m_colorBuffer[index + 2] = blue; m_colorBuffer[index + 3] = alpha;&#125; 3、网格顶点数据&emsp;&emsp;三维的渲染程序中的顶点数据通常包含顶点位置、顶点颜色、纹理坐标、顶点法线，然后在此基础上利用一组给定顺序的顶点数据表示一个网格，渲染时网格的数据将被送入管线进行处理。为此，有必要对顶点数据做一定的封装。 1234567891011121314class Vertex&#123;public: Vector4D position; Vector4D color; Vector2D texcoord; Vector3D normal; Vertex() = default; Vertex(Vector4D _pos, Vector4D _color, Vector2D _tex, Vector3D _normal) :position(_pos),color(_color),texcoord(_tex),normal(_normal) &#123;&#125; Vertex(const Vertex &amp;rhs) :position(rhs.position),color(rhs.color),texcoord(rhs.texcoord),normal(rhs.normal)&#123;&#125;&#125;; &emsp;&emsp;顶点数据经过顶点着色器的处理之后，会被送到下一个渲染管线的阶段处理。顶点着色器的顶点数据输出与输入有些差异，为此我们也定义一个类表示为顶点着色器的输出，这对于构建渲染管线尤为重要。 12345678910111213141516171819class VertexOut&#123;public: Vector4D posTrans; //世界变换后的坐标 Vector4D posH; //投影变换后的坐标 Vector2D texcoord; //纹理坐标 Vector3D normal; //法线 Vector4D color; //颜色 double oneDivZ; //1/z用于深度测试 VertexOut() = default; VertexOut(Vector4D _posT, Vector4D _posH, Vector2D _tex, Vector3D _normal, Vector4D _color, double _oneDivZ) :posTrans(_posT),posH(_posH),texcoord(_tex), normal(_normal),color(_color),oneDivZ(_oneDivZ) &#123;&#125; VertexOut(const VertexOut&amp; rhs) :posTrans(rhs.posTrans), posH(rhs.posH), texcoord(rhs.texcoord), normal(rhs.normal), color(rhs.color), oneDivZ(rhs.oneDivZ) &#123;&#125;&#125;; &emsp;&emsp;然后就是关于网格的表示，为了节省空间（特别是对于很大的模型），我们直接采用索引来组织网格。若想详细了解OpenGL的顶点索引概念请看这里。一个网格有两个数组，分别是$Vertex$数组和$Index$数组。下面的代码中，有一个$asTriangle$方法，这是一个三角形网格，调用这个方法之后网格存储的就是一个三角形，用于后面的光栅化调试，光栅化的基本单元就是三角形。通常情况，所有的网格模型都可以用一定数量的三角形构成，因而我们实现的软渲染器的基本图元就是三角形。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class Mesh&#123;public: std::vector&lt;Vertex&gt; vertices; std::vector&lt;unsigned int&gt; indices; Mesh() = default; ~Mesh() = default; Mesh(const Mesh&amp; mesh) :vertices(mesh.vertices), indices(mesh.indices)&#123;&#125; Mesh&amp; operator=(const Mesh&amp; mesh) &#123; if (&amp;mesh == this) return *this; vertices = mesh.vertices; indices = mesh.indices; return *this; &#125; void setVertices(Vertex* _vs, int count) &#123; vertices.resize(count); new(&amp;vertices[0])std::vector&lt;Vertex&gt;(_vs, _vs + count); &#125; void setIndices(int* _es, int count) &#123; indices.resize(count); new(&amp;indices)std::vector&lt;unsigned int&gt;(_es, _es + count); &#125; void asBox(double width, double height, double depth); void asTriangle(const Vector3D p1, const Vector3D p2, const Vector3D p3);&#125;;void Mesh::asTriangle(Vector3D p1, Vector3D p2, Vector3D p3)&#123; vertices.resize(3); indices.resize(3); vertices[0].position = p1; vertices[0].normal = Vector3D(0.f, 0.f, 1.f); vertices[0].color = Vector4D(1.f, 0.f, 0.f, 1.f); vertices[0].texcoord = Vector2D(0.f, 0.f); vertices[1].position = p2; vertices[1].normal = Vector3D(0.f, 0.f, 1.f); vertices[1].color = Vector4D(0.f, 1.f, 0.f, 1.f); vertices[1].texcoord = Vector2D(1.f, 0.f); vertices[2].position = p3; vertices[2].normal = Vector3D(0.f, 0.f, 1.f); vertices[2].color = Vector4D(0.f, 0.f, 1.f, 1.f); vertices[2].texcoord = Vector2D(0.5f, 1.f); indices[0] = 0; indices[1] = 1; indices[2] = 2;&#125; 4、简单的着色器&emsp;&emsp;着色器方面时软渲染中较为高级的内容，目前我们只是搭建一个框架，因而着色器不需要什么复杂的操作，只需简单地传递数据就行了。博主实现的软渲染器只包含必不可少的顶点着色器和片元着色器，目前的顶点着色器将顶点原封不动地输出，片元着色器也是如此，这样我们后面要实现光照效果的时候直接在着色器里写上就行了。为了更加有条理，我们设计一个着色器的虚类，这样实现不同效果的着色器时我们直接继承这个虚类即可。 123456789101112class BaseShader&#123;public: BaseShader() = default; virtual ~BaseShader() = default; virtual VertexOut vertexShader(const Vertex &amp;in) = 0; virtual Vector4D fragmentShader(const VertexOut &amp;in) = 0; virtual void setModelMatrix(const Matrix4x4 &amp;world) = 0; virtual void setViewMatrix(const Matrix4x4 &amp;view) = 0; virtual void setProjectMatrix(const Matrix4x4 &amp;project) = 0;&#125;; 12345678910111213141516171819202122232425262728293031323334353637383940414243class SimpleShader : public BaseShader&#123;public: SimpleShader() = default; virtual ~SimpleShader() = default; virtual VertexOut vertexShader(const Vertex &amp;in); virtual Vector4D fragmentShader(const VertexOut &amp;in); virtual void setModelMatrix(const Matrix4x4 &amp;world); virtual void setViewMatrix(const Matrix4x4 &amp;view); virtual void setProjectMatrix(const Matrix4x4 &amp;project);&#125;;VertexOut SimpleShader::vertexShader(const Vertex &amp;in)&#123; VertexOut result; result.posTrans = in.position; result.posH = in.position; result.color = in.color; result.normal = in.normal; result.oneDivZ = 1.0; result.texcoord = in.texcoord; return result;&#125;Vector4D SimpleShader::fragmentShader(const VertexOut &amp;in)&#123; Vector4D litColor; litColor = in.color; return litColor;&#125;void SimpleShader::setModelMatrix(const Matrix4x4 &amp;world)&#123;&#125;void SimpleShader::setViewMatrix(const Matrix4x4 &amp;view)&#123;&#125;void SimpleShader::setProjectMatrix(const Matrix4x4 &amp;project)&#123;&#125; &emsp;&emsp;可以看到$SimpleShader$仅仅是将顶点数据直接输出，不进行任何处理。 5、搭建基本的渲染管线&emsp;&emsp;目前我们已经有了一些渲染管线的基本组件，现在就需要把这些组件串起来。首先是渲染循环的问题，$Qt$有它自己的事件循环，而且主线程的事件循环要尽量避免大量的运算（否则UI控件会陷入未响应），因此将渲染循环放到子线程里是一个不错的渲染，这样也可以避免我们的软渲染逻辑与$Qt$的接口耦合得太高。 渲染线程&emsp;&emsp;$Qt$提供了$QThread$类构建线程，我采用的方式为：渲染循环类继承$QObject$，然后调用$moveToThread$番方法挂到子线程上运行，最后将线程的启动信号与$loop$渲染循环关联即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class RenderLoop : public QObject&#123; Q_OBJECTpublic: explicit RenderLoop(int w, int h, QObject *parent = nullptr); ~RenderLoop(); void stopIt() &#123;stoped = true;&#125; void setFpsZero()&#123;fps = 0;&#125; int getFps()&#123;return fps;&#125;signals: void frameOut(unsigned char *image);public slots: void loop();private: bool stoped; int fps; int width, height, channel;&#125;;RenderLoop::RenderLoop(int w, int h, QObject *parent) : QObject(parent), width(w), height(h), channel(4)&#123; fps = 0; stoped = false;&#125;RenderLoop::~RenderLoop()&#123;&#125;void RenderLoop::loop()&#123; // pipeline initialization ...... // fps counting. fps = 0; while(!stoped) &#123; // render logic ...... ++ fps; &#125;&#125; &emsp;&emsp;然后在主窗口中创建$RenderLoop$对象，挂到$QThread$上启动。此外还有一点要注意的是在子线程中最好不用使用$QTimer$类，因此我在主窗口中创建$QTimer$类，设定为每秒触发，触发时主线程读取子线程的$fps$，这样就达到了显示帧率的目的了。 12345678910111213141516171819202122232425262728293031在Window类声明处：private: QTimer *timer; QThread *loopThread; RenderLoop *loop;在Window类构造函数处： loop = new RenderLoop(width(), height(), nullptr); loopThread = new QThread(this); // fps counting. timer = new QTimer(); connect(timer,&amp;QTimer::timeout,this,&amp;Window::fpsTimeOut); // render thread. loop-&gt;moveToThread(loopThread); connect(loopThread,&amp;QThread::finished,loop, &amp;RenderLoop::deleteLater); connect(loopThread,&amp;QThread::started,loop,&amp;RenderLoop::loop); connect(loop,&amp;RenderLoop::frameOut,this,&amp;Window::receiveFrame); // begin the thread. loopThread-&gt;start(); timer-&gt;start(1000);Window的其他函数：void Window::fpsTimeOut()&#123; int fps = loop-&gt;getFps(); loop-&gt;setFpsZero(); this-&gt;setWindowTitle(QString(\" fps: %1\").arg(fps));&#125; 渲染流程&emsp;&emsp;回顾一下$OpenGL$的渲染流程（这里只考虑一般的情况，即不包含几何着色器、细分着色器等），首先外部处理网格，将网格顶点数据和网格顶点索引送入渲染管线，设置基本图元（如三角形）、渲染方式（如线框模式）。渲染管线的第一阶段为顶点着色器阶段（在这之前还有个缓冲清理阶段），顶点着色器对网格数据逐顶点处理（包含坐标空间变换、投影变换等等），随之输出。然后渲染管线对输出的顶点数据进行裁剪，送入光栅化部件，计算几何图元覆盖的像素点，其中进行了大量的线性插值操作。接着片元着色器获取光栅化后的像素，对每个像素做颜色计算等，然后输出颜色数据、深度数据，最后根据这些缓冲数据做深度测试。 &emsp;&emsp;所以一个最基本的渲染管线应该有如下几个步骤： &emsp;&emsp;初始化（如缓冲区创建）$\\to$输入顶点缓冲、索引缓冲$\\to$清除缓冲区$\\to$设置着色器、渲染方式$\\to$绘制$\\to$交换双缓冲$\\to$输出。根据这些步骤，创建$Pipeline$类如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192class Pipeline&#123;private: int m_width, m_height; // width and height of viewport. BaseShader *m_shader; // shaders including vertex shader and fragment shader. FrameBuffer *m_frontBuffer; FrameBuffer *m_backBuffer; Matrix4x4 viewPortMatrix; // viewport transformation matrix. std::vector&lt;Vertex&gt; m_vertices; // vertex buffer. std::vector&lt;unsigned int&gt; m_indices;// index buffer.public: Pipeline(int width, int height); ~Pipeline(); void initialize(); void clearBuffer(const Vector4D &amp;color, bool depth = false); void setVertexBuffer(const std::vector&lt;Vertex&gt; &amp;vertices)&#123;m_vertices = vertices;&#125; void setIndexBuffer(const std::vector&lt;unsigned int&gt; &amp;indices)&#123;m_indices = indices;&#125; void setShaderMode(ShadingMode mode); void drawIndex(RenderMode mode); void swapBuffer(); unsigned char *output()&#123;return m_frontBuffer-&gt;getColorBuffer();&#125;&#125;;Pipeline::Pipeline(int width, int height) :m_width(width),m_height(height) ,m_shader(nullptr),m_frontBuffer(nullptr) ,m_backBuffer(nullptr)&#123;&#125;Pipeline::~Pipeline()&#123; if(m_shader)delete m_shader; if(m_frontBuffer)delete m_frontBuffer; if(m_backBuffer)delete m_backBuffer; m_shader = nullptr; m_frontBuffer = nullptr; m_backBuffer = nullptr;&#125;void Pipeline::initialize()&#123; if(m_frontBuffer) delete m_frontBuffer; if(m_backBuffer) delete m_backBuffer; if(m_shader) delete m_shader; viewPortMatrix.setViewPort(0,0,m_width,m_height); m_frontBuffer = new FrameBuffer(m_width, m_height); m_backBuffer = new FrameBuffer(m_width, m_height); m_shader = new SimpleShader();&#125;void Pipeline::drawIndex(RenderMode mode)&#123; 输入顶点着色器; 光栅化; 输入片元着色器; 写入缓冲区;&#125;void Pipeline::clearBuffer(const Vector4D &amp;color, bool depth)&#123; (void)depth; m_backBuffer-&gt;clearColorBuffer(color);&#125;void Pipeline::setShaderMode(ShadingMode mode)&#123; if(m_shader)delete m_shader; if(mode == ShadingMode::simple) m_shader = new SimpleShader(); else if(mode == ShadingMode::phong) ;&#125;void Pipeline::swapBuffer()&#123; FrameBuffer *tmp = m_frontBuffer; m_frontBuffer = m_backBuffer; m_backBuffer = tmp;&#125; &emsp;&emsp;注意到我创建了帧缓冲，分别是$m_frontBuffer$和$m_backBuffer$，前者存储着当前显示的像素，后者缓冲区用于写入像素。这就是著名的双缓冲原理，可以避免画面的闪烁、撕裂等现象。除此之外，还有一个值得特别说明的就是视口变换矩阵$viewPortMatrix$，这个一般很少见到，因为被内嵌在了渲染管线里面了。经过投影变换、透视除法操作之后，顶点数据都在标准化设备空间中，即$x$轴、$y$轴、$z$轴取值范围为$[-1,1]$。但是屏幕的像素坐标范围并非如此，通常屏幕的$x$轴坐标范围为$[0,width]$，$y$轴坐标范围为$[0,height]$，屏幕像素坐标原点在左上角，$x$轴正向朝右，$y$轴正向朝下，所以我们还要把标准化设备坐标顶点数据变换到屏幕的坐标范围中，这就是视口变换（$z$轴一般保持不变）。视口变换矩阵的构造并没有难度，因为这仅仅是简单的线性映射，因此不再赘述。视口变换矩阵如下所示： viewPortMatrix= \\left[ \\begin{matrix} \\frac{w}{2}&0&0&s_x+\\frac{w}{2}\\\\ 0&-\\frac{h}{2}&0&s_y+\\frac{h}{2}\\\\ 0&0&1&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {1}&emsp;&emsp;其中$(s_x,s_y)$是视口左上角的坐标，$(w,h)$为屏幕的宽度和高度。 12345678void Matrix4x4::setViewPort(int left, int top, int width, int height)&#123; loadIdentity(); entries[0] = static_cast&lt;float&gt;(width)/2.0f; entries[5] = -static_cast&lt;float&gt;(height)/2.0f; entries[12] = static_cast&lt;float&gt;(left)+static_cast&lt;float&gt;(width)/2.0f; entries[13] = static_cast&lt;float&gt;(top)+static_cast&lt;float&gt;(height)/2.0f;&#125; &emsp;&emsp;$Pipeline$还有个非常重要的函数$drawIndex$，它是渲染管线的核心部分，涉及到了图元装配、顶点着色器调度、光栅化、片元着色器调度、写入帧缓冲这几个重要的步骤。我们实现的软渲染器几何图元默认为三角形，所以图元装配就是每三个顶点装成一个图元。 123456789101112131415161718192021222324252627282930313233343536373839void Pipeline::drawIndex(RenderMode mode)&#123; if(m_indices.empty())return; for(unsigned int i = 0;i &lt; m_indices.size()/3;++ i) &#123; //! vertices assembly to triangle primitive Vertex p1,p2,p3; &#123; p1 = m_vertices[3*i+0]; p2 = m_vertices[3*i+1]; p3 = m_vertices[3*i+2]; &#125; //! vertex shader stage. VertexOut v1,v2,v3; &#123; v1 = m_shader-&gt;vertexShader(p1); v2 = m_shader-&gt;vertexShader(p2); v3 = m_shader-&gt;vertexShader(p3); &#125; //! rasterization and fragment shader stage. &#123; v1.posH = viewPortMatrix * v1.posH; v2.posH = viewPortMatrix * v2.posH; v3.posH = viewPortMatrix * v3.posH; if(mode == RenderMode::wire) &#123; // bresenham rasterization &#125; else if(mode == RenderMode::fill) &#123; // edge walking rasterization &#125; &#125; &#125;&#125; &emsp;&emsp;有了以上的$Pipeline$函数，我们的渲染循环逻辑的一般形式如下： 1234567891011while(!stoped)&#123; pipeline-&gt;clearBuffer(Vector4D(0.502f,0.698f,0.800f,1.0f)); pipeline-&gt;drawIndex(RenderMode::fill); pipeline-&gt;swapBuffer(); emit frameOut(pipeline-&gt;output()); ++ fps;&#125; 二、光栅化算法&emsp;&emsp;顶点着色器处理的还是一个个离散的几何顶点，在顶点着色器之后我们还需要进行光栅化操作，将几何覆盖的屏幕像素计算出来，送入片元着色器计算每个点的像素数据。光栅化一般有两种模式：一种是线框模式，即只描绘几何的边；二是填充模式，即将几何的面片全部填充完。Bresenham算法是经典的描线算法，它采用迭代的形式将所需的算术操作降低到最少。除此之外还有DDA描线算法，效率上不如Bresenham算法，所以我没有实现。 1、Bresenham描线算法&emsp;&emsp;我们要描绘的是从$(x_0,y_0)$到$(x_1,y_1)$的一条直线线段。一些数学符号标记如下： \\Delta x= x_1-x_0>0,\\ \\Delta y=y_1-y_0>0,\\ m=\\frac{\\Delta y}{\\Delta x}&emsp;&emsp;其中$m$即直线线段的斜率，为了便于讨论，我们假设$|m|\\leq 1$，其他情况很容易推广。 &emsp;&emsp;在如上的情况下，Bresenham算法从$x=x_0$开始，每次将$x$坐标值加一，然后推算相应的$y$坐标值。记第$i$次迭代获得的点为$(x_i,y_i)$。那么第$i+1$次迭代时获取的点就在$(\\overline x_i+1,\\overline y_i)$和$(\\overline x_i+1,\\overline y_i+1)$这两个中选取。那如何判断应该选哪个呢？即选择这两个点之一的判断标准是什么？直观上，我们应该选取距离的直线线段在该$y$轴上的交点最近的点，如下图1所示。 图1 判别标准 &emsp;&emsp;直线的一般表达式为$y=mx+B$，$m$为直线的斜率，那么$(x_{i+1},y_{i+1})$表示为如下（注意$y_{i+1}$表示的是直线在$x_{i+1}$上真正的$y$值）： x_{i+1}=x_i+1\\\\ y_{i+1}=mx_{i+1}+B=m(x_i+1)+B \\tag {2} 图2 交点到右边的点、右上的点的距离 &emsp;&emsp;故$d_{upper}$和$d_{lower}$的取值如下： d_{upper}=\\overline y_i+1-\\overline y_{i+1}=\\overline y_i+1-m\\overline x_{i+1}-B\\\\ d_{lower}=y_{i+1}-\\overline y_i=mx_{i+1}+B-\\overline y_i \\tag {3}&emsp;&emsp;显然，如果$d_{lower}-d_{upper}&gt;0$，则应该取右上方的点；如果$d_{lower}-d_{upper}0$的符号。 d_{lower}-d_{upper}=m(x_i+1)+B-\\overline y_i-(\\overline y_i+1-m(x_i+1)-B)\\\\ =2m(x_i+1)-2\\overline y_i+2B-1 \\tag {4}&emsp;&emsp;式$(4)$中的$m$是直线的斜率，因此将式$(4)$作为判断标准需要做非常昂贵的浮点数除法运算。为了消去除法，注意到$m=\\frac{\\Delta y}{\\Delta x}$，两边同时乘上$\\Delta x&gt;0$，正负符号不变。 p_i=\\Delta x\\cdot (d_{lower}-d_{upper}) =2\\Delta y\\cdot(x_i+1)-2\\Delta x\\cdot \\overline y_i+(2B-1)\\Delta x\\\\ =2\\Delta y\\cdot x_i-2\\Delta x\\cdot\\overline y_i+c\\\\ where \\ \\ c=(2B-1)\\Delta x+2\\Delta y \\tag {5}&emsp;&emsp;所以可以用$p_i$的符号作为选取的标准。但是，式$(5)$的计算能够进一步简化，考虑$p_i$和$p_{i+1}$（注意我们根据$p_i$的符号来选取$\\overline y_{i+1}$）： p_{i+1}-p_{i} = (2\\Delta y\\cdot x_{i+1}-2\\Delta x\\cdot\\overline y_{i+1}+c) - (2\\Delta y\\cdot x_i-2\\Delta x\\cdot\\overline y_i+c) \\\\= 2\\Delta y-2\\Delta x(\\overline y_{i+1}-\\overline y_i) \\tag {6}&emsp;&emsp;若$p_i\\leq 0$，那么选择右边的点，此时$\\overline y_{i+1}=\\overline y_i$，那么有： p_{i+1}=p_i+2\\Delta y \\tag {7}&emsp;&emsp;若$p_i&gt;0$，那么选择右上角的点，此时$\\overline y_{i+1}=\\overline y_i+1$，那么有： p_{i+1}=p_i+2\\Delta y-2\\Delta x \\tag {8}&emsp;&emsp;所以我们可以根据$p_i$的符号快速计算出$p_{i+1}$的符号，如此迭代下去： Bresenham Algorithm: $draw (x_0, y_0);$ Calculate $\\Delta x$,$\\Delta y$,$2\\Delta y$,$2\\Delta y-2\\Delta x$,$p_0=2\\Delta y-\\Delta x$; for $x$ from $x_0$ to $x_1$: &emsp;&emsp;if $p_i\\leq 0$ &emsp;&emsp;&emsp;&emsp;draw $(x_{i+1},\\overline y_{i+1})=(x_i+1,\\overline y_i)$ ; &emsp;&emsp;&emsp;&emsp;compute $p_{i+1}=p_i+2\\Delta y$; &emsp;&emsp;if $p_i &gt; 0$ &emsp;&emsp;&emsp;&emsp;draw $(x_{i+1},\\overline y_{i+1})=(x_i+1,\\overline y_i+1)$ ; &emsp;&emsp;&emsp;&emsp;compute $p_{i+1}=p_i+2\\Delta y-2\\Delta x$; &emsp;&emsp;$x += 1;$ &emsp;&emsp;上面我们讨论的都是$|m|1$的情况呢？其实这是对称的，这时把$x$看成$y$，把$y$看成$x$即可。另外，当$\\Delta x &lt;0$时，我们的$x$不是递增$1$，而是递减$1$，具体实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465void Pipeline::bresenhamLineRasterization(const VertexOut &amp;from, const VertexOut &amp;to)&#123; int dx = to.posH.x - from.posH.x; int dy = to.posH.y - from.posH.y; int stepX = 1, stepY = 1; // judge the sign if(dx &lt; 0) &#123; stepX = -1; dx = -dx; &#125; if(dy &lt; 0) &#123; stepY = -1; dy = -dy; &#125; int d2x = 2*dx, d2y = 2*dy; int d2y_minus_d2x = d2y - d2x; int sx = from.posH.x; int sy = from.posH.y; VertexOut tmp; // slope &lt; 1. if(dy &lt;= dx) &#123; int flag = d2y - dx; for(int i = 0;i &lt;= dx;++ i) &#123; // linear interpolation tmp = lerp(from, to, static_cast&lt;double&gt;(i)/dx); // fragment shader m_backBuffer-&gt;drawPixel(sx,sy,m_shader-&gt;fragmentShader(tmp)); sx += stepX; if(flag &lt;= 0) flag += d2y; else &#123; sy += stepY; flag += d2y_minus_d2x; &#125; &#125; &#125; // slope &gt; 1. else &#123; int flag = d2x - dy; for(int i = 0;i &lt;= dy;++ i) &#123; // linear interpolation tmp = lerp(from, to, static_cast&lt;double&gt;(i)/dy); // fragment shader m_backBuffer-&gt;drawPixel(sx,sy,m_shader-&gt;fragmentShader(tmp)); sy += stepY; if(flag &lt;= 0) flag += d2x; else &#123; sx += stepX; flag -= d2y_minus_d2x; &#125; &#125; &#125;&#125; 2、Edge-Walking三角形填充算法&emsp;&emsp;三角形光栅化填充对输入给定的三个三角形顶点，计算这个三角区域覆盖的所有像素。三角形填充的光栅化算法有很多种，这里仅实现了Edge-Walking算法，此外还有Edge-Equation算法。关于Edge-Walking算法的前世今生我不再赘述了，这个算法的思路比较简单，但是实现起来比较麻烦一点。 &emsp;&emsp;话不多少，直接上伪代码（懒得自己写了伪代码了）： &emsp;&emsp;大致的思想就是从上往下（或从下往上）扫描，获取每对$X_L$、$X_R$，然后在$[X_L,X_R]$范围内从左到右扫描。显然就是双重循环。一般，我们的三角形光栅化对象有如下四种情况： 图3 四类三角形 &emsp;&emsp;先来看平底三角形的情况，如下图4所示。显然，平底三角形很容易地实现从下往上扫面，竖直方向上仅需考虑左右两条边。当然这里有个问题，就是如何确定$X_L$和$X_R$？如果直接采用算法伪代码中的利用$dx/dy$迭代获取$X$值，因为$X$值是整数，而$dx/dy$是浮点数，当$dx/dy&lt;1$时，把$dx/dy$加到$X$上面计算机对整数类型坐标自动向下取整，结果相当于没加。（即便是浮点数类型，最终也要取整，因为屏幕空间的像素坐标必须是整数） 图4 平底三角形 &emsp;&emsp;一种解决方案就是线性插值，算法从下往上扫描时，$y-=1$，我们根据当前的$y$值来获取$x$值： X_L = (1.0f-\\frac{y1-y}{y1-y0})*x1+\\frac{y1-y}{y1-y0}*x0 \\\\ X_y = (1.0f-\\frac{y2-y}{y2-y0})*x2+\\frac{y2-y}{y2-y0}*x0&emsp;&emsp;平顶的三角形光栅化亦类似，不再赘述。那么除了平底和平顶的情况之外，我们该如何处理其余的情况？一个技巧就是将其他情况的三角形分割乘一个平底三角形、一个平顶三角形，如下图所示： 图5 三角形分割 &emsp;&emsp;这样我们通过调用平底三角形光栅化方法、平顶三角形光栅化方法即可实现一般情况的三角形光栅化： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114void Pipeline::scanLinePerRow(const VertexOut &amp;left, const VertexOut &amp;right)&#123; VertexOut current; int length = right.posH.x - left.posH.x + 1; for(int i = 0;i &lt;= length;++i) &#123; // linear interpolation double weight = static_cast&lt;double&gt;(i)/length; current = lerp(left, right, weight); current.posH.x = left.posH.x + i; current.posH.y = left.posH.y; // fragment shader m_backBuffer-&gt;drawPixel(current.posH.x, current.posH.y, m_shader-&gt;fragmentShader(current)); &#125;&#125;void Pipeline::rasterTopTriangle(VertexOut &amp;v1, VertexOut &amp;v2, VertexOut &amp;v3)&#123; VertexOut left = v2; VertexOut right = v3; VertexOut dest = v1; VertexOut tmp, newleft, newright; if(left.posH.x &gt; right.posH.x) &#123; tmp = left; left = right; right = tmp; &#125; int dy = left.posH.y - dest.posH.y + 1; for(int i = 0;i &lt; dy;++i) &#123; double weight = 0; if(dy != 0) weight = static_cast&lt;double&gt;(i)/dy; newleft = lerp(left, dest, weight); newright = lerp(right, dest, weight); newleft.posH.y = newright.posH.y = left.posH.y - i; scanLinePerRow(newleft, newright); &#125;&#125;void Pipeline::rasterBottomTriangle(VertexOut &amp;v1, VertexOut &amp;v2, VertexOut &amp;v3)&#123; VertexOut left = v1; VertexOut right = v2; VertexOut dest = v3; VertexOut tmp, newleft, newright; if(left.posH.x &gt; right.posH.x) &#123; tmp = left; left = right; right = tmp; &#125; int dy = dest.posH.y - left.posH.y + 1; for(int i = 0;i &lt; dy;++i) &#123; double weight = 0; if(dy != 0) weight = static_cast&lt;double&gt;(i)/dy; newleft = lerp(left, dest, weight); newright = lerp(right, dest, weight); newleft.posH.y = newright.posH.y = left.posH.y + i; scanLinePerRow(newleft, newright); &#125;&#125;void Pipeline::edgeWalkingFillRasterization(const VertexOut &amp;v1, const VertexOut &amp;v2, const VertexOut &amp;v3)&#123; // split the triangle into two part VertexOut tmp; VertexOut target[3] = &#123;v1, v2,v3&#125;; if(target[0].posH.y &gt; target[1].posH.y) &#123; tmp = target[0]; target[0] = target[1]; target[1] = tmp; &#125; if(target[0].posH.y &gt; target[2].posH.y) &#123; tmp = target[0]; target[0] = target[2]; target[2] = tmp; &#125; if(target[1].posH.y &gt; target[2].posH.y) &#123; tmp = target[1]; target[1] = target[2]; target[2] = tmp; &#125; // bottom triangle if(equal(target[0].posH.y,target[1].posH.y)) &#123; rasterBottomTriangle(target[0],target[1],target[2]); &#125; // top triangle else if(equal(target[1].posH.y,target[2].posH.y)) &#123; rasterTopTriangle(target[0], target[1], target[2]); &#125; // split it. else &#123; double weight = static_cast&lt;double&gt;(target[1].posH.y-target[0].posH.y)/(target[2].posH.y-target[0].posH.y); VertexOut newPoint = lerp(target[0],target[2],weight); newPoint.posH.y = target[1].posH.y; rasterTopTriangle(target[0], newPoint, target[1]); rasterBottomTriangle(newPoint,target[1],target[2]); &#125;&#125; 三、程序结果&emsp;&emsp;最终，不借用任何图形接口通过自己实现的光栅化算法画出了三角形： 参考资料$[1]$ https://blog.csdn.net/cppyin/article/details/6232453 $[2]$ https://blog.csdn.net/y1196645376/article/details/78937614 $[3]$ https://blog.csdn.net/y1196645376/article/details/78907914","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/about/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/about/tags/Soft-Renderer/"},{"name":"Rasterization","slug":"Rasterization","permalink":"https://yangwc.com/about/tags/Rasterization/"}]},{"title":"软渲染器Soft Renderer：3D数学篇","slug":"SoftRenderer-Math","date":"2019-05-01T02:37:49.425Z","updated":"2019-07-02T03:54:51.336Z","comments":true,"path":"2019/05/01/SoftRenderer-Math/","link":"","permalink":"https://yangwc.com/about/2019/05/01/SoftRenderer-Math/","excerpt":"本章开始博主将手动搭建一个渲染管线，深入理解3D渲染的整个流程。线性代数中的向量和矩阵是计算机图形学的常客，深入理解和掌握对于图形渲染有着非常重要的意义，本节主要是关于3D数学库的内容。","text":"本章开始博主将手动搭建一个渲染管线，深入理解3D渲染的整个流程。线性代数中的向量和矩阵是计算机图形学的常客，深入理解和掌握对于图形渲染有着非常重要的意义，本节主要是关于3D数学库的内容。 向量 矩阵 一、向量&emsp;&emsp;$n$维向量本质就是一个$n$元组，从几何意义上来说，向量是有大小和方向的有向线段。向量的大小就是向量的长度（模）向量有非负的长度，而向量的方向描述了空间中向量的指向。向量的相关内容高中就已涉及，因此不再赘述。若想要重新深入了解相关内容，可以查看这个地址。 &emsp;&emsp;图形渲染中通常使用的向量为$2$到$4$维，如下分别是$2$维、$3$维、$4$维向量类的常用方法，主要是运算操作符重载以及点乘、叉乘、模、标准化、线性插值等基本操作。向量的内容简单，没什么要特别说明的。 1、2D向量类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Vector2D&#123;public: float x,y; // constructors Vector2D():x(0.0f), y(0.0f) &#123;&#125; Vector2D(float newX, float newY):x(newX), y(newY)&#123;&#125; Vector2D(const float * rhs):x(*rhs), y((*rhs)+1) &#123;&#125; Vector2D(const Vector2D &amp; rhs):x(rhs.x), y(rhs.y)&#123;&#125; ~Vector2D() = default; // setter,getter void set(float newX, float newY)&#123;x=newX;y=newY; &#125; void setX(float newX) &#123;x = newX;&#125; void setY(float newY) &#123;y = newY;&#125; float getX() const &#123;return x;&#125; float getY() const &#123;return y;&#125; // normalization void normalize(); Vector2D getNormalize()const; // length float getLength() const &#123; return static_cast&lt;float&gt;(sqrt(x*x + y*y));&#125; float getSquaredLength()const&#123;return static_cast&lt;float&gt;(x*x + y*y);&#125; // overloaded operators Vector2D operator+(const Vector2D &amp;rhs) const &#123;return Vector2D(x + rhs.x, y + rhs.y);&#125; Vector2D operator-(const Vector2D &amp;rhs) const &#123;return Vector2D(x - rhs.x, y - rhs.y);&#125; Vector2D operator*(const float rhs) const &#123;return Vector2D(x*rhs, y*rhs);&#125; Vector2D operator/(const float rhs) const &#123;return (rhs==0) ? Vector2D(0.0f, 0.0f) : Vector2D(x / rhs, y / rhs);&#125; bool operator==(const Vector2D &amp;rhs) const &#123;return (equal(x,rhs.x) &amp;&amp; equal(y,rhs.y));&#125; bool operator!=(const Vector2D &amp;rhs) const &#123;return !((*this)==rhs);&#125; void operator+=(const Vector2D &amp;rhs)&#123;x+=rhs.x; y+=rhs.y;&#125; void operator-=(const Vector2D &amp;rhs)&#123;x-=rhs.x; y-=rhs.y;&#125; void operator*=(const float rhs)&#123;x*=rhs;y*=rhs;&#125; void operator/=(const float rhs)&#123;if(!equal(rhs, 0.0))&#123;x/=rhs;y/=rhs;&#125;&#125; Vector2D operator-() const &#123;return Vector2D(-x, -y);&#125; Vector2D operator+() const &#123;return *this;&#125; // interpolation Vector2D lerp(const Vector2D &amp;v2,const float factor)const &#123;return (*this)*(1.0f - factor) + v2*factor;&#125; Vector2D quadraticInterpolate(const Vector2D &amp; v2, const Vector2D &amp; v3, const float factor) const &#123;return (*this)*(1.0f-factor)*(1.0f-factor) + v2*2.0f*factor*(1.0f-factor) + v3*factor*factor;&#125;&#125;; 2、3D向量类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Vector3D&#123;public: float x,y,z; // constructors Vector3D():x(0.0f), y(0.0f), z(0.0f)&#123;&#125; Vector3D(float newX, float newY, float newZ):x(newX), y(newY), z(newZ)&#123;&#125; Vector3D(const float * rhs):x(*rhs), y(*(rhs+1)), z(*(rhs+2))&#123;&#125; Vector3D(const Vector3D &amp;rhs):x(rhs.x), y(rhs.y), z(rhs.z)&#123;&#125; ~Vector3D() = default; // setter,getter void set(float newX, float newY, float newZ)&#123;x=newX;y=newY;z=newZ;&#125; void setX(float newX) &#123;x = newX;&#125; void setY(float newY) &#123;y = newY;&#125; void setZ(float newZ) &#123;z = newZ;&#125; float getX() const &#123;return x;&#125; float getY() const &#123;return y;&#125; float getZ() const &#123;return z;&#125; // normalization void normalize(); Vector3D getNormalized() const; // length caculation float getLength() const &#123;return static_cast&lt;float&gt;(sqrt(x*x+y*y+z*z));&#125; float getSquaredLength() const &#123;return x*x+y*y+z*z;&#125; // product float dotProduct(const Vector3D &amp;rhs) const &#123;return x*rhs.x + y*rhs.y + z*rhs.z;&#125; Vector3D crossProduct(const Vector3D &amp;rhs) const &#123;return Vector3D(y*rhs.z - z*rhs.y, z*rhs.x - x*rhs.z, x*rhs.y - y*rhs.x);&#125; // linear interpolation Vector3D lerp(const Vector3D &amp;v2, float factor) const &#123;return (*this)*(1.0f-factor) + v2*factor;&#125; Vector3D QuadraticInterpolate(const Vector3D &amp;v2, const Vector3D &amp;v3, float factor) const &#123;return (*this)*(1.0f-factor)*(1.0f-factor) + v2*2.0f*factor*(1.0f-factor) + v3*factor*factor;&#125; // overloaded operators Vector3D operator+(const Vector3D &amp;rhs) const &#123;return Vector3D(x + rhs.x, y + rhs.y, z + rhs.z);&#125; Vector3D operator-(const Vector3D &amp;rhs) const &#123;return Vector3D(x - rhs.x, y - rhs.y, z - rhs.z);&#125; Vector3D operator*(const float rhs) const &#123;return Vector3D(x*rhs, y*rhs, z*rhs);&#125; Vector3D operator/(const float rhs) const &#123;return (equal(rhs,0.0f))?Vector3D(0.0f, 0.0f, 0.0f):Vector3D(x/rhs, y/rhs, z/rhs);&#125; bool operator==(const Vector3D &amp;rhs) const &#123;return (equal(x,rhs.x) &amp;&amp; equal(y,rhs.y) &amp;&amp; equal(z,rhs.z));&#125; bool operator!=(const Vector3D &amp;rhs) const &#123;return !((*this)==rhs);&#125; void operator+=(const Vector3D &amp;rhs) &#123;x+=rhs.x;y+=rhs.y;z+=rhs.z;&#125; void operator-=(const Vector3D &amp; rhs) &#123;x-=rhs.x;y-=rhs.y;z-=rhs.z;&#125; void operator*=(const float rhs)&#123;x*=rhs;y*=rhs;z*=rhs;&#125; void operator/=(const float rhs)&#123;if(!equal(rhs,0.0f))&#123;x/=rhs; y/=rhs; z/=rhs;&#125;&#125; Vector3D operator-() const &#123;return Vector3D(-x, -y, -z);&#125; Vector3D operator+() const &#123;return *this;&#125;&#125;; 3、4D向量类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Vector4D&#123;public: float x,y,z,w; // constructors Vector4D():x(0.0f), y(0.0f), z(0.0f), w(0.0f)&#123;&#125; Vector4D(float newX, float newY, float newZ, float newW):x(newX), y(newY), z(newZ), w(newW)&#123;&#125; Vector4D(const float * rhs):x(*rhs), y(*(rhs+1)), z(*(rhs+2)), w(*(rhs+3))&#123;&#125; Vector4D(const Vector4D &amp;rhs):x(rhs.x), y(rhs.y), z(rhs.z), w(rhs.w)&#123;&#125; Vector4D(const Vector3D &amp; rhs): x(rhs.x), y(rhs.y), z(rhs.z), w(1.0f)&#123;&#125; ~Vector4D() = default; // setter,getter void set(float newX, float newY, float newZ, float newW)&#123;x=newX;y=newY;z=newZ;w=newW;&#125; void setX(float newX) &#123;x = newX;&#125; void setY(float newY) &#123;y = newY;&#125; void setZ(float newZ) &#123;z = newZ;&#125; void setW(float newW) &#123;w = newW;&#125; float getX() const &#123;return x;&#125; float getY() const &#123;return y;&#125; float getZ() const &#123;return z;&#125; float getW() const &#123;return w;&#125; // product float dotProduct(const Vector4D &amp;rhs) const &#123;return x*rhs.x + y*rhs.y + z*rhs.z + w*rhs.w;&#125; // linear interpolation Vector4D lerp(const Vector4D &amp;v2, float factor) const &#123;return (*this)*(1.0f-factor) + v2*factor;&#125; Vector4D QuadraticInterpolate(const Vector4D &amp;v2, const Vector4D &amp;v3, float factor) const &#123;return (*this)*(1.0f-factor)*(1.0f-factor)+v2*2.0f*factor*(1.0f-factor)+v3*factor*factor;&#125; // overloaded operators Vector4D operator+(const Vector4D &amp;rhs) const &#123;return Vector4D(x+rhs.x, y+rhs.y, z+rhs.z, w+rhs.w);&#125; Vector4D operator-(const Vector4D &amp;rhs) const &#123;return Vector4D(x-rhs.x, y-rhs.y, z-rhs.z, w-rhs.w);&#125; Vector4D operator*(const float rhs) const &#123;return Vector4D(x*rhs, y*rhs, z*rhs, w*rhs);&#125; Vector4D operator/(const float rhs) const &#123;return (equal(rhs,0.0f))?Vector4D(0.0f, 0.0f, 0.0f, 0.0f):Vector4D(x/rhs, y/rhs, z/rhs, w/rhs);&#125; bool operator==(const Vector4D &amp;rhs) const &#123;return (equal(x,rhs.x)&amp;&amp;equal(y,rhs.y)&amp;&amp;equal(z,rhs.z)&amp;&amp;equal(w,rhs.w));&#125; bool operator!=(const Vector4D &amp;rhs) const &#123;return !((*this)==rhs);&#125; void operator+=(const Vector4D &amp;rhs) &#123;x+=rhs.x;y+=rhs.y;z+=rhs.z;w+=rhs.w;&#125; void operator-=(const Vector4D &amp; rhs) &#123;x-=rhs.x;y-=rhs.y;z-=rhs.z;w-=rhs.w;&#125; void operator*=(const float rhs)&#123;x*=rhs;y*=rhs;z*=rhs;w*=rhs;&#125; void operator/=(const float rhs)&#123;if(!equal(rhs,0.0f))&#123;x/=rhs; y/=rhs; z/=rhs; w/=rhs;&#125;&#125; Vector4D operator-() const &#123;return Vector4D(-x, -y, -z, -w);&#125; Vector4D operator+() const &#123;return *this;&#125;&#125;; 二、矩阵&emsp;&emsp;矩阵本质就是向量的进一步扩展的，一个$n\\times m$的矩阵可看成$n$个$m$维行向量组成或者$m$个$n$维列向量组成，关于矩阵的基本概念、操作请看这里。通常我们采用方阵来描述线性变换。所谓线性变换，即变换之后保留了直线而不被弯曲，平行线依然平行，原点没有变化，但其他的几何性质如长度、角度、面积和体积可能被变换改变了。直观来说，线性变换可能“拉伸”坐标系，但不会“弯曲”或“卷折”坐标系。 &emsp;&emsp;矩阵在计算机中有行主序存储、列主序存储两种方式，行主序存储即按照顺序逐行存储，列主序存储则按照顺序逐列存储。图形学渲染中我们通常采用的是列主序的方式，以下的讨论都是列主序的矩阵存储方式。那么矩阵是如何变换向量的？ &emsp;&emsp;向量在几何上能被解释成一系列与轴平行的位移，一般来说，任意向量$\\vec v$都能写成如下的形式： \\vec v=\\left[\\begin{matrix}x\\\\y\\\\z\\end{matrix}\\right]=\\left[\\begin{matrix}x\\\\0\\\\0\\end{matrix}\\right]+\\left[\\begin{matrix}0\\\\y\\\\0\\end{matrix}\\right]+\\left[\\begin{matrix}0\\\\0\\\\z\\end{matrix}\\right]=x\\left[\\begin{matrix}1\\\\0\\\\0\\end{matrix}\\right]+y\\left[\\begin{matrix}0\\\\1\\\\0\\end{matrix}\\right]+z\\left[\\begin{matrix}0\\\\0\\\\1\\end{matrix}\\right] \\tag {1}&emsp;&emsp;公式$(1)$右边的单位向量就是$x$、$y$、$z$轴方向的向量，向量的每个坐标都表明了平行于相应坐标轴的有向位移。我们记$\\vec p$、$\\vec q$、$\\vec r$分别为公式$(1)$中右边的$x$、$y$、$z$轴的单位列向量，则有： \\vec v=x\\vec p+y\\vec q+z\\vec r=\\left[\\begin{matrix}\\vec p &\\vec q&\\vec r\\end{matrix}\\right]\\left[\\begin{matrix}x \\\\y\\\\z\\end{matrix}\\right] \\tag {2}&emsp;&emsp;向量$\\vec v$就变成了向量$\\vec p$、$\\vec q$、$\\vec r$的线性表示，向量$\\vec p$、$\\vec q$、$\\vec r$称作基向量。以上仅仅讨论的是笛卡尔坐标系，但更通用的情况是，一个$3$维坐标系能用任意$3$个线性无关的基向量表示，以列向量$\\vec p$、$\\vec q$、$\\vec r$构建$3\\times 3$的矩阵$M$： M=\\left[\\begin{matrix}\\vec p &\\vec q&\\vec r\\end{matrix}\\right]=\\left[\\begin{matrix}p_x &q_x&r_x\\\\p_y &q_y&r_y\\\\p_z &q_z&r_z\\end{matrix}\\right] \\tag {3}&emsp;&emsp;结合公式$(2)$和公式$(3)$，即有： \\vec v=M\\left[\\begin{matrix}x \\\\y\\\\z\\end{matrix}\\right] \\tag{4}&emsp;&emsp;坐标系变换矩阵的每一列（如果是行主序，就是每一行）都是该坐标系的基向量，一个点$v$右乘该矩阵就相当于执行了一次坐标系转换。求解线性变换矩阵的关键就是根据当前的坐标系求解变换之后的坐标系的基向量，然后将基向量填入向量位置！ &emsp;&emsp;一个矩阵类通常有如下方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class Matrix4x4&#123;public: float entries[16]; // constructors Matrix4x4()&#123;loadIdentity();&#125; Matrix4x4(float e0, float e1, float e2, float e3, float e4, float e5, float e6, float e7, float e8, float e9, float e10,float e11, float e12,float e13,float e14,float e15); Matrix4x4(const float *rhs); Matrix4x4(const Matrix4x4 &amp;rhs); ~Matrix4x4() = default; // setter,getter void setEntry(int position, float value); float getEntry(int position) const; Vector4D getRow(int position) const; Vector4D getColumn(int position) const; void loadIdentity(); void loadZero(); // overloaded operators Matrix4x4 operator+(const Matrix4x4 &amp; rhs) const; Matrix4x4 operator-(const Matrix4x4 &amp; rhs) const; Matrix4x4 operator*(const Matrix4x4 &amp; rhs) const; Matrix4x4 operator*(const float rhs) const; Matrix4x4 operator/(const float rhs) const; bool operator==(const Matrix4x4 &amp; rhs) const; bool operator!=(const Matrix4x4 &amp; rhs) const; void operator+=(const Matrix4x4 &amp; rhs); void operator-=(const Matrix4x4 &amp; rhs); void operator*=(const Matrix4x4 &amp; rhs); void operator*=(const float rhs); void operator/=(const float rhs); Matrix4x4 operator-() const; Matrix4x4 operator+() const &#123;return (*this);&#125; Vector4D operator*(const Vector4D rhs) const; // inverse, transpose void inverted(); Matrix4x4 getInverse() const; void transpose(); Matrix4x4 getTranspose() const; void invertTranspose(); Matrix4x4 getInverseTranspose() const; // operation on space void setTranslation(const Vector3D &amp; translation); void setScale(const Vector3D &amp; scaleFactor); void setRotationAxis(const double angle, const Vector3D &amp; axis); void setRotationX(const double angle); void setRotationY(const double angle); void setRotationZ(const double angle); void setRotationEuler(const double angleX, const double angleY, const double angleZ); void setPerspective(float fovy, float aspect, float near, float far); void setPerspective(float left, float right, float bottom, float top, float near, float far); void setOrtho(float left, float right, float bottom, float top, float near, float far);&#125;; 1、线性变换、仿射变换&emsp;&emsp;满足$F(a+b)=F(a)+F(b)$和$F(ka)=kF(a)$的映射$F(a)$就是线性的。对于映射$F(a)=Ma$，当$M$为任意方阵时，也可以说明$F$映射是一个线性变换。在计算机图形学中，缩放、旋转的变换操作都是线性的，但是平移不是线性变换。 &emsp;&emsp;具有$v’=Mv’+b$形式的变换都是仿射变换。平移作为最常用的变换之一，然而却不是线性变换；所以为了包括平移变换提出了仿射变换。仿射变换是指线性变换后接着平移。因此，仿射变换的集合是线性变换的超集，任何线性变换都是仿射变换，但不是所有的仿射变换都是线性变换。为了统一用矩阵表示低维度的仿射变换，我们可以通过高维度的线性变换来完成，为此引入了$4$维齐次坐标。（当然引入第$4$维$w$还有其他的用途，如当$w=0$时，可解释为无穷远的“点”，其意义是描述方向），关于齐次坐标的更多内容请查看这里。 &emsp;&emsp;从而，对于高维度来说只是经历了一次切变+投影变换就可以实现低维度的平移（更多内容查看这里），在$3D$渲染中，我们采用$4\\times 4$的矩阵做相应的变换。关于平移和缩放不再赘述： 123456789101112131415void Matrix4x4::setTranslation(const Vector3D &amp;translation)&#123; loadIdentity(); entries[12] = translation.x; entries[13] = translation.y; entries[14] = translation.z;&#125;void Matrix4x4::setScale(const Vector3D &amp;scaleFactor)&#123; loadIdentity(); entries[0] = scaleFactor.x; entries[5] = scaleFactor.y; entries[10] = scaleFactor.z;&#125; 2、绕任意轴旋转&emsp;&emsp;在3D中，绕坐标轴旋转，而不是绕点旋转，此时首先需要定义的是何为旋转正方向： 左手坐标系中定义此方向的规则为左手法则。首先，要明确旋转轴指向哪个方向。当然，旋转轴在理论上是无限延伸的，但我们还是要认为它有正端点和负端点。与笛卡尔坐标轴定义坐标系相同，左手法则是这样的:伸出左手，大拇指向上，其余手指弯曲。大拇指指向旋转轴的正方向，此时，四指弯曲的方向就是旋转的正方向。右手坐标系则根据右手法则利用右手判断旋转正方向，本文讨论的是常见的右手坐标系。 &emsp;&emsp;在旋转变换中，一个常见的特殊情况就是绕$x$轴、绕$y$轴、绕$z$轴旋转，这类的旋转矩阵求解比较简单，只需牢牢记住列主序矩阵的列向量就是变换后的坐标系的基向量即可快速推导出相应的旋转矩阵： R_x(\\theta)=\\left[ \\begin{matrix} 1&0&0\\\\ 0&cos\\theta&-sin\\theta\\\\ 0&sin\\theta&cos\\theta \\end{matrix}\\right] \\\\ R_y(\\theta)=\\left[\\begin{matrix}cos\\theta&0&sin\\theta\\\\0&1&0\\\\-sin\\theta&0&cos\\theta \\end{matrix}\\right]\\\\ R_z(\\theta)=\\left[\\begin{matrix}cos\\theta&-sin\\theta&0\\\\ sin\\theta&cos\\theta&0\\\\0&0&1\\end{matrix}\\right] \\tag {5}1234567891011121314151617181920212223242526void Matrix4x4::setRotationX(const double angle)&#123; loadIdentity(); entries[5] = static_cast&lt;float&gt;(cos(M_PI*angle/180)); entries[6] = static_cast&lt;float&gt;(sin(M_PI*angle/180)); entries[9] = -entries[6]; entries[10] = entries[5];&#125;void Matrix4x4::setRotationY(const double angle)&#123; loadIdentity(); entries[0] = static_cast&lt;float&gt;(cos(M_PI*angle/180)); entries[2] = -static_cast&lt;float&gt;(sin(M_PI*angle/180)); entries[8] = -entries[2]; entries[10] = entries[0];&#125;void Matrix4x4::setRotationZ(const double angle)&#123; loadIdentity(); entries[0] = static_cast&lt;float&gt;(cos(M_PI*angle/180)); entries[1] = static_cast&lt;float&gt;(sin(M_PI*angle/180)); entries[4] = -entries[1]; entries[5] = entries[0];&#125; &emsp;&emsp;但是更一般的情况是绕任意轴进行旋转，构建这样的矩阵稍微有点麻烦，我们接下来就做一些绕任意轴旋转的矩阵构建推到。在这里我们不考虑平移，因而围绕旋转的轴一定是通过原点的。如下图1所示，将$\\vec v$旋转到$\\vec v ‘$，任意轴用单位向量$\\vec n$表示，绕$\\vec n$旋转$\\theta$角度的矩阵记为$R(\\vec n, \\theta)$，$\\vec v’$是向量绕轴$\\vec n$旋转后的向量，即$\\vec v’=R(\\vec n,\\theta)\\vec v$。 图1 绕任意轴旋转&emsp;&emsp;我们的目标就是用$\\vec v$、$\\vec n$和$\\theta$来表示$\\vec v’$，从而构造出$R(\\vec n, \\theta)$。首先将$\\vec v$分解成平行于$\\vec n$的向量$\\vec v_{||}$和垂直于$\\vec n$的分量$\\vec v_{⊥}$，而$\\vec v’_{⊥}$是垂直于$\\vec n$的分向量。注意，$\\vec n$是单位向量，但$\\vec v$不是单位向量，可得$\\vec v$在$\\vec n$方向的投影向量$\\vec v_{||}$为： \\vec v_{||}=(\\vec v\\cdot\\vec n)\\vec n \\tag {6}&emsp;&emsp;从而根据$\\vec v_{||}$和$\\vec v$可知$\\vec v_{⊥}$和$w$，$w$是垂直于$\\vec n$和$\\vec v_{⊥}$的向量： \\vec v_{⊥}=\\vec v-\\vec v_{||} \\tag {7} w=\\vec n \\times \\vec v_{⊥} = \\vec n\\times (\\vec v-\\vec v_{||})\\\\ =\\vec n\\times\\vec v-\\vec n\\times\\vec v_{||}=\\vec n\\times\\vec v-0=\\vec n\\times \\vec v \\tag{8}&emsp;&emsp;$\\vec w$和$\\vec v_{⊥}$相互垂直，$\\vec w$、$\\vec v_{⊥}$和$\\vec v’_{⊥}$在同一个平面上，$\\vec v’_{⊥}$和$\\vec v_{⊥}$的夹角为$\\theta$，从而$\\vec v’_{⊥}$可由$\\vec w$和$\\vec v_{⊥}$线性表示为： \\vec v'_{⊥}=cos\\theta\\vec v_{⊥}+sin\\theta\\vec w\\\\ =cos\\theta(\\vec v-(\\vec v\\cdot\\vec n)\\vec n)+sin\\theta(\\vec n\\times\\vec v)\\tag {9}&emsp;&emsp;最后，根据公式$(6)$和公式$(9)$我们已知$\\vec v_{||}$和$\\vec v’_{⊥}$，从而可以得出$\\vec v’$： \\vec v'=\\vec v_{||}+\\vec v'_{⊥}\\\\ =cos\\theta(\\vec v-(\\vec v\\cdot\\vec n)\\vec n)+sin\\theta(\\vec n\\times\\vec v)+(\\vec v\\cdot\\vec n)\\vec n \\tag {10}&emsp;&emsp;由公式$(10)$可知，我们已经用$\\vec v$、$\\vec n$和$\\theta$表示$\\vec v’$，那如何根据上述的公式$(10)$构建旋转矩阵$R(\\vec n, \\theta)$？还是那个思路：列主序变换矩阵的列向量就是变换后的坐标系的基向量。我们只需求出笛卡尔坐标系的$\\vec x$、$\\vec y$、$\\vec z$三个轴方向上的基向量按照公式$(10)$旋转之后的基向量$\\vec x’$、$\\vec y’$、$\\vec z’$，然后填入矩阵$R(\\vec n, \\theta)$即可，以$\\vec x=[1\\ \\ 0 \\ \\ 0]^T$为例： \\vec x'=cos\\theta(\\vec x-(\\vec x\\cdot\\vec n)\\vec n)+sin\\theta(\\vec n\\times\\vec x)+(\\vec x\\cdot\\vec n)\\vec n =\\left[\\begin{matrix} n^2_x(1-cos\\theta)+cos\\theta \\\\n_xn_y(1-cos\\theta)+n_zsin\\theta \\\\n_xn_z(1-cos\\theta)-n_ysin\\theta) \\end{matrix}\\right] \\tag {11}&emsp;&emsp;$\\vec y=[0\\ \\ 1\\ \\ 0]^T$和$\\vec z=[0\\ \\ 0\\ \\ 1]^T$同理： \\vec y' =\\left[\\begin{matrix} n_xn_y(1-cos\\theta)-n_zsin\\theta \\\\n^2_y(1-cos\\theta)+cos\\theta \\\\n_yn_z(1-cos\\theta)+n_xsin\\theta \\end{matrix}\\right] \\tag {12} \\vec z' =\\left[\\begin{matrix} n_xn_z(1-cos\\theta)+n_ysin\\theta \\\\n_yn_z(1-cos\\theta)-n_xsin\\theta \\\\n^2_z(1-cos\\theta)+cos\\theta \\end{matrix}\\right] \\tag {13}&emsp;&emsp;将$\\vec x’$、$\\vec y’$、$\\vec z’$合并到$R(\\vec n, \\theta)$中： R(\\vec n, \\theta) =\\left[\\begin{matrix} \\vec x'&\\vec y'&\\vec z' \\end{matrix}\\right] \\\\=\\begin{bmatrix} {n_x}^2(1-cos\\theta)+cos\\theta&n_xn_y(1-cos\\theta)-n_zsin\\theta&n_xn_z(1-cos\\theta)+n_ysin\\theta \\\\n_xn_y(1-cos\\theta)+n_zsin\\theta&n^2_y(1-cos\\theta)+cos\\theta&n_yn_z(1-cos\\theta)-n_xsin\\theta \\\\n_xn_z(1-cos\\theta)-n_ysin\\theta)&n_yn_z(1-cos\\theta)+n_xsin\\theta&n^2_z(1-cos\\theta)+cos\\theta \\end{bmatrix} \\tag {14}12345678910111213141516171819202122void Matrix4x4::setRotationAxis(const double angle, const Vector3D &amp;axis)&#123; Vector3D u = axis.getNormalized(); float sinAngle = static_cast&lt;float&gt;(sin(M_PI*angle/180)); float cosAngle = static_cast&lt;float&gt;(cos(M_PI*angle/180)); float oneMinusCosAngle = 1.0f - cosAngle; loadIdentity(); entries[0] = (u.x)*(u.x) + cosAngle*(1-(u.x)*(u.x)); entries[4] = (u.x)*(u.y)*(oneMinusCosAngle) - sinAngle*u.z; entries[8] = (u.x)*(u.z)*(oneMinusCosAngle) + sinAngle*u.y; entries[1] = (u.x)*(u.y)*(oneMinusCosAngle) + sinAngle*u.z; entries[5] = (u.y)*(u.y) + cosAngle*(1-(u.y)*(u.y)); entries[9] = (u.y)*(u.z)*(oneMinusCosAngle) - sinAngle*u.x; entries[2] = (u.x)*(u.z)*(oneMinusCosAngle) - sinAngle*u.y; entries[6] = (u.y)*(u.z)*(oneMinusCosAngle) + sinAngle*u.x; entries[10] = (u.z)*(u.z) + cosAngle*(1-(u.z)*(u.z));&#125; 3、透视投影、正交投影&emsp;&emsp;$3D$空间中的物体最终都要通过投影显示到$2D$的屏幕上，这一过程就是投影变换。投影变换矩阵将视图空间中的顶点数据变换到裁剪空间，裁剪空间中的顶点最后通过透视除法被变换到标准化设备坐标（$NDC$）。通常由两类投影：透视投影、正交投影。 透视投影矩阵&emsp;&emsp;关于透视投影矩阵的前世今生我不过多说，直接上透视投影矩阵的推导过程。一个视锥体我们目前用六个参数表示：$left$，$right$，$bottom$，$top$，$near$，$far$，简写为$l$、$r$、$b$、$t$、$n$和$f$，即视锥体的六个面。我们的目标就是将视图空间中在视锥体内的点变换到标准化设备坐标中的立方体内。即$x$轴方向从$[l,r]$映射到$[-1,1]$，$y$轴方向从$[b,t]$映射到$[-1,1]$，$z$轴方向从$[-n,-f]$映射到$[-1,1]$。 &emsp;&emsp;可能你会觉得奇怪，$z$轴方向为什么是从$[-n,-f]$映射到$[-1,1]$？这是因为摄像机空间的坐标系是右手坐标系，在视图空间中摄像机是朝向视图坐标系的$z$轴的负方向，如下图左边所示，$+Y$、$+Z$、$+X$标准摄像机坐标系的三个轴，而摄像机的观察视锥体是朝向$-Z$方向的。而$NDC$又是左手坐标系，朝向$+Z$方向，所以我们要取负。 图2 透视投影视锥和标准化设备坐标 图3 从-Y方向看去的视锥横截面 图4 从-X方向看去的视锥横截面&emsp;&emsp;在视锥体中的顶点$(x_e,y_e,z_e)$被投影到视锥体的近平面，近平面上的点我们记为$(x_p,y_p,-n)$。如图3和图4所示，根据三角形相似的原理，我们有： \\frac{x_p}{x_e}=\\frac{-n}{z_e}\\ \\rightarrow\\ x_p=\\frac{-n\\cdot x_e}{z_e}=\\frac{n\\cdot x_e}{-z_e} \\tag {15} \\frac{y_p}{y_e}=\\frac{-n}{y_e}\\ \\rightarrow\\ y_p=\\frac{-n\\cdot y_e}{z_e}=\\frac{n\\cdot y_e}{-z_e} \\tag {16}&emsp;&emsp;注意到公式$(15)$和$(16)$中分母都是一个$-z_e$，这与我们将裁剪空间中的顶点做透视除法相对应，透视投影然后做透视除法如下公式$(17)$、$(18)$所示： \\left( \\begin{matrix} x_{clip}\\\\ y_{clip}\\\\ z_{clip}\\\\ w_{clip} \\end{matrix} \\right) =M_{projection}\\cdot \\left( \\begin{matrix} x_{eye}\\\\ y_{eye}\\\\ z_{eye}\\\\ w_{eye} \\end{matrix} \\right) \\tag {17} \\left( \\begin{matrix} x_{ndc}\\\\ y_{ndc}\\\\ z_{ndc} \\end{matrix} \\right) = \\left( \\begin{matrix} x_{clip}/w_{clip}\\\\ y_{clip}/w_{clip}\\\\ z_{clip}/w_{clip} \\end{matrix} \\right) \\tag {18}&emsp;&emsp;为了便于构建矩阵（$x_e$和$y_e$均与$-z_e$相除，不好构建矩阵），我们令裁剪空间中的$w_{clip}$为$-z_e$，将除以$-z_e$的这一步挪到了透视除法去做。故目前的透视矩阵就变为： \\left( \\begin{matrix} x_{c}\\\\ y_{c}\\\\ z_{c}\\\\ w_{c} \\end{matrix} \\right) = \\left( \\begin{matrix} .&.&.&.\\\\ .&.&.&.\\\\ .&.&.&.\\\\ 0&0&-1&0 \\end{matrix} \\right) \\left( \\begin{matrix} x_{e}\\\\ y_{e}\\\\ z_{e}\\\\ w_{e} \\end{matrix} \\right) \\tag {19}&emsp;&emsp;其中”$.$”均表示未知。得到在近平面的$x_p$和$y_p$之后，我们还要将$x_p$映射到$[-1,1]$范围，同理$y_p$也是。以$x_p$为例，我们知道其值域为$[l,r]$。为了将$x_p$其映射到$[-1,1]$，我们首先将其映射到$[0,1]$，不难得到如下式子： \\frac{x_p-l}{r-l}\\in[0,1] \\tag {20}&emsp;&emsp;式$(20)$乘上一个$2$再减去$1$就映射到了$[-1,1]$，映射之后记为$x_n$： x_n=2\\frac{x_p-l}{r-l}-1=\\frac{2x_p}{r-l}-\\frac{r+l}{r-l}\\in[-1,1] \\tag {21}&emsp;&emsp;同理$y_p$到$y_n$的映射： y_n=\\frac{2y_p}{r-l}-\\frac{t+b}{t-b}\\in[-1,1] \\tag {22}&emsp;&emsp;然后将公式$(15)$中的$x_p$带入公式$(21)$，将公式$(16)$中的$y_p$带入公式$(22)$，以$x_p$为例： x_n=\\frac{2x_p}{r-l}-\\frac{r+l}{r-l} =\\frac{2\\frac{n\\cdot x_e}{-z_e}}{r-l}-\\frac{r+l}{r-l}\\\\ =\\frac{2n\\cdot x_e}{(r-l)(-z_e)}-\\frac{r+l}{r-l} =\\frac{\\frac{2n}{r-l}\\cdot x_e}{-z_e}-\\frac{r+l}{r-l}\\\\ =\\frac{\\frac{2n}{r-l}\\cdot x_e}{-z_e}+\\frac{\\frac{r+l}{r-l}\\cdot z_e}{-z_e} =\\underbrace{(\\frac{2n}{r-l}\\cdot x_e+\\frac{r+l}{r-l}\\cdot z_e)}_{x_c}/-z_e \\tag {23}&emsp;&emsp;其中$x_c$即公式$(19)$中的裁剪空间中的$x$轴坐标值。$y_p$同理可得$y_c$: y_n =\\underbrace{(\\frac{2n}{t-b}\\cdot y_e+\\frac{t+b}{t-b}\\cdot z_e)}_{y_c}/-z_e \\tag {24}&emsp;&emsp;现在我们已经知道了$x_c$和$y_c$分辨关于$x_e$、$y_e$以及$z_e$的表达形式，我们可以填充式$(19)$中的投影矩阵第一行与第二行： \\left( \\begin{matrix} x_{c}\\\\ y_{c}\\\\ z_{c}\\\\ w_{c} \\end{matrix} \\right) = \\left( \\begin{matrix} \\frac{2n}{r-l}&0&\\frac{r+l}{r-l}&0\\\\ 0&\\frac{2n}{t-b}&\\frac{t+b}{t-b}&0\\\\ 0&0&A&B\\\\ 0&0&-1&0 \\end{matrix} \\right) \\left( \\begin{matrix} x_{e}\\\\ y_{e}\\\\ z_{e}\\\\ w_{e} \\end{matrix} \\right) \\tag {25}&emsp;&emsp;现在我们还剩下投影矩阵的第三行还不知道。因为我们知道$z$的投影与$x_e$和$y_e$无关，只与$z_e$、$w_e$有关，故可以假设投影矩阵的第三行如上式$(25)$所示，$A$和$B$就是我们假设的要求解的未知表达式。此外，在视图空间中的$w_e$是等于$1$的，$w_c$即前面提到的$-z_e$，从而有： z_n=z_c/w_c=\\frac{Az_e+Bw_e}{-z_e}=\\frac{Az_e+B}{-z_e} \\tag {26}&emsp;&emsp;为了求出公式$(26)$中的$A$和$B$，我们取两个极端的例子：在$-n$处的$z$值被映射到$-1$，在$-f$处的$z$值被映射到$1$，将$(z_n,z_e)=(-1,-n)$和$(z_n,z_e)=(1,-f)$带入式$(26)$中，可得方程组： \\begin{cases} \\frac{-An+B}{n}=-1\\\\ \\frac{-Af+B}{f}=1\\\\ \\end{cases}\\ \\rightarrow\\ \\begin{cases} {-An+B}=-n\\\\ {-Af+B}=f\\\\ \\end{cases} \\tag {27}&emsp;&emsp;求解方程$(27)$，可得$A$与$B$如下所示： A=-\\frac{f+n}{f-n}\\\\ B=-\\frac{2fn}{f-n} \\tag {28}&emsp;&emsp;将公式$(28)$带入公式$(26)$中： z_n=\\underbrace{(-\\frac{f+n}{f-n}z_e-\\frac{2fn}{f-n})}_{z_c}/{-z_e} \\tag {29}&emsp;&emsp;我们最终得到了$z_c$关于$z_e$的表达式，将$A$与$B$填入式$(25)$的投影矩阵即可，$M_{projection}$就是我们一直在寻求的透视投影矩阵： M_{projection}= \\left( \\begin{matrix} \\frac{2n}{r-l}&0&\\frac{r+l}{r-l}&0\\\\ 0&\\frac{2n}{t-b}&\\frac{t+b}{t-b}&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right) \\tag {30}&emsp;&emsp;公式$(30)$中的透视投影矩阵只是一个通用的形式，在视图空间中的视锥体通常都是关于$x$轴和$y$轴对称的，从而有$r=-l$、$t=-b$，将式$(30)$简化成如下形式： M_{projection}= \\left( \\begin{matrix} \\frac{2n}{r-l}&0&0&0\\\\ 0&\\frac{2n}{t-b}&0&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right) \\tag {31}&emsp;&emsp;&emsp;但是通常我们传入构建透视矩阵函数的参数是$fovy$（$y$轴方向的视域角）、$aspect$（屏幕的宽高比）、$near$（近平面）以及$far$（远平面），如何根据这些参数构造式$(31)$的透视投影矩阵呢？注意到$r-l=width$即近平面宽度，$t-b=height$即近平面的高度，我们可以根据$fovy$和$aspect$得出$width$和$height$，具体细节不再赘述： r-l=width=2*near*aspect*tan(fovy/2)\\\\ t-b=height=2*near*tan(fovy/2) M_{projection}= \\left( \\begin{matrix} \\frac{1}{aspect*tan(fovy/2)}&0&0&0\\\\ 0&\\frac{1}{tan(fovy/2)}&0&0\\\\ 0&0&-\\frac{f+n}{f-n}&-\\frac{2fn}{f-n}\\\\ 0&0&-1&0 \\end{matrix} \\right) \\tag {32}123456789101112void Matrix4x4::setPerspective(float fovy, float aspect, float near, float far)&#123; loadZero(); // convert fov from degrees to radians float rFovy = fovy*M_PI/180; const float tanHalfFovy = tanf(static_cast&lt;float&gt;(rFovy*0.5f)); entries[0] = 1.0f/(aspect*tanHalfFovy); entries[5] = 1.0f/(tanHalfFovy); entries[10] = -(far+near)/(far-near); entries[11] = -1.0f; entries[14] = (-2.0f*near*far)/(far-near);&#125; 正交投影矩阵&emsp;&emsp;理解了透视投影矩阵的构造之后，正交投影就简单太多了，正交投影只需做简单的线性映射就行了。只需将$x$轴方向从$[l,r]$映射到$[-1,1]$，$y$轴方向从$[b,t]$映射到$[-1,1]$，$z$轴方向从$[-n,-f]$映射到$[-1,1]$，而这个映射的过程很简单，正如前面公式$(20)$和$(21)$那样，先映射到$[0,1]$，再映射到$[0,2]$，最后映射到$[-1,1]$，这个过程我也不细说了，直接上结果： M_{projection}= \\left( \\begin{matrix} \\frac{2}{r-l}&0&0&-\\frac{r+l}{r-l}\\\\ 0&\\frac{2}{t-b}&0&-\\frac{t+b}{t-b}\\\\ 0&0&\\frac{-2}{f-n}&-\\frac{f+n}{f-n}\\\\ 0&0&0&1 \\end{matrix} \\right) \\tag {33}&emsp;&emsp;然后又因为视锥体关于$x$轴、$y$轴对称，简化的正交投影矩阵就为： M_{projection}= \\left( \\begin{matrix} \\frac{2}{r-l}&0&0&0\\\\ 0&\\frac{2}{t-b}&0&0\\\\ 0&0&\\frac{-2}{f-n}&-\\frac{f+n}{f-n}\\\\ 0&0&0&1 \\end{matrix} \\right) \\tag {33}12345678910void Matrix4x4::setOrtho(float left, float right, float bottom, float top, float near, float far)&#123; loadIdentity(); entries[0] = 2.0f/(right-left); entries[5] = 2.0f/(top-bottom); entries[10] = -2.0f/(far-near); entries[12] = -(right+left)/(right-left); entries[13] = -(top+bottom)/(top-bottom); entries[14] = -(far+near)/(far-near);&#125; 4、lookAt函数构造视图矩阵&emsp;&emsp;视图矩阵的工作目标是将世界坐标系中的所有物体的顶点的坐标从世界坐标系转换到摄像机坐标系。这是因为摄像机坐标系的原点不一定与世界坐标系重合，同时由于自身的旋转，坐标轴也一定不与世界坐标系的坐标轴平行。为完成工作任务，需要分为两步走：首先整体平移，将摄像机平移至世界坐标系原点，然后将顶点从世界坐标系变换至摄像机坐标系。 &emsp;&emsp;lookAt函数的输入参数分别为：$eye$摄像机的位置，$target$摄像机目标点，$up$世界空间的上向量,。首先我们要根据这些参数确定摄像机坐标系的三个轴向量，其中需要非常注意的就是变换到视图空间中时摄像机是朝向视图空间的$-Z$方向的，所以求视图空间中的$Z$轴时是摄像机的位置减去目标点的位置： Z = normalize(eye - target)\\\\ X = normalize(cross(up, Z))\\\\ Y = normalize(cross(Z,X))&emsp;&emsp;通过以上的方式我们就求出了视图空间的三条轴向量，再加上摄像机的位置我们就可以求出将世界坐标变换到与视图坐标重合的矩阵了，记为$M=T\\cdot R$，其中$T$是平移到摄像机位置$eye$的变换矩阵，$R$是旋转到摄像机坐标轴方向的旋转矩阵： M=T\\cdot R= \\left[ \\begin{matrix} 1&0&0&eye_x\\\\ 0&1&0&eye_x\\\\ 0&0&1&eye_x\\\\ 0&0&0&1 \\end{matrix} \\right] \\cdot \\left[ \\begin{matrix} X_x&Y_x&Z_x&0\\\\ X_y&Y_y&Z_y&0\\\\ X_z&Y_z&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {34}&emsp;&emsp;然而公式$(34)$并不是我们要求的视图矩阵，上式中的矩阵$M$仅仅是将世界坐标轴变换到摄像机坐标轴。摄像机只是一个虚拟的物品，我们不能将上述的矩阵$M$作用于摄像机，因为摄像机根本不存在！我们视图矩阵最终作用的世界空间中的物体，这就涉及到了一个相对运动的概念！ &emsp;&emsp;当我们向前移动摄像机的时候，可以看成是摄像机不动，而物体朝着与摄像机朝向相反的方向移动。当我们向右旋转摄像机时，相当于摄像机不动而物体朝着摄像机的左边移动。摄像机的构造得益于相对于运动的理论，计算机图形学中的虚拟$3D$摄像机实际上是通过物体的移动来实现的，所以我们要构造的视图矩阵是公式$(34)$中的逆矩阵。 viewMatrix = M^{-1}=(T\\cdot R)^{-1}=R^{-1}\\cdot T^{-1} = \\left[ \\begin{matrix} X_x&Y_x&Z_x&0\\\\ X_y&Y_y&Z_y&0\\\\ X_z&Y_z&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} \\cdot \\left[ \\begin{matrix} 1&0&0&eye_x\\\\ 0&1&0&eye_x\\\\ 0&0&1&eye_x\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} \\tag {35}&emsp;&emsp;由上式可知，构造视图矩阵涉及到$R$和$T$的求逆，其中的平移矩阵$T$的求逆则是直接取平移量的相反数即可： T^{-1}= \\left[ \\begin{matrix} 1&0&0&eye_x\\\\ 0&1&0&eye_x\\\\ 0&0&1&eye_x\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} = \\left[ \\begin{matrix} 1&0&0&-eye_x\\\\ 0&1&0&-eye_x\\\\ 0&0&1&-eye_x\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {36}&emsp;&emsp;至于旋转矩阵$R$，我们知道旋转矩阵都是正交矩阵，正交矩阵的一个特点就是它的逆等于它的转置： R^{-1}= \\left[ \\begin{matrix} X_x&Y_x&Z_x&0\\\\ X_y&Y_y&Z_y&0\\\\ X_z&Y_z&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right]^{-1} = \\left[ \\begin{matrix} X_x&X_y&X_z&0\\\\ Y_x&Y_y&Y_z&0\\\\ Z_x&Z_y&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {37}&emsp;&emsp;最后，我们得到视图矩阵： viewMatrix=R^{-1}\\cdot T^{-1}= \\left[ \\begin{matrix} X_x&X_y&X_z&0\\\\ Y_x&Y_y&Y_z&0\\\\ Z_x&Z_y&Z_z&0\\\\ 0&0&0&1 \\end{matrix} \\right] \\cdot \\left[ \\begin{matrix} 1&0&0&-eye_x\\\\ 0&1&0&-eye_x\\\\ 0&0&1&-eye_x\\\\ 0&0&0&1 \\end{matrix} \\right] \\\\= \\left[ \\begin{matrix} X_x&X_y&X_z&-(\\vec X\\cdot \\vec {eye})\\\\ Y_x&Y_y&Y_z&-(\\vec Y\\cdot \\vec {eye})\\\\ Z_x&Z_y&Z_z&-(\\vec Z\\cdot \\vec {eye})\\\\ 0&0&0&1 \\end{matrix} \\right] \\tag {38}1234567891011121314151617181920212223242526void Matrix4x4::setLookAt(Vector3D cameraPos, Vector3D target, Vector3D worldUp)&#123; Vector3D zAxis = cameraPos - target; zAxis.normalize(); Vector3D xAxis = worldUp.crossProduct(zAxis); xAxis.normalize(); Vector3D yAxis = zAxis.crossProduct(xAxis); yAxis.normalize(); loadIdentity(); entries[0] = xAxis.x; entries[4] = xAxis.y; entries[8] = xAxis.z; entries[1] = yAxis.x; entries[5] = yAxis.y; entries[9] = yAxis.z; entries[2] = zAxis.x; entries[6] = zAxis.y; entries[10] = zAxis.z; entries[12] = -(xAxis.dotProduct(cameraPos)); entries[13] = -(yAxis.dotProduct(cameraPos)); entries[14] = -(zAxis.dotProduct(cameraPos));&#125; 参考资料$[1]$ http://www.songho.ca/opengl/gl_projectionmatrix.html $[2]$ https://blog.csdn.net/zsq306650083/article/details/8773996 $[3]$ https://blog.csdn.net/y1196645376/article/details/78463248 $[4]$ https://www.cnblogs.com/J1ac/p/9340622.html $[5]$ https://learnopengl-cn.github.io/01%20Getting%20started/08%20Coordinate%20Systems/","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/about/categories/Soft-Renderer/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Soft Renderer","slug":"Soft-Renderer","permalink":"https://yangwc.com/about/tags/Soft-Renderer/"},{"name":"3D Math","slug":"3D-Math","permalink":"https://yangwc.com/about/tags/3D-Math/"}]},{"title":"流体模拟Fluid Simulation：流体模拟基础","slug":"fluidSimulation","date":"2019-05-01T02:37:38.127Z","updated":"2019-09-12T08:52:00.962Z","comments":true,"path":"2019/05/01/fluidSimulation/","link":"","permalink":"https://yangwc.com/about/2019/05/01/fluidSimulation/","excerpt":"本文主要参考文献《FLUID SIMULATION SIGGRAPH 2007 Course Notes》，结合我的理解单纯地讲述一下流体渲染的一些基础知识，本人水平有限，如有错误，欢迎指出。本文只是单纯针对流体模拟领域，可能一些地方不太严谨，但是对于虚拟模拟来说是可行的。即便如此，本文涉及到大量的数学方法。","text":"本文主要参考文献《FLUID SIMULATION SIGGRAPH 2007 Course Notes》，结合我的理解单纯地讲述一下流体渲染的一些基础知识，本人水平有限，如有错误，欢迎指出。本文只是单纯针对流体模拟领域，可能一些地方不太严谨，但是对于虚拟模拟来说是可行的。即便如此，本文涉及到大量的数学方法。 矢量微积分 Naiver-Stokes偏微分方程组 N-S方程的分步求解 对流算法 一、矢量微积分&emsp;&emsp;高等数学中太多数讨论的是一维的微积分，而矢量微积分则是一维微积分的高维扩展。矢量微积分的三个基础算子：梯度（符号为$∇$），散度（符号为$∇\\cdot$)，旋度（符号为$∇\\times$），在此基础上流体力学中经常用到的还有拉普拉斯算子。 1、梯度（Gradient）&emsp;&emsp;梯度实际上就是矢量的空间偏导数，且结果依然是一个矢量，$2$维的梯度如下： ∇f(x,y)=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}) \\tag {1.1}&emsp;&emsp;依此类推，$3$维的梯度有如下形式： ∇f(x,y,z)=(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y},\\frac{\\partial f}{\\partial z}) \\tag {1.2}&emsp;&emsp;有时也会采用如下形式来表示梯度： ∇f=\\frac{\\partial f}{\\partial \\vec x} \\tag {1.3}&emsp;&emsp;梯度通常用来近似计算函数值（实际上就是一维形式的推广)： f(\\vec x+\\Delta \\vec x)\\approx f(\\vec x)+∇f(\\vec x)\\cdot \\Delta \\vec x \\tag {1.4}&emsp;&emsp;同样的，多个函数的梯度就构成了一个矩阵： ∇\\vec F=∇(f,g,h)=\\left( \\begin{matrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} & \\frac{\\partial f}{\\partial z} \\\\ \\frac{\\partial g}{\\partial x} & \\frac{\\partial g}{\\partial y} & \\frac{\\partial g}{\\partial z} \\\\ \\frac{\\partial h}{\\partial x} & \\frac{\\partial h}{\\partial y} & \\frac{\\partial h}{\\partial z} \\\\ \\end{matrix} \\right) =\\left( \\begin{matrix}∇f\\\\ ∇g\\\\ ∇h\\\\ \\end{matrix} \\right) \\tag {1.5}2、散度（Divergence）&emsp;&emsp;散度算子仅仅应用于向量场，它衡量在某一点出相应的矢量聚集或者发散程度，测量方向为径向，结果为标量。$2$维、$3$维形式的散度算子如下所示： ∇\\cdot \\vec u=∇\\cdot (u,v)=\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y} ∇\\cdot \\vec u=∇\\cdot (u,v,w)=\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}+\\frac{\\partial w}{\\partial z} \\tag {1.6}&emsp;&emsp;输入是矢量，而输出为标量。类比梯度，散度符号$∇\\cdot \\vec u$可以理解为梯度$∇$与矢量$\\vec u$的点乘： ∇\\cdot \\vec u=(\\frac{\\partial}{\\partial x},\\frac{\\partial}{\\partial y},\\frac{\\partial}{\\partial z})\\cdot (u,v,w)=\\frac{\\partial}{\\partial x}u+\\frac{\\partial}{\\partial y}v+\\frac{\\partial}{\\partial z}w \\tag {1.7}&emsp;&emsp;若矢量场散度为$0$，则称该矢量场无散度。 3、旋度（Curl）&emsp;&emsp;旋度衡量围绕某一点的旋转速度，测量方向为切向，三维形式的旋度是一个向量： ∇\\times \\vec u=∇\\times (u,v,w) =(\\frac{\\partial w}{\\partial y}-\\frac{\\partial v}{\\partial z}, \\frac{\\partial u}{\\partial z}-\\frac{\\partial w}{\\partial x}, \\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y}) \\tag {1.8}&emsp;&emsp;倒推到$2$维，我们取上式中的$w=0$，即矢量场为$(u,v,0)$，$2$维向量场的旋度是一个标量： ∇\\times \\vec u=∇\\times (u,v)=\\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y} \\tag {1.9}&emsp;&emsp;同样地，旋度符号$∇\\times \\vec u$我们可以理解为梯度$∇$与矢量场$\\vec u$的叉乘： ∇\\times \\vec u= (\\frac{\\partial }{\\partial x}, \\frac{\\partial }{\\partial y}, \\frac{\\partial }{\\partial z})\\times(u,v,w) \\tag {1.10}&emsp;&emsp;若矢量场旋度为$0$，则称该矢量场无旋度。 4、拉普拉斯算子（Laplacian）&emsp;&emsp;拉普拉斯算子定义为梯度的散度，符号表示为$∇\\cdot∇$，显然$∇\\cdot$是散度，而后面的$∇$则为梯度，故拉普拉斯算子即梯度的散度，是一个二阶微分算子。$2$维、$3$维形式分别如下： ∇\\cdot∇f=\\frac{\\partial^2f}{\\partial x^2}+\\frac{\\partial^2f}{\\partial y^2} ∇\\cdot∇f=\\frac{\\partial^2f}{\\partial x^2}+\\frac{\\partial^2f}{\\partial y^2}+\\frac{\\partial^2f}{\\partial z^2} \\tag {1.11}&emsp;&emsp;简言之，拉普拉斯算子定义如下： ∇\\cdot∇f=\\Sigma_{i=1}^n\\frac{\\partial^2f}{\\partial x_i^2} \\tag {1.12}&emsp;&emsp;偏微分方程$∇\\cdot ∇f=0$被称为拉普拉斯方程；而如果右边为某个非$0$常数，即$∇\\cdot ∇f=q$，我们称之为泊松方程。更一般地，如果梯度再乘上一个标量$a$（如$1/\\rho$)，即$∇\\cdot (a∇f)=q$，我们依旧称之为泊松问题。 二、$Naiver-Stokes$偏微分方程组&emsp;&emsp;流体模拟器的构建主要围绕著名的不可压缩$Navier-Stokes$方程展开，它是一个流体力学领域的偏微分方程，方程形式如下： \\frac{\\partial \\vec u}{\\partial t}+\\vec u\\cdot ∇\\vec u+\\frac1\\rho∇p=\\vec g+\\nu∇\\cdot∇\\vec u \\tag {2.1} ∇\\cdot\\vec u=0 \\tag {2.2}&emsp;&emsp;这个方程组看起非常地复杂，接下来我们就把它剖析成一个个比较简单的部分来理解。 1、符号标记&emsp;&emsp;我们有必要定义一些物理量的符号用以标记： &emsp;&emsp;符号$\\vec u$在流体力学中通常表示为流体的速度矢量，记$3$维的速度矢量$\\vec u=(u,v,w)$； &emsp;&emsp;希腊字符$\\rho$是流体的密度，对于水，该值大约为$1000kg/m^3$，而空气则大约为$1.3kg/m^3$； &emsp;&emsp;字符$p$代表压力，流体对任何物体施加的单位面积力； &emsp;&emsp;字符$\\vec g$则是我们熟悉的重力加速度，通常取$(0,-9.81,0)m/s^2$。我们约定$y$轴向上，而$x$轴和$z$轴在水平面上。另外补充一点，我们把其他的一些类似的力都累加到$\\vec g$上，也就是我们统一用$\\vec g$表示所有类似力之和，这类力我们称之为体积力（称之为体积力是因为它们的力是作用到整个流体而不只是流体的表面）； &emsp;&emsp;希腊字符$\\nu$是流体的运动粘度，它衡量流体的黏滞性。糖蜜之类的流体有非常高的粘度，而像酒精之类的流体有很低的粘度； &emsp;&emsp;其它一些矢量微积分的符号算子前面已经提到过，不再赘述。 2、动量方程&emsp;&emsp;偏微分方程$(2.1)$我们称之为动量方程，它本质上就是我们熟悉的牛顿定律$\\vec F=m\\vec a$的形式，描述了施加在流体上的力是如何影响流体的运动。 &emsp;&emsp;假设我们用粒子系统来模拟流体，每个粒子代表流体的一小滴，每个粒子有各自的质量$m$、体积$V$和速度$\\vec u$。为了让整个粒子系统运作起来，我们必须弄清楚每个粒子所承受的力的作用。牛顿第二定律告诉我们：$\\vec F=m\\vec a$，而根据加速度定义，我们有： \\vec a=\\frac{D\\vec u}{Dt} \\tag {2.3}&emsp;&emsp;符号$D$是指物质导数，所谓物质导数，就是对流体质点求导数，而且是针对流体质点（在这里就是流体粒子）而不是空间的固定点。因而牛顿第二定律就变成： m\\frac{D\\vec u}{Dt}=\\vec F \\tag {2.4}&emsp;&emsp;那么流体粒子承受的力有哪些呢？一个最简单的力就是重力：$m\\vec g$。而其他的流体质点（或其他流体粒子）也会对当前的流体粒子产生作用力。流体内部的相互作用力之一便是压力，较大压力的区域会向较低压力区域产生作用力。值得注意的是，我们只关注施加在粒子上的压力的净合力。例如，若施加在粒子上压力在每个方向上都相等，那么它的压力的合力便为0。我们用压力的负梯度（取负是因为方向是由压力大的区域指向压力小的区域）来衡量在当前流体粒子处压力的不平衡性，即取$-∇p$。那么流体粒子所承受的压力就是对$-∇p$在整个流体粒子的体积上进行积分，为了简化，我们简单地将$V$与$-∇p$相乘，故粒子压力部分为$-V∇p$。 &emsp;&emsp;其他的流体相互作用力则是由流体的黏性产生的，我们直观地把这种力理解为尽可能使得粒子以周围区域的平均速度移动的力，也就是使得粒子的速度与周围区域粒子速度的差距最小化。拉普拉斯算子是衡量一个量与之周围区域该量平均值之差的算符，因而$∇\\cdot∇\\vec u$是当前粒子速度矢量与周围区域平均速度矢量之差。为了计算粘滞力，我们同样对$∇\\cdot∇\\vec u$在整个粒子体积$V$上进行积分，与前面类似，我们简单取$V∇\\cdot∇\\vec u$。除此之外，我们还引进一个称为动力粘度系数的物理量，符号为$\\mu$。因而粘滞力为$V\\mu∇\\cdot∇\\vec u$。 &emsp;&emsp;把重力、压力和粘滞力综合一起，我们可得： m\\frac{D\\vec u}{Dt}=\\vec F=m\\vec g-V∇p+V\\mu∇\\cdot∇\\vec u \\tag {2.5}&emsp;&emsp;当粒子系统中的粒子数量趋于无穷大，而每个粒子大小趋于$0$时，会产生一个问题：此时每个粒子的质量$m$和体积$V$变为$0$，此时上式变得没有意义。为此，我们把$(2.5)$式调整一下，两边同除以体积$V$，又因$\\rho=m/V$，故有： \\rho\\frac{D\\vec u}{Dt}=\\rho\\vec g-∇p+\\mu∇\\cdot∇\\vec u \\tag {2.6}&emsp;&emsp;两边同除以$\\rho$，移项调整： \\frac{D\\vec u}{Dt}+\\frac1\\rho∇p=\\vec g+\\frac\\mu\\rho∇\\cdot∇\\vec u \\tag {2.7}&emsp;&emsp;为了进一步简化，定义运动粘度为$\\nu=\\mu/\\rho$，式$(2.7)$变为： \\frac{D\\vec u}{Dt}+\\frac1\\rho∇p=\\vec g+\\nu∇\\cdot∇\\vec u \\tag {2.8}&emsp;&emsp;我们已经快把动量方程推导出来，现在我们要把物质导数$\\frac{D\\vec u}{Dt}$弄清楚，为此，我们需要了解两种描述方法：拉格朗日描述和欧拉描述。 3、拉格朗日描述与欧拉描述&emsp;&emsp;当我们尝试研究流体或可变形固体的运动的时候，通常有两种方法来描述：拉格朗日描述（ Lagrangian viewpoint）、欧拉描述（Eulerian viewpoint）。 &emsp;&emsp;拉格朗日描述方法是我们比较熟悉的方法，这种描述方法把物体看成是由类似于粒子系统的形式组成，固体或流体的每个点看作一个独立的粒子，粒子有各自相应的位置$\\vec x$和速度$\\vec u$。我们可以把粒子理解为组成物体的分子。对于我们通常采用拉格朗日描述法进行建模模拟，即用一系列离散的粒子集来构建，粒子之间通过网格相联系。 &emsp;&emsp;欧拉描述方法则采用了完全不同的角度，它常被用于流体力学中。与拉格朗日描述追踪每个物体粒子的方法不同，欧拉描述关注点是空间中的一个固定点，并考察在这个固定点上流体性质（如密度、速度、温度等）是如何随着时间变化的。流体流动经过这个固定点可能会导致这个固定点的物理性质发生一些变化（如一个温度较高的流体粒子流经这个固定点，后面紧跟着一个温度较低的流体粒子流过固定点，那么这个固定点的温度会降低，但是并没有任何一个流体粒子的温度发生了变化）。 &emsp;&emsp;用天气测量举个简单的例子：拉格朗日描述方法就是你乘坐在一个随风而飘的热气球上，测量周围空气的压力、密度和浑浊度等天气指标；而欧拉描述方法就是你固定在地面上，测量流过的空气的天气指标。 &emsp;&emsp;欧拉描述法似乎看起来带来了一些不必要的复杂度，但是目前大多数的流体模拟器都是基于欧拉描述法，这是因为欧拉描述法相比于拉格朗日描述法有一些不可比拟的优点：欧拉描述法能够更加方便地计算一些物理量的空间导数（例如压力梯度和粘度）；而如果用粒子方法的话（即拉格朗日描述法），那么计算物理量相对于空间位置的变化是比较难的。 &emsp;&emsp;把拉格朗日描述法和欧拉描述法联系起来的关键点就是物质导数。首先从拉格朗日描述法出发，假设有一群粒子，每个粒子都有各自的位置$\\vec x$和速度$\\vec u$。记$q$为通用的物理量（如密度、速度和温度等），每个粒子有其对应的$q$值。方程$q(t,\\vec x)$描述在时间点$t$而位置为$\\vec x$的粒子对应的物理量值$q$。则一个粒子的物理量$q$随时间$t$的变化率是多少？这是一个拉格朗日描述角度下的问题，我们取对时间$t$的导数（注意用到了求导链式法则，以及$\\frac{\\partial q}{\\partial \\vec x}=∇q$和$\\vec u=\\frac{d\\vec x}{dt}）$： \\frac d{dt}q(t,\\vec x)=\\frac{\\partial q}{\\partial t}+∇q\\cdot\\frac{d\\vec x}{dt}=\\frac{\\partial q}{\\partial t}+∇q\\cdot\\vec u\\equiv\\frac{Dq}{Dt} \\tag {2.9}&emsp;&emsp;这就是物质导数。把式$(2.9)$代入式$(2.8)$我们就得到了流体动量方程$(2.1)$。物质导数针对的是流体质点（在这里就是流体粒子）而不是空间的固定点。式$(2.9)$写完整一点就是： \\frac{Dq}{Dt}=\\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}+v\\frac{\\partial q}{\\partial y}+w\\frac{\\partial q}{\\partial z} \\tag {2.10}&emsp;&emsp;对于给定的速度场$\\vec u$， 流体的物理性质如何在这个速度场$\\vec u$下变化的计算我们称之为对流（advection）。一个最简单的对流方程，就是其物理量的物质导数为$0$，如下所示： \\frac{Dq}{Dt}=0\\implies\\frac{\\partial q}{\\partial t}+\\vec u\\cdot ∇q = 0 \\tag {2.11}&emsp;&emsp;公式$(2.11)$的意义即在拉格朗日视角观察下，每个流体粒子的物理量保持不变。 4、不可压缩性&emsp;&emsp;关于流体的压缩性在此不做过多的物理细节描述，只需知道一点：通常情况下流体的体积变化非常小（除开一些极端的情况，而且这些极端情况我们日常生活中较少出现）。可压缩流体的模拟涉及到非常复杂的情况，往往需要昂贵的计算资源开销，为此在计算机流体模拟中我们通常把所有的流体当作是不可压缩的，即它们的体积不会发生变化。 &emsp;&emsp;任取流体的一部分，设其体积为$\\Omega$而其边界闭合曲面为$\\partial\\Omega$，我们可以通过围绕边界曲面$\\partial\\Omega$对流体速度$\\vec u$在曲面法线方向上的分量进行积分来衡量这块部分流体的体积变化速率： \\frac d{dt}Volume(\\Omega)=\\int\\int_{\\partial\\Omega}\\vec u\\cdot n \\tag{2.12}&emsp;&emsp;对于不可压缩的流体，其体积保持为某个常量，故其体积变化速率为$0$： \\int\\int_{\\partial\\Omega}\\vec u\\cdot n=0 \\tag {2.13}&emsp;&emsp;由高斯散度定理，我们可以把式$(2.13)$转换为体积分： \\int\\int_{\\partial\\Omega}\\vec u\\cdot n=\\int\\int\\int_\\Omega∇\\cdot \\vec u=0 \\tag{2.14}&emsp;&emsp;式$(13)$应该对任意的$\\Omega$成立，意即无论$\\Omega$取何值，积分值均为$0$。这种情况下只有令积分函数值取$0$方可成立，即对$0$积分无论$\\Omega$取何值结果均为$0$。所以有： ∇\\cdot \\vec u=0 \\tag{2.15}&emsp;&emsp;这就是$Navier-Stokes$方程中的不可压缩条件$(2.2)$。满足不可压缩条件的速度场被称为是无散度的，即在该速度场下流体体积既不膨胀也不坍缩，而是保持在一个常量。模拟不可压缩流体的关键部分就是使得流体的速度场保持无散度的状态，这也是流体内部压力的来源。 &emsp;&emsp;为了把压力与速度场的散度联系起来，我们在动量方程$(2.1)$两边同时取散度： ∇\\cdot\\frac{\\partial \\vec u}{\\partial t}+∇\\cdot(\\vec u\\cdot ∇\\vec u)+∇\\cdot\\frac1\\rho∇p=∇\\cdot(\\vec g+\\nu∇\\cdot∇\\vec u) \\tag {2.16}&emsp;&emsp;对于上式$(2.16)$第一项，我们转变一下求导次序： \\frac {\\partial}{\\partial t}∇\\cdot\\vec u \\tag {2.17}&emsp;&emsp;如果满足流体不可压缩条件，那么式$(2.17)$取值$0$（因为无散度），然后我们调整一下式$(2.16)$可得关于压力的方程： ∇\\cdot\\frac1\\rho∇p=∇\\cdot(-\\vec u\\cdot ∇\\vec u+\\vec g+\\nu∇\\cdot∇\\vec u) \\tag{2.18}5、丢弃粘度项&emsp;&emsp;在某些流体如蜂蜜、小水珠等的模拟中，粘滞力起着非常重要的作用。但是在大多数流体动画模拟中，粘滞力的影响微乎其微，为此秉持着方程组越简单越好的原则，我们常常丢弃粘度项。当然这也不可避免地带来一些误差，事实上，在计算流体力学中尽可能地减少丢弃粘度项带来的误差是一个非常大的挑战。下面的叙述都是基于丢弃粘度项的前提。 &emsp;&emsp;丢弃了粘度项的$Navier-Stokes$方程被称为欧拉方程，而这种理想的流体则是无粘度的。丢弃了粘度项的欧拉方程如下： \\frac{D\\vec u}{Dt}+\\frac1\\rho∇p=\\vec g \\tag {2.19} ∇\\cdot\\vec u=0 \\tag{2.20}&emsp;&emsp;大多数的流体模拟的计算方程都是欧拉方程。 6、边界条件&emsp;&emsp;目前为止我们讨论的都是流体内部的情况，然而边界部分也是流体模拟非常关键的部分。在流体模拟中我们仅仅关注两种边界条件：固体墙（solid walls）、自由面（free surfaces）。 &emsp;&emsp;固体墙顾名思义就是流体与固体接触的边界，用速度来描述很简单：流体既不会流进固体内部也不会从固体内部流出，因此流体在固体墙法线方向上的分量为$0$： \\vec u\\cdot n=0 \\tag {2.21}&emsp;&emsp;当然，上述是固体自身不移动的情况下。通常来说，流体速度在法线方向上的分量与固体的移动速度在法线方向上的分量应该保持一致： \\vec u\\cdot n=\\vec u_{solid}\\cdot n \\tag{2.22}&emsp;&emsp;上述的两个公式都是仅对流体速度在法线方向上的分量做了限制，对于无粘度的流体，切线方向上的流体速度与固体的移动速度无必然的联系。 &emsp;&emsp;自由面是另外一个非常重要的边界条件，它通常就是与另外一种流体相接壤的边界部分。例如在模拟水花四溅时，水流表面不与固体接触的都是自由面（如与空气这种流体接触）。因空气密度远小于水导致空气对水体的仿真影响非常小，为了简化模拟，我们将空气所占的空间设为某个固定大气压的区域，设为$0$是最方便的方案，此时自由面就是压强$p=0$的水体表面。 &emsp;&emsp;在小规模的流体仿真中，自由面的表面张力占据着非常重要的地位。在微观分子层面下，表面张力的存在是因为不同的分子相互吸引产生的力。从几何的角度来解释就是，表面张力就是促使流体的表面积尽可能小的一种力。物理学上，两种不同的流体之间实际上存在着与表面平均曲率成正比的压力骤变： [p]=\\lambda k. \\tag {2.23}&emsp;&emsp;公式$(2.23)$中的$[p]$记为压力之差。$\\lambda$是表面张力系数，可以根据模拟的流体类型查找对应的张力系数（例如空气与水在室温下张力系数为$\\lambda \\approx 0.073N/m$）。而$k$就是平均曲率，单位为$m^{-1}$。又因为我们常常设空气的压力为$0$，因此水与空气交界的自由面的压力为： p=\\lambda k \\tag {2.24}​ 三、N-S方程的分步求解&emsp;&emsp;有了对以上对$Navier-Stokes$方程的理论支撑，接下来我们就要如何用计算机来对该组偏微分方程进行离散化求解。为了程序的松耦合性以及使计算尽可能地高效、简单，在流体模型领域，我们将流体方程分成几个独立的步骤，然后按顺序先后推进。对于不可压缩的无粘度流体方程（即前面的欧拉方程$(2.19)$和$(2.20)$，我们将其离散化成对流项（advection）如公式$(3.1)$、体积力项（body force）如公式$(3.2)$、压力/不可压缩项如公式$(3.3)$： \\frac{Dq}{Dt}=0 \\tag {3.1} \\frac{\\partial \\vec u}{\\partial t}=\\vec g \\tag {3.2} \\begin{cases} \\frac{\\partial \\vec u}{\\partial t}+\\frac{1}{\\rho}∇p=0\\\\ ∇\\cdot\\vec u=0 \\end{cases} \\tag {3.3}&emsp;&emsp;需要注意的是，在对流项公式$(3.1)$中我们用了一个通用量的符号$q$是因为我们不仅仅要对流体的速度进行对流，还需要对其他物理量进行对流。我们记对流项公式$(3.1)$的对流计算算法为$advect(\\vec u, \\Delta t, q)$，即对于给定的时间步长$\\Delta t$和速度场$\\vec u$，对物理量q进行对流。 &emsp;&emsp;对于体积力项$(3.2)$，我们采用简单的前向欧拉法即可：$\\vec u \\leftarrow \\vec u + g\\Delta t$。 &emsp;&emsp;对于压力/不可压缩项$(3.3)$，我们用一个称为$project(\\Delta t, \\vec u)$的算法，通过$project(\\Delta t, \\vec u)$计算出正确的压力以确保速度场$\\vec u$的无散度性质。欧拉方案不会着重研究具体粒子间的作用力，因而不会正向去求解$\\frac{1}{\\rho}∇p$，它是利用流体不可压缩的特性，将速度场$\\vec u$投影到散度为$0$的空间上，间接地解算了压力项。这种思想相当于，已知一个中间量$\\vec u_{temp}$，对这个中间量的唯一一个操作（如正向求解压力$\\frac{1}{\\rho}∇p$）不可行，但是直到最终量$\\vec u_{fianl}$符号的一个性质（散度为$0$），于是只要将$\\vec u_{temp}$投影到符合散度为$0$的特性平面上，即可间接地还原正向求解压力的操作，得到最终的速度场$\\vec u_{temp}$。 &emsp;&emsp;对流项$advect(\\vec u, \\Delta t, q)$的输入速度场$\\vec u$要确保为无散度的状态，投影项$project(\\Delta t, \\vec u)$确保了流体体积保持不变，因而投影项输出的速度场必然是无散度的。所以我们只要确保投影项$project(\\Delta t, \\vec u)$输出的速度场$\\vec u$作为对流项$advect(\\vec u, \\Delta t, q)$的输入即可，这时我们的分步求解流体方程的优势就体现出来了，其伪代码如下所示。 算法1 Fluid Simulation($\\vec u_n$, $\\Delta t$): 1: 初始化速度场$\\vec u_n$,使得$\\vec u_n$无散度 2: 对于每个时间步$n = 0,1,2,…$ 3: &emsp;&emsp;决定一个合理的时间步长$\\Delta t = t_{n+1}-t_n$ 4: &emsp;&emsp;对流项计算$\\vec u_A=advect(\\vec u_n,\\Delta t,\\vec q)$ 5: &emsp;&emsp;体积力项计算$\\vec u_B=\\vec u_A+\\Delta t\\vec g$ 6: &emsp;&emsp;无散度投影$\\vec u_{n+1}=project(\\Delta t,\\vec u_B)$ 1、时间步长&emsp;&emsp;在流体模拟算法中，确定适当的时间步长是算法的第一步。因为计算流体模拟的最后结果是呈现在屏幕上的，所以$\\Delta t$的选取与屏幕的刷新率有重要的关系。若选取的$\\Delta t$有$t_n+\\Delta t &gt; t_{frame}$，那么必须做一个截断使$\\Delta t=t_{frame}-t_n$。此外，流体模拟的三个步骤即对流项、体积力项、无散度投影项对时间步长$\\Delta t$的要求不尽相同，要选择一个满足所有要求的最小时间步长能确保计算的收敛性。此外，一方面为了流体模拟的真实性，我们可能需要选取一个足够小的时间步长来复现流体的高质量细节。另一方面，有时高性能的需求又使得我们不能选取太小的时间步长去渲染一帧。假设一帧至少要进行三个时间步的模拟，那么$\\Delta t$应该至少设成帧间隔时间的三分之一。 2、网格结构&emsp;&emsp;欧拉法的整个流程都是基于网格的，所以合理的网格结构是算法高效的关键点。$Harlow$和$Welch$提出了一种经典的$MAC$（marker and cell）网格结构，许多不可压缩流体模拟的算法都在这个网格结构上呈现出了良好的效率。$MAC$网格是一种交叉排列的网格，不同类型的物理量被存储于网格的不同位置。以二维的网格为例，如图3-1左图所示，流体粒子的压力数据存储于网格的中心点$P_{i,j}$，而速度则沿着笛卡尔坐标被分成了两部分。水平方向的$u$成分被存储在了网格单元竖直边的中心处，例如网格单元$(i,j)$和$(i+1,j)$之间的水平速度记为$u_{i+1/2,j}$。垂直方向的$v$成分则被存储在了网格单元水平面的中心上。这样的存储方案十分有利于估算流体流进/流出某个网格单元的量。 图3-1 MAC网格,左图二维,右图三维 &emsp;&emsp;扩展到三维的情况，$MAC$网格同样是交错排列的结构网格，如图3-1右图所示。压力数值存储在立方体网格单元的中心，三个速度分量分别被记录在立方体网格单元的三个表面的中心点上。在数值计算时，这样的分配方式使得我们可以准确地采用中心差分法计算压力梯度和速度的散度，同时克服了中心差分法的一个普遍的缺点。一维的情况为例，在网格顶点位置$…,q_{i-1},q_i,q_{i+1}…$上估算量场$q$的导数，为了无偏（所谓无偏，就是不偏向左边或者右边）估计网格顶点$i$处的$\\frac{\\partial q}{\\partial x}$，一种比较自然的方式就是采用一阶中心差分法： (\\frac{\\partial q}{\\partial x})_i\\approx \\frac{q_{i+1}-q_{i-1}}{2\\Delta x} \\tag {3.4}&emsp;&emsp;公式$(3.4)$是无偏的，且精确度为$O(\\Delta x^2)$。而前向欧拉差分法偏向右边且精确度只有$O(\\Delta x)$： (\\frac{\\partial q}{\\partial x})_i\\approx \\frac{q_{i+1}-q_i}{\\Delta x} \\tag {3.5}&emsp;&emsp;然而，公式$(3.4)$存在着一个非常严重的问题：网格点$i$的估算导数完全忽略了$q_i$的值。数学上，只有常数函数的一阶导数为零。但是公式$(3.4)$遇到了锯齿函数如$q_i=(-1)^i$时，它错误地将该类函数的导数估算为$0$，这种问题被称为零空间问题（null-space problem）。 &emsp;&emsp;交叉错排的$MAC$网格完美地克服了中心差分法的零空间问题，同时也保持了它的无偏二阶精度。在$MAC$网格上运用中心差分法，网格点$i$处的估算导数公式如下所示： (\\frac{\\partial q}{\\partial x})_i\\approx\\frac{q_{i+1/2}-q_{i-1/2}}{\\Delta x} \\tag {3.6}&emsp;&emsp;$MAC$网格确实给流体的压力计算和不可压缩性的处理带来了很大的便利，但与此同时也带来了一些其他方面的麻烦。如果我们要估算某个地方的速度向量，即便采样点恰好在网格点上我们也要做一些插值才能获取相应的速度向量。在网格点处，我们通常采用平均法，以二维为例： \\vec u_{i,j}=(\\frac{u_{i-1/2,j}+u_{i+1/2,j}}{2},\\frac{v_{i,j-1/2}+v_{i,j+1/2}}{2}),\\\\ \\vec u_{i+1/2,j}=(u_{i+1/2,j},\\frac{v_{i,j-1/2}+v_{i,j+1/2}+v_{i+1,j-1/2}+v_{i+1,j+1/2}}{4}),\\\\ \\vec u_{i,j+1/2}=(\\frac{u_{i-1/2,j}+u_{i+1/2,j}+u_{i-1/2,j+1}+u_{i+1/2,j+1}}{4},v_{i,j+1/2}).\\tag {3.7}&emsp;&emsp;最后，在实现中下标索引一般没有浮点数之说，前面直接采用$i+1/2$的记法是为了便于叙述。一般约定如下： p(i,j,k)=p_{i,j,k},\\\\ u(i,j,k)=u_{i-1/2,j,k},\\\\ v(i,j,k)=v_{i,j-1/2,k},\\\\ w(i,j,k)=w_{i,j,k-1/2}. \\tag{3.8}&emsp;&emsp;因而对于$nx\\times ny\\times nz$分辨率的网格，压力数值存储在$nx\\times ny\\times nz$的数组中，速度的$u$成分存储在$(nx+1)\\times ny\\times nz$数组中，速度的$v$成分存储在$nx\\times (ny+1)\\times nz$数组中，速度的$w$成分存储在$nx\\times ny\\times (nz+1)$数组中。 四、对流算法&emsp;&emsp;求解如下所示的对流方程是流体模拟的关键一步： \\frac{Dq}{Dt}=0 \\tag {4.1}&emsp;&emsp;我们把这个对流数值计算的算法记为： q^{n+1}=advect(\\vec u,\\Delta t,q^n) \\tag {4.2}&emsp;&emsp;公式$(4.2)$中的各个符号含义： &emsp;&emsp;$\\vec u$：在$MAC$网格上的离散化的速度场； &emsp;&emsp;$\\Delta t$：时间步长； &emsp;&emsp;$q^n$：当前的物理量场$q$（如流体密度、速度、燃烧物浓度等）； &emsp;&emsp;$q^{n+1}$：经过对流后得到的新的量场。 &emsp;&emsp;在这里要特别注意，输入对流算法的速度场$\\vec u$必须是无散度的，否则模拟结果会出现一些奇怪的失真现象。 1、半拉格朗日对流算法（Semi-Lagrangian Advection）&emsp;&emsp;一维情况下，对流方程$(4.1)$写成偏微分的形式如下： \\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}=0 \\tag {4.3}&emsp;&emsp;分别采用前向欧拉差分法计算对时间的偏导和中心差分法计算对空间的偏导，我们有： \\frac{q^{n+1}_{i}-q^n_i}{\\Delta t}+u^n_i\\frac{q^n_{i+1}-q^n_{i-1}}{2\\Delta x}=0 \\tag {4.4}&emsp;&emsp;转成以$q^{n+1}_i$为计算目标的显式公式，得： q^{n+1}_i=q^n_i-\\Delta t u^n_i\\frac{q^n_{i+1}-q^n_{i-1}}{2\\Delta x} \\tag {4.5}&emsp;&emsp;公式$(4.5)$看起来没什么问题，但是却存在非常严重的漏洞。首先，前向欧拉法被证明是无条件不稳定的空间离散方法：无论取多么小Δ𝑡，随着时间步的推进，累积误差终将发散。即使使用更稳定的时间积分方法来取代前向欧拉方法，解决了时间上的PDE（Partial Differential Equation，偏微分方程）计算，空间上的PDE计算还是会带来重大的麻烦。标准中心差分方法不可避免地会出现的零空间问题，具有高频震荡性质的速度场对空间的导数被错误地计算为$0$或几乎为$0$，低离速度分量被分离出来，从而导致模拟效果中出现许多奇怪的高频摆动和震荡。 &emsp;&emsp;针对这些问题，研究者们提出了一个解然不同的、更加简单和更具物理直观意义的半拉格朗日法。之所以叫半拉格朗日法，是因为这种方法是以拉格朗日视角去解决欧拉视角的对流方程（“半”字的由来）。假设我们的目标是求解网格点$\\vec x_G$的在第$n+1$个时间步时关于物理量$q$的新值，记为$q^{n+1}_G$。在拉格朗日的视角下，我们可以寻找在第$n+1$时间步之前，是空间中的哪一个点上的流体粒子在速度场$\\vec u$的作用下“流向”了$\\vec x_G$，我们记这个粒子在第$n$个时间步时的网格位置为$\\vec x_P$，则第$n+1$个时间步时$\\vec x_G$的$q^{n+1}_G$即为第$n$个时间步时$\\vec x_P$的$q^{n}_P$。如下图4-1为半拉格朗日对流法的示意图。 图4-1 半拉格朗日对流法&emsp;&emsp;半拉格朗日对流法的第一步就是要找出$\\vec x_P$，为此我们根据$\\vec x_G$做反向的追踪。粒子位置对时间的导数就是速度场： \\frac{d\\vec x}{dt}=\\vec u(\\vec x) \\tag {4.6}&emsp;&emsp;经过一个时间步长$\\Delta t$之后，粒子由$\\vec x_P$移动到$\\vec x_G$。为了得到$\\vec x_P$，最简单的方法就是采用前向欧拉法进行倒推： \\vec x_P=\\vec x_G-\\Delta t\\vec u(\\vec x_G) \\tag {4.7}&emsp;&emsp;然而前向欧拉法只有一阶的精度，若在不改变$\\Delta t$的情况下提高精度，我们可以采用高阶的龙格库塔法（Runge-Kutta method）。采用二阶的龙格库塔法如下所示： \\vec x_{mid}=\\vec x_G-\\frac12\\Delta t\\vec u(\\vec x_G),\\\\ \\vec x_P=\\vec x_G-\\Delta t\\vec u(\\vec x_{mid}). \\tag {4.7}&emsp;&emsp;倒推得到$\\Delta t$之前的网格位置$\\vec x_P$一般不会恰好在网格顶点上，为此我们需要做些插值。三维模拟通常采用三线性插值，而二维的则采用双线性插值。 q^{n+1}_G=interpolate(q_n,\\vec x_P) \\tag {4.8}2、边界情况&emsp;&emsp;若我们倒推得到的$\\vec x_P$仍然在流体的内部，那么做插值是完全没问题的。但若$\\vec x_P$在流体的边界之外呢？这种情况的出现的原因通常有两个：一个是$\\vec x_P$确确实实在流体的外部且即将流入流体内部，另一个是由前向欧拉法或龙格库塔法的数值计算方法带来的误差导致。 &emsp;&emsp;在一种情况下，我们应该知道当流体流入时其携带的物理量，此时我们将这个外部流入的物理量作为返回值即可。例如，第$n$个时间步时的外部流体以速度$\\vec U$和温度$T$在第$n+1$个时间步时注入流体内部$\\vec x_G$的位置，那么$\\vec T^{n+1}_G$的值就为$T$。 &emsp;&emsp;在第二种由误差导致的情况下，一个适当的策略就是根据边界上的最近点外推出所求得物理量。在模拟某些流体时，外推变得很简单。例如，在模拟烟雾时我们简单地假设烟雾流体外部即空气的速度风场为某个常数$\\vec U$（可能为$0$），这样边界上的速度场都取$\\vec U$。但还有一些必须根据流体内部的已知量外推出未知量，这时情况就变得比较复杂了。具体如何外推将在后面介绍，目前我们只需要知道大概的步骤：首先寻找边界上的最近点，然后在最近点的领域内插值获取相应的物理量场。 3、时间步长大小&emsp;&emsp;对任何一种数值计算方法的主要的考虑点就是它是否稳定。幸运的是，半拉格朗日对流法已经被证明是一种无条件稳定的算法：无论$\\Delta t$取多大，它永远不会出现数值爆炸的现象。因为每一个新值$q$的确定，都是通过对旧值得插值，无论是线性插值、双线性插值还是三线性插值，$q$的大小都是处于插值点之间，不会得到比原来插值点更大或者更小的值，因而$q$是有上下界的。这使得我们可以尽情地根据所需的模拟质量和模拟效率去调整时间步长。 &emsp;&emsp;但是在实践中，时间步长的大小也不能选得太过极端，否则会产生一些奇观的现象。Foster和Fekiw提出了一个对$\\Delta t$的限制：流体粒子在$\\Delta t$内的倒推轨迹最多经过某个常数个网格单元为宜，例如5个： \\Delta t \\leq \\frac{5\\Delta x}{u_{max}} \\tag {4.9}&emsp;&emsp;公式$(4.9)$中，$u_{max}$是速度场的最大值，我们可以简单地取 存储在网格中的最大速度值。一个更鲁棒的方法考虑了体积力（如重力、浮力等）对最大速度的影响： u_{max}=max(|u^n|)+\\Delta t|g| \\tag {4.10}&emsp;&emsp;将不等式$(4.9)$的最大值带入公式$(4.10)$，我们有： u_{max}=max(|u^n|)+\\frac{5\\Delta x}{u_{max}}|g| \\tag {4.11}&emsp;&emsp;取一个简单的速度上界（简化了公式$(4.11)$），$u_{max}$： u_{max}=max(|u^n|)+\\sqrt{5\\Delta xg} \\tag {4.12}&emsp;&emsp;这样确保了$u_{max}$始终为正，且避免公式$(4.9)$的除$0$错误。 &emsp;&emsp;关于时间步长的讨论离不开$CFL$（以Courant、Friedrichs、Lewy三人的名字命名）条件。$CFL$条件是一个简单而直观的判断计算是否收敛的必要条件。它的直观物理解释就是时间推进求解的速度必须大于物理扰动传播的速度，只有这样才能将物理上所有的扰动俘获到。满足$CFL$条件意味着当$\\Delta x$和$\\Delta t$趋于取极限$0$时，数值计算所求的解就会收敛到原微分方程的解。 &emsp;&emsp;对于半拉格朗日对流法，其满足$CFL$条件当且仅当在极限情况下，追踪得到的粒子轨迹足够逼近真实的轨迹。足够逼近的意思是经过正确的网格插值能够得到正确的依赖域（即差分格式的依赖域包含了原微分方程的依赖域），追踪的轨迹就会收敛到正确真实的轨迹。 &emsp;&emsp;因而，对于采用标准的显式有限差分法的对流方程求解，为了保证收敛，我们要求$q^{n+1}$的新值是由以当前网格点为中心、以$C\\Delta x$（$C$是一个小的整数常量）为半径的邻域范围内插值得到： \\Delta t \\leq C\\frac{\\Delta x}{|\\vec u|} \\tag {4.13}&emsp;&emsp;公式$(4.13)$中的$C$被称为$CFL$数，因而不等式$(4.9)$可以看成是公式$(4.13)$取$CFL$数为$5$得到。 4、数值耗散&emsp;&emsp;对流算法在对流获取新的物理量场$q^{n+1}_i$时会进行一些插值操作，插值不可避免地会平滑物理量场，这带来了一些数值耗散。一次两次的数值耗散不会由太大的影响，但是在流体模拟中我们会在每个时间步都进行对流运算，反反复复的平滑操作将数值耗散不断扩大，损失大量的流体细节。 &emsp;&emsp;以一维的对流项计算为例，流体速度为常量$u&gt;0$： \\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}=0 \\tag {4.14}&emsp;&emsp;假设$\\Delta t &lt; \\frac{\\Delta x}{u}$，即单个时间步长内粒子追踪轨迹长度小于单个网格单元的大小。我们的目标点是$x_i$，则倒推得到的粒子位置就落在了$[x_{i-1},x_i]$上的$x_i-\\Delta tu$，然后进行线性插值得到$q^{n+1}_i$： q^{n+1}=\\frac{\\Delta tu}{\\Delta x}q^n_{i-1}+(1-\\frac{\\Delta tu}{\\Delta x})q^n_i \\tag {4.15}&emsp;&emsp;将公式$(4.15)$整理一下，有： q^{n+1}_i=q^n_i-\\Delta tu\\frac{q^n_i-q^n_{i-1}}{\\Delta x} \\tag {4.16}&emsp;&emsp;公式$(4.16)$实际上正好就是采用时间上的前向欧拉差分法和空间上的单向有限差分法的欧拉方案，把$q^n_i$看成是$q^n$关于$x_i$的函数，对$q^n_{i-1}$进行泰勒级数展开： q^n_{i-1}=q^n_i-(\\frac{\\partial q}{\\partial x})^n_i\\Delta x+(\\frac{\\partial^2q}{\\partial x^2})^n_i\\frac{\\Delta x^2}{2}+O(\\Delta x^3) \\tag {4.17}&emsp;&emsp;将公式$(4.17)$代入公式$(4.16)$，并做一些变量消去，可得： q^{n+1}_i=q^n_i-\\Delta tu(\\frac{\\partial q}{\\partial x})^n_i+\\Delta tu\\Delta x(\\frac{\\partial^2q}{\\partial x^2})^n_i+O(\\Delta x^2) \\tag {4.18}&emsp;&emsp;在二阶截断误差的情况下，结合公式$(4.18)$和公式$(4.14)$，有： \\frac{\\partial q}{\\partial t}+u\\frac{\\partial q}{\\partial x}=u\\Delta x(\\frac{\\partial^2q}{\\partial x^2}) \\tag {4.19}&emsp;&emsp;右边就是对流方程计算时引入的额外类似粘度乘上系数$u\\Delta x$的项。这也就是说，当我们采用简单的半拉格朗日法去求解无粘度的对流方程时，模拟的结果却看起来我们像时在模拟有粘度的流体。这就是数值耗散！当然，当$\\Delta x\\to 0$时，这个数值耗散系数也会趋于$0$，所以取时间步无穷小时能够得到正确的模拟结果，但这需要耗费巨额的计算资源开销。我们通常模拟的流体大多数都是无粘度的，所以如何减少这个数值耗散是个至关重要的难题。 &emsp;&emsp;一个简单有效的修复数值耗散的方法就是采用更加锐利的插值方法，从而尽可能地减少由插值带来的数值耗散。在一维的情况时，我们采用三次插值（cubic interpolant）如下公式$(4.21)$，而不是简单的一次线性插值$(4.20)$： q\\approx(1-s)x_i+sx_{i+1} \\tag {4.20} q\\approx[-\\frac13s+\\frac12s^2-\\frac16s^3]q_{i-1}+[1-s^2+\\frac12(s^3-s)]q_i\\\\ +[s+\\frac12(s^2-s^3)]q_{i+1}+[\\frac16(s^3-s)]q_{i+2} \\tag {4.21}&emsp;&emsp;扩展到二维或者三维就是双三次插值（bicubic interpolation）或三三次插值（tricubic interpolation）。以二维情况为例，我们可以先沿着$x$轴做第一遍的三次插值如公式$(4.22)$，然后再沿着$y$轴做第二遍插值如公式$(4.23)$： q_{j-1}=w_{-1}(s)q_{i-1,j-1}+w_0(s)+q_{i,j-1}+w_1(s)q_{i+1,j-1}+w_2(s)q_{i+2,j-1},\\\\ q_{j}=w_{-1}(s)q_{i-1,j}+w_0(s)+q_{i,j}+w_1(s)q_{i+1,j}+w_2(s)q_{i+2,j},\\\\ q_{j+1}=w_{-1}(s)q_{i-1,j+1}+w_0(s)+q_{i,j+1}+w_1(s)q_{i+1,j+1}+w_2(s)q_{i+2,j+1},\\\\ q_{j+2}=w_{-1}(s)q_{i-1,j+2}+w_0(s)+q_{i,j+2}+w_1(s)q_{i+1,j+2}+w_2(s)q_{i+2,j+2}. \\tag {4.22} q=w_{-1}(t)q_{j-1}+w_0(t)q_j+w_1(t)q_{j+1}+w_2(t)q_{j+2} \\tag {4.23}&emsp;&emsp;当然也可以先沿着$y$轴，然后再沿着$x$轴做插值操作。","categories":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/categories/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/categories/Fluid-Simulation/"}],"tags":[{"name":"Computer Graphics","slug":"Computer-Graphics","permalink":"https://yangwc.com/about/tags/Computer-Graphics/"},{"name":"Fluid Simulation","slug":"Fluid-Simulation","permalink":"https://yangwc.com/about/tags/Fluid-Simulation/"},{"name":"Naiver-Stokes Equations","slug":"Naiver-Stokes-Equations","permalink":"https://yangwc.com/about/tags/Naiver-Stokes-Equations/"},{"name":"Advection","slug":"Advection","permalink":"https://yangwc.com/about/tags/Advection/"}]}]}